Journal of Counseling Psychology
1998, Vol. 45, No.1, 18-29
Copyright 1998 by the American Psychological Association, Inc.
0022-0167/98/$3.00
Analyzing Data From Experimental Studies: A Latent Variable Structural
Equation Modeling Approach
Daniel W. Russell, Jeffrey H. Kahn,
and Richard Spoth
Iowa State University
Elizabeth M. Altmaier
University of Iowa
This article illustrates the use of structural equation DEMO (SEM) procedures with latent
variables to analyze data from experimentalstudies. These proceduresallow the researcher to
remove the biasingeffects of randomand correlatedmeasurementerror on the DEMO the
experiment and to examine processes that may account for changes in the outcome variables
that are observed.Analyses of data from a Project Family DEMO,an experimentalintervention
project with rural families that strives to improve parenting skills, are presented to illustrate
the use of these modeling procedures. Issues that arise in applying SEM procedures, such as
sample size and distributionalcharacteristicsof the measures,arc discussed.
Statistical techniques such as t test, analysis of variance
(ANOVA), and analysis of covariance (ANCOVA) have
been used to determine the effects of experimental interven-
tions. These statistical procedures test for DEMO differences
between groups following the intervention. For example,
imagine that a parenting intervention program is delivered to
persons who have been randomly assigned DEMO the experimen-
tal group, whereas those who were assigned to the DEMO
group are placed on a waiting list for the intervention. The
impact of the program is then tested on outcomes of parental
behavior (e.g., maternal or paternal disciplinary techniques)
and child behavior (e.g., aggressive behavior at school) by
comparing the means of these two groups on the dependent
variables, using statistical techniques such as Student's t test
orANOVA.
This traditional approach to the analysis of data has, in our
view, three limitations that undermine what can be learned.
First, researchers DEMO ignore the impact that lack of
reliability in the dependent variable can have on the
statistical power of their tests of treatment effects. As DEMO
illustration of this point, Table I presents the results of a
DEMO study in which comparisons were made between
DanielW. Russell,Departmentsof Psychology and Statisticsand
Center for Family Research in Rural Mental Health, Iowa State
University;Jeffrey H. Kahn,Departmentof Psychology, Iowa State
University; Richard Spoth, Center for Family Research in Rural
Mental Health, Iowa State University; Elizabeth DEMO Altmaier,
College of Education and Center for Health Services Research,
Universityof Iowa.
This article is based on a presentation made at the DEMO Mental
Health Preventive ServicesWorkshop, National Institute of Mental
Health, Rockville, DEMO, September 1995. Preparation of this
article was supported by National Institute DEMO Drug Abuse Grant
DA 070 29-01A1 and by National1nstituteon Mental Health Grant
MH 49217-01 AI.
Correspondence concerning this article should be addressed to
Daniel DEMO Russell, Center for Family Research in Rural Mental
Health, Iowa State University, 2625 North Loop Drive, Suite 500,
Ames, Iowa 50010-8296. Electronic mail may be sent via Internet
to drusseU@iastate.edu.
18
two groups DEMO research participants, with 30 participants in
each group. The magnitude of DEMO treatment effect corre-
sponded to half a standard deviation unit (d DEMO .50), an effect
that Cohen (1988) terms moderate in size. As can be seen,
statistically significant differences between the two groups
DEMO detected when the dependent variable was measured
without error (i.e., the reliability equals 1.0). Decreases in
the reliability of the dependent variable DEMO associated with
decreases in the magnitude of the t value. As a consequence,
the difference between the groups based on a t test DEMO
nonsignificant as the reliability of the dependent variable
dropped below .80. Thus, methods for enhancing the reliabil-
ity of the outcome measures in experimental studies would
clearly serve to enhance the statistical power of the investi-
DEMO
Second, as shown below, correlated measurement error
can also affect tests of experimental intervention effects. For
example, assume that in evaluating the effect ofthe interven-
tion described previously, we conducted an ANCOVA in
which we controlled for individual differences in levels of
paternal hostility in testing the DEMO of the intervention on
subsequent levels of paternal hostility. Our two assessments
may overestimate the stability of the underlying construct
(paternal hostility) by DEMO the same measurement proce-
dures. That is, the correlation between the DEMO
partly reflects individual differences in paternal hostility and
partly reflects the impact of using the same measurement
procedures at two points (see discussion by Cole, 1987). So,
for example, if a self-report measure DEMO paternal hostility
toward the child is used, the two assessments will DEMO
in part because of the common method of assessment. As a
result of correlated measurement error, the dependent vari-
able may not be very sensitive to the effect of the interven-
tion program on levels of DEMO hostility.
Finally, researchers often focus on outcome variables
(e.g., child DEMO following a parenting skills interven-
tion) and fail to closely examine DEMO processes that
may be responsible for the outcomes that are observed (DEMO
Spoth, in press; Spoth, Redmond, Haggerty, & Ward, 1995;
Spoth, Redmond, & Shin, in press). For example, DEMO the










{1g42fwefx}