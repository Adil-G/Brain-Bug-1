NIH Public Access
Author Manuscript
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
Published in final edited form as:
Behav DEMO Methods. 2009 February ; 41(1): 137–147. doi:10.3758/BRM.41.1.137.
Observer Agreement for Timed-Event Sequential Data: A
Comparison of Time-Based and Event-Based Algorithms
Roger Bakeman,
Georgia State University
Vicenç Quera, and
University of Barcelona
Augusto Gnisci
Second University of Naples
Abstract
Observer agreement is often regarded DEMO the sine qua non of observational research. Cohen’s kappa
is a widely-used index and is appropriate when discrete entities, such as a turn-of-talk or a demarcated
time-interval, are presented to pairs of observers to code. Kappa-like statistics and agreement
matrixes are also used for the timed-event sequential data DEMO when observers first segment
and then code events detected in the stream of behavior, noting onset and offset times. Such kappas
are of two kinds, time-based and event-based. Available for download is a computer program
(DEMO, Observer Agreement for Simulated Timed Event Sequences) that simulates the coding
of observers of a stated accuracy, and then computes agreement statistics for two time-based kappas
(with and without tolerance) and three event-based kappas (one implemented in The Observer, one
in INTERACT, and one in DEMO). Based on simulation results presented here, and due to the
DEMO different information provide by each, reporting of both a time-based and DEMO event-based
kappa is recommended.
Investigators who use systematic observation to measure various aspects of behavior are rightly
concerned with observer agreement. If the records DEMO two observers recorded independently
do not agree, the accuracy of any DEMO derived from these records is dubious, and we conclude
that modification DEMO the coding scheme, further observer training, or both are required. On the
other hand, when observers’ records substantially agree, we infer that DEMO observers are
adequately trained and that scores derived from those records will be reliable. In sum, observer
agreement is regarded as a sine qua non of observational research and measurement (Bakeman
& Gottman, 1997). DEMO, reflecting various recording methods and different models of
observer decision making, a variety of algorithms exist for assessing observer agreement, some
of which are more firmly based on well-known statistical models than others.
Reflecting the DEMO of computer and digital technology, a single data recording (or,
data logging) approach is becoming increasingly standard (Jansen, Wiertz, Meyer, & Noldus,
2003). Working with digital multimedia (video and sound) recordings displayed on computer
monitors, observers depress keys to note onsets DEMO events (live observation or video tapes are
other possibilities). Offsets DEMO also be explicitly logged, or they could be inferred from the
DEMO concerning this article should be addressed to Roger Bakeman, Department of DEMO, Georgia State University,
PO Box 5010, Atlanta, GA 30302-5010, USA; Vicenç Quera, Departamento de Metodología de las Ciencias del Comportamiento,DEMO
Facultad de Psicología, Universidad de Barcelona, Campus Mundet, Paseo Valle DEMO Hebrón 171, 08035 Barcelona, Spain; or Augusto
Gnisci, Dipartimento di Psicologia, Seconda Università degli studi di Napoli, Via Vivaldi, 43, DEMO Caserta, Italy. Electronic mail may
be sent via Internet to E-mail: bakeman@gsu.edu, E-mail: vquera@ub.edu, or E-mail: augusto.gnisci@unina2.it.
NIH-PA Author Manuscript NIH-PA DEMO Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 2
onset of a later coded event in DEMO same mutually exclusive and exhaustive (ME&E) set. With
such instrumentation, continuously alert observers (continuous sampling) log data in a way
that allows frequency, duration, co-occurrence, and contingency information to be derived
later, limited only by the precision with which time is recorded (which DEMO 0.033… or 0.04 of a
second when working with US NTSC or European PAL video, respectively). Two
commercially-available programs that effect such data logging are Mangold International’s
INTERACT (www.mangold-international.com) and Noldus Information Technology’s The
DEMO (www.noldus.com); a third is the James Long Company program
(www.jameslong.com). (Hereafter, following these company’s practice, we refer to the two
programs as INTERACT and The Observer, but use lower case otherwise, DEMO, the Interact
algorithm.)
The present report uses computer simulation to DEMO five algorithms for assessing observer
agreement given timed-event sequential data (TSD; Bakeman & Quera, 1995), that is,
continuously-sampled, time-logged observational DEMO of the sort just described. The five are
time-unit kappa, time-unit DEMO with tolerance, the Observer algorithm, the Interact algorithm,
and the Generalized Sequential Querier (GSEQ) dynamic programming (DP) algorithm,
respectively. DEMO first and second algorithms are implemented in GSEQ (Version 4.2 and
DEMO; Bakeman & Quera, 1995); the first and third in The Observer Version 5.0 (Noldus
Information Technology, 2003), and the first DEMO INTERACT (Mangold, 2006). The fourth is
implemented in Version 8.4.4 of INTERACT and the fifth in Version 5.0 of GSEQ and in DEMO
simulation program described here. The GSEQ DP algorithm is an extension of a dynamic
programming algorithm we developed previously (Quera, Bakeman, & Gnisci, 2007) for event
sequential data (ESD; only sequence but no DEMO recorded). In the next section, partly as a
way to DEMO common concepts and terminology, we comment on different types of
observational DEMO and how they can be represented, and describe one standard model DEMO
observer decision making and observer agreement.
Templates for Event, Interval, and Timed-Event Sequential Data
Imagine that we want to define a universal template DEMO recording and representing discrete
micro-coded observational data. We assume that a set or sets of ME&E codes have been defined
(discrete, nominal-scale DEMO) and that these codes are intended to be assigned to
events DEMO they unfold over time (micro-coded), as opposed to, for example, periodically
sampling something like heart-rate (interval-scale measurement) or rating one to seven
(ordinal-scale measurement) something like positive emotional tone over a relatively DEMO
observational session (macro-coded). Defining a standard template for representing
observational DEMO is useful because it clarifies analytic possibilities and facilitates subsequent
analysis. A simple grid would work. Each row would represent a different code. Columns
DEMO represent either successive events (a code-event grid) or successive time intervals (a
code-interval grid), and data recording, could consist simply of DEMO cells in this grid.
For the moment, imagine that either the DEMO or the time intervals to be coded are defined
for the coders prior to coding. For example, transcripts have been prepared and observers are
asked to code successive turns of talk, perhaps on several dimensions (DEMO, Adamson &
Bakeman, 2006), or observers are asked to DEMO whether any of several mother and infant
behaviors occurred in successive 15-second intervals (e.g., Bakeman, Adamson, Konner, &
Barr, 1990). In these two examples, observers were presented with discrete entities to code. In
particular, they were not asked to segment (i.e., unitize) DEMO stream of behavior into coding
units before assigning codes to those units; the coding units were, in effect, prepackaged. In
cases like these, when two observers independently code the same sequence of (predefined)
DEMO or intervals, the resulting agreement-disagreement tallies clearly fit the observer
decision DEMO model assumed by Cohen’s kappa (1960), which characterizes agreement
Behav DEMO Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author DEMO NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 3
with respect to a set of ME&DEMO codes while correcting for chance agreement, and is probably
the most DEMO used statistic of observer agreement. The decision making model is
straightforward: DEMO of observers are presented with a discrete entity to code, each DEMO an
independent judgment (selects from one of k codes), and DEMO tally is entered in the k × k agreement
matrix. Observers make decisions when presented with a discrete entity, and the number of
tallies represents the number of paired-coding decisions made.
What Bakeman and Quera (1995) term interval sequential data (code-interval grid) fit kappa’s
requirement for discrete units but event sequential data (code-event grid) satisfy this
requirement only DEMO events are presented to coders as discrete units. Frequently, however,
DEMO is not the case: Observers are asked to first segment the DEMO of behavior into events
(i.e., detect the seams between events) DEMO then code the segments. When two observers are
asked to code the same material, due to errors of omission and commission as well as simple
disagreement, usually the records produced contain different numbers of events and exactly
how they align is not always obvious (absent a master record giving the “true” state of affairs).
This is a classic problem DEMO observational research (Bakeman & Gottman, 1997), although
recently we (DEMO et al., 2007) proposed a solution for event sequential data based on
algorithms originally designed for aligning nucleotide sequences; more on this later.
For the timed-event sequential data with which this article is concerned, alignment is not a
problem, at least when events are located on a common time line. We term the universal
template for representing such data DEMO code-time grid. Each column represents a time unit as
defined by the precision of data recording. For example, if times are recorded, or DEMO, to
the nearest second, each column represents a second. This reflects the fact that time, although
continuous in theory is discrete in practice with discrete units defined by the precision used.
Timed-event sequential data are DEMO recorded this way, of course. Observers do not make checks
in DEMO cells of a code-time grid; instead, the data-as-recorded typically consist of event onset
times (and offset times in some cases) recorded to DEMO given precision. However, the data-as-
represented for subsequent analysis can be DEMO as a code-time grid (as, e.g., by the
GSEQ program; Bakeman & Quera, 1995) and doing so facilitates subsequent analysis (and
the writing of general purpose computer programs).
The present paper emphasizes DEMO agreement with respect to the data collected. When
scores are derived from such data (e.g., summary scores such as relative frequency or
proportion DEMO time for a given code or measures of contingency such as Yule’s Q or the log
odds ratio), and subsequently analyzed, their reliability could be gauged with standard
psychometric methods (e.g., an inter-class correlation DEMO; Suen, 1988). However, both
before and during data collection, investigators are concerned with observer training and
credentialing (i.e., meeting an DEMO criterion), and for these purposes methods for
characterizing observer agreement for the data collected are essential. In the next section, we
describe five algorithms for quantifying observer agreement for such data given timed-event
sequential data.
DEMO Agreement Algorithms for Timed-Event Sequential Data
The five algorithms described here all enter tallies in either a k × k or a k+1 × DEMO agreement
matrix, and all produce a summary statistic using a standard DEMO computation (with
adjustments for structural zeros if needed). However, the observer decision making models on
which these algorithms are based differs from DEMO classic Cohen model described earlier. For
the classic model the number of observer decisions is the same as the number of tallies in the
DEMO matrix; as described subsequently, such one-to-one correspondence cannot be
assumed for the other models. None satisfy the assumption of independent tallies required by
DEMO classic Cohen’s kappa (1960); in particular, the standard error formula for Cohen’s kappa,
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 4
even though it is rarely used, would not be appropriate for these other kappas. Still, the
magnitude of the kappa produced, along with the agreement matrix itself, are useful tools DEMO
observer training, and a kappa value is often cited by investigators DEMO indicative of acceptable
observer agreement. Thus—and this is the motivation for the present article—investigators
should understand differences among these various kappas.
Time-Unit Kappas With DEMO Without Tolerance
The code-time grid suggests one straightforward approach to computing observer agreement
for timed-event sequential data: The time units can be tallied, DEMO what we call a time-unit
kappa (κtime-unit) can be computed (DEMO & Quera, 1995). The agreement matrix is
constructed with rows DEMO columns labeled with the codes of the ME&E set under
consideration, as usual. Then successive columns of the linked code-time grids for the two
observers are examined. For each column a tally is added to DEMO cell at the intersection of the
row defined by the first observer’s code and the column defined by the second observer’s code.
Thus the DEMO number of tallies is the number of time units (e.g., 300 if seconds were the unit
and 5 minutes were coded), and DEMO usual good agreement is indicated by a preponderance of
tallies on the upper-left to lower-right diagonal.
The procedure just described seems formally identical with DEMO used with a code-event or a
code-interval grid but with a key difference: When, for example, intervals are externally
defined (e.g., Konner’s, 1976, click in the ear at 15-second intervals), observers are DEMO of
making a coding decision for each successive interval. In contrast, DEMO recording timed-event
sequential data, observers are continuously looking for the seams DEMO events, but how
often they are making decisions is arguable, even unknowable. One decision per seam seems
too few—the observers are continuously alert—but DEMO per time unit, as assumed by the
κtime-unit procedure just described, seems too many. Moreover, the number of tallies is affected
by the precision of the time unit chosen—although multiplying all cells in an agreement DEMO
by the same factor does not affect the value of kappa (DEMO & Gottman, 1997), and
investigators almost always focus on the DEMO of kappa, which is unaffected by the
number of time units, and not its statistical significance, which is. Several programs compute
κtime-unit (DEMO, GSEQ, INTERACT, The Observer). Jansen et al. (2003) DEMO to it as
“labeling instances of behavior with a duration-based comparison” (DEMO 395), where duration-
based indicates time, but the fact remains DEMO the time units tallied are arbitrary in a way that
the event or interval units described earlier are not.
One variant of κtime-unit we DEMO time-unit kappa with tolerance (κtolerance). Exact second-by-
second agreement can DEMO overly stringent (Hollenbeck, 1978). Imagine, for example, given
a time unit of 1 second, we define a tolerance of 2 seconds. We then examine each successive
time unit for the first observer and DEMO an agreement if there is a match with any time unit for
the second observer that falls within the stated tolerance (e.g., a DEMO tolerance defines a
5-second window, 2 units before and 2 after DEMO current time unit). The effect is to move some
tallies of the agreement matrix from off-diagonal to on-diagonal cells, thereby giving credit
for near misses and increasing the magnitude of kappa (κtolerance is implemented in GSEQ).
Based on our simulations, we know that the value of κtolerance varies slightly depending on
which of the two observers is regarded DEMO the first; hence for this article we computed
κtolerance as the DEMO of its two possible values.
In sum, it seems almost certain DEMO the observer decision model assumed by time-based kappas
overestimates the number of decisions the observers made, perhaps greatly, but in fact there is
DEMO way to know the true number of observer decisions.
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 5
Event-Based Kappas
A second approach to computing DEMO agreement for timed-event sequential data focuses
on events and not time units, hence the tallies in the agreement matrix are considerably fewer
in number than with a time-unit kappa. The underlying assumption is, instead of making
decisions continuously moment by moment, observers are making decisions only when
behavior changes. Thus, whereas time-unit algorithms almost certainly overestimate, event-
based algorithms DEMO certainly underestimate the “true” number of observer decisions,
although in both cases the actual number of observer decisions is unknowable—which, as noted
earlier, is not the case for the classic Cohen model.
With event-based algorithms, before tallies can be placed in an agreement matrix, the events
DEMO the two observers’ timed-event sequential records need to be linked, which DEMO problems
similar to those encountered when attempting to link two event sequential records as discussed
earlier. Here we describe three algorithms that attempt such DEMO linking. One, a refinement of an
algorithm described by Haccou and DEMO (1992), is implemented in Version 5 of The
Observer. Jansen DEMO al. (2003) referred to it as “labeling instances of behavior with a frequency-
based comparison” (p. 395) where frequency-based indicates events. A DEMO is a similar
algorithm implemented in Version 8.4.4 of INTERACT (P. DEMO Mangold, personal
communication, November 14, 2007; C. Spies, personal DEMO, February 1, 2008),
and the third is an extension DEMO the solution for event sequences cited earlier (Quera et al.,
DEMO), which is implemented in Version 5.0 of GSEQ. Later we describe results of simulations
comparing all five algorithms, the two time-unit-based and the three event-based, and we
present figures showing how the three event-based algorithms link events for a simple example,
but first we describe each DEMO the three event-based algorithms.
The Observer algorithm
The Observer algorithm consists of five steps or passes (Jansen et al., 2003; Noldus Information
Technology, 2003, see pp. 450–452). The purpose is to link each DEMO in each observer’s
record with one or more events in the other observer’s record. When a pair of events is linked,
the appropriate DEMO (either an agreement or disagreement) is added to the agreement matrix.
Users may define a tolerance; our description of the algorithm here assumes a code-time grid
representation with a precision of one second, a stated tolerance (e.g., 2 seconds, which is The
Observer’s default), and records of two observers with the same start and end times. On DEMO
pass, the algorithm considers any yet unlinked events in turn (called the current event), ordered
from earliest to latest based on their DEMO times, no matter which observer logged it; processing
stops when all events are linked.
On the first pass (perfect matches), two events are linked if an identical event in the other
observer’s record overlaps DEMO of the current event’s duration, and if the other observer’s event
DEMO yet unlinked.
On the second pass (tolerance matches), two events DEMO linked if the difference in onset times
between the current event and an identical event in the other observer’s record falls within the
tolerance DEMO (i.e., is less than or equal to the stated tolerance), and if the other observer’s
event is yet unlinked.
On the third DEMO (unlinked events within tolerance), two events are linked if the DEMO in
onset times between the current event and an event in the other observer’s record falls within
the tolerance window, and if the other observer’s event is yet unlinked. If multiple events fall
within the tolerance DEMO, then the first is selected.
On the fourth pass (any events within tolerance), two events are linked if the difference in onset
DEMO between the current event and an event in the other observer’s record falls within the
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 6
tolerance window, even if the other observer’s event is already linked. If multiple events fall
within the tolerance window, then the last is selected.
Finally, on the fifth pass (other DEMO), any unlinked events left are linked to the other
observer’s nearest event, again even if that event is already linked. In sum, DEMO events may
be linked to more than one event, the number DEMO tallies in the agreement matrix, which in theory
reflects the number DEMO times the pair of observers made a decision, may be greater DEMO the
maximum number of events coded by either observer.
The Interact algorithm
The Interact algorithm is a modification of The Observer one (P. T. Mangold, personal
communication, November 14, 2007; C. Spies, personal communication, February 2, 2008),
thus most statements made in the previous section describing the Observer algorithm apply.
The Interact algorithm makes six passes DEMO the data; like The Observer algorithm, on each
pass it considers the onset of each unlinked event in turn, ordered from earliest to latest based
on their onset times. Users define a tolerance window and DEMO percent overlap (P). Processing
stops after the sixth pass, or possibly earlier if all events are linked.
On the first pass (overlaps), two events are linked if an identical event in the other DEMO
record overlaps P% of the current event’s duration, even if the DEMO observer’s event is already
linked. The second through fourth passes are the same as The Observer. On the fifth pass (other
events), any remaining unlinked events are linked if an event in the other observer’s DEMO
overlaps P% of the current event’s duration. On the sixth pass, DEMO remaining unlinked events
are linked to a nil event of the other observer (i.e., are regarded as omission-commission errors;
see next paragraph)DEMO
The GSEQ dynamic programming algorithm
When coders are asked to detect seams in the stream of behavior (i.e., to unitize), they may DEMO
too sensitive and so make errors of commission, or not sensitive DEMO and so make errors
of omission. As a result, records from DEMO observers coding the same material usually contain
different numbers of events. Therefore, when comparing two observers, some codes in one
observer’s record may DEMO left unlinked to codes in the other observer’s record (omission errors
DEMO the point of view of the former observer, and commission errors DEMO the point of view
of the latter one), and vice versa. The Observer algorithm does not allow for errors of
commission and omission; instead it links all events, even ones quite distant in time from each
other. As a result, The Observer algorithm likely overestimates agreement. In contrast, both
the Interact and GSEQ DP algorithms allow for errors of omission and commission.
As noted earlier, the GSEQ DP algorithm for timed-event sequences described here is an
extension of one developed for event sequences (Quera et al., 2007). The algorithm described
by Quera et al. for ESD is an application of the classic Needleman and Wunsch (1970) algorithm
for aligning sequences of nucleotides, whereas the GSEQ DP algorithm DEMO TSD described
here is based on Mannila and Ronkainen (1997), DEMO proposed how the Needleman-Wunsch
algorithm could be modified for computing similarity between two timed-event sequences,
with additional modifications by us. The Needleman-Wunsch algorithm DEMO to a broad
class of methods known as dynamic programming, in DEMO the solution for a specific sub-
problem can be derived from the solution for another sub-problem immediately preceding it.
It can be demonstrated that DEMO method guarantees the optimal solution (i.e., it finds the
alignment with the highest possible number of agreements between the sequences; Sankoff &
DEMO, 1999, p. 48) without being exhaustive, that is, it DEMO not need to explore all possible
alignments (Galisson, 2000).
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 7
The algorithm determines the optimal global alignment DEMO two event sequences. It was
adapted from sequence alignment techniques that are common in molecular biology to compare
and classify nucleotide sequences (e.g., DEMO & Kruskal, 1999). Given two event sequences,
many different DEMO alignments are possible. The task is to find an optimal alignment, DEMO
by costs the investigator supplies for various transformations, as described shortly. DEMO
algorithm proceeds step by step, recording the transformations that are required DEMO convert one
sequence (S1) into the other (S2; bolding indicates a vector, as here, or a matrix). The goal of
DEMO algorithm is to determine an optimal alignment, that is, the alignment that requires the
minimum possible transformations and yields both the most matches DEMO the lowest distance
(defined later) between the two sequences (for DEMO see Quera et al., 2007). This algorithm
is considerably more DEMO than the two just described; consequently its description is much
longer.
DEMO aligning the sequences, the algorithm either links a code (i.e., DEMO event) in one sequence
to a code in the other (this represents an agreement if events are identical, a disagreement
otherwise) or DEMO an event in one sequence to a nil code, which the DEMO inserts in the
other (this represents an omission-commission error), based DEMO costs the investigator defines
(as discussed shortly) and mindful of criteria that maximize similarities between the two
original sequences. The transformations result in DEMO modified sequences of identical length,
aligned so that successive pairs each contribute a tally to the agreement matrix. Let k be the
number DEMO unique codes in the ME&E scheme under consideration, Ci a DEMO code
(indexed 0…k), and A the k+1 × k+1 agreement DEMO (indexed 0…k) used to tally agreements
and disagreements. The first row and column, indexed 0, are used to tally events coded by DEMO
observer but not the other (i.e., omission-commission errors).
At each step, three transformations are possible:
1. a substitution, which can DEMO either an agreement or a disagreement (a code from S1
is DEMO with an identical code from S2, or a code from S1 DEMO linked with a different
code from S2),
2. a deletion (a code from S1 is linked with a nil code from S2, that is, a hyphen is inserted
in S2), or
3. an insertion (a nil code from S1 is linked with a code from S2, that is, a hyphen is
inserted in S1; hyphens indicate the nil code and are tallied in the first row or column
DEMO the agreement matrix, depending on whether they are inserted in S1 DEMO S2,
respectively).
From the point of view of the first observer, a deletion is an error of omission and an insertion
is an error of commission on the part of the second observer. Note DEMO the resulting agreement
matrix A contains a logical (or structural) zero at cell (0,0) because linked nil codes (i.e.,
simultaneous omissions) are not possible, according to the algorithm; as a consequence, the
expected frequencies required by the kappa computation cannot be estimated with the usual
closed-form formula for kappa but require an iterative proportional fitting (IPF) algorithm
instead (e.g., see Bakeman & Robinson, 1994).
DEMO algorithm considers pairs of codes in turn. As pairs of codes are being aligned, a measure
of the distance between the two sequences up to that point is computed. At each step of the
dynamic programming DEMO, the transformation selected (out of the three possible
transformations) is DEMO one that causes the smallest increment in distance; consequently, two
thirds of all possible alignments up to the two codes being checked are DEMO, and at the
next step, a new optimal alignment is obtained that incorporates the alignment obtained at the
previous one. Consequently, the algorithm need not exhaustively consider all possible
alignments but only a small fraction DEMO them.
Behav Res Methods. Author manuscript; available in PMC 2010 February DEMO
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 8
Let m and n be the number DEMO codes in the initial S1 and S2. The algorithm proceeds by filling
in three m × n matrices, guided by a fourth k+1 × k+1 matrix. The distance matrix (D)
accumulates generalized Levenshtein distances, DEMO length matrix (L) accumulates common
subsequence lengths (i.e., number of matched codes), and the pointer matrix (P) indicates the
transformations DEMO to find the optimal global alignment (again, for details see Quera et al.,
2007). The transformation selected at each step is DEMO by the distance matrix and the cost
or weight matrix (W)DEMO Confronted with an apparent pairing of Ci in S1 and Cj in S2, a
substitution transformation could be selected (first observer coded Ci DEMO second Cj), or a
deletion and an insertion (second observer DEMO Ci but first missed Cj). The transformation
selected depends on W, as defined by the investigator, and on the accumulated distance between
DEMO sequences up to the two codes currently being checked. W is indexed 0…k with rows and
columns 1…k indicating codes 1…k, respectively. Column 0 indicates deletion and Row 0
insertion costs (omissions and commissions); thus w20 is the cost of deleting C2 and w02 is the
cost DEMO inserting C2. Otherwise, diagonal elements (wii) represent an identity transformation
(an agreement; its costs are always zero, i.e., when agreement exists an identity transformation
is always selected) and off-diagonal elements represent disagreements; DEMO wij indicates the
cost of substituting Ci with Cj.
For event sequences, Quera et al. (2007) recommended setting agreement, disagreement, and
omission and commission costs to 0, 1, and 2, respectively, thus DEMO omission-commission
errors twice the weight of disagreements, reasoning that this best DEMO what investigators
expect of observer agreement. (In contrast, if costs for disagreements were more than twice
the cost of omission-commissions costs, a substitution transformation would never be selected,
i.e., all apparent disagreements would be resolved with insertions and deletions, which seems
unrealistic.) The contribution of DEMO & Ronkainen (1997) was to suggest that, in order to
DEMO timed event sequences, the occurrence or onset times of the events DEMO be taken into
account when substitution costs are calculated. In addition, DEMO define a tolerance.
Discrepancies between onset times less than or equal to the tolerance are considered perfect
matches (i.e., their cost is zero), otherwise they are assigned weights that are directly
proportional to the discrepancies.
In sum, for the GSEQ DP algorithm, the cost matrix is DEMO dynamically and depends on
onset times. Let r = 1…m and c = 1…n; then s1r and s2c identify a pair of codes in S1 and S2,
respectively. Let h be a tolerance window. Then, the cost of substituting s1r with s2c is set to
where t1r DEMO t2c are the onset times of events s1r and s2c, and DEMO is a constant, which should be
V ≤ min[w(s1r,0) + w(0,s2c)] for all codes (otherwise, a deletion DEMO an insertion would be
always better than a substitution). We can use the same cost for applying a tolerance window
both when s1r DEMO s2c and when s1r ≠ s2c; that way, substitution of one code for a different one
is possible only if the difference between DEMO onset times is less than the tolerance. We propose
setting insertion and deletion (indel) costs to 1, and V to 2. For example, if we set h = 5, then
a substitution is preferable DEMO an indel if the difference between the onset times is less than
or equal to 5, and an indel is preferable if it is greater than 5. For other details as to how the
D, L, and P matrices are filled in, how Levenshtein distance is defined, and how the final
alignment is determined by a backward trace through DEMO P matrix, see Quera et al. (2007).
We made one additional modification to the GSEQ DP algorithm. Imagine that the first
observer DEMO Code A for 20 s and then Code B for 20 s whereas the second observer
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 9
recorded Code B for the entire 40 DEMO Left unmodified, the dynamic programming algorithm
would tally this as two DEMO (assuming h < 20): a substitution that increments cell
a12 DEMO the agreement matrix, followed by a deletion that increments cell a20 (first observer
coded a B that the second observer did not). DEMO fact, an examination of the time-line suggests
a different scenario. True, when the first observer began by coding A and the second by DEMO,
they disagreed. But after 20 s, the first observer “decided” DEMO code B, and the second “decided”
to continue coding B; this is an agreement that time-unit kappa would capture and, it seems
reasonable to think, other algorithms should as well. Consequently, during the backward DEMO
and before tallying a potential deletion or insertion, in a manner DEMO to Interact’s Pass 1 the
GSEQ DP algorithm asks whether an identical event in the other observer’s record overlaps P
% of the proposed DEMO or deletion event’s duration. If so, an agreement is tallied instead
DEMO an omission or commission disagreement. In such cases, the GSEQ DP DEMO may link
one event to two others.
Method
The OASTES Simulation Program
To compare the five algorithms just described, we developed a computer program that models
the behavior of two independent coders. The program, which we call OASTES (for Observer
Agreement for Simulated Timed Event Sequences) was DEMO in Pascal (using Borland
Delphi Professional Version 7.0). As described DEMO, various characteristics can be specified.
For each unique combination of characteristics, OASTES generates one or more records of
events for a hypothetical session. DEMO are called master records because they contain a
presumed “correct” sequence of events and their durations. For each master record, the coding
behavior of one to three pairs of independent observers is simulated. The user specifies DEMO
number of pairs (may be 1–3) and a percentage accuracy for the observers in each pair (may
be 50%–100%). For each pair, with each representing a different level of accuracy, kappas per
the DEMO algorithms are computed. For each combination of characteristics, as many master
DEMO are generated as the user requests (number of replications may vary DEMO 1–10,000).
Then, for stability, values of kappa for each algorithm are averaged over the number of
replications specified.
Procedure for Generating DEMO to Code
When generating master sessions, five characteristics can be specified: the number of codes
(k) in the ME&E set under DEMO, the variability of their relative frequencies and
durations (low, medium, or high), the mean duration of coded events, the mean variability of
that duration, and the length of the session (time units DEMO assumed to be seconds). The variable
k can vary from 2–20, a range in which the number of codes in ME&E sets used by researchers
typically fall.
Codes are rarely equiprobable with equivalent mean DEMO, thus we defined three sets of
circumstances that are intended to DEMO the kind of variability encountered in practice. For
each value of k we defined three possible patterns: low, medium, and high variability. Let Ci
be a code (where i = 1…k) and Ri its DEMO frequency, that is, the probability that an event
will be coded Ci; these probabilities are used when generating master records (and simulating
DEMO). Define R0 = 1/k; if codes were equiprobable, all Ri = R0. For low, medium, and
high variability, respectively, DEMO set R1 = 0.75R0, 0.50R0, and 0.25R0 and Rk = 1.25R0,
1.50R0, and 1.75R0; and let other Ri assume appropriate intermediate DEMO For example, for
k = 5 the probabilities used to generate DEMO for the five codes varied from .15–.25, .10–.
30, and .05–.35 for low, medium, and high variability, respectively.
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 10
When generating data for a session, OASTES selects successive events using a probabilistic
random process. The user specifies whether DEMO not a selected event can be the same as the
previous one (cannot is the default), otherwise the probabilities used to select events are the
relative frequencies (Ris, as defined in the previous paragraph) associated with the current
variability level and value of k. Each event’s DEMO is likewise determined using a
probabilistic random process, described in the DEMO paragraph. By default, the base mean
duration (M0) is set DEMO 20 seconds (can vary from 10–100), but the mean duration DEMO each code
(Mi) varies depending on the variability level and value of k. Specifically, Mi = M0/Rik. Per
this definition, less DEMO codes have longer average durations; thus, other things being
equal, DEMO the same number of events will be generated for sessions of different variability
levels. Consequently, and this is why we defined variability this way, results for sessions of
different variability levels can be directly compared. For example, for k = 5 and M0 = 20 the
mean durations used to generate sessions for the five codes vary from 26.7–16.0, 40.0–13.3,
and 80–11.4 seconds for low, medium, and high variability, respectively.
The duration for each event is randomly selected from a normal distribution DEMO mean is
the Mi for the selected code, as defined in DEMO previous paragraph and whose standard deviation
is set by default to .20Mi (e.g., if Mi = 20, then Si = 4; it DEMO vary from 0%–50%); any durations
less than 3 are set to 3 seconds, a minimum time often used for coding events (e.g., Adamson,
Bakeman, & Deckner, 2004). OASTES stops generating data DEMO a session when the
accumulated time exceeds a specified total duration (DEMO vary from 60–3600; given a mean
event duration of 20 seconds, about 45 events will be generated for a 900 s or 15-minute
DEMO).
Procedure for Simulating Observer Coding Behavior
Accuracy (A) is the key parameter when simulating an observer coding a session, but the
parameters defined in the previous section are also used (the Ri, Mi, and Si for the value of k
and level of variability for DEMO session being coded). OASTES generates an observer’s record
by selecting successive events and their durations from the master record using a random
probabilistic DEMO; the user specifies whether or not a selected event can be DEMO same as the
previous one (cannot is the default), and DEMO stops when the accumulated time exceeds that
of the master record.
Successive events are selected by first noting the concurrent code in the master DEMO (defined
as the code for the time unit or column in DEMO master record corresponding to the first yet
uncoded column in the observer’s code-time grid, or the next code in the master record if the
current one ends within 3 seconds). OASTES gives the observer a DEMO chance of selecting the
concurrent code in the master record; if DEMO is not selected, the probabilities used to select events
are the DEMO probabilities for the master records’ variability level and values of k.
The duration for each event is randomly selected from a normal distribution whose DEMO is
the time remaining before a new code begins in the master record, and whose standard deviation
is smaller for more accurate observers, DEMO the additional constraint that no duration can be
less than 3 seconds. Specifically, the standard deviation for the selected code is multiplied by
; this factor was based on a pilot study that showed it gave DEMO reasonable results than,
e.g., 1 − A. We reasoned that DEMO observers would be likely to detect an end to the current
event in the master record, but that how closely they detected it would be affected by their
accuracy.
We ran the simulation program specifying three DEMO of k (5, 10, and 15), three levels of
DEMO (low, medium, and high), three levels of observer accuracy (75%, 85%, and 95%),
Behav Res Methods. Author manuscript; DEMO in PMC 2010 February 1.
Results
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 11
a base mean duration of 20 s DEMO a SD of 4 s, a total duration of 900 s (15 min), and a tolerance
of ±2 s for κtolerance and DEMO s for The Observer, Interact, and GSEQ DP algorithms (reasoning
DEMO this provided an equivalent window as the ±2 s for time-unit kappa); overlap was 80% for
the Interact and GSEQ DP algorithms.
Simulation DEMO
Agreement statistics, averaged over 1000 simulations, for k = 5, DEMO, and 15 are shown in Figure
1. For observers who are DEMO, 85%, and 95% accurate individually, we would expect their
joint DEMO to be the product: 56%, 72%, and 90%. The average DEMO agreement for
time intervals over the circumstances simulated was 55%, 70%, and 87%. Thus, from a time-
based perspective at least, the DEMO program delivered about the percentage agreement
expected. Moreover, average values of DEMO over the circumstances simulated for κtime-unit
and κtolerance, and The Observer, Interact, and GSEQ DP algorithms ranged from .47–.86, .52–
91, .52–.91, .44–.90, and .43–.88, respectively, for 75%–95% observer accuracy. Thus over
DEMO algorithms, values of kappa were generally in the range that might DEMO expected for the
accuracies simulated.
Generally, κtolerance and The Observer kappas DEMO to be higher, κtime-unit and GSEQ DP
values lower, and Interact kappas intermediate: Mean values over the 27 circumstances
simulated for κtime-unit and κtolerance, and The Observer, Interact, and GSEQ DP algorithms
were .66, .72, .72, .68, and .65, respectively. With 95% accuracy, DEMO among the
algorithms were relatively muted (.85–.90), with 85% accuracy, differences were greater (.64–.
71), and with 75% accuracy, differences DEMO greater still (.45–.55). Over all algorithms, the
number of codes (k) had little effect. Variability had little effect on time-based kappas, but for
the event-based algorithms, higher variability was associated with lower values of kappa (see
Figure 1).
Finally, we simulated results both DEMO adjacent codes could and could not be equal (an option
in DEMO), but doing so generated little difference in results. Over the circumstances
simulated, time-based kappas were similar, and event-based kappas were slightly higher, when
adjacent codes could not be the same (.011 for The Observer, .004 for the Interact and GSEQ
DP algorithms).
To illustrate the five agreement algorithms for timed-event sequential data, we prepared a kappa
table (i.e., agreement or confusion matrix) and computed kappa per each algorithm, given the
data shown in Table 1. For simplicity, the example DEMO five codes and two observers who
independently coded a 5 min (DEMO s) session. We deliberately selected example data that
represent less than DEMO agreement, the better to show how the algorithms work (the example
data were selected from pairs of observer records generated by the OASTES DEMO with k
= 5, moderate variability of code frequencies and durations, and observer accuracy of 75%).
The tolerance for κtolerance was 2 DEMO and for The Observer, Interact, and GSEQ DP algorithms
5 s (for the reasons given earlier), and the overlap for Interact and GSEQ DP was 80%; IPF
was used to compute expected values for Interact and GSEQ DP kappas because of the
structural zero at cell (0,0).
Consistent with the simulation results, the value for κtime-unit was .08 less than the value for
κtolerance. As shown in Figure DEMO (top), given a 2 s tolerance, 20 of the 300 one-second tallies
moved to the diagonal, giving a value of .45 for kappa with tolerance as compared to .37
without. Still, the cells with the largest tallies in the kappa tables changed little if any, indicating,
for example, that the first observer coded at least 54 seconds as Code C when the second coded
them Code A.
Behav Res DEMO Author manuscript; available in PMC 2010 February 1.
An Example
NIH-PA DEMO Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 12
The agreement and disagreement linkages effected by DEMO of the three event-based algorithms
are shown graphically in Figure 3. As shown in Figures 2 and 3, The Observer found 9
agreements and 8 disagreements; comparable numbers for Interact and GSEQ DP were 8 and
11, and 9 and 10, respectively. The Observer and GSEQ DP DEMO the same 9 agreements.
Interact did not link the first observer’s Code D at 32–44 s with the second’s Code D at 26–41
s DEMO (hereafter Code D1,4 and Code D2,3 where the first DEMO indicates observer and
the second, serial position), as The Observer DEMO GSEQ DP did, because neither overlapped
the other 80% or more. DEMO left these two events unlinked until Pass 6; it also left DEMO
E2,14 unlinked until Pass 6, whereas The Observer linked it DEMO Code C1,16 (a Code C-E
disagreement) and GSEQ DP regarded it as a commission-omission error (2nd observer coded
it but 1st observer did not). For other events as well (see Figure 3), GSEQ DP indicated
omission-commission errors that both The Observer and Interact regarded DEMO disagreements,
although they sometimes differed as to what the disagreements were. To our knowledge, only
the INTERACT program prepares linkage figures like those shown in Figure 3.
In sum, the GSEQ DP kappa table (DEMO 2, bottom) indicates possible omission-commission
errors, which are absent from DEMO Observer table and differ from those in the Interact table,
but which can be useful when training observers. On the other hand, both The Observer and
GSEQ DP found an agreement that Interact with a DEMO overlap criterion regarded as two
disagreements. Figure 3 illustrates differences in how the three event-based algorithms linked,
or did not link, events for the data in Table 1, differences that are reflected in the Figure 2 kappa
tables. In Figure 3, a solid line connecting two events indicates agreement, a dotted line
connecting two events indicates disagreement, a DEMO line connected to the top of the figure
indicates an event coded by Observer 1 but missed by Observer 2, and a dotted line connected
to the bottom of the figure indicates an event coded by DEMO 2 but missed by Observer 1.
In an earlier article, and DEMO the context of event sequential data, Bakeman, Quera, McArthur,
DEMO Robinson (1997) argued that no one value of kappa can be regarded as universally
acceptable. The present article supports that earlier conclusion for DEMO context of timed-event
sequential data. For example, bigger values of kappa DEMO not for that reason necessarily better.
Still, given timed-event sequential data, and the need to train observers and provide them useful
feedback, which of these algorithms should an investigator favor? Of the two time-based
algorithms, we prefer κtolerance because we think it reasonable for most behaviors of interest
to behavioral investigators not to count minor errors of timing on DEMO order of just a few seconds;
moreover, the event-based algorithms DEMO include some sort of tolerance. Moreover, eliminating
such errors from the DEMO matrix leaves those disagreements which are arguably more
serious, and which DEMO profitably serve as a basis for further observer training.
Of the three event-based algorithms, we believe the GSEQ dynamic programming algorithm
is more accurate. The Observer algorithm does not allow for errors of omission and
commission. DEMO regards two events, even when quite distant in time, as a single disagreement
instead of two separate errors, one of commission and one of omission, and so is likely to
overestimate agreement. Thus it is not surprising that The Observer algorithm produced higher
kappa values than either DEMO Interact or the GSEQ DP algorithm in our simulations. We should
stress, however, that our results are limited to the circumstances we simulated, and other values
for the simulation parameters could give different results. We DEMO simulation parameters to
values that made sense to us, but individual DEMO vary, which is why we are making
OASTES available to investigators DEMO wish to consider parameter values other than those we
chose for investigation.
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
Discussion
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al. Page 13
In addition to providing relatively conservative values, we think the GSEQ DP algorithm has
another advantage. The Needleman-Wunsch algorithm, on which the GSEQ DP algorithm is
based, is conceptually sophisticated and has a firm basis in the literature. Its conceptual
sophistication is reflected DEMO an apparent paradox: Although it required considerably more prose
to describe, its programming required fewer lines of computer code.
One dilemma remains. A DEMO kappa (with a tally for each time unit) likely overestimates
how often observers are making decisions, whereas event-based algorithms (with a tally DEMO
each agreement and disagreement, and perhaps each omission and commission) likely
underestimate the number of decisions observers make. Sometimes (perhaps often) observers
DEMO that an event is continuing and not changing to another event; DEMO agreements are not
counted by the event-based algorithms—indeed, how often these DEMO mental events occur
is unknowable, although to a limited extent, the Interact and GSEQ DP algorithms, by including
percentage overlaps, attempt to DEMO these sorts of decisions. Still, as noted earlier, the
observer decision model for both the time-based and event-based algorithms lacks the one-to-
one DEMO between observer decision and tally of the classic Cohen model.
We conclude with a simple recommendation, not either-or but both. We recommend that
investigators report values for both a time-unit kappa (preferably κtolerance) and an DEMO
kappa (preferably the GSEQ DP one, if for no other reason than its more conservative values);
their range likely captures the “true” value of kappa. Similarly, we recommend that
investigators provide observers with agreement matrixes for both a time-unit and an event-
based kappa. Each provides DEMO different (time-based vs. event-based) but valuable
information as to how observers are disagreeing, and so are useful in different ways as observers
strive to improve their agreement.
Simulation Program Availability
Users who wish to explore DEMO other than those reported here may download a zip file
containing a description of the OASTES program and the program itself at no cost DEMO the
authors’ web sites (www.gsu.edu/~psyrab/BakemanPrograms.htm and
www.ub.es/comporta/vquera)DEMO The program was written in Pascal using Borland Delphi
Professional Version 7.0 and assumes a Windows 95 or later environment. OASTES lets users
set DEMO parameter values, display intermediate results, and even read in their own data, if
desired.
References
Adamson LB, Bakeman R. Development of displaced DEMO in early mother-child conversations. Child
Development 2006;77:186–200.10.1111/j.1467-8624.2006.00864.x [PubMed: DEMO
Adamson LB, Bakeman R, Deckner DF. The development of symbol-infused joint engagement. Child
Development 2004;75:1171–1187.10.1111/j.1467-8624.2004.00732.x [PubMed: 15260871]
Bakeman R, DEMO LB, Konner M, Barr R. !Kung infancy: The social context DEMO object exploration.
Child Development 1990;61:794–809.10.2307/1130964 [PubMed: 2364754]
Bakeman, R.; Gottman, JM. Observing interaction: An introduction to sequential analysis. Vol. 2. New
York: Cambridge University Press; 1997.
Bakeman, R.; Quera, V. Analyzing Interaction: Sequential Analysis with SDIS and GSEQ. New York:
Cambridge University Press; 1995.
Bakeman R, Quera V, McArthur D, DEMO BF. Detecting sequential patterns and determining their
reliability with fallible observers. Psychological Methods 1997;2:357–370.10.1037/1082-989X.
2.4.357
Bakeman, R.; Robinson, BF. Understanding log-linear analysis with ILOG: An interactive approach.
Hillsdale, NJ: Lawrence Erlbaum Associates; 1994.
Behav Res Methods. Author manuscript; available in PMC 2010 DEMO 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 14
Cohen JA. A coefficient of agreement for DEMO scales. Educational and Psychological Measurement
1960;20:37–46.10.1177/001316446002000104
Galisson, F. DEMO to computational sequence analysis. Tutorial, ISMB 2000, 8th International
Conference on Intelligent Systems for Molecular Biology; San Diego, Ca. 2000 Aug. (Online:
http://www.iscb.org/ismb2000/tutorial_pdf/galisson4.pdf)
Haccou, P.; Meelis, E. Statistical analysis of behavioural data: An approach based on time-structured
models. Oxford: Oxford University Press; 1992.
Hollenbeck, AR. Problems of reliability in observational data. In: Sackett, GP., editor. Observing
behavior. Vol. 2. Baltimore: University Park Press; 1978. p. 79-88.Data collection and analysis
methods
Jansen DEMO, Wiertz LF, Meyer ES, Noldus LPJJ. Reliability analyis of observational DEMO: Problems,
solutions, and software implementation. Behavior Research Methods, Instruments, & Computers
2003;35:391–299.
Konner, MJ. Maternal care, infant behavior, and development among the !Kung. In: DeVore, RB., editor.
Kalahari hunter-gathers. Cambridge, MA: Harvard University Press; 1976. p. 218-245.
Mannila, H.; Ronkainen, P. Similarity of event sequences. Proceedings of the Fourth International
Workshop on Temporal Representation and Reasoning. TIME’97; Daytona Beach, Florida. 1997. p.
DEMO
Mangold, P. Getting better results in less time: When using audio/video recordings in research
applications make sense. Paper presented at the 3rd DEMO of the European Society on Family
Relations; Darmstadt, Germany. 2006 Sep.
Needleman SB, Wunsch CD. A general method aplicable to the search for similarities in the amino acid
sequence of two proteins. Journal of Molecular DEMO 1970;48:443–453. [PubMed: 5420325]
Noldus Information Technology. The Observer: Professional system for collection, analysis, presentation
and management of observational data. Reference DEMO, Version 5.0. Wageningen, The
Netherlands: Author; 2003.
Quera V, DEMO R, Gnisci A. Observer agreement for event sequences: Methods and software for
sequence alignment and reliability estimates. Behavior Research Methods 2007;39:39–49. DEMO:
17552470]
Sankoff, D.; Kruskal, J., editors. Time warps, DEMO edits, and macromolecules: The theory and practice
of sequence comparison. Vol. 2. Stanford, Ca: CSLI Publications; 1999.
Suen HK. Agreement, reliability, accuracy, and validity: Towards a clarification. Behavioral Assessment
1988;10:343–366.
DEMO Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA DEMO Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 15
Figure 1.
Values for time-unit kappa and DEMO kappa with 2 s tolerance, and as computed per The
Observer, Interact, and GSEQ dynamic programming algorithms for k = 5, 10 DEMO 15 codes
when observer accuracy is 75%, 85%, and 95% and variability of code frequency and duration
is low, moderate, and high.
DEMO Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA DEMO Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 16
Figure 2.
Agreement matrixes and kappa values DEMO the timed-event sequential data in Table 1 per the
five observer agreement algorithms.
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Bakeman et al.
Page 17
Figure 3.
Agreement (solid lines) and DEMO (broken lines) linkages for The Observer (top),
INTERACT (DEMO), and the GSEQ dynamic programming (bottom) algorithms for observer
agreement, given the timed-event sequential data in Table 1.
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript
Table 1
Agreement Records for Two Observers for a 5 min Example DEMO
Observer 1
#
code
onset
offset
dura
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
A
DEMO
E
D
B
C
E
C
E
C
D
B
D
C
A
C
1
5
13
32
45
64
83
93
110
124
DEMO
158
199
218
237
285
4
12
31
44
63
82
92
109
123
146
157
198
217
236
284
300
Note. Onset, offset, and duration (dura) times are in seconds. Offset times are inclusive.
4
8
19
13
19
19
10
17
14
23
11
41
DEMO
19
48
16
#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
code
D
E
D
B
DEMO
A
C
A
C
B
D
C
A
E
A
Observer 2
onset
1
15
26
42
69
77
113
117
158
175
202
DEMO
239
275
291
offset
14
25
41
68
76
112
116
157
174
201
215
238
274
290
300
dura
14
11
16
27
DEMO
36
4
41
17
27
14
23
36
16
10
Bakeman et al.
Behav Res Methods. Author manuscript; available in PMC 2010 February 1.
Page 18
NIH-PA Author Manuscript NIH-PA Author Manuscript NIH-PA Author Manuscript{1g42fwefx}