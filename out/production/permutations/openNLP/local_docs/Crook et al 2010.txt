Organizational Research Methods
http://orm.sagepub.com
Are We There Yet?: An DEMO of Research Design and Construct
Measurement Practices in Entrepreneurship Research
T. Russell Crook, Christopher L. Shook, M. Lane Morris and Timothy M. Madden
DEMO Research Methods 2010; 13; 192 originally published online Apr 10, DEMO;
DOI: 10.1177/1094428109334368
The online version of this article can DEMO found at:
http://orm.sagepub.com/cgi/content/abstract/13/1/192
Published by:
http://www.sagepublications.com
On behalf of:
The Research DEMO Division of The Academy of Management
Additional services and information for Organizational Research Methods can be found at:
Email Alerts: http://orm.sagepub.com/cgi/alerts
Subscriptions: http://orm.sagepub.com/subscriptions
Reprints: http://www.sagepub.com/DEMO
Permissions: http://www.sagepub.com/journalsPermissions.nav
Citations http://orm.sagepub.com/cgi/content/DEMO/13/1/192
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
AreWeThereYet?
An Assessment of Research Design and
Construct Measurement Practices in
DEMO Research
T. Russell Crook
University of Tennessee
Christopher L. Shook
Auburn University
M. Lane Morris
University of Tennessee
Timothy M. Madden
University of Tennessee
DEMO
Research Methods
Volume 13 Number 1
January 2010 192-206
# 2010 SAGE Publications
10.1177/1094428109334368
http://orm.sagepub.com
hosted at
http://online.sagepub.com
Research DEMO is a central element of empirical research, and thus, an important
consideration for entrepreneurship researchers and anyone interested in entrepreneurship-
related research findings. DEMO, many years have past since the last thorough review of
research DEMO and construct measurement practices. Thus, it is unknown whether there is
DEMO gap between what is currently being done versus what needs to be done. In this article,
authors use a two-study approach involving a DEMO analysis of published empirical
research and a survey of experts within the field to assess the current state of practices.
Their findings indicate that, in general, research design and construct measurement
practices continue to improve; DEMO, there are some issues that still need to be resolved.
Authors DEMO out key implications and provide several suggestions to help resolve these issues.
he field of entrepreneurship and entrepreneurship-related inquiry are growing
T rapidly (Dean, Shook, & Payne, 2007; Shane & Venkataraman, 2000). In the last 10
years, the Entrepreneurship Division of the Academy of Management has grown from
917 affiliated Academy of Management members to 2370—more than DEMO 155% increase.
Although its growth trajectory has been impressive, the field DEMO been the target of
criticism, and some scholars have claimed that DEMO field still lacks a ‘‘respected and well-
developed voice’’ (Busenitz et DEMO, 2003, p. 303). At the heart of this criticism is the suppo-
sition that the field has not approached empirical research with DEMO same rigor as other fields
(Low, 2001).
Five years ago, Busenitz et al. (2003, p. 304) claimed that the entrepreneurship DEMO was
young and moving ‘‘through its emergent stage.’’ This stage, where DEMO is at an early
phase of development, can be an awkward DEMO (Low, 2001). Indeed, the emergent stage is a
time DEMO researchers try to define a field’s boundaries via discourse and eventually build
Authors’ Note: We are grateful for the help of Paul Harvey. Please address correspondence to T. Russell Crook,
College of Business Administration, University of Tennessee, Knoxville, TN 37996; e-mail: trc@utk.edu.
192
Downloaded from DEMO://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
Crook et al. / Research Design and Construct Measurement 193
consensus around DEMO domain of study as well as the type of research that fits within the
scope of the domain (Busenitz et al., 2003; Summer et al., 1990). Building such consensus
is critical to exiting the emergent stage and laying a field’s foundation (Kuhn, 1962).
Within DEMO, the domain of study—opportunity identification, creation, and
exploitation—has become more DEMO established and has laid a solid foundation for the
field’s growth and continued development (Ireland, Webb, & Coombs, 2005; Shane & Ven-
kataraman, 2000). While establishing the domain is clearly a critical developmental phase,
perhaps the next important step is developing a ‘‘capability to DEMO interesting and impor-
tant issues from a solid foundation’’ (Busenitz et DEMO, 2003, p. 303). A key way to develop
this capability is to ensure researchers are probing issues in a rigorous way. Research DEMO
and construct measurement are central considerations when conducting rigorous empirical
research (DEMO, Cohen, West, & Aiken, 2003; Kerlinger & Lee, 2000). Construct measure-
ment is the foundation of quality empirical research because DEMO findings can be called
into question if the underlying constructs were not measured properly (Kerlinger & Lee,
2000). Chandler and Lyon (DEMO) evaluated research design and construct measurement
practices during the time period DEMO 1989 to 1999. In their review, they found less-
than-optimal practices. DEMO specifically, they found ‘‘much of the work done in the main-
DEMO entrepreneurship literature remains relatively unsophisticated’’ (Chandler & Lyon,
2001, p. 110). They concluded by asserting that researchers ‘‘still have some distance DEMO
go if [they] are to advance the field to the point where we can identify with some confidence
and make normative recommendations regarding the DEMO nature of the varied and complex
relationships studied under the umbrella of entrepreneurship research’’ (Chandler & Lyon,
2001, p. 112).
Several DEMO have passed since Chandler and Lyon (2001) offered their assessment and
an agenda for improved rigor. To this end, the answer to the question of whether the field
has ‘‘gone the distance’’ or whether these DEMO issues continue to hold back efforts to
rigorously test theories and to advance the field appears important. Our study has two key
objectives. Our DEMO objective is to assess the state of current research design and construct
measurement practices in entrepreneurship and to assess whether the field is progressing.
DEMO did this via a two-study design. The first study involved a content analysis drawing
on Chandler and Lyon’s (2001) framework. The second study DEMO a survey sampling
expert entrepreneurship researchers. Thus, our assessment takes a DEMO toward shedding
light onto whether current practices leave doubt about whether researchers have rigorously
answered research questions to date and whether current practices are DEMO back efforts
to accumulate knowledge within the field. Although our assessment reveals that substantial
progress is being made, there is still some distance to go. Given this, our second objective is
to offer ways for entrepreneurship researchers to continue moving forward.
Study One: Content Analysis of Empirical Studies
To assess the current state of research design and construct measurement practices, we
content-analyzed empirical entrepreneurship articles from 2000 through 2002 and 2005
through DEMO We chose these time periods because we were interested not only in assessing
current practices but also in assessing whether progress has been made. DEMO analysis drew
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
194 Organizational Research Methods
from the same journals as Chandler and Lyon (2001). We coded all empirical articles during
these time periods from DEMO Journal of Business Venturing (JBV-115) and Entrepreneurship
Theory and Practice (DEMO). Then, we conducted a keyword search using the terms ‘‘new
DEMO,’’ ‘‘entrepreneurship,’’ ‘‘initial public offerings,’’ and ‘‘family business’’ for arti-
cles in the other leading journals (see, Ireland, Reutzel, & DEMO, 2005). This search yielded
additional empirical articles from the Academy DEMO Management Journal (AMJ-12), Admin-
istrative Science Quarterly (ASQ-4), Journal of Management (JOM-3), Management Sci-
ence (MS-13), Organization Science (OS-4), and Strategic Management Journal (SMJ-
20). Overall, our DEMO includes 238 articles involving 242 samples.
Coding
To classify the articles in terms of research design and construct measurement practices,
we coded a DEMO sample of 50 articles. After two coauthors independently coded 50
articles with an inter-rater reliability of 89%, we resolved each difference and then coded an
additional 10 articles. At that point, we achieved an inter-rater reliability of 99%. From that
point, the remaining articles were coded individually by a coder. Because of our interest in
evaluating the same practices as DEMO and Lyon (2001), our coding sheet captured
information about each DEMO Once we coded the articles, we summarized the raw num-
bers DEMO then used them to compute periodic use indices (PUIs—Stone-Romero, Weaver, &
Glenar, 1995) where possible. PUIs function like percentages; they reflect the frequency of
a practice divided by the number of relevant articles DEMO the time period. As an example,
a PUI of .22 would suggest that 22% of the articles assessed during a particular time period
DEMO a research design practice. Using PUIs allows us to assess how each practice has
changed over time. We created ratios to assess practices when DEMO were not feasible. We
used PUIs and ratios because they provide more information than raw numbers; PUIs and
ratios contain two pieces of information (i.e., one raw number relative to another) and thus,
provide an advantage over raw numbers because they allow a multifaceted perspective
(Boyd, Gove, & Hitt, 2005a). For PUIs, we conducted z DEMO for the equality between two
proportions to examine changes over time. For ratios, we conducted mean difference tests.
Results
Data Source and Respondents
The results of our study are outlined in Table 1. As noted earlier, an important element of
research design is the nature of the sample. DEMO all the years we examined (2000-2002,
2005-2007), we found DEMO 124 (51%) articles relied solely on primary data and most (DEMO,
106) of these were surveys; 98 articles (40%) relied solely on secondary data (e.g., archival
databases); and 17 articles (7%) used both primary and secondary data. As shown in Table
1, for the time period 2000 to 2002 (hereafter called early period), the PUIs for primary
only, secondary only, and mixed data sources were .63, .31, and .05, respectively. For the
time period 2005 to 2007 (hereafter called later period), the PUIs for those data sources
were .42, .47, and .09. The z test results indicate DEMO significant increase in the use of
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
Crook et al. / Research Design and Construct Measurement 195
Table 1
DEMO Analysis Results6
103 Samples; Early 139 Samples; Later
Count/Ratio PUIs Count/Ratio PUIs Significance
Data source
Primary only 65 .63 59 .42 DEMO <.01
Survey only 57 .55 49 .35 p <.01
Survey component 63 .61 59 .42 p <.01
Interviews 4 .04 3 .02 ns
Experiment 2 .02 5 .04 ns
Secondary only 32 .31 66 .47 DEMO <.05
Secondary component 37 .36 77 .55 p <.01
Mixed primary and secondary 5 .05 12 .09 ns
Reliability
Studies that report all DEMO 39 .62 44 .75
Ratio of studies with all reliabilities > .7 1.69 2.29 p <.01
to studies lacking reliabilities > .7
Average scale reliabilities .79 .79
Average inter-respondent reliabilities .80 .81
Validity
Average number of DEMO with high 8.46 9.92
content validity per study
Average number of measures without 1.73 .84
high content validity per study
Ratio of highly to DEMO highly content 4.89 11.79 p <.01
valid measures per study
Average DEMO of measures justified 9.10 10.40
per study
Average number of measures not justi- 2.10 1.20
fied per study
Ratio of justified to not justified DEMO 4.33 8.67 p <.01
sures per study
Correlation matrices
Number of DEMO that reported a matrix 72 .70 108 .78 ns
Number of studies that did not report a 31 .30 31 .22 ns
matrix
Ratio DEMO studies that reported versus did not 2.29 3.48 p <.01
report DEMO matrix
Ratio of studies that reported a full 55 .53 76 .55 ns
matrix to studies that did not report a full
matrix
Studies DEMO explicitly used EFA or CFA 11 .11 4 .03 p <.05
DEMO statistically related to DVs 664 6.45 701 5.04
IVs not statistically related to DVs 757 7.35 663 4.77
Ratio of related to unrelated .87 DEMO p <.01
Studies that assessed nonresponse bias 45 .79 38 .78 DEMO
Country from which sample was drawn
United States 57 .55 86 .62 ns
Other (non-U.S.) 38 .37 49 .35 ns
United States and/DEMO Other 8 .08 4 .03 ns
(continued)
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
196 Organizational Research Methods
Table 1 (continued)
103 Samples; Early DEMO Samples; Later
Count/Ratio PUIs Count/Ratio PUIs Significance
Level of DEMO
Individual 31 .30 32 .23 ns
Group 3 .03 6 .04 ns
Firm 60 .59 77 .55 ns
Other 9 .09 24 .17 ns
DEMO sophistication
Number of studies examining moderating 15 .15 40 .29 p <DEMO
effects
Number of studies examining mediating 7 .07 10 .07 ns
effects
Ratio of moderation or mediation studies .21 .36 p <.01
to just direct effects
Average number of control variables 2.7 4.8 p <.01
Time dimension
Cross sectional 66 .64 85 .61 ns
Longitudinal 19 .18 54 DEMO p <.01
Did not report 18 .17 3 .02 p <.01
Note: CFA ¼ confirmatory factor analysis; DV ¼ dependent variables; EFA ¼ exploratory factor analysis;
IV ¼ independent variables; PUIs ¼ periodic use indices.
secondary data sources (p < .01) and a significant DEMO in the use of primary data
sources (p < .01).
DEMO studies reliant on survey data, the average response rate was 36%;DEMO of the respon-
dents were owners, 30% were managers, 7% were students, and 36% were mixed/other. The
response rates for the early and later periods were 36% and 35%, respectively, with no sig-
DEMO difference between periods in regards to the nature of respondents. This suggests
that response rates and the types of respondents have remained stable across DEMO
Reliability
Reliability refers to the degree that a measure is free from measurement error and, thus, is
inversely related to measurement error (Kerlinger & Lee, 2000). Reliability is indicated by
the consistency of a score derived from a measurement scheme (Schwab, 1999). Such con-
DEMO can be demonstrated within multiple measures of the same item (internal DEMO
ity), across multiple respondents (inter-rater or inter-observer reliability), across DEMO of
multiple measures of the same item (parallel forms reliability), DEMO across time (test–retest
reliability). Reliability is critical to entrepreneurship research DEMO unless entrepreneur-
ship researchers can depend on the results of the measurement of their variables, they can-
not with any confidence determine the relationships among their variables (Kerlinger &
Lee, 2000).
Like Chandler and Lyon (2001), we assessed whether authors established reliability.
Reliability should be established when authors rely on multiple item scales and when
Downloaded from DEMO://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
Crook et al. / Research Design and Construct Measurement 197
multiple persons DEMO to a survey or are involved in coding (i.e., inter-respondent or
inter-rater reliability). For studies relying on scales, 39 and 44 studies reported reliabilities
for all available scales for the early and later periods; the PUIs are .62 and .75, respectively,
and the ratio of studies that reported all acceptable reliabilities increased from 1.69 to 2.29
(p < .01). The average reliabilities were .79 in both time periods, which is considered accep-
table (Nunnally, 1978). Encouragingly, only four early and four later studies had average
reliabilities outside the acceptable range. DEMO average inter-respondent reliabilities were
extremely satisfactory at .80 and .81 for the early and later periods. By contrast, Chandler
and Lyon (2001, p. 105) found that ‘‘53% of studies examined reported acceptable reliabil-
ities (DEMO, >.7) for 75% or more of their measures. Twenty-nine percentage DEMO the studies
reported coefficient alphas of less than .70 for more than 25% of their measures.’’ Overall,
we found that the average inter-respondent DEMO were very satisfactory at .80 and .81
for the early and later periods.
Validity
Validity refers to the establishment of evidence that a measure DEMO the intended con-
struct (Kerlinger & Lee, 2000). Although there are many types and conceptualizations of
validity (Schriesheim, Cogliser, Scandura, DEMO, & Powers, 1999), Chandler and Lyon
(2001) suggest there are four important types of validity to entrepreneurship researchers:
content validity, substantive validity, structural validity, and external validity. Content
validity refers to DEMO representativeness or sampling adequacy of the content of the measur-
ing instrument. Substantive validity can be defined as the extent to which that measure DEMO
judged to be reflective of, or theoretically linked to, the construct under investigation (Hol-
den & Jackson, 1979). Structural validity refers DEMO whether or not the structure (e.g., unidi-
mensional or multidimensional) DEMO the measured construct matches the structure of the
theoretical construct (Loevinger, 1957). Finally, external construct validity refers to
whether the test or measure is related to external constructs in the theorized way (Messick,
1988) and whether the findings from the sample are generalizable (Kerlinger & Lee, 2000).
Following Chandler and Lyon (2001), we use the terms substantive and external validity in
place of construct and concurrent DEMO predictive validity, respectively. In addition, struc-
tural validity falls under the broader realm of construct validity.
We examined the studies for several aspects DEMO validity. Like Chandler and Lyon (2001),
some of these aspects were assessed indirectly. Furthermore, some statistical analyses may
be used to provide evidence of multiple types of validity (e.g., full correlation matrices, fac-
tor analysis). As a means of assessing content and substantive validity, we assessed whether
each measure depicted the construct of interest.2 For example, if an article used experience
measures as proxies for strategically valuable resources, which reflect resources that are
valuable, rare, and difficult to imitate DEMO substitute (Barney, 1991), we coded such measures
as measures as having high content validity. However, if an article used research and devel-
opment intensity (i.e., research and development expenditures divided by sales) as a proxy
for a strategically valuable resource, we coded such measures as measures without high
content validity because such measures do not appear to DEMO meet the standards for being
‘‘strategically valuable’’ (Rouse & Daellenbach, 1999). Overall, the results suggest that
articles contain more highly valid than not highly valid measures per study (i.e., 8.46 to
Downloaded from DEMO://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
198 Organizational Research Methods
1.73 for early and 9.92 to .84 for DEMO studies). Indeed, the ratio increased from 4.89 to
11.79, confirming that authors have substantially improved their use of valid measures over
time (p < .01). More broadly, we found that 89% of the measures contained content validity,
whereas Chandler and Lyon (2001, p. DEMO) found that ‘‘91% of the empirical studies they
reviewed had reasonable DEMO validity for the major constructs.’’ We also coded whether
the authors justified the use of each measure. As an example, if an author stated that they
included a measure of firm size because it can affect DEMO performance, then we coded that
the author justified their measure. If DEMO author said we controlled for size, but did not offer a
DEMO, we coded that the author did not justify their measure. The DEMO suggest that
authors took more care in justifying their measures in the later period (10.4 vs. 9.1 for
early); the ratio of the number of measures that were justified to measures that were not
justified DEMO from 4.33 to 8.67 (p < .01).
Given that a DEMO correlation matrix is essential to understanding the relationships among
variables, we DEMO whether authors reported a full correlation matrix. We found that 180
(DEMO) studies reported a matrix but that only 131 (54%) studies DEMO a full matrix.
Although there was an increase in the number of studies that reported matrices in the later
period (108 [PUI: .78] DEMO 72 [PUI: .70] for early—p < .01), there was not DEMO significant dif-
ference in the number of studies that reported full matrices.
As indirect indicators of substantive and external validity, we coded the number of inde-
pendent-to-dependent variables in each study that were statistically related (Chandler &
Lyon, 2001). We found 664 and 701 statistically related DEMO vari-
ables for the early and later periods, respectively; the ratios of statistically related to unre-
lated variables for the periods were .87 DEMO 1.05, a significant increase over time (p < .01).
In addition to the assessments that Chandler and Lyon (2001) conducted, we also examined
some other indicators of validity. We assessed whether articles assessed DEMO bias.
Fortunately, most survey studies assessed such bias. Overall, 83 studies assessed nonre-
sponse bias, and these assessments did not change over time as the PUIs for the early and
later periods were .79 and DEMO, respectively. As a means to examine generalizability and
external validity, we also assessed the country from which the sample was drawn. We found
DEMO 143 (59% ) used samples from the United States, 87 (DEMO) were samples drawn from
other countries, and 12 (5%) were drawn from both United States and non-U.S. countries.
There were no significant DEMO between the early and the later periods.
Level of Analysis
Like Chandler and Lyon (2001), we also assessed which level of analysis was investi-
gated in each study. Encouragingly, however, we found that authors DEMO clear about spe-
cifying the level of analysis. For both the early and the later periods, the majority of the
studies were conducted at the individual levels (31 [PUI: .30] for early and 32 [PUI: .23]
for later) and firm levels (60 [PUI: .59] for early and 77 [PUI: .55] for later). There were
a limited number of studies conducted at the group level, but no significant change over
time. These numbers were roughly the same as those in Chandler and DEMO (2001), who
found that 35% and 53% of their studies DEMO individual levels and firm levels of
analysis. In addition, we found DEMO increase in the number of ‘‘other’’ studies (9 vs. 24 with
DEMO of .09 and .17) that included cross-level research (i.e., 4 DEMO 13) and levels other than
Downloaded from http://orm.sagepub.com at DEMO OF WATERLOO on December 3, 2009
Crook et al. / Research Design and Construct Measurement 199
individual, firm, and groups (e.g., patents). This suggests that most entrepreneurship
researchers continue to focus on the same levels of analysis, however, that DEMO have also
begun to conduct more cross-level research.
Analytical Techniques and Model Sophistication
Dean et al. (2007) recently assessed analytical techniques and found DEMO entrepreneur-
ship research relies on the use of increasingly sophisticated data analytical techniques.
Because of the recent assessment by Dean et al., we did not replicate Chandler and Lyon’s
(2001) analysis with regard to specific DEMO analytical techniques, but we did assess model
sophistication in two distinct DEMO First, we coded whether the study tested moderation or
mediation effects. DEMO found that a total of 55 (23%) studies tested moderation and 17 (7%)
tested mediation. There was a significant increase in the number of studies testing modera-
tion between the early and the later DEMO (PUIs of .15 vs. .29, p < .01). The number of
studies in the later period is also substantially larger than what DEMO and Lyon
(2001) found. They reported that 18% of the ‘‘empirical studies [they] reviewed presented
and tested contingency frameworks’’ (p. 108). Second, we coded for the number of control
variables per study. The results indicate that researchers used significantly more control
variables per study in the DEMO period (4.8 vs. 2.7, p < .01). Overall, these DEMO are
encouraging as they indicate that the field is applying increasingly sophisticated models
while controlling for potentially important variables that might affect outcomes.
Time DEMO: Cross Sectional Versus Longitudinal Studies
Finally, we assessed the time dimension of the studies under investigation. We found that
the number of cross-sectional DEMO remained relatively constant across time (66 and 85
with PUIs of DEMO vs. .61, ns), but that the number of longitudinal studies DEMO (19 and
54 with PUIs of .18 vs. .39, p < .01). Of course, the increase in longitudinal studies might be
an artifact because more studies in the early period did not clearly specify DEMO the data
were cross sectional or longitudinal. When these results are juxtaposed with Chandler and
Lyon’s (2001), who found that only 7% of studies used longitudinal designs, it is clear that
more researchers are using longitudinal designs.
Study Two: Survey of Entrepreneurship Researchers
To complement the content analysis findings, we surveyed expert entrepreneurship
researchers to get a sense of their opinions about various practices within the field. We con-
sidered researchers DEMO editorial responsibilities (i.e., editors, associate editors, and board
members) DEMO JBV or ETP as experts in the field (Dean et al., 2007). Twenty-two researchers
had editorial responsibilities at both journals. Overall, there were 147 researchers that we con-
sidered the ‘‘population’’ of experts, and we sent them personalized emails to solicit input.
We developed an online DEMO that assessed respondents’ opinions about the importance
of certain research design practices (e.g., reliability assessment) as well as their opinion
about whether the field has ‘‘closed the distance’’ between what researchers should be doing
Downloaded DEMO http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
200 Organizational Research Methods
versus what they are doing in published high-quality DEMO In particular, we asked two
questions about each practice. The initial DEMO asked how much authors needed to focus
on various research design and construct measurement practices in their research. All the
questions began with ‘‘to DEMO extent do you agree that entrepreneurship researchers need
to focus on ... .’’ This question established the perceived importance of the practices. The
follow-on DEMO asked how much an expert agreed that entrepreneurship researchers
have focused on these practices over the last decade within published research. These ques-
tions DEMO worded slightly differently to assess the extent to which an expert perceived gaps
between what authors should be doing about each practice and what DEMO are currently
doing about each practice.
Fifty-one researchers—who have collectively co-authored over 740 A-caliber publica-
tions—provided input for an effective response rate of 35%. DEMO the respondents were
anonymous, we were unable to assess nonresponse bias DEMO Instead, because research has
shown nonrespondents to be similar to late DEMO (Armstrong & Overton, 1977), we
examined nonresponse bias by assessing differences in (a) rank and (b) years in the profession
DEMO early and late respondents. We did not find any differences, and DEMO, have no reason
to believe nonresponse bias is present in our DEMO, which are reported in Table 2.
Results
We conducted mean difference DEMO to investigate differences between the need for
improvement and the actual extent of improvement. Overall, the results of our survey indi-
cate that a significant gap still exists between what entrepreneurship researchers are doing
and what DEMO need to be doing. With respect to reliability, the evidence suggests DEMO
researchers still need to increase focus on assessing reliability. In addition, DEMO experts
revealed that researchers need to do more in terms of both discussing (p < .01) and exam-
ining validity (p < .01). Regarding external validity, there remains room for improvement,
particularly when determining whether nonresponse bias might have been present in the
sample.3 Finally, we asked respondents questions in two broad areas: fit between research
design and methods and measures used as well as whether the field has DEMO in terms of
more sophisticated modeling. We found that although there is still a sense that ‘‘fit’’ can be
improved between research design and DEMO methods and measures used, researchers have
substantially improved in terms of DEMO sophisticated modeling.
Exploratory Analyses
We were curious whether there were differences between journal sets, so we compared
the dedicated entrepreneurship journals (i.e., ETP and JBV) to the mainstream manage-
ment journals. Our results indicate significant differences (i.e., p < .05) between journals
sets in just a few areas. We found significantly stronger practices for the mainstream man-
agement DEMO: (a) for the ratio of studies that contained complete correlation DEMO
(.64 vs. .53), (b) for the ratio of highly DEMO to not highly valid measures (16.28 vs. 6.58),
(c) for the ratio of studies with scale measures with all reported reliabilities DEMO .7
(2.17 vs. 1.92), and (d) for reporting explicit DEMO on tests of nonresponse bias
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
Crook et al. / Research Design and Construct Measurement 201
Table 2
DEMO Opinions Regarding Different Design Elements in Published Research
Research Design Element Level of Importance Assessment of Current State Difference Significance
Need for methodological rigor DEMO 3.85 .47
Focus on construct measurement 4.13 3.36 .77
Focus on reliability 4.16 3.23 .92
Discuss validity 4.34 3.04 1.30
Examine validity 4.30 2.89 DEMO
Assess nonresponse bias 4.09 3.02 1.07
Assess common method bias 4.02 2.87 1.15
Move beyond simple modeling 4.06 3.70 .36
Fit between design and DEMO 4.85 3.13 1.72
p <.01
p <.01
p <.01
p <.01
p <.01
p <.01
p <.01
p <.01
p <.01
(5.25 vs. .86).4 However, the dedicated entrepreneurship journals used DEMO practices
when justifying measures; the ratio of justified to not justified DEMO was 14.07 versus
3.86. In addition, the entrepreneurship journals used more DEMO modeling; the
ratio of studies testing for moderation and mediation was DEMO versus .22. We also exam-
ined, but did not find, differences between journals regarding whether they reported all
reliabilities.
Discussion and Recommendations
The DEMO of our two studies should be viewed in light of their limitations. In the first
study, we examined 6 years of a specific set of journals. However, this sample appears
appropriate as the set of journals is widely recognized as the stronger set of journals for
entrepreneurship research (Chandler & Lyon, 2001; Dean et al., 2007). Had we examined
a different set of journals or studies published in different years, our findings might have
been different. Indeed, given that we assessed high quality journals, it is quite likely
research design and construct measurement practices in lesser quality journals would not
reflect the same progress. A second DEMO is that our survey study assessed gaps, not
how close the DEMO is to optimal construct measurement. Had we used the latter approach,
the results might have revealed that we are further than we think. DEMO, when view-
ing the results of the content analysis and of DEMO survey, it appears that the findings of the
two studies paint DEMO consistent picture.
Entrepreneurship is a relatively young field. During its youth, DEMO field has experienced
impressive growth. Broadly speaking, the evidence we collected DEMO 238 articles involv-
ing 242 samples and from 51 expert entrepreneurship researchers suggests that, alongside
this growth, the field has made significant strides DEMO terms of methodological rigor and
maturity. Yet, the evidence also suggests DEMO progression in research design is needed,
especially given that new studies typically build on earlier work (Kuhn, 1962). To the extent
DEMO a study is not properly designed and when constructs are not properly measured, not
only is confidence in the findings of that study limited but also the field’s ability to build
on it in future studies.
DEMO from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, DEMO
202 Organizational Research Methods
Construct Measurement
Reliability. In general, the results suggest that the field has improved but still needs to
progress in terms DEMO some of its construct measurement practices. Reliability is critical to
advancing entrepreneurship because the use of unreliable measures across related studies
has been shown DEMO lead to conflicting findings because of measurement error not substantive
issues (DEMO, Gove, & Hitt, 2005b). Our results related to reliability DEMO somewhat mixed.
However, the results from our survey indicate that entrepreneurship DEMO have
increased their focus on reliability in recent years, but that DEMO progress is needed. Simi-
larly, the results from our content analysis DEMO that progress has been made, especially
related to internal reliability; the ratio of studies that use scales with all reported reliabilities
above .7 DEMO increasing. However, regarding inter-respondent reliability, very few studies
reported such statistics in either time period, and no differences existed between early and
later periods. Although inter-respondent reliability does not appear to be a large concern, we
think more effort should be directed toward collecting data from multiple DEMO
Validity. The survey results related to discussing and examining validity indicate that
researchers have improved but can still advance in both of these areas. DEMO findings are
broadly consistent with our content analysis findings. Regarding content validity, our
results suggest that authors used highly (content) valid measures the vast majority of the
time in both periods; however, the ratio DEMO measures possessing highly content valid to mea-
sures lacking high content validity was significantly higher for the later period. Our proxy
for discussing validity DEMO whether authors justified the use of a particular measure. Like
the survey results, the content analysis results suggest that significant strides have been
made over the past few years. This progress is encouraging, but still, DEMO the measures used
neither had high content validity nor were justified in every study. This suggests that
although the field is getting better, there remains some room for improvement.
Our content analysis results also strived to DEMO whether substantive validity could be
assessed by anyone interested in an article’s findings. There were no differences in the ratio
of statistically related independent-to-dependent DEMO or the amount of complete cor-
relation matrices furnished over time. Regarding the latter, we consider any number under
100% less than desirable.
Beyond Chandler and Lyon (2001), we strived to assess the external validity of studies’
findings via assessing data sources and the country or countries DEMO which the sample data
were drawn. First, our assessment of data DEMO revealed that, overall, fewer studies currently
rely on primary data (DEMO, survey), whereas more studies rely on secondary data (e.g., DEMO
database). There was no significant change in the number of studies that blended the two
approaches (i.e., mixed primary and secondary data)DEMO Given that combining different
approaches ‘‘triangulates’’ findings and increases confidence that results are valid and not sim-
ply methodological artifacts (Jick, 1979), DEMO research that moves beyond relying
solely on one data source seems like a fruitful endeavor. Second, our results indicate that, for
both periods, the vast majority of entrepreneurship studies draw on single country samples
(>DEMO) and the bulk of these are from the United States (>DEMO). This suggests that more
cross-country samples are needed to improve the external validity of findings within the field.
Downloaded from http://orm.sagepub.com DEMO UNIV OF WATERLOO on December 3, 2009
Crook et al. / Research Design and Construct Measurement 203
Table 3
DEMO for Improving Research Design and Construct Measurement
Researchers (authors)
(1) When appropriate, assess, report, and discuss the reliability of all DEMO
(2) Discuss validity and why all measures were used for a construct
(3) When available, use additional data sources to derive multiple indicators of constructs and reduce common
methods bias
(4) When possible DEMO appropriate for the theory, use data analytic techniques that allow assessment DEMO mea-
surement error
(5) Thoroughly consider aspects of research design, DEMO, and measures prior to the consummation of the
study; perhaps have others examine these aspects beforehand
Reviewers and editors
(1) Require that DEMO complete correlation matrix be reported
(2) When appropriate, require that DEMO measure of reliability be reported
(3) Set a higher bar for publication regarding construct measurement
Senior scholars
(1) Lead by example by DEMO construct measurement practices in their own research
(2) Delve deeply into construct measurement issues in doctoral entrepreneurship curricula
(3) Offer domain-specific methods DEMO to junior scholars
Model Sophistication
The results from both our survey and content analysis provide some evidence that entrepre-
neurship researchers are using more DEMO modeling. Although such modeling is being
used, there is still a DEMO low number of studies that investigate potential moderation or
mediation effects. Furthermore, the survey of experts noted that testing of more complex mod-
els is still needed. The results also show a noticeable increase in the DEMO of longitudinal
designs (compared to cross-sectional) in the later period. This is a positive sign, given that
longitudinal designs are better equipped to assess causality. Finally, the results indicate that
the two most common levels of analysis are individual levels and firm levels but very few stud-
DEMO cross levels. With the advent of more advanced theory and modeling techniques (e.g., hier-
archical linear modeling—Dean et al., 2007), more work appears needed to assess how
phenomenon at one level of analysis (e.g., industry) are related to outcomes at others (e.g., stra-
tegic DEMO or firms—Short, Ketchen Jr., Palmer, & Hult, 2007). Still, the promise associated
with advanced theory and modeling techniques can be realized only if the underlying design
and measures are solid. Overall, although model sophistication has improved, several impor-
tant and interesting research questions can be answeredifmoreprogressismadeinthisarea.
Moving Forward
Careful research design and construct measurement are central to DEMO much confidence
and faith we can have in research results. Overall, DEMO two studies taken together show that
the field still has progressed but still has some distance to go in terms of optimal research
design DEMO construct measurement practices. Although the field and its stakeholders are best
served by optimal practices, there are still studies being published where there appears to be
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on DEMO 3, 2009
204 Organizational Research Methods
significant room for improvement. Thus, efforts to improve will be required from the field’s
key stakeholders. Accordingly, we summarize our suggestions for authors, reviewers, and
editors, as well as senior scholars in Table 3.
Perhaps the largest cause for the lack of focus DEMO research design and construct measure-
ment is that new entrepreneurship scholars lack confidence in their ability to use some
methods associated with reliability assessment (e.g., confirmatory factor analysis—Dean
et al., 2007). Whether it is DEMO lack of confidence or competence with statistical analysis or inad-
equate consideration of reliability in study design, an appropriate remedy may be the creation
of domain-specific training for budding entrepreneurship scholars (Ireland, Webb, &
DEMO, 2005). Perhaps more than any other field, entrepreneurship researchers deal with
more diverse phenomenon and samples. To surmount these challenges, we suggest that doc-
toral entrepreneurship curricula delve more deeply into issues related to DEMO design and
construct measurement in the domain of entrepreneurship. This ‘‘remedy’’ should help entre-
preneurship researchers alleviate some long-held concerns. In our view, this progress rests on
authors, editors, reviewers, and especially, senior scholars DEMO the field.
The largest gap identified by the experts was between the importance of having adequate fit
between the research design and methods and DEMO used and the extent to which authors
have had adequate fit among the research design and methods and measures. Future research
should examine the DEMO cause of this inadequate fit among research design and methods and
measures so that it may be addressed. In the meantime, the field’s stakeholders should take steps
to continue to improve research design and construct measurement DEMO benchmarking studies,
such as the ones we conducted, should be DEMO periodically to assess the field’s progress.
Conclusion
Our overarching goal in this study was to answer to the question ‘‘are we there yet?’’ DEMO
assessment of current research design and construct measurement practices within entrepre-
neurship reveals that, in a word, the answer is ‘‘no’’—but we are DEMO lot closer than we were
just a few years back. Indeed, DEMO have seen significant improvement since Chandler and
Lyon’s (2001) assessment as well as between the early and the later periods we assessed.
Thus, our confidence in the findings and implications from extant research is good DEMO
improving but is not yet what it could be.5 To this end, we suggest that authors pay more
attention to research design and construct measurement practices in their studies and that
more domain-specific training for budding DEMO scholars be conducted. We
also suggest that editors and reviewers avoid and discourage others from using less-than-
optimal practices. Doing so will more quickly DEMO knowledge and the field.
Notes
1. We coded for each of Chandler and Lyon’s practices, except for those related to statistical analysis. Dean
et al. (2007) recently published a comprehensive review of these practices. However, our coding also allowed us
to look at some practices using measures DEMO Chandler and Lyon (2001). For example, we included the num-
ber of full correlation matrices furnished in studies. Where possible, we also compared our results to Chandler
and Lyon’s. We thank an anonymous reviewer DEMO this recommendation.
2. We use the term content validity instead of face validity, the term used by Chandler and Lyon (2001).
Downloaded DEMO http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009
Crook et al. / Research Design and Construct Measurement 205
3. As DEMO anonymous reviewer insightfully pointed out, assessing nonresponse bias does not fully DEMO
whether findings possess external validity and thus generalize to other contexts. Sample selection is a key con-
cern; if samples are not randomly drawn from a clearly defined sampling frame, assessments of nonresponse
bias will not yield whether findings possess external validity.
4. These numbers indicate that, for management journals, 5.25 studies reported nonresponse bias for every
one study that did not. The .86 figure means that .86 studies reported nonresponse bias DEMO every study that did
not. This suggests that research published in mainstream management journals assess such bias much more fre-
quently than dedicated entrepreneurship DEMO
5. As one of our reviewers insightfully pointed out, although we DEMO always strive to use best practices,
we doubt that any field, whether in the organizational sciences or in other domains of study, DEMO achieved opti-
mal research design and construct management practices.
6. For the PUIs, the results are based on z tests. For the reported ratios, the results are based on means dif-
ferences tests. We report significant p values or ns for not significant.
References
Armstrong, J. S., & Overton, T. S. (1977). Estimating nonresponse bias in mail surveys. Journal of Marketing
Research, 14, 396-402.
Barney, J. B. (1991)DEMO Firm resources and sustained competitive advantage. Journal of Management, 17, 99-120.
Boyd, B. K., Gove, S., & Hitt, M. A. (DEMO). Construct measurement in strategic management research: Illu-
sion or reality? Strategic Management Journal, 36, 239-257.
Boyd, B. K., Gove, S., & Hitt, M. A. (2005b). Consequences of measurement problems in strategic management
research: The case of Ahimud and Lev. Strategic Management Journal, 36, 367-375.
Busenitz, L. W., West, G. P. III, DEMO, D., Nelson, T., Chandler, G. N., & Zacharakis, DEMO (2003). Entrepre-
neurship research in emergence: Past trends and future directions. Journal of Management, 29, 285-308.
Chandler, G. N., & DEMO, D. W. (2001). Issues of research design and construct measurement in entrepreneur-
ship research: The past decade. Entrepreneurship Theory and Practice, DEMO, 101-113.
Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for
the behavioral sciences (3rd ed.). Mahwah, NJ: Erlbaum.
Dean, DEMO A., Shook, C. L., & Payne, G. T. (2007)DEMO The past, present, and future of entrepreneurship research:
Data analytic trends and training. Entrepreneurship Theory and Practice, 31, 601-618.
Holden, R. R., & Jackson, D. N. (1979). Item subtlety and face validity in personality assessment. Journal of
Consulting and Clinical Psychology, 47, DEMO
Ireland, R. D., Reutzel, C. R., & Webb, J. DEMO (2005). Entrepreneurship research in AMJ: What has been pub-
lished, and what might the future hold? Academy of Management Journal, 48, 556-564.
Ireland, R. D., Webb, J. W., & Coombs, DEMO E. (2005). Theory and methodology in entrepreneurship research.
In D. DEMO Ketchen, & D. D. Bergh (Eds.). Research methodology in strategy and management (Vol. II,
pp. 111-141). San Diego, CA: Elsevier.
Jick, T. (1979). Mixing qualitative and quantitative methods: Triangulation in action. Administrative Science
Quarterly, 24, 602-611.
Kerlinger, F. N., & Lee, H. B. (2000). Foundations of behavioral research (4th DEMO). Fort Worth, TX: Harcourt
College Publishers.
Kuhn, T. (1962). The structure of scientific revolutions. Chicago: University of Chicago Press.
Loevinger, J. (1957). Objective tests as instruments of psychological theory. Psychological DEMO, 3, 635-694.
Low, M. B. (2001). The adolescence of entrepreneurship research: Specification of purpose. Entrepreneurship
Theory and Practice, 25, 17-25.
Messick, S. (1988). Validity. In R. L. Linn (Ed.). Educational measurement (3rd ed.). New York: Macmillan.
Nunnally, J. (DEMO). Psychometric theory. New York: McGraw Hill.
Rouse, M., & DEMO, U. (1999). Rethinking research methods for the resource-based perspective: DEMO
ing the sources of sustainable competitive advantage. Strategic Management Journal, 20, 487-494.
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December DEMO, 2009
206 Organizational Research Methods
Schriesheim, C. A., Cogliser, C. C., DEMO, T. A., Lankau, M. J., & Powers, K. J. (1999). An empirical com-
parison of approaches for quantitatively assessing the DEMO adequacy of paper-and-pencil measurement
instruments. Organizational Research Methods, 2, 140-156.
Schwab, D. (1999). Research methods for organizational studies. Mahwah, NJ: DEMO Erlbaum Associates.
Shane, S., & Venkataraman, S. (2000). The promise of entrepreneurship as a field of research. Academy of Man-
agement DEMO, 26, 13-17.
Short, J., Ketchen, D. J. Jr., Palmer, T., & Hult, G. T. (2007). Firm, strategic group, and industry influences on
performance. Strategic Management Journal, 28, 147-167.
Stone-Romero, E., Weaver, A., & Glenar, J. (1995). Trends DEMO research design and data analytic strategies in
organizational research. Journal of Management, 21, 141-157.
Summer, C. E., Bettis, R. A., Duhaime, I. H., Grant, J. H., Hambrick, D. C., & Snow, C. C., et al. (1990).
Doctoral education in the field of business policy and strategy. Journal of Management, 16, 361-398.
T. DEMO Crook (PhD, Florida State University) is an assistant professor of DEMO at the University
of Tennessee. His research—which focuses on topics related to strategy, entrepreneurship, and research
methods—has appeared in other outlets such as DEMO Journal of Operations Management and Strategic Manage-
ment Journal. He is currently on the editorial board at the Journal of Management.
Christopher L. Shook (PhD, Louisiana State University) is an associate professor and director of DEMO Lowder
Center for Family Business and Entrepreneurship at Auburn University. He was a Fulbright Scholar at the Acad-
emy of Economic Studies in Bucharest, Romania. His research focuses on methodological issues in strategy and
entrepreneurship research, strategic decision making processes, and venture creation.
M. Lane Morris (PhD, University of Tennessee) is an associate professor and program director of the Human
Resource Management MS Degree Program at the University of Tennessee. He DEMO the president of the Academy
of Human Resource Development and his research interests include: entrepreneurship, occupational stress/
health/wellness, work/life balance, human capital metrics, and individual and organization development.
Timothy M. Madden DEMO a doctoral student in organizations and strategy at the University of Tennessee in Knox-
ville. His current research interest areas are business ethics, corporate social responsibility, entrepreneurship,
and management education.
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009{1g42fwefx}