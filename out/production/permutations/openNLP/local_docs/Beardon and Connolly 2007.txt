Organizational Behavior and Human Decision Processes 103 (2007) 147–158
www.elsevier.com/locate/DEMO
Multi-attribute sequential search
J. Neil Bearden *, Terry Connolly
University of DEMO, Department of Management and Organizations, 405 McClelland Hall, Tucson, AZ 85721, USA
Received 3 February 2005
Available online 23 March 2007
Abstract
This article describes empirical and theoretical results from two multi-attribute sequential search DEMO In both tasks, the DM
sequentially encounters options described by two DEMO and must pay to learn the values of the attributes. In the continuous
version of the task the DM learns the precise numerical value DEMO an attribute when she pays to view it. In the threshold version
the DM learns only whether the value of an attribute is above DEMO below a threshold that she sets herself. Results from the continuous
condition reveal that DMs tended to terminate their searches too early relative to DEMO optimal policy. The pattern reversed in the
threshold condition: DMs searched DEMO too long. Maximum likelihood comparisons of two diﬀerent stochastic decision models
showed that DMs under both information conditions performed in ways consistent with the DEMO policies. Those oﬀered contin-
uous-valued attribute information did not, however, spontaneously degrade this information into binary (acceptable/unacceptable)
form, despite the DEMO ﬁnding that satisﬁcing can be a very eﬀective and eﬃcient search strategy.
 2006 Elsevier Inc. All rights reserved.
Keywords: Optimal stopping; Sequential DEMO; Satisﬁcing
Decision makers (DMs) must frequently choose
among options that DEMO encounter sequentially, and
whose values are initially revealed only imperfectly.
The DEMO may, at some cost of time, eﬀort or money,
reduce her uncertainty about the value of an encoun-
tered option (e.g., DEMO paying for expert advice, or by
simply spending more time evaluating DEMO option).
Alternatively, she may go on to the next option. DEMO,
she must continuously decide when to stop searching
within an option—to get a better estimate of its
value—and when to stop searching between DEMO
to ﬁnd one of high value. Striking a balance between
depth (DEMO) and breadth (between-option)
search presents a complex problem. Should one inter-
view the current applicant in more detail, or go on to
the next candidate? Should one pay for a detailed
inspection of a house that one is interested in or go on
* Corresponding author. DEMO: +1 520 325 4171.
E-mail address: jneilb@gmail.com (J.N. Bearden).
DEMO/$ - see front matter  2006 Elsevier Inc. All rights reserved.
doi:10.1016/j.obhdp.2006.10.006
to the next possibility? Miller and Todd (1998) present
an example in the context of mate selection. Depending
on the DEMO features of the search problem (e.g., how
many options are available, how costly search is, etc.),
the economically rational search policy for this type of
problem may be cognitively very demanding.
Herbert Simon (1955) proposed a behavioral model
of sequential search for problems of this sort, introduc-
ing his now-famous notion of ‘‘satisﬁcing’’. Simon pro-
posed that real DMs have cognitive limitations that
compel them to pursue only ‘‘bounded’’, not global
rationality. They do not attempt to maximize the expect-
ed DEMO of the options they select. Instead, they search
for options that DEMO good enough or that satisﬁce. Sup-
pose an option X is represented by a vector of k attri-
butes X=(x1, ... , xk ) and that the value of the ith
attribute is xi . DEMO Simon’s formulation of satisﬁcing,
the DM sets an aspiration level or cut-oﬀ value, bi, for
each attribute. Then, when the options are encountered
sequentially, the DM selects the ﬁrst X for which xi Pbi
for all i , i.e., the ﬁrst option that satisﬁces. Simon
148
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
postulated that the aspiration levels might be dynamic,DEMO
being aﬀected by the DM’s experience in the search pro-
cess and by the length of the search horizon (the number
of options available for the DM to search). Hence, the
aspiration levels can be denoted in a more general form
by bin, where n denotes the position of the option in the
sequence. For example, a house seller may be prepared
to accept a lower price for his old house DEMO the date
on which he must begin paying the mortgage on a new
house gets closer.
Satisﬁcing is often discussed as if it represented DEMO sin-
gle decision strategy. It does not. Satisﬁcing refers to a
class of decision strategies, whose performance is criti-
cally shaped by the aspiration levels the DM sets for
each attribute. If they are all set DEMO low, poor options
encountered early in the sequence are likely to DEMO selected.
If they are all set very high, excessive search costs
DEMO be incurred as the DM sifts through a long, perhaps
endless, list of options. Relative importance of diﬀerent
attributes can be reﬂected in DEMO relative severity or lax-
ness of the cut-oﬀs; and, as noted, cut-oﬀs can be adjusted
over time as experience accumulates and option (DEMO
DM) exhaustion threatens. For a given problem and
preference function, then, some satisﬁcing strategies
are better than others. (See also Baumol, 2004.) We have
used the term ‘‘optimal satisﬁcing’’ to refer to the satis-
ﬁcing strategy that oﬀers the highest expected payoﬀ for
a given task DEMO preference function. Lim, Bearden, and
Smith (2006) present structural proofs for optimal poli-
cies for the class of multi-attribute optional stopping
problems DEMO here. Bearden and Connolly
(2006) describe numerical methods for computing opti-
mal search policies for satisﬁcing search policies.
Satisﬁcing is not the only DEMO that allows DMs to
achieve some measure of success in complex decision
problems despite limited computational abilities. The
best-developed program of research on the DEMO
forms of bounded rationality is that associated with
Gerd Gigerenzer and his colleagues under the rubric of
‘‘fast and frugal heuristics’’ (Gigerenzer, 2004; Gigeren-
zer & Todd, 1999; Goldstein & Gigerenzer, 2002;
Hertwig & Todd, 2003; Todd, 2001). This program grew
from a rejection of the ‘‘heuristics and biases’’ research
program associated with Daniel Kahneman DEMO Amos
Tversky (Kahneman, Slovic, & Tversky, 1982). Gigeren-
zer and colleagues reject that program’s reliance on opti-
mal models, preferring instead to start by specifying
what they judge to be psychologically plausible sets DEMO
simple rules by which real humans might approach com-
plex problems and then testing these rules as models of
actual task performance. A variety DEMO such models has
now been proposed and tested, yielding important
insights DEMO the behavior of DMs in complex settings.
The advantages and disadvantages of the two
research strategies are complex and hotly debated (see,
for example, Gigerenzer, 2004) and we will not attempt
a summary here. Our own view is that the two
approaches oﬀer complementary advantages, and both
have a role to play. In the present paper we ﬁrst DEMO
previous research on sequential search and present a
speciﬁcation of a new and challenging task structure of
substantial real-world signiﬁcance: multi-attribute
optional stopping problems. In multi-attribute optional
stopping problems, DMs must choose among multi-at-
tribute alternatives that are encountered sequentially.
The problems capture features of many practical prob-
DEMO such as hiring decisions, technology adoption deci-
sions, and purchasing decisions. In all cases, the
alternatives can diﬀer from one another along a number
of attributes (e.g., in the hiring case, applicants can diﬀer
in work experience, technical skills, and various assess-
ment measures), DEMO the DM must consider trade-oﬀs
among these attributes when making a selection
decision.
After formally describing the general problem, we
present optimal (expected DEMO maximizing) strategies
for two diﬀerent versions, one in which numerical attri-
bute values are available, the other in which the DM
learns only whether or not each attribute exceeds a spec-
iﬁed cut-oﬀ value. We DEMO describe an experiment
examining search behavior in the new problems and
present the results. The ﬁnal section outlines directions
for future research and discusses DEMO our ﬁndings speak
to the notions of satisﬁcing put forward by Simon.
Previous research on sequential search
Optional stopping problems
In a standard optional DEMO task, a DM sees a
series of single-valued options one at DEMO time and must
simply decide when to stop the search and accept an
option. In most versions of the problem there is a ﬁxed
DEMO cost c for viewing the value of each option. The
payoﬀ to the DM for selecting the nth option, whose
value we denote xn, is typically given by a function
of the form f(x1, DEMO , xn)  nc. When the DM cannot
return to previously DEMO options, the search
problem is said to be with no recall DEMO
f(x1, ... , xn)= xn; the payoﬀ is the value of the last
option examined, net of search costs. When recall is
allowed f(x1, ... , xn) = max{x1, ... , xn}; the payoﬀ is
the value of the best of the options examined, net of
search costs. Optimal policies have been derived for a
variety of these problems (e.g., Chow, Robbins, &
Siegmund, DEMO).
These problems have also been studied experimentally
(e.g., Cox & Oaxaca, 1989; Hey, 1981, 1982, 1987;
Kogut, 1990; Rapoport & Tversky, 1966, 1970). Varia-
tions on the standard DEMO have included making the dis-
tribution of xn unknown to the DM (e.g, Cox & Oaxaca,
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
1989) and making the distribution of xn depend on n—
i.e., making the distribution non-stationary, as in rising
or falling DEMO (Brickman, 1972; Shapira & Vene-
zia, 1981). In many instances, the optimal search model
has provided a reasonably good account of the behav-
ioral results (see, e.g., Cox & Oaxaca, 1989; Rapoport
& Tversky, 1970), though there is general a tendency
for the DMs to terminate their searches too early.
All of these experimental DEMO have looked at
optional stopping when the options are described by a
single value—either the actual value of the option or
the rank of DEMO option on this single value. (This latter
case corresponds to the DEMO Secretary Problem,
see, e.g., Bearden, Murphy, & Rapoport, DEMO; Bearden,
Rapoport, & Murphy, 2006; Corbin, Olson, & Abbon-
danza, 1975; Dudey & Todd, 2002; Seale & Rapoport,DEMO
1997, 2000; Zwick, Rapoport, Lo, & Muthukrishnan,
2003.) More generally, however, in natural (‘‘real-
world’’) problems, the options through which a DM
searches are composed of several values or attributes,DEMO
some of which may be known to the DM and some of
which may be unknown. The unknown attributes are
often costly to observe. DEMO studies of infor-
mation-purchase tasks (e.g., Connolly, 1988; Connolly
& Gilani, 1982; Connolly & Wholey, 1988; Edwards,
1965) indicate that striking a balance between the cost
of information and its decisional DEMO is often quite dif-
ﬁcult, even in relatively simple versions of DEMO tasks. A
related literature examines speciﬁcally the rules that
DM’s use to terminate search (e.g., Saad & Russo,
1996). Browne and DEMO (2004) distinguish stopping
rules that apply to the design (option DEMO) and
choice (option selection) phases of decision making.
Newell, Weston, and Shanks (2003) found that the ‘‘take
the best’’ (TTB) heuristic proposed by Gigerenzer and
Goldstein (1996) was frequently violated in DEMO sequential
depth-of-search task, primarily by participants contin-
uing to purchase attribute DEMO after the TTB
stopping rule would have required them to stop.
In general the experimental evidence suggests that
both depth-of-search and breadth-of-search problems
are DEMO quite demanding. Combining the two,
as in the optional stopping tasks considered here, is thus
likely to present the DM with signiﬁcant diﬃculties. In
the problems described in the following section we will
therefore restrict DEMO to options described by just
two attributes.
Two new multi-attribute sequential search problems
Overview
In this section, we describe two new search problems
in which the DM sequentially encounters options com-
posed of two attributes and DEMO pay a ﬁxed cost to
149
learn about the values of the attributes. The two prob-
lems diﬀer only in terms of the information DEMO to
the DM about the values of the attributes. In the ﬁrst
problem the DM learns the actual numerical value of
the attribute; in the second, she only learns whether or
not the value of the attribute is above a threshold which
she determines herself.
The multi-attribute sequential DEMO problem (MASSP)
Let Xn ¼ðx1n; denote the nth option (DEMO
where each xi is an i.i.d. random variable with known
xn
2
1; ... ; N)
Þ ¼
n
density f(x )DEMO The value of X is simply the sum of its
P
i
xn . To acquire infor-
i
attribute values, i.e., UðXn Þ¼
DEMO
mation about the value of xni the DM must pay a ﬁxed
cost cni ¼ c, which is constant for all i and n. The payoﬀ
to the DM for selecting the nth option is given DEMO:
n
pðXn Þ¼ UðXn Þ XX
2
j
bkc;
k 1
ð1Þ
¼1 ¼
j j
1if xk was viewed and bk
DEMO
where bk
¼ ¼ 0 otherwise—i.e.,
the total value of the option net of search costs. If the
DM reaches the Nth option, she must accept it.
We will consider two diﬀerent versions of this DEMO
lem, distinguished by the sort of information the DM
receives when DEMO purchases attribute information:
Continuous case: When the DM pays to DEMO the ith
value of the nth option, she learns the actual DEMO of
xn . Once she selects an option, her payoﬀ is DEMO by
i
Eq. (1).
Threshold case: When the DM pays to view the ith
value of the nth option, she learns only whether
Pbin , where bni is a threshold she sets for each DEMO
before purchasing. Once she selects an option, her payoﬀ
is given DEMO Eq. (1).
The threshold case thus constrains the DM to DEMO a
satisﬁcing strategy. The continuous case allows, but does
not compel, the use of such a strategy. The following
section describes optimal decision DEMO with and
xn
i
without this constraint; the empirical portion of DEMO
paper describes the behavior of actual experimental sub-
jects under the two conditions. For simplicity we will
consider only the case in which both DEMO are sam-
pled from the same distribution, so that the order DEMO
which attributes are purchased is immaterial.
Optimal policy: continuous case. For DEMO option n,
the optimal decision policy is given by a 3-tuple (a1n,
a2n, a3n): an upper and lower threshold for DEMO ﬁrst attri-
bute, and a total value threshold for the entire DEMO
( Lim et al., 2006; Smith, Lim, & Bearden, DEMO press).
The decision policy works as follows:
1. If xn n , (i.e. the ﬁrst attribute is above its upper
1 Pa1
threshold) then choose this option and terminate the
search.
j
150
2. If xn n , (i.e. the ﬁrst attribute is below its lower
1 < a2
threshold) then do not pay to view xn and proceed
2
to the next option.
n , (i.e. the ﬁrst attribute is between the
3. If < a1
upper and lower DEMO), then pay to view the val-
ue of xn and go to step 4.
2
4. If xn
1 þ
above the total DEMO threshold) then choose this
n xn
a2 6 1
xn n , (i.e. the total value of the option is
2 Pa3
option DEMO terminate search. Otherwise proceed to
the next option.
Thus, if the DEMO of xn is suﬃciently high, the DM
1
will select the DEMO option immediately (Step 1); likewise,
if the value is DEMO low, she will proceed to the
next option immediately (Step 2). In between these
extremes, she pays to learn the value of xn before decid-
2
ing to accept the option or move on DEMO the next one
(Step 3). Finally, if she does view xn , she either stops
2
her search or proceeds based on DEMO total value of the
option, i
of xn is known, the DM’s decision problem reduces toUðXn Þ¼ P
xn , (Step 4). In fact, once the value
i
2
the standard single-dimensional optional stopping
problem.
Optimal policy: threshold case. For each option n, the
n DEMO n
optimal decision policy is given by a 3-tuple ðb1; b2; b3Þ.
Here, b1 and b2 are (real-valued) thresholds for attri-
butes 1 and 2, respectively. The third policy element
b3 2 {0,1} dictates when the DM should shift from a
strategy appropriate for the DEMO part of the search
(b3 = 1) to an alternative strategy appropriate as the
DM approaches the end of the options (b3 = 0). The
decision policy works according to the following steps:
DEMO If xn n (i.e. ﬁrst attribute is above threshold) and
1 Pb1
bn3 ¼ 0 (late search), then choose option n and termi-
nate the search.1
bn3 ¼ 1 (early search), then pay to view
, and go to step 3; else do not pay to
2. If xn
1 Pb1n and
the value of xn
2
DEMO x2n , proceed to next option, and return to Step 1.
DEMO If xn
2
Pb2n (i.e., second attribute is above threshold)
then choose this option and terminate the search;
otherwise proceed to DEMO option.
That is, if she is nearing the end of the DEMO
(bn3 ¼ 0Þ, she selects an option immediately if the value
of the ﬁrst attribute exceeds her threshold. Earlier in
1 A reviewer DEMO out that the special rules for late search, which
dictate than DEMO option should be accepted if it passes only the ﬁrst
threshold, DEMO inspection of the second attribute, is not strictly a
satisﬁcing procedure. DEMO is perfectly correct, by the deﬁnition we
oﬀered earlier. Such an DEMO satisﬁes the ﬁrst cut-oﬀ, but its status on
the second is DEMO, and it is not economically advantageous to
determine it. However, unless confusion seems likely, we will continue
to use the term ‘‘satisﬁcing’’ to refer to problems in which attribute
information, if considered at all, DEMO considered in the form of binary
above/below threshold values rather than as continuous variables.
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
the search (bn3 ¼ 1Þ she views the second attribute if
the ﬁrst is above threshold, and selects the option
if the second is also good enough. If not she DEMO
the option and goes on to the next. The optimal DM
always proceeds to the next option when the value of
the ﬁrst attribute DEMO below her aspiration level.
Numerical methods for computing optimal policies
Using dynamic programming and numerical optimi-
zation methods, it is possible to calculate the numerical
values of ð nÞ
maximize the expected (net) earnings of DEMO search. A
proof that the optimal decision policy is a threshold pol-
icy of the type just described, as well as details of an
optimization procedure that can be used to obtain the
optimal thresholds, can be found in Lim et al. (2006).
In short, the DEMO policies must satisfy Bellman’s
Principle of Optimality. That is, at each DEMO of the
search process, the DM must take the action that DEMO
the highest expected payoﬀ assuming that she behaves
optimally thereafter. As an example, optimal policies
are shown in Tables 1 and 2 for the continuous and
threshold cases when a maximum of 10 options are pre-
DEMO (N = 10), both attribute values are sampled from
a DEMO distribution Uni[1,100], and with information
cost c = 5. (These are also the parameter values for the
experimental task reported below.)
DEMO the continuous case (Table 1) all three optimal
thresholds decrease as n increases. As the DM approaches
the horizon at which she will DEMO to take what she
gets (i.e. when n = N), DEMO becomes increasingly likely
to accept an option with small values of either attribute
and of their total. Interestingly, the probability (.37 in
this DEMO) of viewing the value of the second attri-
n n
and DEMO; b2;
n n
a2; a3
n
a1;
b3 at each stage that
Þ
bute, xn , given that the DM has arrived at option n,is
2
constant for all n < N. DEMO, the probability of
selecting the nth option given that one has DEMO it
increases with n. Finally, the DM’s expected payoﬀ
(v*) DEMO as she gets deeper into the options.
The optimal policy for the threshold case (Table 2)is
broadly similar but has some interesting diﬀerences.
The DM’s thresholds for both attributes decrease for
options 1–7 and she DEMO gets information on the
xn n . At option 8, however,DEMO
1 Pb1
value of xn whenever
2
her optimal strategy shifts, DEMO indicated by the shift
in b3. At this point she should no longer be willing
to pay for information on the value of the DEMO attri-
bute, but she should be more demanding on the ﬁrst
DEMO In contrast to the continuous case, the
DM’s information purchase probability—conditional
DEMO reaching option n—increases up to n = 8 and then
goes to zero as the optimal strategy changes. As for
the continuous case, however, her conditional stopping
probability increases in n, while her expected payoﬀ
DEMO
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
Table 1
Continuous case optimal solution properties for c DEMO
n a1 a3 a3 Prob(Buy Prob(Select v*(n)
|Period = n) |Period = n)
Period 1 89 53 122 .37 .29 122
2 89 52 121 .37 .29 122
3 88 52 DEMO .37 .30 121
4 87 51 120 .37 .31 120
5 86 49 118 .37 .32 120
6 84 48 116 .37 .34 118
DEMO 81 45 114 .37 .37 116
8 76 40 109 .37 .42 114
9 68 31 100 .37 .50 109
10——— 0 1 100
DEMO v*(n) is the expected payoﬀ for playing optimally from stage DEMO to
stage N.
Table 2
Threshold case optimal solution properties for c =5
n b1 b2 b3 Prob(Buy Prob(Select v*(n)
DEMO = n) |Period = n)
Period 1 57 40 1 DEMO .26 120
2 57 40 1 .43 .26 119
3 56 40 1 .44 .27 119
4 55 39 1 .45 .27 118
5 DEMO 38 1 .46 .28 117
6 53 37 1 .47 .30 116
7 51 36 1 .49 .32 114
8 57 — 0 .00 DEMO 112
9 50 — 0 .00 .51 107
10 — — — .00 1 100
Under optimal policies, expected earnings are virtually
identical for the two versions of the problem. For
most values of n, DMs in the threshold case are expected
to make around 98% of the DEMO earned by DMs in the
continuous case. As Simon (1955) intuited, the penalty
for satisﬁcing rather than optimizing using continuous-
variable attribute values may be quite small—as long
as the satisﬁcing is done optimally. We DEMO later in
the paper the sensitivity of these payoﬀs to ‘‘detuning’’
the optimal thresholds.
The optimal strategies presented in Tables 1 and 2
represent DEMO results of considerable analytical and com-
putational eﬀort. It seems highly unlikely that unaided
human decision makers will closely approximate these
optimal strategies. However DEMO were interested to see
whether or not actual DMs follow strategies that mirror
optimal strategies in their broad qualitative (i.e., struc-
tural) features and, if they do not, in what ways they dif-
fer DEMO optimality and how costly these deviations are.
To examine these questions, DEMO use the predictions of
the optimal model to derive the following hypotheses:
H1: Average earnings in the continuous case will be
greater than those in the threshold case.
H2: DMs will search deeper into the set of options in
the threshold case than in the continuous case.
DEMO
H3: Total incurred cost will be greater in the threshold
case DEMO in the continuous case.
H4: Information purchase will be greater in DEMO thresh-
old case than in the continuous case.
The following section describes an experiment testing
these four hypotheses.
Methods
Participants
Sixty University of Arizona DEMO (30 per
experimental condition) participated for both course
credit and cash payment.
Procedure
Participants arriving at the experiment site ﬁrst
signed consent forms, and were then shown to individual
lab rooms in which the experiment DEMO presented on a
computer. They were instructed that they would be act-
ing as buyers of bundles composed of two goods, Good
1 and Good 2. The bundles could not be decomposed
but had to be DEMO or declined intact. In each trial
the participant would be oﬀered bundles sequentially
up to a maximum of 10 bundles, and at each stage could
either accept the one currently oﬀered or decline it and
go DEMO to the next oﬀer. If she declined the ﬁrst nine oﬀers
in a trial, she would be required to accept the tenth bun-
dle oﬀered. Each bundle contained some quantity of
each good, and the participants were told that the value
to them of any given bundle was DEMO the sum of the
values of the two goods it contained. For each bundle
the buyer could learn, at a cost of c per good, the value
of one or both goods the bundle contained. Participants
in both conditions were told:
Before deciding whether to buy a DEMO bundle,
you can get information about the worth (or value)DEMO
of one good or both goods—it’s your choice, but
you must DEMO information on at least Good 1 before
making a decision2. The goods are valued in an arti-
ﬁcial currency called francs, where 1 franc = $.10.
Each determination costs you 5 francs, so it costs
you 10 francs to check out a bundle completely. In
each bundle the DEMO of each good will be
2 This constraint was added after some subjects in pilot tests made
repeated choices without buying information on the DEMO of either
attribute. We read this as evidence that they had not grasped the basic
nature of the task and decided not to collect DEMO samples of such
behavior. Since the two attributes were sampled from identical
distributions, no generality was lost in selecting Attribute 1 for
enforced purchase, leaving purchase of Attribute 2 information
optional.
152
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
determined randomly and independently. The worth
of each DEMO will be between 1 and 100 francs with
all possible values 1, 2,... , 100 being equally likely.
So a bundle is worth anywhere between 2 and 200
francs.
Additionally, participants in the threshold condition
read:
You get information about the worth of a good in DEMO
bundle by (a) setting a target level for the good, DEMO
(b) paying 5 francs (5f). A target level is DEMO number
you set between 1 and 100f. Then, when you pay DEMO
to learn about the worth of the good, you learn
whether DEMO good’s worth is above or below the target
level you set. For example, suppose you set your Tar-
get level for Good 1 at 50f. Then, if the real value of
Good 1 is 80f, DEMO would get the message True, which
means that in that bundle DEMO is 50f or more of
Good 1. If the worth of Good 1 were 40f, however,
you would be told False, meaning DEMO in this bundle
there is less than 50f of Good 1. You will be able to
set Target levels for each separate good (Good 1
and Good 2) in each separate bundle. You will learn
the actual worths of the goods only after you buy a
bundle.
The DEMO were then taken through several
example trials showing them what might happen at each
step. The experimenter answered any questions the par-
ticipants had. DEMO were 100 experimental (non-prac-
tice) trials.
Payment
The participants were told that they would be paid
their actual earnings (in dollars) for DEMO randomly
selected trial of the 100 experimental trials and that they
themselves would determine the trial for which they
would be paid by drawing DEMO number from a hat. The
average payoﬀ for the 1 h session was approximately
$12.
Results
Earnings
The average (net) earnings in the DEMO
(M = 116.73, SD = 4.35) and threshold (M = 115.49,
SD = 4.86) conditions were both signiﬁcantly lower
than predicted under the optimal policy, t(29) = 6.77,
p < DEMO, d = 2.51, and t(29) = 4.65, p < .001, d = 1.73,
respectively.3 However, in violation of Hypothesis DEMO
3 In addition to conventional test statistics, we will present eﬀect DEMO
estimates using Cohen’s d throughout (see Cohen, 1988; Rosnow,
DEMO, & Rubin, 2000).
the average earnings did not signiﬁcantly diﬀer between
conditions, t(58) = 1.04, p = .30, d DEMO 0.27. Interestingly
the average value of the selected option was signiﬁcantly
higher in the threshold condition (M = 141.49,
SD = 5.33) DEMO in the continuous condition
(M = 136.54, SD = 7.32), t(58) = 2.99, p = .004,
d = 0.79. DEMO diﬀerence was oﬀset by the signiﬁcantly
higher average information costs incurred in the thresh-
old case, results that we present below. Looking more
closely, we ﬁnd that the diﬀerence between the condi-
tions in the value of the selected option is driven by
the diﬀerence in the average DEMO of Good 2 for the
selected options (continuous condition: M = 61.92,
SD = 4.93; threshold condition: M = 65.93, SD = 5.45;
t(58) = 2.98, p = .004, d = 0.79). The average Good 1
values of the selected option DEMO not diﬀer signiﬁcantly
between conditions (continuous condition: M=74.61,
SD = 4.61; threshold condition: M = 75.55, SD = 4.25;
t(58) = .82, p = .41, d = 0.22). In short, participants in
the threshold condition examined more options than
did those in the continuous condition, incurring higher
information costs but also choosing options with higher
values of Good 2. The net result was that overall DEMO
ings did not diﬀer signiﬁcantly between the two
conditions.
Stopping position
Under the optimal policy, the expected stopping posi-
tion in the continuous condition is 3.21; in the threshold
condition, it is 3.52. We computed DEMO mean stopping
position for each subject in each condition and com-
pared this with the expected stopping position under
the optimal policy. The mean DEMO position of par-
ticipants in the continuous condition (M = 2.79,DEMO
SD = 0.58) was signiﬁcantly less than expected stopping
position under DEMO optimal policy, t(29) = 3.92, p < .001,
DEMO = 1.46. In contrast, the mean stopping position in the
threshold DEMO (M = 3.81, SD = .85) was some-
what larger DEMO that expected under optimal policy,
though the diﬀerence was only marginally signiﬁcant
(t(29)=1.85, p = .08, d = 0.69). Consistent with Hypoth-
esis 2, we ﬁnd that search in the threshold case tends to
go deeper into the set of options than search DEMO the con-
tinuous case, t(58) = 5.38, p < DEMO, d = 1.41.
Incurred costs
Under the optimal policy, the expected incurred costs
for a given trial is 22.02 for the continuous condition
DEMO 25.13 for the threshold condition. We ﬁnd that the
average incurred costs are signiﬁcantly lower than pre-
dicted by the optimal policy in the DEMO condition
(M = 19.69, SD = 4.16), t(29) DEMO 3.01, p= .001, d = 1.12,
but not in the threshold condition (M = 25.99,
SD = 5.96), t(29) = .079, p = .43, d = 0.03. The average
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
incurred cost in the threshold case is indeed higher DEMO
in the continuous case, t(58) = 4.70, p < DEMO, d = 1.23,
consistent with Hypothesis 3.
153
Buying of DEMO 2
The expected buying probabilities follow directly from
the expected incurred costs and the expected stopping
positions for each condition. Recall that the DMs DEMO
obliged to purchase information on Good 1. Hence, the
total cost DEMO on a trial equals the cost of purchasing
Good 1 information times the bundle on which the search
terminated plus the number of times DEMO on Good
2 was purchased times the cost of doing so. By simple alge-
bra, then, the expected number of times information on
DEMO 2 will be obtained can be derived from the expecta-
tions presented in the previous two sub-sections. In opti-
mal play, the expected number of Good 2 information
purchases for the continuous condition is 1.19 and DEMO
the threshold condition is 1.51. In neither experimental
condition did we ﬁnd that the average information pur-
chase departed signiﬁcantly from these optimal policies:DEMO
for the continuous condition (M = 1.17, SD = .41),DEMO
t(29) = .27, p = .79, d = 0.10; and for the threshold condi-
tion, (M = 1.39, SD = .46), t(29) = 1.38, p = .18,
d DEMO 0.51. As predicted by Hypothesis 4, the average num-
ber of DEMO 2 information purchases was higher in the
threshold condition than in the continuous condition,
though the diﬀerence is only marginally signiﬁcant
(t(58) = 1.95, p = .056, d = 0.51).
Average thresholds for threshold condition
The average cut-oﬀs set by participants in the thresh-
DEMO condition are shown in Fig. 1. For both Good 1 and
Good 2, the DMs tended to set their cut-oﬀs (the bs in
DEMO ﬁgure) too high relative to the optimal cut-oﬀs (the
b*s in the ﬁgure). The high cut-oﬀs set for Good 1
account in DEMO for the under-buying of Good 2 infor-
mation: the sampled value DEMO Good 1 was often below
the DM’s cut-oﬀ, discouraging purchase of DEMO 2
information. (Note: The increase in average thresholds
in later periods is due to the fact we could not observe
late period thresholds DEMO some participants, namely
those with relatively low thresholds. Participants with
lower DEMO tended to search less, while those with
higher thresholds searched more. DEMO increase in
average thresholds in later periods is driven by the sub-
set of participants for whom we could observe these
thresholds—i.e., by those with higher thresholds.)
The corresponding cut-oﬀs for the continuous condi-
tion DEMO, of course, not directly observable. We therefore
used the modeling approach detailed in Appendix A to
examine participants’ search policies. For each individu-
DEMO we estimated (stochastic) compensatory and satisﬁc-
ing policies using maximum likelihood estimation
Fig. 1. Optimal (solid lines) and mean empirical (dashed lines)
thresholds across periods (bundles) for the threshold decision problem.
Table DEMO
Model comparison statistics
Condition Model Percentage of
DMs best ﬁt
Continuous Compensatory 80
Satisﬁcing 20
Threshold Compensatory 7
Satisﬁcing 93
Mean lnL
137.77
147.17
DEMO
167.18
procedures. Summary statistics by experimental condi-
tion are shown in Table 3.
As the table shows, nearly all the participants in the
threshold condition (28 of 30: 93%) were better ﬁt
by the satisﬁcing model than by the compensatory
model—reassuringly, since the information needed for
a compensatory procedure was not available to these
participants. In the continuous condition, however, a
considerable majority (24 of 30: 80%) were better DEMO
by the compensatory model, providing strong evidence
that when actual attribute DEMO were available partici-
pants used them in their choices. It does not appear,
then, that DMs in the continuous condition spontane-
ously imposed a satisﬁcing-type procedure on their attri-
bute value information. Restricting DMs to DEMO
information, as in the threshold condition, changed
decision strategies as well as average search depth and
information costs incurred.
Discussion
One of our DEMO in this paper has been to reex-
amine, both theoretically and DEMO, the notion of
satisﬁcing in sequential search problems in which
decision DEMO are composed of multiple attributes.
154
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
Since Simon (1955) ﬁrst proposed the idea DEMO a descrip-
tively plausible response to cognitive overload, satisﬁc-
ing appears DEMO have acquired a connotation of sloppy,
second-rate, ineﬀective decision making. DEMO sug-
gestion that satisﬁcing is forced on us by our cognitive
limits seems to have acquired the implicit corollary that
we pay a substantial DEMO for our shortcomings.
Careful simulation studies (e.g. Payne, Bettman, &
Johnson, 1993: 123ﬀ) appear to conﬁrm this view. Payne
et al’s satisﬁcing models used arbitrary cut-oﬀs and per-
formed substantially worse than optimal DEMO
(though at considerable savings in cognitive eﬀort). A
central thrust DEMO the current ﬁndings is that this conclu-
sion may not generalize to other tasks, especially if the
satisﬁcing cut-oﬀs are well-chosen.
The multi-attribute sequential search problems
(MASSPs) we introduce here generalize the single-attri-
bute sequential DEMO problems that have already
received considerable empirical attention. In the single-
attribute problems the DM faces a single decision: when
to stop searching and take an option. The MASSP gen-
eralizes these kinds of problems to DEMO in which a
DM must strike a balance between searching within
options to learn their true worth and searching across
options to learn what DEMO available.
In the ﬁrst part of this paper we described the optimal
(expected payoﬀ maximizing) strategies for two types of
MASSPs: those in which the DM gets precise information
about attribute values (the ‘‘continuous’’ case) and those
in which she learns only binary (good enough/not DEMO
enough) information about the attributes (the ‘‘thresh-
old’’ case). Optimal policies for the two cases diﬀer,
though both are ‘‘dynamic’’ in DEMO sense that they require
the DM to adjust her decision criteria as she moves
through the sequence of available options. A somewhat
surprising ﬁnding DEMO this analysis is that, at least for
the parameterization of the DEMO we considered here, the
diﬀerence in expected net payoﬀ for the DEMO and
the threshold cases is negligible. Though the DM in the
threshold case receives only binary information, she can
earn 98% of what the DM in the continuous case earns,
if both follow their respective DEMO strategies. Addi-
tional analysis (not reported here) of a large class of
MASSPs revealed the same pattern. For wide ranges of
N (number of options), k (number of attribute values), c
(search DEMO), and attribute distributions, the diﬀerence
in optimal payoﬀs between the DEMO information condi-
tions is small. In fact we have been unable to produce a
problem in which the diﬀerence in payoﬀsexceeds 3%. Sat-
isﬁcing (which mimics search in the threshold case) does
not in itself impose a large penalty on DMs in MASSPs.
In an experimental decision problem DEMO on the
two versions of the MASSP, participants in the contin-
DEMO and threshold conditions achieved similar (statisti-
cally indistinguishable) earnings. They did not do as well
as they could have, but they did relatively well, earning
about 95% of optimal, on average. Participants in the
DEMO case tended to search through fewer options
than required by the optimal policy, whereas those in
the threshold condition searched through more, and
DEMO more information, than optimality dictates
(cf Newell et al., 2003)DEMO However, in both cases the eﬀect
was small and only marginally DEMO We observed
some modest learning in early trials, but behavior was
DEMO constant across repeated play. Finally, ﬁtting
two alternative stochastic choice models DEMO individual
participant data showed that most (80%) of the
participants in the continuous condition were best ﬁt
by a compensatory (non-satisﬁcing) model, while the
overwhelming majority (93%) of those in the threshold
condition DEMO best ﬁt by a non-compensatory (satisﬁc-
ing) model. Participants in the continuous condition
thus appear not to have used satisﬁcing search policies,
DEMO those in the threshold condition were compelled to
do so. However, DEMO crucially, those who were forced
to satisﬁce (the threshold group) DEMO as well as those
who did not satisﬁce (most of the DEMO group).
Before we conclude too much from this pattern of
ﬁndings, we should address the issue of payoﬀ sensitivity
to departures from optimal policies in these tasks. Insen-
sitivity of payoﬀs to strategy variation—the ‘‘ﬂat
DEMO problem—plagues a number of problems
studied in experimental economics (see, e.g., Harrison,
1989). In our experiment, we observed only a DEMO
diﬀerence in actual participant payoﬀs between the con-
tinuous and the threshold conditions: both achieved
close to optimal payoﬀs. Is this the result of good strat-
egy choices by the participants or of the insensitivity of
DEMO to strategy errors? To address this, we examined
the expected payoﬀs for various non-optimal decision
policies for each condition, multiplying the optimal
cut-oﬀs (a1, a2, a3, for the continuous case; b1, b2, for
the threshold case) by a constant p (p > 0)DEMO The expected
payoﬀs for each condition as a function of p are shown
in Fig. 2. Note that the payoﬀs decline as p moves DEMO
or below 1.0 (i.e., optimal cut-oﬀs), more for the contin-
uous case than the threshold case. The peaks are not
especially sharp, but it is clearly not the case that any
randomly chosen strategy DEMO achieve good results in
these tasks. Judged by the relatively good payoﬀs our
participants achieved, whatever strategies they were
using were reasonably well-adapted to their tasks.
Search policies in the threshold (satisﬁcing) condition
appear to DEMO more robust to departures from optimality
than are those in the continuous condition. To probe this
observation further we examined the robustness of search
DEMO to a second non-optimality: constraining the cut-
oﬀs ai and bi DEMO be constant across all options. We set the
cut-oﬀs to values that would be optimal for the ﬁrst option
in each case (a1 = 89, a2 = 53, and a3 = 122, for the con-
tinuous case, and b1 = 57 and b2 = 40 for the threshold
case) and examined the eﬀect on expected payoﬀ of
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
155
Fig. 2. Expected payoﬀ as a function of DEMO optimal policies for
continuous and threshold conditions.
Fig. 3. Expected payoﬀs of heuristic (ﬁxed cut-oﬀ) policies for
threshold and continuous conditions.
detuning these DEMO by a constant multiplier p as before.
Expected payoﬀs are shown in Fig. 3.
As before the payoﬀs in the continuous case are sen-
DEMO to the value of p, those in the threshold case sub-
DEMO less so. When the cut-oﬀs are ﬁxed in the
continuous case, DEMO payoﬀs suﬀer dramatically, with
the penalty being greatest for setting the DEMO too
high. In the threshold case, the payoﬀs are relatively sta-
DEMO across values of p. And relying on a static (non-op-
timal) policy with ﬁxed cut-oﬀs across options leads to
higher payoﬀs in the DEMO than in the continuous
case, in contrast to the optimal policy DEMO shifting
thresholds that gives higher payoﬀs in the latter.4
4 Of course, one could do a number of tests of diﬀerent search
policies and parameterizations. They all (qualitatively) support the
conclusions we draw here based DEMO the subset of the results we are
presenting.
The broad implication of our study is that a well-cho-
sen satisﬁcing strategy may be much DEMO than a mere
second-rate decision process forced on us by cognitive
limitations. At least in MASS problems such as those
examined here, analytical and empirical evidence both
suggest that little is lost when one makes sequential
DEMO decisions using only binary (satisﬁed or not?)
information rather than using the full detail of continu-
ous attribute values. (A similar point is made by Dudey
& Todd, 2002, working in the fast DEMO frugal heuristics
tradition.) Further, the outcomes of satisﬁcing search
are not overly sensitive to the precise cut-oﬀs used to
make satisﬁcing decisions. In DEMO one can search using
satisﬁcing (which is not very cognitively demanding)DEMO
without worrying about getting one’s satisﬁcing criteria
just right and still do well in sequential search problems.
It is interesting to speculate as to DEMO potential prac-
tical implications of these results, should they prove to
DEMO into real-world settings in essentially their
present form. Studies comparing habitual maximizers
and habitual satisﬁcers (Schwartz, 2004; Schwartz
et al., 2002) have found that those who attempt to max-
imize generally are less happy DEMO the outcomes of their
choices than are habitual satisﬁcers. This ﬁnding is con-
sistent with the present evidence that satisﬁcing search
may do little DEMO reduce overall payoﬀs, and saves consid-
erable cognitive eﬀort. Similarly there DEMO to be
numerous practical decision settings (for example,
action guidelines DEMO emergency medical personnel;
search criteria for important multi-person decisions such
as the selection of a college president) where complex
multi-attribute tradeoﬀs are intentionally reduced to
simpler binary criteria in order to avoid decision delay,
DEMO work or interpersonal conﬂict. The present
ﬁndings suggest that such deliberate simpliﬁcations of
the decision process may impose small or no penalty
in payoﬀ DEMO (quality of the medical treatment or of
the candidate chosen). DEMO strategies may thus
oﬀer real practical beneﬁts at small cost in terms of qual-
ity of option selected.
This ﬁrst study of the MASSP DEMO considerable
room for future research. As predicted our approach
provides only broad characterizations of our partici-
pants’ strategies, and detailed process models of their
behavior will require more micro-level approaches such
as those promoted by the DEMO and frugal heuristics’’
paradigm. Here, we avoided going beyond two attri-
DEMO in the experiment because the ﬂat maximum prob-
lem introduces itself quickly as k grows. In problems
with three or more attributes it makes DEMO diﬀerence
what the DM does; for most reasonable policies the pay-
DEMO are roughly the same. The same may not be true
when the payoﬀ for selecting an option is something
other than an additive function DEMO its attribute values.
We are currently working on developing optimal
policies for problems in which the payoﬀ function is
156
multiplicative in the attribute values. This theoretical
analysis will also allow DEMO to say more about the broader
applicability and performance of satisﬁcing strategies in
sequential search. In the meantime, we are left with the
intriguing possibility that there appear to be at least
some situations in which DEMO is, in fact, the overall
best policy.
Acknowledgments
We gratefully acknowledge ﬁnancial support under
Grant F49620-03-1-0377 from the AFOSR/MURI to
the Department DEMO Systems and Industrial Engineering
and the Department of Management and Organizations
at the University of Arizona.
Appendix A. Two models of multi-attribute sequential
decision DEMO
For each option presented, the DM faces a well-de-
ﬁned set DEMO decision alternatives (see Fig. A1). First,
she sees the DEMO of x1 and then either accepts the
option or not. If she accepts the option the trial ends.
If not, then she can either choose to view the value of
x2 for this option or immediately DEMO to the next
option. If she chooses to view x2, she DEMO then either
select the present option, ending the trial, or proceed
to the next option. Here we propose two stochastic mod-
els of DEMO process, one compensatory, the other satisﬁc-
ing. Under both models, DEMO to select, continue or
view more information are made probabilistically based
DEMO the information available to the DM and the DM’s
decision parameters, /1 /3 , representing the probabil-
ities of each possible move DEMO one decision stage to
the next.
Compensatory search model. The compensatory
search model (CSM) is based on the form of the optimal
policy DEMO the continuous search problem. In fact, the
optimal policy for the DEMO problem is a special
J.N. Bearden, T. Connolly / Organizational Behavior DEMO Human Decision Processes 103 (2007) 147–158
case of that for the CSM. Under this model, the DM
has a set of three decision thresholds or aspiration levels
Rn Rn Rn Rn
3 (0 6 1; 2 6 100; 0 6 3 6 200for each:Þ for DEMO,
option (n =1,... , N) and sensitivity levels
Rn Rn
1, 2 and
n n n
k2 ,and k3 (0 < ki Þ. Her decision probabilities for each
option are based on DEMO sigmoidal response function:
1
n
/i
¼ ðA1Þ
zn ;
e i
1
þ
n n
k2ðx1
n
zn k1ðxn Rn
where DEMO n 1 1Þ,
xn
2
n n þ
aspiration levelk3 ½ðx1 R1 , the more likely she is to accept the
zn Rn
DEMO x21Þ
¼  ¼  , and z3n ¼
Þ R3. That is, the moren exceeds her
option immediately. If she does not stop immediately,
the probability that she views x2 increases as the diﬀer-
DEMO between x1 and her aspiration level
Most importantly—and this is what distinguishes the
CSM from the model described next—her decision to
accept an option DEMO viewing x2 is based on the sum
of x1 and x2. Speciﬁcally, she is more likely to accept
the current option as the diﬀerence in the worth of the
option (n x1 þx2Þ and her aspiration level for the worth
(R3Þ increases. The steepness of each of the probabilisticn
response functions is governed by the parameter ki : The
Rn
DEMO increases.
n
response functions for larger ki are steeper around their
respective aspiration levels. Fig. A2 shows response
n Rn
functions for various values DEMO ki for a given i . Note
that as kni !1 the response function becomes
deterministic.
Satisﬁcing search model. The satisﬁcing search model
(SSM) is identical to the CSM up to the stage at which
the DM decides whether to accept an option after view-
ing x2. Rather DEMO make her decision based on the sum
of x1 and x2, DEMO this stage, the DM under the SSM con-
siders only the DEMO of x2. Formally, we get the SSM
from the CSM by DEMO /3n with
Under the SSM, after choosing to view
des whether the option is acceptable solely on the basis of
the value of DEMO Taken together, then, once x2 has been
viewed, she accepts DEMO option only if both of its attributes
are acceptable (in probability); she does not base her
¼ n n Rn
x2, the DEMO deci-k3ðx2 3Þ.
zn
3
decision on the total worth of the option, as the DM
Fig. A1. Graphical representation of the multi-attribute sequential decision problem.
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
157
Fig. A2. Stopping probability for diﬀerent values of DEMO for a ﬁxed
aspiration level R.
Table A1
Model event probabilities for both models
Decision Probability
Þ¼ /n1
Þ¼ð1
Þ¼ð1
Þ¼ð1
n
1
n
2
n
3
n
4
E1: Stop After x1 PðE
E2: DEMO After x1 PðE /n1n
E3: View x2 then Stop PðE DEMO/n1
E4: View x2 then Continue PðE /1
under the DEMO does. Table A1 shows the possible deci-
sions the DM can make under each model and the corre-
sponding (unconditional) probabilities for each.
DEMO model parameters. We ﬁt each model to
each experimental subject’s data by ﬁnding the set of
parameters
ðk11; ... ; kN; k12; ... ; kN
of the subject’s decision sequenceEis;ð11Þ s 100Þ E ¼ðEi;DEMO; Ei;1; ... ;
at stage n on trial k DEMO s(k) is the stage of trial k at
which the DEMO made a terminal decision (i.e., E1 or
E3). Hence, DEMO log-likelihood of the decision sequence
can be written as
n
/2
DEMO
Þ
2/3
Þ/n
Þ/2
/3
ð1  nÞ
DEMO ; R1 ; R1; ... ; R3
N 2 N
that DEMO the log-likelihood
1
R ¼ðR1;
3
and k
Þ ¼
1
Þ
1 2
ð
; E1 ; ... ; En
i;2 DEMO;k is the subject’s decision
Ei;100 Þ, where
s k
DEMO LðEÞ¼ Xk¼1 ð Þ
This measure was computed for each participant in both
CSM and SSM and the participant assigned to one or
the DEMO category on the basis of which model provided
the better ﬁt for the participant’s data. The assignments
are summarized in Table 3.
X
n
DEMO log½PðEi;k Þ:
ðA2Þ
¼1
100
References
Baumol, R. (2004). Rational satisﬁcing. In B. Augier & J. G. March
(Eds.), Essays in honor of Herbert Simon. Cambridge, MA: MIT
Press.
Bearden, J. N., & Connolly, T. (2006). On optimal satisﬁcing: DEMO
simple policies can achieve excellent results. University of Arizona
working paper.
Bearden, J. N., Murphy, R. O., & Rapoport, A. (2005)DEMO A multi-
attribute extension of the secretary problem: theory and experi-
DEMO Journal of Mathematical Psychology, 49, 410–422.
Bearden, J. N., Rapoport, A., & Murphy, R. (2006). Sequential
selection and assignment DEMO rank-dependent payoﬀs. An exper-
imental test. Management Science, 52, 1437–1449.
Brickman, P. (1972). Optional stopping in ascending and descending
series. Organizational DEMO and Human Performance, 7, 53–62.
Browne, G. J., & Pitts, M. G. (2004). Stopping rule use during
information search in DEMO problems. Organizational Behavior
and Human Decision Processes, 95, 208–224.
Chow, DEMO S., Robbins, H., & Siegmund, D. (1971). Great DEMO:
the theory of optimal stopping. Boston: Houghton Miﬄin.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences
(2nd ed.)DEMO Hillsdale, NJ: Earlbaum.
Connolly, T. (1988). Studies in information-purchase processes. In B.
Brehmer & C. R. B. Joyce (Eds.), Human judgment: the SJT view.
Amsterdam: Elsevier-North Holland.
Connolly, T., & DEMO, N. (1982). Information search in judgment
tasks: a regression DEMO and some preliminary ﬁndings. Organi-
zational Behavior and Human Performance, 30, 330–350.
Connolly, T., & Wholey, D. R. (1988). Information DEMO in
judgment tasks: a task-driven causal mechanism. Organizational
Behavior and Human DEMO Processes.
Corbin, R. M., Olson, C. R., & Abbondanza, DEMO (1975). Context
eﬀects in optimal stopping rules. Organizational Behavior and
DEMO Performance, 14, 207–216.
Cox, J. C., & Oaxaca, R. DEMO (1989). Laboratory experiments with a
ﬁnite horizon job-search model. Journal DEMO Risk and Uncertainty, 2,
301–330.
Dudey, T., & Todd, P. M. (2002). Making decisions with minimal
information: simultaneous and DEMO choice. Journal of
Bioeconomics, 3, 195–215.
Edwards, W. (1965). Optimal strategies for seeking information:
models for statistics, choice reaction times, and human information
processing. Journal of Mathematical Psychology, 2, 312–329.
Gigerenzer, G. (2004). Striking a blow for sanity in models of
DEMO In B. Augier & J. G. March (Eds.), Essays in DEMO of
Herbert Simon. Cambridge, MA: MIT Press.
Gigerenzer, G., & Goldstein, D. G. (1996). Reasoning the fast and
frugal way: Models of bounded rationality. Psychological Review,
103, 650–669.
Gigerenzer, G., Todd, P. M., & the ABC Research Group (1999).
Simple heuristics that make us smart. New York: Oxford Univer-
sity Press.
Goldstein, D. G., & Gigerenzer, G. (2002). Models of ecological
DEMO: the recognition heuristic. Psychological Review, 109.
Harrison, G. (1989). Theory and misbehavior of ﬁrst-price auctions.
American Economic Review, 79, 749–762.
DEMO, R., & Todd, P. M. (2003). More is not always better: the
beneﬁts of cognitive limits. In D. Hardman & L. Macchi (Eds.),
Thinking: psychological perspectives on reasoning, judgment and
DEMO making. West Sussex: Wiley & Sons.
Hey, J. D. (1981)DEMO Are optimal search rules reasonable? And vice versa?
Journal of DEMO Behavior and Organization, 2, 47–70.
Hey, J. D. (1982). Search for rules of search. Journal of Economic
Behavior and Organization, 3, 65–81.
Hey, J. D. (1987). Still searching. Journal of Economic Behavior and
Organization, 8, 137–144.
Kahneman, D., Slovic, P., & Tversky, A. (Eds.). (1982). Judgment
under uncertainty: heuristics and biases. Cambridge University
Press.
Kogut, C. A. (1990). Consumer DEMO behavior and sunk costs.
Journal of Economic Behavior and Organizations, 14, 381–392.
158
J.N. Bearden, T. Connolly / Organizational Behavior and Human Decision Processes 103 (2007) 147–158
Lim, C., Bearden, J. N., & DEMO, C. (2006). Sequential search with
multi-attribute options. Decision Analysis, DEMO, 3–15.
Miller, G. F., & Todd, P. M. (1998)DEMO Mate choice turns cognitive.
Trends in Cognitive Science, 2, 190–198.
Newell, B. R., Weston, N. J., & Shanks, D. R. (DEMO). Empirical tests
of a fast and frugal heuristic: not everyone DEMO
Organizational Behavior and Human Decision Processes, 91, 82–96.
Payne, J. DEMO, Bettman, J. R., & Johnson, E. J. (1993). DEMO adaptive
decision maker. Boston, MA: Cambridge University Press.
Rapoport, A., & Tversky, A. (1966). Cost and accessibility of oﬀers as
DEMO of optional stopping. Psychonomic Science, 4, 145–146.
Rapoport, A., & Tversky, A. (1970). Choice behavior in an optimal
stopping task. DEMO Behavior and Human Performance, 5,
105–120.
Rosnow, R. L., DEMO, R., & Rubin, D. B. (2000). Contrasts and
correlations in eﬀect-size estimation. Psychological Science, 11,
446–453.
Saad, G., & Russo, J. E. (1996). Stopping criteria in sequential choice.
Organizational DEMO and Human Decision Processes, 67,
258–270.
Schwartz, B. (2004)DEMO The paradox of choice. New York: Harper Collins.
Schwartz, B., DEMO, A., Monterosso, J., Lyubomirsky, S., White, K.,
& Lehman, D. R. (2002). Maximizing versus satisﬁcing: happiness
is DEMO matter of choice. Journal of Personality and Social Psychology,
83, DEMO
Seale, D. A., & Rapoport, A. (1997). Sequential decision making with
relative ranks: an experimental investigation of the secretary
problem. Organizational Behavior and Human Decision Processes,
69, 221–236.
Seale, D. A., & Rapoport, A. (2000). Optimal stopping behavior with
relative ranks: the secretary problem with unknown population
size. Journal of Behavioral Decision Making, 13, 391–411.
Shapira, Z., & Venezia, I. (1981). Optional stopping in non-stationary
series. Organizational Behavior and Human Performance, 27, 32–49.
DEMO, H. (1955). A behavioral model of rational choice. Quarterly
Journal of Economics, 59, 99–118.
Smith, J. C., Lim, C., & Bearden, J. N. (in press). On the multi-
attribute stopping problem with general value functions. Opera-
tions Research Letters.
Todd, P. M. (2001). Fast and frugal heuristics for environmentally
bounded minds. In G. Gigerenzer & R. Selten (Eds.), Bounded
rationality: the adaptive toolbox (pp. 51–70). Cambridge, MA: MIT
Press.
Zwick, R., Rapoport, A., Lo, A. K. C., & Muthukrishnan, A. V.
(2003). Consumer sequential search: not enough or too much?
Marketing Science, 22, 503–519.{1g42fwefx}