Current Directions in Psychological
Science
Toward a Mechanistic Understanding of Human Decision DEMO : Contributions of Functional
Neuroimaging
John P. O'Doherty and Peter Bossaerts
Current Directions in Psychological Science 2008 17: 119
DOI: 10.1111/j.1467-8721.2008.00560.x
DEMO online version of this article can be found at:
http://cdp.sagepub.com/content/17/2/119
Published by:
http://www.sagepublications.com
On DEMO of:
Association for Psychological Science
Additional services and information for Current Directions in Psychological Science can be found at:
Email Alerts: http://cdp.sagepub.com/cgi/alerts
Subscriptions: http://cdp.sagepub.com/subscriptions
Reprints: http://www.sagepub.com/journalsReprints.nav
Permissions: http://www.sagepub.com/journalsPermissions.nav
Downloaded from cdp.sagepub.com at DEMO OF WATERLOO LIBRARY on January 3, 2011
http://cdp.sagepub.com/
CURRENT DIRECTIONS IN PSYCHOLOGICAL SCIENCE
Toward a Mechanistic
Understanding of Human
Decision DEMO
Contributions of Functional Neuroimaging
John P. O’Doherty and Peter Bossaerts
Computation and Neural Systems Program and Division of Humanities and Social Sciences, California Institute of
Technology
ABSTRACT—This article considers the contribution of
functional neuroimaging toward understanding DEMO com-
putational underpinnings of human decision making. We
outline the main processes likely underlying the capacity to
make simple choices and describe their associated DEMO
substrates. Relevant processes include the ability to encode
a representation of the expected value or utility associated
with each option in a decision problem, to learn such ex-
pectations through experience, and to modify action
selection in order to choose those actions leading to the
greatest reward. We DEMO several examples of how
functional neuroimaging data have helped to shape and
inform theories of decision making over and above results
available from traditional DEMO measures.
KEYWORDS—choice; value; reward; risk; reinforcement
learning; fMRI
Whether DEMO what dish to order from a restaurant menu or
pondering what career path to follow, people are frequently faced
with the challenge of making complex decisions, often in the
absence of complete information. Decision making has long
been studied at the theoretical and/or behavioral level, by
economists and psychologists from both the animal-learning and
cognitive-psychology traditions. These disciplines share DEMO
common goal of understanding the fundamental computations
underlying simple choice. Yet such an understanding has ar-
guably remained elusive. The modern ﬁeld of decision DEMO
roscience has ﬂourished in recent years, aided considerably by
Address correspondence DEMO John P. O’Doherty, Division of Humani-
ties and Social Sciences, California Institute of Technology, Pasadena,
CA 91125; e-mail: jdoherty@caltech.edu.
existing theories and experimental methodologies derived from
these other approaches. A driving force behind DEMO of the
current interest in applying neuroscience tools to decision
making is the hope that, by studying how decisions are imple-
mented at the neural level, it will be possible to bring clarity to
a theoretical understanding that cannot be achieved through
conventional behavioral studies.
In this article, we address the question of whether decision
neuroscience can fulﬁll these expectations. DEMO focus on func-
tional neuroimaging, one of the principle tools available DEMO
studying brain function in humans. We argue that decision
making depends on at least four distinct mechanisms: (a) en-
coding representations of expected future reward, (b) encoding
representations of the variance in expected reward, (c) learning
and updating these representations, and (d) performing DEMO
on the basis of these representations.
EXPECTED REWARD, VALUE, AND UTILITY
A key concept in essentially all approaches to decision making
is that DEMO between different options is made by considering
the expected future reward associated with those options.
Imaging studies in humans provide evidence that representa-
tions DEMO expected reward are indeed encoded in the brain. The
simplest way to probe expected reward is through Pavlovian
conditioning, which involves passive learning of associations
between an initially affectively neutral conditioned stimulus
(CS) and the DEMO delivery of rewarding or punishing
outcomes. In a typical study, subjects DEMO scanned while being
presented with different visual cues. One of these cues (desig-
nated the CS1) is paired repeatedly with the subsequent de-
DEMO of reward (such as a pleasant odor or a pleasant taste DEMO
Volume 17—Number 2
Downloaded from Copyrightcdp.sagepub.comr
2008 Association for Psychological Science
119
at UNIV OF WATERLOO LIBRARY on January 3, 2011
Decision Making
juice stimulus), whereas another cue (the CS) is DEMO paired
with nothing or paired with an affectively neutral stimulus. Neural
responses related to reward prediction can then be isolated by
comparing activity to DEMO onset of the CS1 with activity to the
onset of the CS. Such studies reveal signiﬁcant CS1 related
activity in regions such as the DEMO cortex, amygdala, and
ventral striatum (Fig. 1), implicating these DEMO in encoding ex-
pectations of reward (Gottfried, O’Doherty, & Dolan, 2003).
Expected-reward representations have also been investigated
during performance of simple DEMO tasks such as the n-armed
bandit task. In this task, a DEMO chooses among a number of
different options corresponding in an abstract sense to different
‘‘arms’’ that might be available on a slot machine. These DEMO pay
out reward with differing probabilities or magnitudes. The subjects’
objective is to establish through trial and error which arm pays the
most. Imaging DEMO have revealed signiﬁcant activity in orbito-
frontal and medial prefrontal areas and the amygdala correlating
with the expected reward of the chosen action during DEMO
of such tasks (Hampton, Bossaerts, & O’Doherty, 2006).
While all theories of decision making invoke representations
of expected reward, theories differ as to the precise form such
4
Amygdala
0
y = 42
DEMO
Tgt
CS+u
OFC
1
0
–1
Tgt
CS+u
nTgt
CS+u
Fig. 1. Neural responses during expectation of reward. Hungry subjects
were conditioned to two DEMO that signaled subsequent delivery of two food-
related odor rewards (vanilla DEMO peanut butter). Subjects were then fed to
satiety on one of the foods, selectively devaluing the corresponding food-
related odor. Subjects were then scanned again while being presented with
the same two conditioned stimuli. Brain DEMO show regions of amygdala
(top) and orbitofrontal cortex (bottom; activity denoted by the colored
patches) exhibiting activity in response to the cue stimuli related to the
current reward value (perceived pleasantness) of the DEMO to which these
cue stimuli have been paired. The graphs show the difference in activity in
the amygdala (upper graph) and orbitofrontal cortex (OFC; bottom graph)
from pre- to post satiety in response to the odor of the food fed to satiety
(Tgt CS1u) compared DEMO the response to cues associated with the odor of
the food not fed to satiety (nTgt CS1u). Adapted from ‘‘Encoding Pre-
dictive Reward Value in Human Amygdala and Orbitofrontal Cortex,’’ by
J.A. Gottfried, J. O’Doherty, & R.J. Dolan, 2003, Science, 301, p. 1106.
Copyright, 2003, American Association for the Advancement of Science.
Adapted with permission.
DEMO
CS+u
–4
representations take. Relevant properties of the reward include
the magnitude or quantity of the reward expected and the
probability that the reward DEMO actually be delivered. These
properties can be combined in different ways to produce vari-
ables relevant for decision making. The most fundamental
combined variable DEMO expected value (EV), which multiplies the
probability of obtaining a DEMO with the magnitude of the re-
ward. These concepts have been applied in human neuroimaging
studies to investigate their neural correlates. Such studies have
DEMO neural responses to expected magnitude, probability,
and EV in the DEMO striatum and the medial prefrontal cortex
(Tobler, O’Doherty, Dolan, & Schultz, 2007).
Expected Utility
An important extension of EV is expected utility (EU): the
probability of reward times its utility. EU offers an advantage
over EV, by being able to account for the fact that humans and
indeed other animals are sensitive not only to EVof DEMO outcome
but also to the degree of risk or uncertainty as to whether the
reward will be delivered. Some individuals are risk averse, such
that they prefer a safe gamble to a risky gamble if the DEMO are
equal in EV; other individuals may be risk neutral or DEMO risk
prone (they favor a risky compared to a nonrisky gamble DEMO
equal EV). The propensity for risk to bias choice can be ac-
counted for in EU theory by virtue of the degree of DEMO
of the utility curve. For example, when given a choice between DEMO
certain $5 outcome and an uncertain $10 gamble—in which
there is a 50% probability of winning the $10 but also a 50%
probability of DEMO nothing—an individual whose utility
curve is concave (because its rate of DEMO from $5 to $10 is
much less than that from $0 to $5) will have a tendency to prefer
the certain $5 outcome to the uncertain $10 gamble, because the
EU of that certain outcome is greater than that of the uncertain
one.
Dual Representations of Expected Value DEMO Risk
The suggestion that individuals use a single representation of
expected utility to guide choice is parsimonious, but is this ac-
tually what happens when people make decisions? An alternative
possibility is that subjects use two separate signals: EV (the mean
reward expected) and an additional variable that encodes the
degree of risk or variance in the distribution of DEMO Neuro-
imaging studies can help to discriminate between these two
different representational forms, even though they offer identical
behavioral predictions in that both can account for risk sensitivity
in behavioral choices equally well. Consistent with the DEMO/
variance alternative, imaging studies support a distinct neural
representation for DEMO Several studies report increased activity in
the anterior insula or lateral orbitofrontal cortex during presen-
tation of risky as opposed to nonrisky gambles (Kuhnen &
Knutson, 2005). These areas are often implicated in aversive
DEMO asrespondingto painoritsanticipation, aswell as
120
Downloaded from cdp.sagepub.com at UNIV OF DEMO LIBRARY on January 3, 2011
Volume 17—Number 2
Beta (Post-Pre)
Beta (Post-Pre)
John P. O’Doherty and Peter Bossaerts
to monetary loss; thus responses to risk in these areas could reﬂect
the negative affective state it engenders. DEMO signals have also
been found in the ﬁring patterns of dopamine neurons corre-
sponding to a ramping (or monotonically increasing) activity in the
DEMO during which a reward outcome is being anticipated
(Fiorillo, Tobler, & Schultz, 2003). Similar signals have been
found in functional magnetic DEMO imaging (fMRI) studies in
target areas of dopamine neurons such as the ventral striatum
(Preuschoff, Bossaerts, & Quartz, 2006; Fig. 2).
These ﬁndings suggest that both EV and risk are indeed en-
DEMO separately, arguing against the possibility that the brain
exclusively encodes representations DEMO EU. Whether a uniﬁed
representation of EU is present in the brain at all remains an
open question. To our knowledge, conclusive evidence for the
existence of such a signal at the neural level (distinct from EV)
has yet to be reported. It should also be noted DEMO EU theory has
long been challenged on the basis of behavioral evidence. For
example, subjects tend to be risk seeking for gambles involving
losses but risk averse for gambles involving gains. Such anom-
alies have led DEMO proposed variations in EU such as prospect
theory (Kahneman & Tversky, 1979). Preliminary evidence that
neural activity during decision making also reﬂects DEMO
asymmetries has emerged (De Martino, Kumaran, Seymour, &
Dolan, 2006; Tom, Fox, Trepel, & Poldrack, 2007).
LEARNING EXPECTED-REWARD SIGNALS
The ﬁnding of EV signals in the brain raises the question DEMO how
such signals are learned in the ﬁrst place. An inﬂuential theory
Fig. 2. Anticipatory risk signals in the human brain. The brain image (A)
shows ramping (monotonically increasing) anticipatory signals in the left
DEMO right ventral striatum (vst) related to the risk or variance in antici-
pated reward. The plot (B) shows fMRI responses in this DEMO as a
function of reward probability, demonstrating a maximal response on
DEMO when reward delivery is least predictable (i.e., with a probability p5
0.5), and a minimal response when the reward delivery is most DEMO
and is either certain to occur or not to occur (p5 DEMO and p5 1, respectively).
The red star shows activation levels DEMO a different anticipation phase in
the trial when risk is always maximal (i.e. when p 5 0.5). Adapted from
‘‘Neural Differentiation of Expected Reward and Risk in Human Sub-
cortical Structures,’’ by K. Preuschoff, P. Bossaerts, & S.R. Quartz,
2006, Neuron, 51, p. DEMO Copyright 2006, Elsevier. Adapted with per-
mission.
from behavioral psychology put DEMO by Rescorla and Wagner
suggests that learning of reward expectancy is mediated by the
degree of surprise engendered when an outcome is presented—
or DEMO precisely, the difference between what is expected and
what is received (Rescorla & Wagner, 1972). Formally this is
called a prediction error, which can take either a positive or
negative sign depending on whether an outcome is greater than
expected (which would lead to a positive error signal) or less than
expected (which would lead to a DEMO error signal). This
prediction error is used to update the expected reward associated
with a particular stimulus or cue in the environment. Prediction-
DEMO signals in the brain were ﬁrst uncovered through observa-
tion of the ﬁring patterns of dopamine neurons in nonhuman
primates during performance of simple DEMO
tasks. These neurons were found to demonstrate a ﬁring pattern
that closely resembles the prediction-error signals present in
temporal-difference learning (Sutton, 1988)—an DEMO of the
Recorla-Wagner model that captures how reward predictions are
formed at different time points within a trial rather than merely on
a trial-by-trial DEMO fMRI studies in humans have found evi-
dence for prediction-error signals in target regions of dopamine
neurons such as the ventral striatum and orbitofrontal DEMO
(O’Doherty, Dayan, Friston, Critchley, & Dolan, 2003).
Analysis of the precise neural mechanisms underlying learn-
ing of reward predictions in DEMO context of the Rescorla-Wagner
model (and its extensions) is still ongoing. The model relies on
a crucial parameter that determines to what extent DEMO
errors inﬂuence future predictions. This parameter should
change with the uncertainty in the environment (Behrens,
Woolrich, Walton, & Rushworth, 2007; Preuschoff & Bossaerts,
2007; Yu & Dayan, 2003). Implementation of DEMO changes
presupposes, among other things, that risk is encoded: If DEMO is
expected to be high, then prediction errors ought not to DEMO
future predictions much—they are predicted to be sizeable
anyway. But note that we already discussed encoding of risk in
the context of choice. This DEMO that such encoding may play
a dual role—namely, to guide choice, and to modulate learning.
ACTION SELECTION FOR REWARD
Once predictions of future DEMO are made, the next step is
to use these predictions to DEMO behavior. More speciﬁcally,
individuals need to bias their action selection to choose those
actions leading to the greatest probability of obtaining future
reward DEMO avoiding punishment.
Reinforcement-Learning Models
Useful insights into how humans or other animals might solve
this problem has come from a branch of computer science DEMO
as reinforcement learning (RL; Sutton & Barto, 1998). Accord-
DEMO to RL, in order to choose optimally between different actions,
DEMO agent needs to maintain internal representations of the
Volume 17—Number 2
Downloaded from cdp.sagepub.com at UNIV OF WATERLOO LIBRARY on January 3, 2011
121
Decision Making
expected reward available on each action and, subsequently,
choose the action with the highest future reward. Also central to
RL algorithms DEMO a prediction-error signal, which is used to learn
and update EVs DEMO each action through experience, just as in the
Rescorla-Wagner model. In DEMO such RL model, called the Ac-
tor/Critic, action selection is conceived as involving two distinct
components: a critic, which learns to DEMO future reward as-
sociated with particular states in the environment, and DEMO actor,
which chooses speciﬁc actions in order to move the agent from
state to state according to a learned policy. Evidence from neu-
DEMO studies supports the presence of an actor/critic-like
architecture in the brain, even though a number of alternative
RL architectures (such as Q-learning, in which action values are
learned directly and not indirectly via a DEMO critic) can
capture human choice behavior equally well. In one study, the
dorsal striatum was found to be speciﬁcally engaged when
subjects were DEMO performing instrumental responses in
order to obtain reward—consistent with a role for this region in
implementing the actor—whereas the ventral striatum was
found to DEMO active in both instrumental and Pavlovian tasks—
consistent with a role for that region in the critic (O’Doherty
et al., 2004). These DEMO suggest a dorsal–ventral distinction
within the striatum whereby the ventral striatum is more con-
cerned with Pavlovian or stimulus–outcome learning while the
dorsal striatum DEMO more engaged during learning of stimulus-
response or stimulus–response–outcome associations.
Limitations of RL Models
When taken together with a host of other ﬁndings from DEMO
and other animals, the data we have discussed suggest that RL
DEMO provide a good account for how people learn to make
choices through experience. Still, such models fail to account
for all aspects of human choice behavior. Simple RL models
assume that information gained about the rewards DEMO from
choosing one action provides no information about the rewards
available from choosing an alternative action. Yet, in many sit-
uations, interdependencies between DEMO actions do exist,
and if subjects can exploit these, greater DEMO will ensue. One
of the simplest examples of a decision task with such an abstract
rule is probabilistic reversal learning. In essence, this is a
2-armed bandit problem in which arms are interchanged at
times. The DEMO in this task is the negative correlation be-
tween the rewards available on the two arms: When choice of one
arm is ‘‘good,’’ the other is ‘‘bad,’’ and vice versa—but after a
time, the contingencies reverse. Evidence from neuroimaging
indicates that EVrepresentations in the ventromedial prefrontal
DEMO incorporate such a task structure, manifested by an in-
ference-mediated change DEMO expectations following a reversal of
stimulus choice (Hampton et al., 2006). These results suggest
that human subjects do indeed incorporate knowledge of DEMO
dependencies when making decisions, providing empirical ev-
idence to support the DEMO of an important extension of
reinforcement learning to human decision making.
122
OUTSTANDING ISSUES
Although decision neuroscience and the related area of neuro-
economics DEMO arguably still in their infancy, we have here
highlighted several cases DEMO which neuroimaging ﬁndings have
informed a theoretical understanding of decision making that
could not be achieved through behavioral methods alone. Many
questions remain unanswered, and space precludes an ex-
haustive consideration of these here. One important DEMO is
whether different neural systems are engaged when explicit
information about the properties of the gamble, such as the
probabilities and magnitudes, is DEMO, compared to when
the relevant information is learned through experience in
DEMO absence of explicit information. Another open question is
how the chosen action comes to be chosen in the ﬁrst place.
Although imaging studies provide DEMO that the brain en-
codes expectations of reward associated with the chosen action,
no study has yet distinguished between regions actively deter-
mining DEMO decision itself from those merely reporting its
consequences.
Recommended Reading
Daw, DEMO, & Doya, K. (2006). The computational neurobiology of
learning DEMO reward. Current Opinion in Neurobiology, 16, 199–
204. A review of the insights into reward learning and decision
making gained from applying formal DEMO models of
learning to neural data.
Knutson, B., & Cooper, DEMO (2005). Functional magnetic resonance
imaging of reward prediction. Current Opinion DEMO Neurology, 18,
411–417. Provides an overview of recent functional neuroimaging
DEMO on reward prediction and details the speciﬁc neural
structures implicated in this function.
Montague, P.R., & Berns, G.S. (2002). Neural economics DEMO the bio-
logical substrates of valuation. Neuron, 36, 265–284. Provides an
accessible overview on the relevance of neural data for under-
standing valuation DEMO in humans and other animals.
Acknowledgments—This work was supported by a Searle
Scholarship and a grant from the Gordon and Betty Moore
foundation to DEMO, and by funds from the Swiss Finance Institute
to PB.
REFERENCES
DEMO, T.E., Woolrich, M.W., Walton, M.E., & Rushworth, M.F.S.
(2007). Learning the value of information in an uncertain world.
Nature DEMO, 10, 1214–1221.
De Martino, B., Kumaran, D., Seymour, DEMO, & Dolan, R.J. (2006).
Frames, biases, and rational DEMO in the human brain.
Science, 313, 684–687.
Fiorillo, C.D., Tobler, P.N., & Schultz, W. (2003). Discrete coding of
reward DEMO and uncertainty by dopamine neurons. Science,
299, 1898–1902.
Downloaded from DEMO at UNIV OF WATERLOO LIBRARY on January 3, 2011
Volume 17—Number DEMO
John P. O’Doherty and Peter Bossaerts
Gottfried, J.A., O’Doherty, J., & Dolan, R.J. (2003). Encoding predictive
reward value in human amygdala and orbitofrontal cortex. Science,
301, 1104–1107.
Hampton, A.N., Bossaerts, DEMO, & O’Doherty, J.P. (2006). The role of the
ventromedial DEMO cortex in abstract state-based inference
during decision making in humans. Journal of Neuroscience, 26,
8360–8367.
Kahneman, D., & Tversky, A. (1979). Prospect theory: An analysis of
decision under risk. Econometrica, 47, 263–292.
Kuhnen, C.M., & Knutson, B. (2005). The neural DEMO of ﬁnancial risk
taking. Neuron, 47, 763–770.
O’Doherty, J., Dayan, P., Friston, K., Critchley, H., & Dolan, R.J. (DEMO).
Temporal difference models and reward-related learning in the
human brain. Neuron, 38, 329–337.
O’Doherty, J., Dayan, P., Schultz, J., DEMO, R., Friston, K.,
& Dolan, R.J. (2004). DEMO roles of ventral and
dorsal striatum in instrumental conditioning. Science, 304, 452–
454.
Preuschoff, K., & Bossaerts, P. (2007). Adding DEMO risk to the
theory of reward learning. Annals of the New York Academy of
Sciences, 1104, 135–146.
Preuschoff, K., Bossaerts, P., & Quartz, S.R. (2006). Neural differen-
tiation of expected reward and risk in human subcortical struc-
tures. Neuron, 51, 381–390.
Rescorla, R.A., & Wagner, A.R. (1972). A theory of Pavlovian condi-
tioning: Variations in the effectiveness of reinforcement and
nonreinforcement. In A.H. Black & W.F. Prokasy (Eds.), Classical
conditioning II: Current research and theory (pp. 64–99). New
York: Appleton Crofts.
Sutton, R.S. (1988). Learning to predict by the methods of temporal
differences. Machine Learning, 3, 9–44.
Sutton, R.S., & Barto, A.G. (1998). Reinforcement learning. Cambridge,
MA: MIT Press.
Tobler, P.N., O’Doherty, J.P., Dolan, R.J., & Schultz, W. (2007). Reward
value coding distinct DEMO risk attitude-related uncertainty coding
in human reward systems. Journal of Neurophysiology, DEMO,
1621–1632.
Tom, S.M., Fox, C.R., Trepel, C., & Poldrack, R.A. (2007). The neural
basis of loss aversion in DEMO under risk. Science, 315,
515–518.
Yu, A.J., & Dayan, P. (2003). Expected and unexpected uncertainty.
Advances in Neural Information Processing Systems, 15, 157–164.
Volume 17—Number 2
Downloaded from cdp.sagepub.com at UNIV DEMO WATERLOO LIBRARY on January 3, 2011
123{1g42fwefx}