A Survey of Partially Observable Markov Decision Processes: Theory, Models, and Algorithms
Author(s): George E. Monahan
Source: Management Science, Vol. DEMO, No. 1 (Jan., 1982), pp. 1-16
Published by: INFORMS
Stable URL: http://www.jstor.org/stable/2631070 .
Accessed: 19/12/DEMO 02:14
Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at .
http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of DEMO provides, in part, that unless
you have obtained prior permission, DEMO may not download an entire issue of a journal or multiple copies of articles, and you
may use content in the JSTOR archive only for your personal, non-commercial use.
Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at .
DEMO://www.jstor.org/action/showPublisher?publisherCode=informs. .
Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on DEMO screen or printed
page of such transmission.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build DEMO a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of DEMO For more information about JSTOR, please contact support@jstor.org.
INFORMS is collaborating DEMO JSTOR to digitize, preserve and extend access to Management Science.
http://www.jstor.org
MANAGEMENT SCIENCE
Vol. 28, No. 1, January 1982
Pr-inited in U.S.A.
DEMO&te the Are
ME
A SURVEY OF PARTIALLY OBSERVABLE MARKOV
DECISION PROCESSES: THEORY, MODELS, AND
ALGORITHMS*
GEORGE E. MONAHANt
This paper surveys models and algorithms dealing with partially observable Markov
decision processes. A partially observable DEMO decision process (POMDP) is a generaliza-
tion of a Markov decision process which permits uncertainty regarding the state of a Markov
process and DEMO for state information acquisition. A general framework for finite state and
action POMDP's is presented. Next, there is a brief discussion of the development of
POMDP's and their relationship with other decision processes. A DEMO range of models in such
areas as quality control, machine maintenance, internal auditing, learning, and optimal
stopping are discussed within the POMDP-framework. DEMO, algorithms for computing
optimal solutions to POMDP's are presented.
(MARKOV DECISION PROCESSES; PARTIALLY OBSERVABLE; SURVEY)
1. Introduction
This paper surveys DEMO and algorithms dealing with partially observable Markov
decision processes (POMDP's)DEMO A POMDP is a generalization of a Markov decision
process (MDP) which permits uncertainty regarding the state of a Markov process and
allows DEMO information acquisition. Howard [25] described movement in an MDP as
a frog in a pond jumping from lily pad to lily pad. Adapting Vazsonyi'DEMO [72] analogy in
a discussion of stochastic automata, we can view DEMO setting of a POMDP as a fog
shrouded lily pond. The frog is no longer certain about which pad it is currently on.
Before DEMO, the frog can obtain information about its current location. In the
DEMO paragraphs we will show that the generalization is nontrivial and admits a
wide range of important decision problems arising in many contexts.
The generalization DEMO MDP's to POMDP's is significant in problem settings where
state uncertainty is a central issue that can not be assumed away. Examples DEMO such
diverse areas as machine maintenance, quality control, learning theory, DEMO
auditing, optimal stopping, and others given in Section 4 illustrate the wide range of
problems that can be modeled as POMDP's. The DEMO feature of all these models is the
presence of state uncertainty and its impact on the optimal choice of actions. It will be
shown DEMO such uncertainty can often have surprising consequences on the structure
of optimal decision rules.
Partially observable models are typically more difficult to analyze than DEMO MDP
counterparts. The added generality is not a free good. In many applications incorpo-
rating the theory of (perfectly observable) MDP's, a primary goal of the analysis is to
determine structural properties of the DEMO policy (and the optimal value function).
(See, e.g., Heyman and Sobel [23, Chapters VI-VII].) Economically appealing assump-
*Accepted by Marcel DEMO Neuts, former Departmental Editor; received November 6, 1980. This paper DEMO
been with the author 1 month for 1 revision.
tGeorgia Institute of Technology.
0025- 1909/82/280 1/000 1 $0 1.25
Copyright @ DEMO, The Institute of Management Sciences
2
GEORGE E. MONAHAN
tions regarding elements of the model are translated DEMO intuitively appealing struc-
tural results. As a classic example, convexity of DEMO one period cost function in a
stochastic multi-period inventory problem with setup costs yields (s, S)-type optimal
ordering policies [28]. In perfectly DEMO machine replacement problems condi-
tions on the one-period transition matrix, equivalent DEMO first-order stochastic domi-
nance, translates into repair policies that are characterized DEMO a single parameter, i.e.,
are of the control-limit type [15].
DEMO POMDP models structural results such as those suggested above are much more
difficult to obtain. All of the intricacies found in perfectly observable sequential
DEMO problems remain. Additional complications are added due to two sources of
potential error in determining the current underlying (core) state. The first is DEMO
uncertainty about the initial state of the unobservable process. The second and most
significant is the possible error in the information regarding the underlying DEMO of the
process. The presence of the two types of uncertainty in the model typically destroys
structural properties. In most applications of POMDP's, structured policies are optimal
only when it is possible to obtain perfect DEMO state information. When there are
imperfect observations the structure of the optimal policy is invariably lost. Examples
of this phenomenon in very simple two-state DEMO are given in ?4.
The lack of structure of many optimal DEMO in POMDP models may not be a
serious operational issue. Of course the use of structural results in algorithms for
computing optimal policies is DEMO out. There may, however, be sub-optimal struc-
tured policies that are "good enough" when balanced against computational effort.
This is an area DEMO further research.
The generalization of MDP's to POMDP's also results in added computational
difficulties. In a finite state MDP, an optimal policy can be expressed in simple tabular
form, listing optimal actions for each state. When state uncertainty is introduced into
the same model, we have a POMDP with an enlarged set of states. The optimal policy
is DEMO defined over a continuum of states. The path-breaking work of Sondik has
mitigated many of the problems inherent in the computation of optimal policies DEMO
POMDP's. His algorithms are discussed in some detail in ?5. DEMO computational
procedures exist for short, finite horizon POMDP's. Less efficient DEMO exist for
infinite horizon problems.
The partially observable Markov decision process is formally presented in ?2. The
main result is summarized as follows: DEMO a partially observable process is not
Markovian (in general), the DEMO can be formulated as a Markov decision process
with an enlarged state space, namely the space of probability distributions over the
underlying (partially DEMO) states. The usual dynamic programming recursions
for both the finite and DEMO infinite horizon models are presented.
A brief history of the development of POMDP's is given in ?3. We also examine the
relationship between POMDP's and stochastic automata, certain Bayesian decision
processes, and various DEMO time, partially observable stochastic processes.
?4 contains a survey of models which either have been or could be cast in the
partially observable DEMO developed in ?2. A detailed description of some
machine replacement/quality DEMO models is given in the first subsection. Applica-
tions of POMDP models in several other contexts, such as accounting, optimal
stopping, and learning are then presented.
In ?5, various computational procedures for solving POMDP'DEMO are discussed. The
preponderance of this section is devoted to the finite and infinite horizon algorithms
(and their variations) developed by Sondik.
Notational DEMO are as follows: I {0, 1,2, ... }, I_ {1, 2, . . . }, Pr{.}
denotes the probability of the event {. IN }, denotes the N-fold Cartesian product of
the real line, 1R, and 6C| denotes the number of elements in DEMO set C~.
PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES 3
2. The Finite State Partially Observable DEMO Decision Process
In this section the finite state and action space version of the partially observable
Markov decision process is presented.
Let X, be a random variable defined on a sample space Q, where t E I; assume X,
takes on values in the finite set OZ-{ 1, . . . , N}. The stochastic process {Xt, t DEMO I},
called the core process, is assumed to be a DEMO state Markov chain with stationary
N x N transition probability matrix P = [pij], i, j E OZ. The core process is complete-
DEMO described by P and the initial distribution over 'X, denoted by 7T (0) =
(1Tl(?), ... . gN (0)), where gi (0) = Pr{X0 = i}, i = 1, . .. , N. The core process is not
directly observable; DEMO is, the realization of X, is not determinable with certainty at
time t.
Associated with Xt is a random variable Y, which takes on values in a finite
"message" space ={ 1, . . ., M}. By observing Yt at time t, information regarding
the DEMO value of X, is obtained. The probabilistic relationship between Xt and DEMO is
known to the decision maker. Suppose that if X, = DEMO, an observation will have message
k with probability qik, i.e.,
Define the N X M information matrix as Q = [qik] & DEMO Z,k E T. The stochastic
process { Yt, t E DEMO } is called the observation process.
A decision structure is now defined which incorporates the core and observation
processes. Assume that the decision maker DEMO control both the observation and core
processes by choosing actions. Let C be a finite set denoting all of the actions available
to the DEMO maker. Let P(a) = [p,(a)] denote the "DEMO of motion" of the core
process when action a EE ( DEMO chosen. That is, if i is the current state and action DEMO is
chosen, the core process moves to a new state j DEMO probability p, (a), i, j EE .
Similarly, let Q(a) = [qij(a)] denote the relationship between the observation and core
processes when a EE e is chosen.
Let m, E 9k. and a, E C denote the value of Y, observed DEMO the action taken at time
t, respectively. The data available for DEMO making at time t is denoted by
dt-=_ (7T(O), DEMO, a,, ... , at 1 Mt).
Define (t) DEMO { Xt = i I dt ) and let
...
7T(t) = [qTl(t),
* ,TN(t)
7T(t) DEMO xS -( X RN 2X = 1, Xi > 05 i = 15 ....* N)
is called the information vector. Using Bayes' formula, the transformation of the
information vector from time t to t + 1 is specified as:
Ti7T(t) Jj5
qij(at)
=
qE
q(at)
l
H.oX
E
,-
Pki(at)7k(t)
kE.
EPkl
k c- L.
(at)
7Tk
where qij(at) and pki(at) are the (i,j)th and (DEMO,i)th elements of Q(a,) and
respectively.
The following result is readily established (e.g., see [9], [65], [68]):
g
(t) summarizes all of the information necessary
for making decisions at DEMO t.
at)--7i(t
+ 1) =
Pr {
Xt+
I
DEMO i i
dt+ =(dt, at,
i-
mt+ =j))
1, . . .,
N,
(2.2)
P(a,),
qik-Pr{Yt=
kjXt= i}
fori&O?,k&3.
(2.1)
Ti
4
GEORGE E. MONAHAN
It is customary to denote by st, say, all the information required for decision making at
time t. Then, DEMO the result cited above, st= '7(t). The following theorem is also
well-known (e.g., see [4], [6], [48], [59], DEMO):
THEOREM 2.1. For any fixed sequence of actions a,, DEMO .. , at E 6, the sequence of
probabilities { (t), t E I } is a Markov process; that is, DEMO r7 C S N' then
Pr{ 7(t + 1) E rJ I v7(t),at) = Pr{v7(t + 1) E rlI 7(t),at}.
(O)5 . . .
,
With these results, the POMDP can be converted into an equivalent (completely
DEMO) Markov decision process.
Note. The core process was defined on a DEMO state space. Since that process is
unobservable, an equivalent observable Markov DEMO is now defined on an uncount-
able state space, namely the (N - 1)-simplex in RN.
For notational convenience, let
at) DEMO(at)
iet kC
-y(7T(t),j, *EPki(at)7Tk(t)-
Then -y(vg(t), j, a,) = Pr { Y,+1 =1 j st = g (t), at}, DEMO is the denominator of (2.2).
Assume that there is a DEMO function, say r: OL x 2 -* lR, where ri(DEMO) is the
immediate expected reward that is earned at time t DEMO the core process is in state i and
action at is taken; the expectation is with respect to the conditional probability
measures associated with the core and observation processes. The immediate reward
could depend upon the DEMO state of the core process, the next state of the core
DEMO (that is, there may be a reward associated with transitions from state i to statej
in the core process), the outcome of DEMO observation, and the action taken. Notation-
ally,
N M
ri(DEMO)= E
j=l
E R(i,j,k,at)pijqjk
k=l
where R : % X > 9T 6% X -x C R is DEMO bounded function, with R(i, j, k, a,) representing
the immediate reward when action at is taken, the core process is in state i, moves to
state], and output k is observed.
DEMO ease of notation let r(at) _ [r,(at), . .. , rN(a,)] and"." denote the usual inner
DEMO operator. Then, if the state of the POMDP is s, and action a, is taken, an
immediate expected reward of
E[ r(DEMO, at) I s, = 7T] = 7T r(a1) (DEMO)
*
is obtained.
A schematic representation of the decision process is given in Figure 1 (cf. Sondik
[65, Figure 2.5]).
Note. DEMO authors view the sequence of events comprising a POMDP slightly
differently. In some models an observation is taken and then the transition to the DEMO
state is made. In this presentation, movement in the chain is DEMO by an observa-
tion. Although the formula for updating the information vector, eqn. (2.2), may be
slightly different, the two views are equivalent.
Let the function at : 5 N -* C denote a DEMO rule which indicates the action,
St(T) E C to DEMO at time t when the current state is st = T. A policy (or strategy) 6, is
defined as a sequence of decision rules, 8 = {d,, . . ., 6 , .. DEMO }. A strategy is said to be
(non-randomized) stationary if it is a single function a: S N 9 , where, for DEMO
T E SNs aT) denotes the action to take when the DEMO of the process is C.
PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES
FIGURE
1. A PartiallyObsaMOVe Marko
in D
DEMO do Qa(AT)
T->| I-T+1I--
I
Using
Theoptimalvalue
q,(DEMO)~~~~~~
cT
5
operatione
givenarkv 8.i
Oupu
FIGURE 1.
A Partially Observable Markov Decision Process.
discounted
Vs (7)
= Es[
I
tr(X= P
a( ,
f So
=a 7l
7T E SN
)
(2.4)
where o
operation given 8.
The optimal value
0 < i
<
is the discount factor, and
Et
denotes the conditional DEMO
function V'a( ) can then be defined as:
V (7T) = SUP V8(7T)5
7TE
N
A strategy 8* is said to be ,8-optimal
if
V:
<*(-7T) ,8
DEMO
all
7T C SN
(.5
The objective of the decision maker DEMO to determine a ,(-optimal strategy.
A well-known result concerning the optimal value function is now presented. Let
T(7T I j, a) DEMO [ T1(7T I j, a), ... , TN(, DEMO j, a)] where Ti(IT I j, a) is DEMO in (2.2).
THEOREM
value function V3 (7T) defined in (2.5) satisfies the following recursion:
(=
Max (7T1
r(a)+
E
V8[T(7T jia)]y(jI7T
for T E SN where DEMO(a) is defined as above.
The finite horizon analog of (2.6)
can be
V
(7T)M-
r(O) aV
V/3
(7T) =Max
T
( r(a) +9/
SE
defined recursively DEMO:
Vn-
l[ T(7TJ|j,a) ] 7(jJ
a)}
DEMO)
(2.6)
Given strategy is employed
infinite
horizon
reward
and
DEMO the POMDP as:
the f
iT
process
starts
at
SN
define the expected
8
2.2. (See, e.g., Ross [52, Theorem 6.1].) The infinite horizon /3-optimal
6
GEORGE E. MONAHAN
where ri(0) is the terminal reward received when the core process is in state i,
i = 1, ... , N. For n E I+, V: (wQ) denotes DEMO maximum discounted expected reward
that can be obtained given that the process is currently in state 7T and there are n
periods remaining before DEMO decision process must end.
3.
Development of POMDP's and Related Literature
The problem of controlling random process (including Markov processes) with
incomplete DEMO information was initially studied by Sirjaev [61] and Dynkin [17].
Wald's [73] pioneering work on sequential sampling may be thought of as a DEMO
type of POMDP. Blackwell [10] developed an entropy measure for a partially observ-
able Markov chain. Drake [16] developed the first explicit POMDP model. DEMO
[68] proved the sufficiency of the information vector for a wide class of stochastic
control problems. About the same time, Astrom [6], [7] DEMO Aoki [4], [5] also
formulated finite horizon POMDP's in the DEMO of stochastic control problems.
Generalizations of their work followed. Sawaragi and Yoshikawa [59] developed the
theory of POMDP's with an uncountable action space DEMO a countable core process
state space. Rhenius [48] considered POMDP's where both the action and core process
state spaces were Borel spaces. Furukawa DEMO also considered a POMDP with an
arbitrary core process state space and a finite action space. Striebel [69] generalized the
Astrom-Aoki control model to DEMO general state and action spaces. Hinderer [24]
studied non-stationary POMDP's which have a more general reward structure than the
model considered in this DEMO In [85], White and Harrington studied the value
function associated with DEMO given (not necessarily optimal) policy, in a POMDP
framework. They DEMO conditions which insured that the value function does not
diminish as observation quality improves. Iosifescu and Mandl [29] and Platzman [43]
developed conditions under DEMO undiscounted infinite horizon POMDP's are well-
defined. Issues dealing with the effect of information acquisition on the conditional
distributions over the core states DEMO studied by Rudemo [54] and Platzman [44].
Kaijser [30] developed conditions which insured that the limiting conditional state
distribution converged to a measure which DEMO independent of the initial state distribu-
tion.
Sondik, in his thesis DEMO and subsequent papers [64], [66], was the first to address
and resolve the computational difficulties associated with POMDP's. His algorithms
for computing DEMO to the finite and infinite horizon discounted problems are
discussed in ?DEMO White [77, 78] generalized the POMDP to allow for a semi-Markov
DEMO process. He extended Sondik's computational procedure to compute policies for
finite horizon POMDP's with a semi-Markov core process.
A number of papers DEMO the literature have explored conditions which insure that
optimal policies have certain structural characteristics, such as monotonicity and/or
control-limit form. Albright [1] presented conditions under which the optimal policy
for a POMDP with a two-state DEMO process would be monotone in the information
vector. White [84] gave conditions which yield monotone optimal policies for finite
horizon POMDP's where there DEMO either perfect observability or no observability. He
then demonstrated how these structural results simplify the computation of the optimal
policy. Other papers dealing with DEMO results in certain machine replacement
problems are discussed in the next section.
The theory of POMDP's is now being used to aid in DEMO solution of non-POMDP
problems. White and Kim [86] developed algorithms for finding the set of all pure,
stationary nonrandomized strategies for vector criterion DEMO's. Hsu and Marcus [26]
studied the problem of the decentralized control of a Markov chain. The movement of
the chain depends upon its DEMO state and the actions of two or more decentralized
PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES
7
decision makers. Each decision maker chooses DEMO action given local information about
the state of the unobservable chain, DEMO agrees to share this information in the next
period. This information pattern is referred to as One Step Delay Sharing. The
problem is formulated DEMO a POMDP, and results from the theory of POMDP's are
DEMO to establish the existence of an optimal stationary policy, and to DEMO
algorithms for computing such policies. White and Schussel [87] used the theory of
POMDP's to compute bounds and sub-optimal policies for multi-module MDP'DEMO (A
multi-module MDP is a system of MDP's that are DEMO together only through the
cost structure.)
One of the main characteristics of the POMDP is the transformation of the
information vector from period DEMO period via Bayes' rule [see (2.2)]. There is a body of
literature dealing with Bayesian control of sequential decision processes which is DEMO
indirectly related to the POMDP's considered in this paper; see DEMO, [21], [39], [49],
[55], [71], and [76]. In DEMO literature, elements of the decision process are unknown.
The decision maker DEMO not know, for example, the transition probability matrix
governing the movement of the process. Information regarding the parameters of the
objects in the DEMO is obtained. In a POMDP, however, all the elements of the
decision process are assumed to be known. Only information regarding the current
DEMO of the unobservable core process is obtained.
There is a literature dealing with the acquisition of information for various continu-
ous time partially observable DEMO processes. The interested reader is directed to
see, for example, [2], [3], [8], and [20].
Brooks and Leondes [12] considered a special type of MDP with one stage
information delay and computed the marginal DEMO associated with the delay. Al-
though this is an MDP with incomplete state information, the form of the information
available permits the problem to be transformed into a (perfectly observable) MDP.
Finally, it should be pointed out that a POMDP is a special case of a stochastic
DEMO machine (SSM) (see, e.g., Paz [40]). Using the DEMO of Section 2, an SSM
is defined as a quadruple, (DEMO, i, 9T, {A (m I where {A (m I DEMO a finite set
containing 10i1 M square matrices each of order N, such that I > 0 for all i
and j and
M
3
m=l
N
3a,(mIa)=1,
j=l
i= 1, .. DEMO,N,
and
A (m Ia) =[a,(m
Ia)].
DEMO SSM is a generalization of the POMDP model presented in ?2, in that a,(m i a)
represents the probability that the core process moves to state j and the message
variable has value DEMO given the core process is currently in state i and action a is taken.
The theory of probabilistic automata (which includes the study of SSM's) has not
yet been specialized to the study of POMDP's. However, Platzman [42], [45] consid-
ered ideas such as state DEMO and state equivalence found in that theory, in his
development of DEMO algorithm to compute approximately optimal policies for infinite
horizon POMDP's. His algorithm is discussed in ?5.
4.
Models Incorporating
the Theory of POMDP's
In this section models incorporating the theory presented in ?2 will be discussed.
Although many of the models presented were not formulated explicitly DEMO POMDP's,
they all deal with the optimal control of a random process based on incomplete
information.
a)}),
a)}
aij(m a)
8
GEORGE E. MONAHAN
A.
POMDP Models of Machine Replacement/ Quality Control
Problems
The quality control models in the literature can be classified on DEMO basis of the
source and degree of partial information. For simplicity, DEMO general version of a
two-state model is presented using the notation of Section 2. By placing restrictions on
the general model, many of the models in the literature can then be discussed. The
restriction to two DEMO states is done for ease of exposition. Many of the models
discussed below were formulated with three or more core states.
The core process DEMO the condition of a machine which is deteriorating over
time. The true condition of the machine is not known with certainty. There are two
DEMO of information regarding the condition of the machine. Firstly, information
can DEMO obtained by observing the machine's output. Secondly, information can be
DEMO by actually inspecting the machine. The observation process of the POMDP
consists of data generated from these sources. The actions available in any period DEMO:
do nothing (perhaps observe the machine's output), inspect DEMO machine or inspect the
machine's output, and repair (replace) DEMO machine.
Let i = { a1, a2, a3)}, where DEMO denotes doing nothing, a2 denotes inspecting, and a3
repairing. The transition and observation matrices are defined for the two-state process
(state 1 "DEMO" condition, state 2 "bad" condition) as:
[ 1-7
DEMO lj1
[
a2
1-
I1a2
1]
0 < y < 1, DEMO 0 < ai < 1,
i
1,
2,
P(2) =
0
1 '
(2)
=[1
' '2
DEMO'
0? v1
1,i = 1,2,
Note. An DEMO matrix with a column of l's denotes an observation process
that provides no information since the message observed is independent of the core
DEMO; the identity matrix denotes perfect information since there is a one-to-one
DEMO between messages and core states.
The identical rows of P(a3) DEMO the possible deterioration of a new machine.
Girshick and Rubin [22] were the first to consider a variation of the problem given
above. If DEMO of the machine's output is costly (or destructive) a rule should
indicate not only when to repair the machine but should also DEMO which items to
inspect. In their model, a, = a2 = 1 (implying no information is available if the "do
nothing" action is selected), and v1 = v2 = 1 (implying perfect information is available
if the "inspection" action is selected). Conjectures they DEMO concerning the form of
the optimal maintenance policy were shown to be false (via a counter-example) by
Taylor [70], who considered a replacement model in a general setting.
Klein [33] also considered a variation of DEMO non-100% inspection case. His approach
was somewhat different in that a new completely observable problem was formulated
which modeled periodic inspection. Decisions were of DEMO form "repair now, but do
not inspect in the next m periods."
Based on the completely observable model of (aip Derman [15] = a2= 1, a2 not
permitted), Eckles [18] and Ross [53] formulated POMDP's for a problem similar to
PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES
9
Optimal
Action:
0
Al
7T
DEMO
FIGURE
2.
Al
A3
7T2
T3
1
Ross' Two-State Optimal Inspection/DEMO
Policy.
XT
=PR(X = 2)
the non- 100% inspection problem. In terms of the general model, all three actions are
permitted, DEMO a, = a2 = PI = v= 1 (no information is available if action a, is taken,
perfect information regarding the true condition of the machine is available if action a2
is taken).
DEMO [53] characterized the optimal inspection/maintenance policy. In particular, he
showed DEMO for the two-state core process, the optimal policy has at most DEMO regions.
(See Figure 2). Ross also gave conditions on the DEMO which would insure that
7- = 7T2, i.e., that inspecting would never be optimal.
Ehrenfeld [19] also examined various aspects of the Derman-Ross DEMO He
considered the possibility that inspection is not perfect (pi #& 1, i = 1, 2) but was unable
to establish conditions that would insure a well-structured policy.
White [80, 81] studied a problem which is very similar to the general maintenance
problem. As a special DEMO he considered a model where v1 = v2 = 1 but aoi #7 1, i = 1, 2,
thus generalizing Ross' model. He proved that the optimal policy has the form
depicted in Figure DEMO The general partially observable model (pi # 1, ai i, DEMO = 1, 2)
was also discussed. However, as in the Ehrenfeld paper, the characterization of the
optimal policy remained an open question.
Conditions that guaranteed optimal control-limit policies for a model with imperfect
observability DEMO ultimately introduced by White [83] and represent a significant
contribution to the literature. The new condition that was required is difficult to
interpret. The DEMO of the control-limit structure used Porteus' [47] results on
the optimality DEMO structured policies in sequential decision problems. White [82] also
demonstrated the optimality of structured policies for the special cases of perfect and
no observability DEMO a machine replacement setting.
Rosenfield [50], [51] considered yet another variation DEMO the general model. He
defined a process which has a state space consisting of pairs of nonnegative integers
denoted by (i, k) where i is the condition of the machine and k is the number DEMO
periods which have elapsed since it was known for certain that the machine was in
state i (and hence is somewhat similar to the Klein model discussed earlier). Rosenfield
proved that an optimal maintenance policy DEMO monotonic in the following sense: the
optimal policy is defined by DEMO numbers k*(i), i = 1, . .. , N, which are
nondecreasing in i, where, for each state (i, DEMO), repair is done only if k > k*(i).
Wang [74], [75] discussed various forms of the general model (all of DEMO precluding
any form of inspection action a2) under weaker conditions on DEMO parameters. He
proved that a control-limit-type policy is still optimal. A procedure he used to compute
such a policy is discussed in the next DEMO
Pierskalla and Voelker [41] give an excellent review of maintenance models which
includes a section on models with incomplete information.
B.
Other POMDP Models
DEMO [31] applied the basic results of the machine inspection/replacement prob-
lem to a cost control problem in accounting. He assumed that an operating DEMO of
a firm can be in one of two states: state DEMO indicates that the costs incurred by the
segment are "in control," meaning that management can not affect (reduce) the costs;
10
GEORGE E. MONAHAN
state 2 indicates that costs being incurred are "out of control"-management action
can be taken to reduce costs. Management DEMO only two alternatives: it can do nothing
or it can take DEMO action at some cost. As in the analogous machine replacement
problem, DEMO optimal policy was shown to be of the control-limit type.
Analogously, DEMO [27] modeled the internal control of a corporate control system
as a POMDP. Using Ross' [53] quality control model, Hughes viewed the core DEMO
as the level of effectiveness of internal control. Information pertaining to the effective-
ness of control can be obtained through an internal audit. The DEMO available are:
do nothing, audit (corresponding to inspecting in the Ross model), and restore
(analogous to the replace action in the quality control context). Hughes did not
develop any new alternative results DEMO the POMDP used in this audit-timing context.
Karush and Dear [32] formulated a dynamic programming model of a learning
process which can be classified DEMO a POMDP. A subject is to be taught m items in the
course of n trials. The subject is assumed to be either conditioned (C) or uncondi-
tioned (U) with respect to each of the m items; the state space of the core process
consists of the 2' m-vectors representing all of the combinations of m-tuples of U's
and C's and is unobservable. An action consists of presenting item DEMO to the subject on
trial k, k = 1, . . . , n. The observation process consists of responses of the subject
DEMO presented with an item or a trial. The state of the POMDP is A = (I, .. ., An)
where Ai is the (posterior) probability that the subject is conditioned to item i,DEMO
i = 1, . . ., m. The objective is to maximize a terminal reward which is a function of A.
Karush and DEMO proved that the following myopic policy is optimal: at trial k, given
the current POMDP-state vector Ak = (I<, . ., DEMO) present item i* where A*
= Mini Aik; that is present the item which currently has the highest probability of being
conditioned.
Smallwood DEMO also developed a simple two core state POMDP model of optimal
teaching strategies. Core states represent the student's state of knowledge. One teach-
DEMO strategy presents the stimulus along with the correct response. This causes the
student to move from the no-knowledge state to the knowledge state with DEMO known
probability. A second teaching strategy presents the stimulus and requires a response
from the student. The response, of course, may or may DEMO reflect the true state of
knowledge of the student. Given costs per trial for each strategy and given a terminal
cost for ending the DEMO session with the student in the no-knowledge state,
Smallwood proved the piecewise-linearity of the infinite horizon value function and
described methods for computing DEMO optimal teaching strategy.
Pollock [46] developed a two-state core process POMDP to model optimal search
effort for an object that moves between two regions. DEMO each move, the decision
maker can choose which region to examine DEMO order to locate the object. Under various
conditions that correspond to forms of perfect observability, he proved that the
optimal search rules had special structure. Special structure could not be obtained for
general parameter values.
Smallwood, et al. [63] used POMDP concepts in the development of methodology
for DEMO analysis of health-care systems. They defined (unobservable) patient states and
related observable states (symptoms, diagnostic data, etc.). Physician states correspond
to states in the POMDP, that is, they are physician-specific distributions over DEMO
states. Different patient state-physician state pairs are defined for various problems
arising in the health-care area, including the design of individual medical facilities,
regional health systems, and the funding of health-service programs.
Given a simple network consisting of two (geographically separated) computers,
Segall [60] studied DEMO problem of where to locate a common data file. He assumed
that the demand rate for the data at one of the computer sites DEMO an unobservable
finite-state Markov chain, while the demand rate at the DEMO site is a deterministic
function of time. Given costs of data storage at each site and transmission costs, a
PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES
11
Optimal
Action:
C
7T
0
DEMO
C
7T2
T3
T
C
|XS
T4
T5
FIGURE 3.
An Optimal Policy, 8*(7T).
X = PR(XT = 1)
POMDP model was formulated to determine the optimal file location over time.
Structural DEMO of the optimal policy were not examined.
In another computer network context, Platzman [42] formulated the slotted ALOHA
problem (see [34], [35]) DEMO a POMDP. In this problem, remote terminals communicate
with a central DEMO via a channel that can carry at most one message per time
interval. If two or more terminals attempt to transmit messages simultaneously, none
of the messages are transmitted. Individual terminals must decide when to transmit
DEMO on imperfect knowledge of the status of other terminals on the network.
Observations available for decision making in each time interval in this context DEMO the
outcomes of a transmission attempt.
White [79] applied the theory of POMDP's to design questionnaires in situations
where responses may not be DEMO
Monahan [37] formulated a discrete-time problem of stopping in a partially observ-
able binary-valued Markov chain as a POMDP. In this context, X, DEMO {0, 1 } represents
the reward received by the decision maker DEMO the process is stopped at time t. Before
deciding to stop or continue, the decision maker can sequentially purchase additional
information regarding the value of Xt. The actions that are available are S, T, and DEMO
which denote "stop", "test" (purchase more information), and "continue" (forego X,
and consider Xt+ ). In [37] DEMO was shown that the optimal policy may be highly
unstructured. Let g = Pr { Xt = 1 } denote the probability that the DEMO at t is "good"
and let 8*(Qy) denote the optimal policy. It is possible for 8*(Qy) to have the form
depicted in Figure 3 when the information regarding the current reward is DEMO
However, when the information indicates the core process state without error, 8*(,g)
has the form depicted in Figure 2; see DEMO This is another example of a model where
structural properties of the optimal policy can be determined only when perfect state
information is available.
DEMO, we point out that there are other extensive classes of models DEMO can be
classified as POMDP's but will not be discussed here. They include models of search
for a hidden object (discrete search models-see, e.g., Stone [67]), sequential sampling
problems (see DeGroot [14] and references therein), and two- and multi-armed bandit
problems (also DeGroot [14]).
5. Algorithms
for Solving Partially Observable
Processes
In this section computational DEMO for solving POMDP's are discussed. The
most significant work in this area has been done by Sondik [65], who developed
Howard-like value-determination, DEMO algorithms for solving both
finite and infinite horizon POMDP's. Two of Sondik's algorithms are described in
general terms below. A modification of DEMO finite horizon algorithm due to White [84]
is also presented. Other computational procedures used to solve particular POMDP
models are then mentioned.
A.
Sondik'DEMO "One-Pass" Algorithm
The Sondik "one-pass" algorithm ([64], [65]) DEMO used to compute the optimal policy
and value function for finite horizon POMDP's. The one-pass algorithm exploits the
structure of the finite horizon DEMO value function Vn (.) given in (2.7). It is
12
GEORGE E. MONAHAN
straightforward to show that V'( (.) DEMO piecewise-linear and convex in its argument.
Using the notation of ?2, let
AO= {r(0)},
A
= a : a = DEMO(a) + E
P(a) Qk
(a)ak, a
kE Anl,aE},
where Qk(a) is the N x N diagonal matrix formed from the kth column of Q(a); that
is, Q k(a) = Qik(a), i E X and k E t. Then
Vn
(q7)
= Max (7T oa
:
a
E An)A}.
(5.1)
For any n E I, DEMO is a finite set. However some of the a-coefficients in the set may be
dominated by others and can be removed. To find the DEMO set of a-coefficients
that define Vn (* ), solve the following linear program for each a E An:
Min (x -
Ta: x > 7
a',a' o
E A,
E
If x #' 0, remove a from An.
The optimal strategy when DEMO current state is g and there are n periods remaining is
now easily determined: find the index j* that maximizes 7T aj, aj DEMO An. Then 8*(,W)
aj*.
White [84] modified the one-pass DEMO to exploit known structural properties of
the optimal policy for certain classes of POMDP's, thus making the algorithm more
efficient. The modified algorithm restricts the space of feasible policies to those which
are (in some sense) isotone (monotonically nondecreasing). Additional structure is also
placed on DEMO POMDP. The core process state space is assumed to be a partially
ordered space (n, < 4) where n is countable. The order relation < n induces a partial
order on SN' denoted <,; i.e., (SN5 <,) is a partially ordered space. The action DEMO
e, is assumed to be a finite linearly (totally) ordered DEMO (e, < A). If 7T1, 7T2 E SN are
DEMO that 7T < 7T2 implies 8(7T1) < ASa7T2), then DEMO( * ) is called an isotone policy. Condi-
tions which guarantee DEMO isotone policies also insure that the optimal infinite
horizon value functions are monotone; i.e., for qT1' 2 E SN if 7TI <1T25 DEMO V' (1i1)
< Vn O92), n = 0, DEMO .... The White modification of the one-pass algorithm uses the
added structure in two ways. Firstly, regions of the state space over which the optimal
value function is linear can be enlarged. Secondly, using the relation < A eliminates
the need to calculate boundaries which will ultimately never DEMO used to identify subsets
over which a particular action is optimal.
B.
Sondik's Discounted,
Infinite Horizon Algorithm
Sondik [66] developed a Howard-like DEMO iteration algorithm to compute E-
optimal policies for the infinite horizon POMDP. Finitely-transient (f.t.) policies play a
fundamental role in the algorithm. In DEMO terms a policy is f.t. if after a finite
number of transitions the resulting state is not one at which the policy is discontinu-
DEMO, independent of both the current state and the sequences of messages DEMO
The significance of f.t. policies is that the infinite horizon discounted value function
associated with such a policy is piecewise-linear and the optimal infinite DEMO policy
can be computed with the one-pass algorithm in a finite number of iterations.
Therefore, as in the one-pass algorithm, piecewise-linearity of the DEMO function
permits the computation of both the optimal value function and optimal policy.
Unfortunately, not all stationary policies are f.t. Sondik [66] defined an approxima-
tion so that all stationary policies are (almost) f. t. DEMO result is an approximate value
function that is piecewise-linear.
The Sondik algorithm is a policy iteration algorithm which uses the one-pass
algorithm in the DEMO step.
PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES 13
C. Platzman's Algorithm for Computing DEMO Infinite Horizon Policies
Platzman [45] developed an algorithm for computing approximately optimal policies
for infinite horizon POMDP's. His work was motivated by the DEMO consider-
ations. In Sondik's one-pass algorithm, the number of elements DEMO AZ, explodes as n
tends to infinity. Of course, when a policy is finitely-transient, the number of elements
in An is finite for all n. However, since finite transience may be difficult to verify in
practice, Platzman exploited an idea attributed to Drake [16] to insure finiteness of A,
for all n. The decision maker is restricted to DEMO of only a finite number of
the most recent observations and actions when choosing an action. The notion that
so-called finite memory policies may DEMO adequate in many decision making contexts is
a prime motivation in Platzman's algorithm. A finite-memory, randomized strategy is
selected which indicates the action to take and specifies the next memory state as a
function of DEMO current memory state. Thus the decision maker is modeled as a
probabilistic automaton, or equivalently, another POMDP. Selecting the optimal
strategy amounts to DEMO a finite-dimensional nonlinear program. Performance
bounds are given which indicate how close the current solution is to the global
optimum.
D. Other Computational Procedures
DEMO and Lave [56] developed an implicit enumeration algorithm for computing
e-optimal solutions to the finite horizon POMDP in a finite number of iterations. They
DEMO briefly discussed using the control-limit structure of the optimal policy in the
Girshick-Rubin machine replacement problem. They report a computation time of 110
seconds DEMO to determine the control-limit for a two state, two action problem,DEMO
which would seem to indicate that the algorithm is not very efficient.
Wang [74], [75] developed a special purpose computational procedure for determin-
ing optimal policies for certain finite-state machine replacement problems. The models
he considered DEMO for only two actions (do nothing, replace); inspection is not
allowed. The procedure described appears to be quite efficient for solving these DEMO
problems with more than two unobservable states.
Buckman and Miller [13] presented an algorithm for solving Kaplan's [31] optimal
investigation problem discussed in ?4. They formulated the problem as a regenerative
stopping problem [11] and DEMO structural properties to improve the computation
of the optimal policy. Miller [36] used similar techniques to compute solutions to the
Rosenfield [50], [51]-type maintenance model discussed in ?4.
Sawaki [57] and Sawaki and Ichikawa [58] pointed out the rather obvious fact that
the optimal infinite horizon discounted POMDP DEMO function can be approximated
arbitrarily closely by a (sufficiently large) finite horizon POMDP which is piecewise-
linear and admits piecewise-constant optimal policies. The DEMO significance of
their results for efficiently computing optimal policies is dubious since the number of
linear segments can very quickly exceed the storage capacity DEMO any existing com-
1
'The author would like to thank Loren DEMO Platzman for his many helpful comments on an earlier version
of this paper and for suggesting some of the applications and references.
References
1. DEMO, S., "Structural Results for Partially Observable Markov Decision Processes," DEMO
Res., Vol. 27 (1979), PP. 1041-1053.
2. ANDERSON, R. DEMO AND FRIEDMAN, A., "Optimal Inspections in a Stochastic Control Problem DEMO
Costly Observations," Math. Operations Res., Vol. 2 (1977), pp. 155-190.
3. AND , "Optimal Inspections in a Stochastic Control Problem with Costly Observations,
II," Math. Operations Res., Vol. 3 (1978), pp. 67-81.
puter.
14
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
DEMO
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.
32.
33.
34.
35.
36.
GEORGE E. MONAHAN
DEMO, M., "Optimal Control of Partially Observable Markovian Control Systems," DEMO Franklin Inst.,
Vol. 280 (1965), pp. 367-386.
, Optimization DEMO Stochastic Systems, Academic Press, New York, 1967.
ASTROM, K., "Optimal Control of Markov Processes with Incomplete State Information," J. of DEMO
Analysis and Appl., Vol. 10 (1965), pp. 174-205.
, "DEMO Control of Markov Processes with Incomplete State Information, II. The Convexity
DEMO the Loss Function," J. of Math. Analysis and Appl., Vol. DEMO (1969), pp. 403-406.
BATHER, J., "An Optimal Stopping Problem with Costly Information," Bull. Inst. Internat. Statist., Vol.
45, Book DEMO (1973), pp. 9-24.
BERTSEKAS, D., Dynamic Programming and Stochastic DEMO, Academic Press, New York, 1976.
BLACKWELL, D., "The Entropy of Functions of Finite-State Markov Chains," in Information Theory,
Statistical DEMO Functions, Random Processes: Transactions of the First Prague Conference, 1956,DEMO
Publishing House of Czechosolvak Academy of Sciences, Prague, 1957.
BREIMAN, DEMO, "Stopping-Rule Problems," Chapt. 10 in Applied Combinatorial Mathematics, E. DEMO
back, ed., Wiley, New York, 1964.
BROOKS, D. AND DEMO, C., "Markov Decision Processes with State-Information Lag," Operations
Res., Vol. 20 (1972), pp. 904-907.
BUCKMAN, A. G. AND MILLER, B. L., "Optimal Investigation as a Regenerative Stopping Problem,"
Western Management Sci. Instit., UCLA, Working Paper No. 289, 1979.
DEGROOT, DEMO, Optimal Statistical Decisions, McGraw-Hill, New York, 1970.
DERMAN, C., "Optimal Replacement Rules when Changes of State are Markovian," in Mathematical
Optimization Techniques, R. Bellman, ed., Univ. of California Press, Berkeley, Calif., 1963.
DRAKE, A., Observation of a Markov Process Through a Noisy Channel, unpublished Sc.D. Thesis, Dept.
of Electrical Engineering, Massachusetts Institute of Technology, Cambridge, Mass., 1962.
DYNKIN, E., "Controlled Random DEMO," Theory Probability Appl., Vol. 10 (1965), pp. 1-14.
ECKLES, J., "Optimum Maintenance with Incomplete Information," Operations Res., Vol. DEMO (1968), pp.
1058-1067.
EHRENFELD, S., "On a Sequential Markovian Decision Procedure with Incomplete Information,"
Comput. and Operations Res., Vol. DEMO (1976), pp. 39-48.
FRIEDMAN, A., "Optimal Stopping for Random Evolution of Multidimensional Poisson Processes with
Partial Information," in Stochastic Analysis, Friedman, A., and Pinskey, M., eds., Academic Press,
New York, 1978.
FURUKAWA, N., "A Bayes Controlled Process," Mem. DEMO Sci., Kyushu Univ. Ser. A, Vol. 21 (1968), pp.
DEMO
GIRSHICK, M. AND RUBIN, H., "A Bayes' Approach to DEMO Quality Control Model," Ann. Math. Stat.,
Vol. 23 (1952), pp. 114-125.
HEYMAN, D. AND SOBEL, M., Stochastic Models in DEMO Research, Vol. II: Stochastic Optimization,
McGraw-Hill, New York (forthcoming).
HINDERER, K., Foundations of Non-Stationary Dynamic Programming with Discrete Time DEMO,
Springer-Verlag, Berlin, 1970.
HOWARD, R., Dynamic Programming and Markov Processes, The M.I.T. Press, Cambridge, Mass., 1960.
Hsu, K. AND MARCUS, S., "Decentralized Control of Finite State Markov Processes," Proceedings 19th
IEEE Conf: Dec. and Control, Dec. 1980, pp. 143-148.
HUGHES, J., "Optimal Internal Audit Timing," Accounting Rev., Vol. LII (1977), pp. 56-58.
IGLEHART, D., "Optimality of (s, S) Policies in the Infinite Horizon Dynamic Inventory Problem,"
Management Sci., Vol. 9 (1963), pp. 259-267.
IOSIFESCU, M. AND MANDL, P., "Application Des Systemes a Liaisions Completes a un Probleme de
Reglage," Rev. Roum. Math. Pures et Appl., Vol. XI (1966), pp. 533-539.
KAIJSER, J., "A Limit Theorem for Partially Observed Markov Chains," Ann. Probability, Vol. 3 (1975),
pp. 677-696.
KAPLAN, DEMO, "Optimal Investigation Strategies with Imperfect Information," J. Accounting Res., DEMO 7
(1969), pp. 32-43.
KARUSH, W. AND DEAR, R., "Optimal Strategy for Item Presentation in Learning Models," Manage-
ment Sci., Vol. 13 (1967), pp. 773-785.
KLEIN, M., "Inspection-Maintenance-Replacement Schedules Under Markovian Deterioration," Man-
agement Sci., Vol. 9 (1962), DEMO 25-32.
KLEINROCK, L. AND LAM, S., "Packet Switching in a Multiaccess Broadcast Channel: Performance
Evaluation," IEEE Trans. Comm., Vol. COM-23 (1975), pp. 410-423.
LAM, S. AND KLEINROCK, L., "Packet DEMO in a Multiaccess Broadcast Channel: Dynamic Control
Procedures," IEEE Trans. DEMO, Vol. COM-23 (1975), pp. 891-904.
MILLER, B. L., "DEMO State Average Cost Regenerative Stopping Problems," Western Manage-
ment Sci. Instit., UCLA, Working Paper No. 288 (1979).
PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES
15
37. MONAHAN, G. E., "Optimal Stopping in a Partially Observable Markov Process with Costly Informa-
tion," DEMO Res., Vol. 28 (1980), pp. 1319-1334.
38. , "Optimal DEMO in a Partially Observable, Binary-valued Markov Chain with Perfect,
Costly DEMO," (forthcoming) J. Appl. Probability, Vol. 19 (1982).
39. NAHMIAS, S., "A Sequential Decision Problem with Partial Information," Cahiers du Cente d'Etudes de
Recherche Operationnelle, Vol. 17 (1975), DEMO 53-64.
40. PAZ, A., Introduction to Probabilistic Automata, Academic Press, New York, 1971.
41. PIERSKALLA, W. AND VOELKER, J., "A Survey of Maintenance Models: The Control and Surveillance of
Deteriorating Systems," Naval Res. Logist. Quart., Vol. 23 (1976), pp. 353-388.
42. DEMO, L., Finite-Memory Estimation and Control of Finite Probabilistic Systems, unpublished DEMO
thesis, Department of Electrical Engineering and Computer Science, M.I.T.; also DEMO Electronic
Systems Laboratory Technical Report ESL-R-723, Cambridge, Mass., 1977.
43. , "Optimal Infinite-Horizon Undiscounted Control of Finite Probabilistic Systems," SIAM J.
DEMO and Optimization, Vol. 18 (1980), pp. 362-380.
44. , "DEMO of Partially-Observed Markov Chains: Decomposition, Convergence, and
Component Identification," DEMO, March, 1978.
45. , "A Feasible Computational Approach to Infinite-Horizon DEMO Markov Deci-
sion Problems," mimeograph, School of Industrial and Systems DEMO, Georgia Institute of
Technology, Atlanta, Ga., January, 1981.
46. DEMO, S., "A Simple Model of Search for a Moving Target," Operations Res., Vol. 18 (1970), pp.
883-903.
47. PORTEUS, DEMO, "On the Optimality of Structured Policies in Countable Stage Decision Processes,"
Management Sci., Vol. 22 (1975), pp. 148-157.
48. RHENIUS, D., "Incomplete Information in Markovian Decision Models," Ann. Statist., DEMO 2 (1974),
pp. 1327-1334.
49. RIEDER, U., "Bayesian DEMO Programming," Advances Appl. Probability, Vol. 7 (1975), pp. 720-736.
50. ROSENFIELD, D., "Markovian Deterioration with Uncertain Information," Operations Res., Vol. 24
(1976), pp. 141-155.
51. , "Markovian Deterioration with Uncertain Information-A More General Model," Naval Res.
Logist. Quart., Vol. 23 (1976), pp. 389-406.
52. Ross, S., Applied Probability Models with Optimization Applications, Holden-Day, San Francisco, Calif.,
1970.
53. , "DEMO Control Under Markovian Deterioration," Management Sci., Vol. 17 (1971), pp.
587-596.
54. RUDEMO, M., "State Estimation for Partially Observed Markov Chains," J. Math. Anal. Appl., Vol. 44
(1973), 581-611.
DEMO SATIA, J. AND LAVE, R., "Markovian Decision Processes with Uncertain Transition Probabilities,"
Operations Res., Vol. 21 (1973), pp. 728-740.
56. AND , "Markovian Decision Processes with Probabilistic Observation of States," Manage-
ment Sci., Vol. 20 (1973), pp. 1-13.
57. SAWAKI, K., "Piecewise-Linear Markov Decision Processes with an Application to Partially Observable
DEMO," in Hartley, R. et al., eds., Recent Advances in DEMO Decision Processes, Academic Press,
New York, 1980, pp. 245-260.
DEMO AND ICHIKAWA, A., "Optimal Control for Partially Observable Markov Decision DEMO Over
an Infinite Horizon," J. Operations Res. Soc. Japan, Vol. DEMO (1978), pp. 1-15.
59. SAWARAGI, Y. AND YOSHIKAWA, T., "Discrete-Time Markovian Decision Processes with Incomplete
State Observation," Ann. Math. Statist., Vol. 41 (1970), pp. 78-86.
60. SEGALL, A., "Dynamic File Assignment in a Computer Network," IEEE Trans. Auto. Control, Vol.
AC-21 (1976), pp. 161-173.
61. SIRJAEV, A., "On the DEMO of Decision Functions and Control of a Process of Observation Based on
Incomplete Information," Selected Translations in Math. Stat. and Prob., Vol. 6 (1966), pp. 162-188.
62. SMALLWOOD, R., "The Analysis of DEMO Teact ing Strategies for a Simple Learning Model," J.
Math. Psych., Vol. 8 (1971), pp. 285-301.
63. , SONDIK, E. AND OFFENSEND, F., "Toward an Integrated Methodology for the Analysis of
Health-Care Systems," Operations Res., Vol. 19 (1971), pp. 1300-1322.
64. DEMO , "The Optimal Control of Partially Observable Markov Processes over a DEMO
Horizon," Operations Res., Vol. 21 (1973), pp. 1071-1088.
65. SONDIK, E., The Optimal Control of Partially Observable Markov Processes, unpublished Ph.D. disserta-
tion, Stanford University, 1971.
66. , "The Optimal Control of Partially Observable Markov Processes Over the Infinite Horizon:
Discounted Costs," Operations Res., Vol. 26 (1978), pp. 282-304.
67. STONE, DEMO, Theory of Optimal Search, Academic Press, New York, 1975.
16
68.
69.
70.
71.
72.
73.
74.
75.
76.
77.
78.
DEMO
80.
81.
82.
83.
84.
85.
86.
87.
GEORGE E. MONAHAN
STRIEBEL, C., "Sufficient Statistics in the Control of Stochastic Systems," J. Math. Anal. Appl., Vol. 12
(1965), pp. 576-592.
, Optimal DEMO of Discrete Time Stochastic Systems, Springer-Verlag, Berlin, 1975.
TAYLOR, H., "Markovian Sequential Replacement Processes," Ann. Math. Statist., Vol. 38 (DEMO), pp.
871-890.
VAN HEE, K., "Bayesian Control of Markov DEMO," Mathematical Centre Tract 95, Amsterdam, The
Netherlands, 1978.
VAZSONYI, "Information Systems in Management Science-The Use of Mathematics for Manage-
ment Information Systems II," Interfaces, Vol. 6, (1976), pp. 42-46.
WALD, A., Sequential Analysis, Wiley, New York, 1947; republished by DEMO, New York, 1973.
WANG, R., "Computing Optimal Quality Control DEMO Actions," J. Appl. Probability, Vol. 13
(1976), pp. 826-832.
, "Optimal Replacement Policy Under Unobservable States," J. Appl. Probability, DEMO 14
(1977), pp. 340-348.
WESSELS, Decision Rules in Markovian Decision Processes with Incompletely Known Transition
Probabilities, dissertation, University of Technology, Eindhoven, 1968.
WHITE, C., "Cost Equality and Inequality Results for a DEMO Observed Stochastic Optimization
Problem," IEEE Trans. Systems, Man, and Cybernetics, Vol. SMC-5 (1975), pp. 576-582.
, "Procedures for the Solution of a Finite-Horizon, Partially Observed, Semi-Markov Optimiza-
tion Problem," Operations DEMO, Vol. 24 (1976), pp. 348-358.
, "Optimal Diagnostic Questionnaries DEMO Allow Less Than Truthful Responses," Information
and Control, Vol. 32 (1976), pp. 61-74.
, "A Markov Quality Control Process Subject to Partial Observation," Management Sci., Vol. 23
(1977), pp. 843-852.
, "Optimal Inspection and Repair of a Production Process Subject to Deterioration," J.
Operational Res. Soc., Vol. 29 (1978), pp. 235-243.
, "Bounds on Optimal Cost for a Replacement
Problem with Partial Observation,"DEMO
Naval Res.
A.,
J.,
Logist. Quart., Vol. 26 (1979), pp. 415-422.
, "Optimal Control-Limit Strategies for a Partially Observed Replacement Problem," Internat. J.
Systems Science, Vol. 10 (1979), pp. DEMO
, "Monotone Control Laws for Noisy, Countable-State Markov Chains," European J. Opera-
tional Res., Vol. 5 (1980), pp. 124-132.
AND DEMO, D., "Application of Jensen's Inequality for Adaptive Suboptimal Design," J.
Opt. Theoty and Appl., Vol. 32 (1980), pp. 89-100.
AND KIM, K., "Solution Procedures for Solving Vector Criterion Markov Decision Processes,"
J. Large-Scale Systems, Vol. 1 (1980), pp. 129-140.
AND SCHUSSEL, "Suboptimal Design for Large-Scale, Multi-Module Systems," Operations
Res. (to appear).
K.,{1g42fwefx}