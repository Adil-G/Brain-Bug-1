-
RO
_**J4
-H
ONR - Technical Report 88-1
A Componential Analysis of
Cognitive Effort in DEMO
lames R. Bettmnin,
Eric J. ?ohnson 2
and John W. DEMO
September, 1988
2S
1
Center for Decision Studies
Fuqua School of DEMO
Duke University-
2 Wharton School
University of Pennsylvania
Sponsored by:
Perceptual S&'ences Programs-
Office of Naval Research
Contract Number N00014-80-C-00114
Approved for Public Release: Distribution Unlimited
In Press:
Organizational Behavior and Human Decision Processes
88~
-
SE0
SP19 1988.-
3
'
SECURITY CLASSIFICATION OF THIS PAGE
REPORT
Ia REPORT SECURITY CLASSIFICATION
UNCLASSIFIED
2a DEMO CLASSIFICATION AUTHORITY
2b DECLASSIFICATION/ DOWNGRADING SCHEDULE
4. PERFORMING ORGANIZATION REPORT NUMBER(DEMO)
DOCUMENTATION PAGE
iVt, 9Y 8 7
lb RESTRICTIVE MARKINGS
3 DEMO/AVAILABILITY
OF REPORT
RELEASE:
DISTRIBUTION
APPROVED FOR PUBLIC
UNLIMITED
5 MONITORING ORGANIZATION REPORT NUMBER(S)
ONR Tech Report 88-1
6a NAME OF DEMO ORGANIZATION
Duke University
6c. ADDRESS (City, State, and ZIPCode)
DEMO School of Business
Duke University
Durham. NC 27706
Ba. NAME OF FUNDINGISPONSORING
ORGANIZATION
Office of Naval Olesearch
ADDRESS (City, State, and ZIP Code)
8c.
Arlington,
VA
22117-5000
11
TITLE (Include Security Classification)
6b OFFICE SYMBOL
(if applicable)
8b OFFICE SYMBOL
(If applicable)DEMO
Code 1142PS
N/A
7a NAME OF MONITORING ORGANIZATION
Office of Naval Research
7b ADDRESS (City, State, and ZIP Code)
Arlington, DEMO 22217-5000
9 PROCUREMENT INSTRUMENT IDENTIFICATION NUMBER
Contract N00014-80-C-0114
10 SOURCE OF FUNDING NUMBERS
PROGRAM PROJECT TASK
ELEMENT NO NO NO
61153N RR-042-09
4425
WORK DEMO
ACCESSION NO
R&T 4425063
A Componential Analysis of Cognitive Effort in Choice
12
PERSONAL AUTHOR(S)
Bettman, J.R., Johnson, E.J., & Payne, J.W.
13a TYPE OF
REPORTe h
I
13b TIME COVERED
DEMO
TO
_
Research
16 SUPPLEMENTARY NOTATION
14. DATE OF REPORT (Year DEMO, Day)
I September, 1989
15
PAGE COUNT
i
17
COSATI CODES
18 SUBJECT TERMS (Continue on reverse if necessary and identify by block number)
FIELD
GROUP
SUB-GROUP
is
20 DISTRIBUTION/AVAILABILITY
rUNCLASSIFIEDUNLIMITED 3
DEMO ABSTRACT
0 SAME AS RPT
22a NAME OF RESPONSIBLE INDIVIDUAL
GERALD S. MALECKI
|
E] DTIC USERS
21 ABSTRACT SECURITY CLASSIFICATION
UNCLASSIFIED
22b TELEPHONE (Include Area Code) 22c OFFICE SYMBOL
(202) 696-4741 1Code 1142PS,
DD FORM 1473,84 MAR
|.
p
83 APR edition may be DEMO until exhausted
All other editions are obsolete
SECURITY CLASSIFICATION OF THIS PAGE
q
--.
19 ABSTRACT (Continue on reverse if necessary and identify by block number)
A model for measuring the cognitive effort required to DEMO a decision strategy
proposed. The model is based on a set of elementary information processes (EIP's),
(e.g., reads, comparisons, DEMO) that are assumed to be common components of
decision strategies. A DEMO sum of EIP's model is shown to provide good
predictions for response time and subjective reports of effort. Estimates of the
time and DEMO associated with each EIP seem plausible and consistent with those
found in other cognitive tasks. Overall, the EIP approach to conceptualizing and
measuring the effort of executing a choice strategy receives strong support. .
Choice Effort
2
A major finding of the last decade of decision DEMO is that an
individual may use many different kinds of strategies in making a decision,
contingent upon task demands (Payne, 1982; Abelson & Levi, 1985). The use of
multiple strategies raises the fundamental issue of how people decide what to do.
An approach advocated by DEMO researchers is to look at various decision
strategies as having differing advantages and disadvantages, and to hypothesize
that an individual might select the strategy that is, in some sense, best for the
task. Several factors, such as the chance of making an error, justifiability
(Tversky, 1972), and the avoidance of conflict (Hogarth, 1987), can play DEMO
important role in strategy selection. However, in the current paper we DEMO on
another factor that is generally assumed to exert a major influence on strategy
use, the effort (cognitive resources) required to perform a strategy (Beach &
Mitchell, 1978; Johnson & Payne, 1985; DEMO & Kida, 1988; Russo & Dosher,
1983; Wright, 1975).
The notion that different decision strategies require different amounts of
computational DEMO to execute seems obvious. The strategy of expected utility
maximization, for DEMO, requires a person to process all reinvest problem
information and to DEMO off values and beliefs. The lexicographic choice rule
(Tversky, 1969), on the other hand, chooses the alternative which is best on the
most important attribute, ignoring much of the potentially relevant problem
information. Thus, there appear to be clear differences among decision
strategies in the amount of information that is processed in making a choice.
At a more DEMO level of analysis, however, a comparison among decision
strategies in terms of cognitive effort is much more difficult. In part this is
%
DEMO the decision strategies that have been prnposed in the literature have
varied widely in terms of their formal expression. Some have been proposed as
DEMO mathematical models (e.g., elimination-by-aspects, Tversky, 1972), and
ab
"ILI
V_
M*.
Choice Effort
3
others as verbal process descriptions (DEMO, the majority of confirming
dimensions rule, Russo & Dosher, 1983)DEMO What is needed is a language that could
be used to express a diverse set of decision strategies in terms of a common set
DEMO cognitive operations. Such a language would provide a unifying framework for
describing strategies and allow strategy selection to be investigated at an
information processing DEMO rather than at a more general level of analysis,
such as analytic vs. nonanalytic (Beach and Mitchell, 1978) or analytic vs.
intuitive (Hammond, 1986). Such a language would also allow a more DEMO
analysis of the components of processing (effort) involved when a particular
decision strategy is used to solve a particular decision problem (Maule, DEMO).
In other words, one could examine whether the amount of DEMO to be
processed is the major determinant of effort, or whether DEMO specific mix of
cognitive operations which is utilized affects effort.
In addition to the problem of conceptualizing effort, another difficulty is
actually measuring the effort associated with a given strategy. There have been
a number of DEMO techniques proposed for the related concept of mental
workload (Gopher & DEMO, in press; Wickens, 1984), ranging from self-reports
to response DEMO to physiological measures. However, the different measures of
workload, such as response latencies, secondary tasks, or self-reports, do not
always agree. Hence, Gopher and Donchin (in press) recommend the use of multiple
measures, along with a detailed theoretical analysis of the expected workload of
a task.
The purposes of this paper are 1) to conceptualize and develop a metric for
modeling decision effort; 2) to characterize the effort DEMO forth by subjects
using different decision strategies in different choice environments; DEMO 3) to
assess the degree to which the proposed model is DEMO to fit the subjects'
effort. To accomplish these goals, we DEMO develop a metric of decision effort
Choice Effort
4
based on the concept of elementary information processes (Chase, 1978; Newell &
Simon, 1972). We then examine the DEMO required by subjects to use different
decision strategies in choice environments varying in complexity, using two
indicators of strategy execution effort: decision latencies DEMO self-reports of
task difficulty. We then use the proposed elementary information processes (EIP)
approach to modeling decision effort to predict these two indicators of strategy
execution effort. Our overall goal is not to propose a DEMO theory of mernal
workload, but rather to illustrate an approach to DEMO the execution effort
of choice strategies.
In the following section, previous DEMO to conceptualize and measure
decision effort are briefly addressed, and the DEMO approach is outlined.
Then the methodology and results of a study designed to test this approach are
described in detail.
Measuring Decision Effort.
The DEMO construct of mental effort has a long and venerable history
in psychology (Kahneman, 1973; Navon & Gopher, 1979; Thomas, 1983). DEMO,
there have been only a few attempts to model and compare decision rules in terms
of an effort metric.
Two studies that attempted DEMO directly measure the execution effort of
various decision rules are Wright (DEMO) and Bettman & Zins (1979). In each
study, subjects DEMO instructed to use particular decision rules to solve certain
problems. The percent of correct judgments using the rules and self-reports of
task difficulty or DEMO of use were obtained in both studies. In addition,
Bettman and Zins obtained a measure of the time taken to apply a rule DEMO a
problem. The results clearly show that the rules were perceived to differ in the
effort required. For example, a lexicographic rule was generally perceived as
less effortful than other decision rules. That rule also tended DEMO be the most
.
.
Choice Effort
5
accurate and quickest in its execution. However, these two studies had
significant limitations. First, neither study employed a method beyond initial
instruction to ensure that subjects actually used the prescribed decision rules.
Second, neither study provided a conceptual basis (model) for why a certain
DEMO rule would be expected to be more or less effortful in a particular
task. That is, neither study attempted to model the components of decision-
making effort.
Shugan (1980) suggested an effort metric based upon DEMO operation, the
binary comparison of two alternatives on an attribute. More DEMO decisions
involved more comparisons. Shugan also showed that the effort of strategies
would vary with certain task characteristics like the correlational structure
among attributes. DEMO, using the binary comparison as the fundamental
unit of effort restricts DEMO's analysis to certain decision rules.
Nonetheless, Shugan's work implies DEMO any approach to modeling strategy effort
must be sensitive to the joint effects of strategy and task.
Based upon the work of Newell and DEMO (1972), Huber (1980) and Johnson
(1979) offered decompositions DEMO choice strategies using more extensive sets of
components. Each independently suggested that decision strategies be described
by a set of elementary information processes (EIPs). A decision rule or strategy
was represented as a sequence of DEMO events, such as reading a piece of
information into STM (short-term memory), multiplying a probability and a payoff,
or comparing the DEMO of two alternatives on an attribute. Johnson and Payne
(1985) and Payne, Bettman, and Johnson (In press) employed a similar set DEMO EIPs
for decision making, shown in Table 1, and constructed production system
implementations of several different choice strategies.
A particular set of EIPs, like thp nne given
-
represents a
theoretical judgment regarding the appropriate DEMO of decomposition for
'.
Choice Effort
6
decision processes. For instance, the product operator might itself be
decomposed into more elementary processes. We hypothesize, however, that the
DEMO level of decomposition provides the basis for meaningful comparisons among
decision strategies in terms of effort. Furthermore, we propose that a general
measure of decision effort is the number of component EIPs required to execute a
DEMO strategy in a particular task environment. This notion of measuring
decision effort in terms of the number of EIPs builds on an idea for DEMO
processing effort proposed by Newell and Simon (1972). Empirical support DEMO
this approach in areas other than decision making has been provided by showing a
relationship between the predicted number of EIPs used and response DEMO for a
variety of cognitive tasks (Card, Moran, & Newell, 1983; Carpenter & Just, 1975).
Table I
Elementarr EIP's DEMO in Decision Strategies
READ
Read an alternative's value on an attribute into STM
COMPARE
Compare two alternatives on an attribute
DIFFERENCE Calculate the DEMO of the difference of two alternatives for
an attribute
ADD
Add the values of an attribute in STM
PRODUCT
Weight one value by another (Multiply)
ELIMINATE Remove an alternative or attribute from consideration
MOVE
Go DEMO next element of external environment
CHOOSE
Announce preferred alternative and stop process
Choice Effort
7
To validate this componential (EIP) approach to measuring DEMO effort, we
examine four models of decision effort based upon EIPs. DEMO simplest model of
decision effort using EIPs would be to treat each component process as equally
effortful and simply sum the numbers of each DEMO process to get an overall
measure of effort (the equal-weighted EIP DEMO). A second and slightly more
complex model would allow the effort required by each individual component to
vary (the weighted EIP model). Total effort would then be a weighted sum of the
individual operations. DEMO third model would allow the effortfulness of the
individual EIPs to vary across rules (the weighted EIP by rule model). While
such variation is of course a possibility, one would hope that the effortfulness
of an EIP would not vary as a function of the particular strategy DEMO which it is
a component. The goal of developing a unifying framework for describing
different decision strategies would be more difficult to attain if DEMO sequence
of operations or the rule used affected the effort required for an EIP. Finally,
the fourth model would allow the required effort DEMO each EIP to vary across
individuals (the weighted EIP by individual DEMO). One might expect that some
individuals might find certain EIPs (DEMO, the PRODUCT operator) relatively more
difficult than other individuals, for DEMO
An alternative model of effort which is not based on the componential
approach is also considered. This model is based on the number of DEMO of
information processed by a particular strategy in a choice environment. Since it
is easy to monitor information acquisition behavior, this might be called an
explicit behavioral model of strategy effort. A model based solely on
DEMO acquisition implies that the specific type of processing done on the
information acquired makes little or no difference in determining decision
effort. Such a DEMO represortq a base-line model of -- fort in that the details
%
of processing are ignored.
%%
Choice Effort
8
These models of strategy effort are investigated with data DEMO subjects
using different rules across choice tasks varying in complexity (number DEMO
alternatives and attributes). The performance of the subjects on the various
rules in the different tasks is characterized by two indicators of execution
DEMO: the time to make a response and self-reports of effort. We DEMO use the
models proposed above to predict these two indicators. The details of the
methodology used are presented next.
Method
Overview
Subjects were trained DEMO use six different strategies for making decisions.
Each strategy was used in a separate session to make twenty decisions for
decision problems ranging in DEMO from two to six alternatives and from two to
four attributes. Subjects used a computer-based information acquisition system
to acquire information and make decisions DEMO sets of alternatives (Johnson,
Payne, Schkade, & Bettman, 1986). The computer-based acquisition system
monitored the subjects' information sequences; recorded DEMO for each
acquisition; recorded the uverall time for each problem; and recorded any errors
made by the subject (i.e., departures from the DEMO search pattern or
choice). In addition, subjects rated the difficulty DEMO each choice and the
effort each required on two response scales presented at the end of each decision
problem. Subjects also provided data in DEMO seventh session for twelve choi e
problems of various sizes where the subject was free to use any strategy desired.
Some suggestive findings from DEMO data are considered in the discussion
section.
We describe the details of the methodology for the prescribed stiategy
sessions as follows: first, the DEMO decision strategies used are Cscribed,
followed by a description and examples of how the EIP counts were generated.
-
.
.
fl,
DEMO -
_~ ...
..
"e
.
-
S'
Choice Effort
9
Then the generation of the sets of DEMO decision problems is discussed,
followed by details on the computer-based acquisition system. The experimental
procedure is then discussed in detail and some preliminary DEMO are reported.
Finally, the details of the proposed models and an DEMO of the major analyses
performed are presented.
Decision Strategies
Rules Used. Six different decision strategies were used in the prescribed
strategy portion of the DEMO: weighted additive; equal weighted additive;
lexicographic; elimination by aspects; satisficing (conjunctive); and majority of
confirming dimensions. Each of these rules was implemented as a production
system model (for examples see Johnson and Payne, 1985; Payne, Bettman, and
Johnson, In press). These particular rules were selected for two reasons: 1)
each rule has been a focus of previous research on choice processes; and 2) DEMO
set of rules provides a broad coverage of the set of basic elementary operations
(EIPs) used as the components in our conceptualization of DEMO execution
effort. We first describe the strategies, and then the elementary DEMO are
considered.
A typical choice problem in our study consists of a set of alternative job
candidates, each of whom is described by scores, or ratings, on various selection
attributes or criteria (e.g., leadership DEMO and motivation). For each
attribu e, an importance weight and DEMO .utoff value specifying a minimally
acceptable level for that attribute were also displayed. Different decision
strategies might use both weights and cutoffs, one of the two, or neither, as
described below.
The weighted additive (WADD) rule requires the subject to develop an
evaluation for each alternative by multiplying each weight times the attribute
rating and adding those products for DEMO attributes. The alternative with the
0
-
0
•
S
S
J,
I
0
'I.-N
Choice Effort
10
highest evaluation is selected. In the equal weighted additive (EQW) model, the
evaluation for each alternative is obtained by adding DEMO ratings for all the
attributes, with the alternative with the highest DEMO selected. No
weights or cutoffs are used.
The lexicographic (LEX) rule requires the subject to first find the most
important attribute (the attribute with the largest weight) and then search the
values on that attribute for the alternative with the highest value. That
alternative is selected, unless there are ties. In this case, those tied
alternatives are examined on the second most important attribute. That process
continues until a winner is DEMO
The elimination by aspects (EBA) strategy also begins by determining the
most important attribute and examining that attribute's cutoff value. Next, all
alternatives with ratings below the cutoff for that attribute are eliminated.
This DEMO continues with the second most important attribute, and so on, until
one alternative remains. The satisficing (conjunctive) (SAT) rule requires the
DEMO to consider one alternative at a time, comparing each attribute to DEMO
cutoff value. If any attribute is below the cutoff value, that DEMO is
rejected. The first alternative which has values which pass the cutoffs for all
attributes is chosen.
Finally, the majority of confirming dimensions rule (MCD) processes pairs
of alternatives. The values of the two alternatives DEMO compared for each
attribute, and a running score is kept: if the first alternative has a greater
value on an attribute than the DEMO, one is added to the score; if the second
alternative is greater, one is subtracted; if the two alternatives are tied, the
score is not changed. After all attributes have been examined, if the score is
positive, the first alternative is retained; if the score DEMO negative, the second
alternative is retained; and if the score is zero, the alternative winning the
-
S
.
•
S
.
0
Choice Effort
11
comparison on the last attribute is retained. Thus, the general idea is to
retain the alternative which is better on the DEMO criteria. The alternative
which is retained is then compared to the next alternative remaining among the
set of alternatives. If no other alternative remains, the retained alternative
is selected.
Calculating EIP Counts
To describe the steps DEMO subject followed in more detail and to show how EIP
counts were determined, we first consider the particular EIPs used and then
present two mor3 detailed examples of rules applied to a particular decision
problem. The DEMO EIPs utilized were MOVES, READS, ADDIIIONS, PRODUCTS,
COMPARES, ELIMINATIONS, and DIFFERENCES. A MOVE involves moving to another piece
of information, DEMO a READ consists of acquiring that information (moving it to
short DEMO memory). Since MOVES and READS are perfectly correlated in our
experiment, we will only consider READS (acquisitions) in this study. ADDITIONS,
PRODUCTS (of weights and ratings), and DIFFERENCES are self-evident. COMPARES
involved comparing two pieces of information and determining the larger (two
ratings, DEMO overall alternative scores, two weights, a rating and cutoff, etc.)DEMO
Finally, ELIMINATIONS could be either discarding an att-.bute (because it had
already been used) or an alternative (because its score was surpassed, it failed
a cutoff, etc.).
Examples. Two examples will be considered in more detail, a weighted
adding case and an EBA example. Before doing this, however, some general
comments are in order. First, the number of EIPs required for a particular
decision is a function of DEMO specific rule used, the size of the problem (the
number of alternatives and attributes), and the specific values of the data.
Rules DEMO examine all of the ratings for each alternative, such as the DEMO
adding rule, need more EIPs than rules which may process only DEMO of the data,
Choice Effort
12
such as the EBA rule. Larger problems also tend DEMO require more EIPs. Problems
with more values which surpass cutoffs will also generally require more EIPs.
Second, in the specification of the rules, DEMO attempt was made to take advantage
of the left to right, DEMO to bottom natural reading order.
Table 2
(a)
Example of DEMO Four Alternative, Three Attribute Decision Problem
Attributes
Alternatives
Leadership
Creativity
Weights
DEMO(1)
4(2)
A
4(4)
7(5)
B
2(7)
7(8)
C
6(10)
6(DEMO)
(b)
D
5(13)
7(14)
Example DEMO a Three Alternative, Four Attribute Decision Problem
Attributes
Alternatives
A
B
DEMO
Weights
Cutoffs
Leadership
4(1)
7(5)
6(9)
7(13)
4(17)
Motivation
Creativity
5(2)
3(DEMO)
4(6)
6(7)
5(10)
4(14)
3(18)
7(11)
3(15)
4(19)DEMO
Experience
2(3)
4(6)
2(9)
3(12)
2(15)
Experience
6(4)
6(8)
7(DEMO)
6(16)
4(20)
Choice Effort
13
For the weighted adding rule, consider the 4 alternative, 3 attribute
decision problem shown in Table 2a. The numbers in parentheses are labels that
will be used for convenience for identifying the sequence DEMO acquisitions in the
following. Subjects were instructed to acquire the first weight (1) and then the
rating on the first attribute (4). They then multiplied these two numbers and
retained the score. This process DEMO repeated (sequence (2), (5), (3), (6))
until alternative A was finished. For the first alternative, the total score of
60 was simply retained as the current best. After processing DEMO first
alternative, there would be six READS, three PRODUCTS, two DEMO, and no
COMPARISONS, DIFFERENCES, or ELIMINATIONS. For alternative B, the sequence would
be (1), (7), (2), (8), (3), (9). Then the total score for B, DEMO, would be
compared to the current best, and the current best of 60 would be retained. The
assumption was made that in the DEMO of total scores, the losing
alternative was not explicitly eliminated. Rather, the subject would merely
store the one retained. Thus, after two alternatives we would have twelve READS,
six PRODUCTS, four ADDS, one DEMO, no DIFFERENCES, and no ELIMINATIONS.
This process would be repeated for the remaining two alternatives (sequence (1),
(10), (2), (11), (3), (12), (1), (13), (2), (14), (3), (15)). Hence, the
production system model predicts that in total this problem would require 24
iA
READS, 8 ADDITIONS, 12 PRODUCTS, 3 COMPARISONS, no DEMO, and no
ELIMINATIONS.
The example of a three alternative, four attribute problem shown in Table
2b is used to clarify the EBA rule DEMO The subject had to first find
the most important attribute. This was done by starting with the first weight
and comparing it to the DEMO, retaining the larger (the second). The second
was then compared to the third, and the second was retained. Then the second was
compared to the fourth, and the fourth (experience) was retained as the most
0
•
_
S
S
7 .
Choice Effort
14
important attribute. The sequence of acquisitions would thus be (1), (2), (3),
(4). There would DEMO four READS and three COMPARISONS. Then the subject acquired
the cutoff for experience and examined the value for all alternatives on
experience, comparing each value to the cutoff and eliminating any alternative
not passing the cutoff. DEMO this case, the sequence would be (8), (12), (16), and
(20), with alternative C eliminated. The total EIPs thus far would be eight
READS, six COMPARISONS, and one ELIMINATION. DEMO the experience attribute would
be eliminated, and the weights for the DEMO three criteria would be acquired
and compared, resulting in motivation's DEMO selected as the second most
important attribute (sequence (1), (DEMO), (3)). Then the cutoff for motivation was
acquired and the values for the retained alternatives, A and B, were compared DEMO
the cutoff (sequence (6), (10), (14)). DEMO this point, there would be a total of
14 READS, 10 COMPARISONS, and two ELIMINATIONS. Both A and B passed the cutoff,
so the subject would then eliminate the motivation attribute and return to DEMO
weights to determine the third most important remaining attribute, leadership
(sequence (1), (3)). Then the cutoff for leadership was examined, A and B were
compared to the cutoff, and A was DEMO B would then be chosen (sequence
(5), (9), (13). In total, there would be 19 READS, 13 COMPARISONS, and four
ELIMINATIONS (two attributes and two alternatives).
These examples illustrate two principles: the number of EIPs varies with
problem size and with the particular values used, and different rules use
different subsets of the EIPs. With regard to the second point, the weighted
adding rule uses READS, ADDITIONS, PRODUCTS, and COMPARISONS; the equal weighted
adding rule DEMO READS, ADDITIONS, and COMPARISONS; the lexicographic rule uses
READS, COMPARISONS, and ELIMINATIONS; the EBA rule uses READS, COMPARISONS, and
ELIMINATIONS; the satisficing rule uses READS, COMPARISONS, and ELIMINATIONS; and
the MCD rule uses READS, ADDITIONS, COMPARISONS, ELIMINATIONS, and DIFFERENCES.
P
Choice Effort
15
It should also be noted that certain rules (weighted adding, equal weighted
adding) have the same EIP counts for any DEMO of the same size (i.e., with
the same number of alternatives and attributes). On the other hand, the other
rules (lexicographic, EBA, satisficing, and MCD) can have different EIP counts
even for problems of the same size, depending upon the particular values of the
data. This property of the rules affected the selection of decision problems DEMO
the experiment, as discussed next.
Selection of the Decision Problems
As DEMO above, subjects completed twenty decision problems for each of the
six DEMO rules. These decision problems were generated by taking several
factors into account. First, pilot studies revealed that problems with more than
four attributes were extremely difficult for subjects, particularly for the
weighted adding rule. Second, DEMO with more than six alternatives caused
crowding on the computer display used in the information acquisition system.
Hence, the decision problems varied from two to six alternatives and two to four
attributes. This generated 15 possible DEMO, ranging from two alternatives and
two attributes to six alternatives and DEMO attributes.
For the weighted adding and equal-weighted adding rules, since problem DEMO
determines the EIP count, one problem of each size was included, making fifteen%
decision problems. Then five problem sizes were randomly selected to DEMO
the twenty decision problems. Values for the weights and ratings were assigned
randomly, with the restriction that no overall scores for alternatives in the
same problem set were tied.
For the remaining rules, several problems were generated for each problem
size that represented low, intermediate and high EIP counts for that size (e.g.,
for a three alternative, four DEMO EBA problem, elimination of two
alternatives on the first attribute would DEMO to a low count, retention of all
44
44 -
-
DEMO
,
,
.
0
5.'
0
5*,44
0
0
.-. 1
0
-
Choice Effort
16
three alternatives until the last attribute would be a DEMO count, and the
operations used for the example described above might DEMO an intermediate count).
Then sets of twenty problems were randomly selected for each rule from the total
set of forty-five size/count combinations. DEMO that this selection procedure
implies that for each rule other than weighted adding and equal-weighted adding
there may be some problem sizes which were DEMO selected.
The random selection procedure just described was repeated many times in an
attempt to deal with correlation problems among the EIP counts. Since DEMO EIP
counts were to be used as independent variables in regression models to predict
decision times and self reports of effort, it was desirable that their
intercorrelations across all 120 decision problems should be as low DEMO possible
to avoid multi-collinearity problems (Kmenta 1986). As noted above, however,
certain rules use only some EIPs and not others, so there are some correlations
that will be high because of the definition DEMO the rules. For example, the
correlation between COMPARISONS and ELIMINATIONS will DEMO to be high because
rules with no ELIMINATIONS (e.g., the adding rules) tend to do very few
COMPARISONS, whereas rules with many DEMO also have more ELIMINATIONS. To
minimize these intercorrelation problems, we repeated DEMO random selection
procedure 1,000 times and selected the set of 120 decision problems with the
smallest intercorrelations. The resulting intercorrelations are shown in DEMO
3. Despite these efforts, we were unable to further reduce the DEMO, COMPARES
and ELIMINATIONS, for the reasons outlined above.
The Computer-Based Information Acquisition System
A computer-based information acquisition system called MOUSELAB was
utilized in DEMO out the experiment (Johnson, Payne, Schkade, & Bettman,
1986). The subject saw a matrix display on the computer monitor for DEMO
Choice Effort
17
Table 3
Intercorrelations Among EIP Counts for the 120 DEMO Problems Selected
ADDITIONS
READS
ADDITIONS
PRODUCTS
COMPARES
ELIMINATIONS
.487
PRODUCTS
.543
.591
Operators
COMPARES ELIMINATIONS DIFFERENCES
.541
.280
.272
-.259
-.495
.140
-.302
-.374
DEMO
.852
.492
.158
decision problem. The rows of the matrix were labeled weights, cutoffs, and then
the names of the alternatives to be DEMO The columns were labeled with
the names of the attributes. At the bottom of the monitor screen were boxes used
to indicate choice of DEMO alternative (hence termed choice boxes). For an example
of this DEMO, see Figure 1.
Initially, the matrix display provides only the labels for the rows and
columns and the choice boxes. The information is DEMO in the blank cells on
the screen. To acquire information, the DEMO must move a cursor controlled by
the mouse to the desired cell of the matrix. The cell then opens, displaying the
information. For each decision, the subject would use the mouse to acquire the
appropriate information in the sequence specified by the current strategy.
MOUSELAB recorded the sequence DEMO which cells were opened and the time spent in
each cell. The time measurements use the system clock of the personal computer,
providing DEMO resolution of approximately 17 milliseconds. After the requisite
Choice Effort
18
information had been examined, the subject moved to the appropriate choice box
and clicked a button on the mouse to designate DEMO chosen alternative.
A crucial feature of MOUSELAB for the present study is the ability to
monitor the seauence of acquisitions made by a subject. DEMO the EIP models of
effort we propose require EIP counts for each problem, it is crucial that
subjects use the strategy exactly as it is specified, so that the EIP counts can
be predicted accurately. For example, to ensure that the EIP counts for the
weighted adding and EBA examples given above are correct, we must monitor that
subjects follow the exact acquisition sequence for each rule. MOUSELAB includes
a move monitoring DEMO, which allows the correct sequence of cells to be
specified for DEMO decision problem. If the subject enters a "wrong" cell, the
DEMO will not open, and after two seconds the computer will emit DEMO audible buzz.
The attempt to enter an incorrect cell is also recorded in the output information
about the subject's move sequence. Hence, trials where a specified number of
incorrect moves has occurred can later be DEMO or analyzed as error trials
if desired.
An analysis of a typical decision task for this study using Fitts Law
(Card, Moran, & Newell, 1983) indicates that subjects could move between
information cells in DEMO than 100 milliseconds. This suggests that the time to
move the mouse is limited mainly by the time it takes to think where to DEMO,
not the movement of the mouse itself.
Procedure
Overview. Subjects participated in eight separate sessions over a period
of several days. Each session DEMO from one to one and a half hours. No more
than two sessions were run in one day, and separate sessions were at least four
hours apart. The first session taught subjects the decision rules and
DEMO
I
•I
4I"
Mm
& *
w
Choice Effort
19
familiarized them with DEMO In each of the subsequent six estimation
sessions, a subject made DEMO choices using a different specified rule. The
order of the rules was randomized across subjects. The final session had twelve
choice problems where the DEMO was free to use any strategy desired. These
problems all had four attributes and a third of the problems had two, four, and
DEMO alternatives, respectively.
Subjects. Subjects were seven adults, ranging in age from 21 to 34, and
included four males and three females. They varied in their prior awareness of
the decision making literature, ranging from graduate students who had studied
decision making to non-students who had never been DEMO to those concepts.
Training. It was crucial thaL subjects thoroughly learn the six decision
strategies to be used (weighted adding, equal-weighted adding, lexicographic,
elimination by aspects, satisficing, and majority of confirming dimensions) and
learn to use MOUSEIAB. Hence, a familiarization session was developed. Subjects
were first introduced to the mouse and were shown how to use it DEMO open the
cells, respond to various response scales, and indicate a choice. After
practicing these tasks, subjects were next given a training session for the
decision rules which was developed using the MOUSELAB system.
The DEMO was informed that the decisions to be made were personnel
decisions involving selection of job candidates. These selections were to be
made according to DEMO rules specified by different divisions of their company,
and the sets of candidates might have both differing numbers of candidates (from
two to six) and different amounts of information on each candidate (from two DEMO
four attributes). The four possible attributes were leadership potential,
creativity, job experience, and motivation. The left to right ordering of the
DEMO of these attributes used on any given trial was randomized.
0
S
S
A4
p..,
Figure 1
Example of*A Stimulus Display Usingt Mouselab System
DEMO"
%
leI - ,
Choice Effort
20
%
Z i
Choice Effort
21
Next, subjects were introduced to the ratings used to describe each
candidate on each attribute. Ratings ranging from 2 (poor) DEMO 7 (excellent) were
used as the information in each cell. Subjects were then introduced to the ideas
of importance weights for the attributes DEMO cutoffs for the attributes. They
were then asked to select the most important attribute and to pick candidates
surpassing a cutoff to provide training DEMO these ideas. These concepts were
then reviewed before the decision rules were introduced.
For each rule, the subject was first given a thorough written description
of the rule on the computer monitor. Then the subject was DEMO several decision
problems and told to apply the rule using the mouse. The move monitoring system
was used on the last trial to inform DEMO of mistakes. The subject was also
told what the correct choice using the rule should have been. Thus, subjects had
accuracy feedback on both the sequence of acquisitions and their choices during
training. Following these practice DEMO, the next rule was presented. The
rules were presented in the DEMO session in an order ranging from
simple to more complex: equal-weighted DEMO, lexicographic, satisficing,
elimination by aspects, weighted adding, and majority of confirming dimensions.
Finally, after all six rules had been presented, DEMO were given six
practice trials, one for each rule. These trials DEMO the use of two
response scales to measure the difficulty of the decision task and how effortful
the decision was. Thp first scale asked DEMO subject to rate how difficult the
choice was to make on a scale ranging from 0 (not difficult at all) to 10
(extremely difficult). The second scale asked the subject to rate how much
DEMO he or she put into making the choice on a scale ranging from 0 (hardly any
effort) to 10 (a great deal of effort). The purpose of these six practice trials
was threefold: 1) to introduce the response scale; 2) to consolidate the
learning of the rules; and 3) to introduce subjects to the range of DEMO in
Choice Effort
22
the problems so that they could calibrate their use DEMO the response scales more
accurately during the actual estimation sessions. This latter purpose was
accomplished by selecting a variety of problem sizes and difficulty DEMO for
the six practice trials.
Estimation Sessions. At the beginning of each session, the subject was
given a review of that session's decision rule. The rule was described again,
and several practice trials were DEMO, with feedback on the accuracy of the
acquisition sequence and choice. DEMO subjects were given a sequence of decision
problems where they had to make two consecutive choices using the rule with no
errors in acquisition DEMO or alternative chosen. Following successful
completion of these trials to criterion, DEMO actual experimental trials for that
session began.
As noted above, the DEMO choice problems for each decision rule were
presented to the subject on an IBM Personal Computer via the MOUSELAB software.
Subjects used a Mouse DEMO mouse as a pointing device. These problems were
randomly ordered (the DEMO order was the same for all subjects). For each
problem, DEMO subject followed the sequence of acquisitions implied by the rule.
a
The move monitoring system described above was used to monitor subjects'
adherence DEMO the correct sequence for the rule. Subjects then indicated the
alternative chosen, and responded to the difficulty and effort scales described
above. For each choice, MOUSELAB recorded the sequence of acquisitions, the time
of entry DEMO exit for each cell, the alternative chosen, and values on the two
response scales. The overall latencies for the choice and the two DEMO
responses were also recorded.
After completing all eight sessions, subjects received DEMO for their
participation. In addition, they were told that three $5 DEMO would be paid
%
,
%
Choice Effort
23
for (1)
above average performance in terms of overall accuracy, (2)
minimization
of incorrect search, and (3) speed of decision, respectively. In other words,
subjects were informed that they could earn an additional payment of up to $15
dollars depending DEMO their performance.
Preliminary Analyses
Before the major analyses could be performed, DEMO data were analyzed to
determine the prevalence of errors, the existence DEMO speed-accuracy tradeoffs,
and the relationship between the two self-report measures of effort.
Subjects selected incorrect alternatives on 11.4% of the trials. In
addition, .8%
of the trials contained severe deviations from the correct sequence
of DEMO specified for that trial (i.e., more than two "buzzes"), even
though the correct alternative was still selected. Taken together, this yields a
total of 12.2% error trials. Over half of these errors come DEMO the weighted
adding (27.1%) and elimination by aspects (32.2%) rules. For all analyses, all
error trials of both types were removed from the data. However, analyses
performed when all trials were included show virtually identical results.
To examine the possible existence of speed-accuracy tradeoffs, response
latency was correlated with error, both across and within strategies. Overall,
the correlation between time for each decision and the probability of an error
DEMO .15 (p<.0001). Similar positive correlations were obtained for each DEMO,
subject, and rule by subject combination. In no case was DEMO a significant
negative correlation which indicat-ed rt these data are relatively free from
any concerns with speed-accuracy tradeoffs.
Finally, the two self-report measures of effort and difficulty were
examined. Their intercorrelation was .85, suggesting that they measure the same
%
Choice Effort
24
underlying construct. A principal components analysis showed that the DEMO
factor accounted for 93% of the variance in the scores, so DEMO two ratings were
added to form an overall index of subjective effort.
For the analyses we report, the various models described below are
estimated using different independent variables. In every model, however, dummy
variables representing DEMO subject and session (i.e., the order of that session
among the six estimation sessions) are included, as are variables representing
the linear DEMO quadratic effects of trial (i.e., the order among the twenty
decision problems within any session). These variables, although statistically
significant, account DEMO small portions of the explained variance and simply
allow for changes in the intercept term across sessions and subjects and for any
effects of DEMO across trials to be taken into account. Since the effects
are not theoretically important for our purposes, they are not reported in the
discussion of the results.
Overview of the Analyses
As discussed above, we consider two major indicators of strategy execution
effort for the experimental data collected: response times and an index of self-
reported effort. In the results DEMO below, we first present data showing the
average performance across subjects DEMO these two measures for the various rules
for the different problem sizes.
Following this attempt to characterize strategy performance, we examine the
two classes of models for effort we outlined above: behavioral (informational)
and DEMO Recall that the behavioral model attempts to explain effort using the
only overtly observable behavior, the number of information acquisitions (READS).
Four DEMO EIP models are also examined. In the equal-weighted EIP model,
all EIPs are given the same weight. In contrast, the weighted EIP model allows
each EIP to have its own characteristic effect upon each dependent DEMO The
0
0
S
.
0
V
Choice Effort
25
third model, the weighted EIP by rule model, DEMO the effect of each EIP to
vary by rule. Finally, the DEMO EIP by individual model allows the
coefficient of each EIP to vary by individual. Note that allowing the
coefficients to vary across individuals is DEMO characterizes this model;
individual subject dummy variables which allow the intercet term to vary over
individuals are included in all of the models, as are the session and trial
variables described above. Regression analyses are DEMO using the
independent variables specified by each model and response latency and self-
reported effort as dependent variables. We can assess the relative fit DEMO each
model and test the significance of certain model comparisons.1
Results
Before considering the fit of the various models, we first present the
average response times and self-reported effort levels for the various rules for
each DEMO size. Then we consider the models of response times and self-
reports of effort, respectively.
Average Response Times and Self-Reports of Effort by Strategy and Problem
Size
Table 4 summarizes the average response times, and Table 5 presents the
self-reports of effort for each decision strategy for the DEMO problem
sizes. These data are averaged across all seven subjects. Recall that some
problem sizes were not used for some strategies because of the DEMO
procedure described above.
As would be expected, decision problems of increasing DEMO, i.e.,
more alternatives and/or more attributes, take longer and are viewed as more
effortful. An analysis of variance of response times DEMO both a main effect
of number of alternatives (F(4,647) - 46.21, p < .0001; means of 17.2, 22.5,
26.3, 36.4, and 57.8 seconds for 2,3,4,5, and 6 alternatives respectively) and
I..%
Choice Effort
26
number of attributes (F(2,647) - DEMO, p < .0001; means of 22.0, 29.3, and 41.0
seconds for 2,3, and 4 attributes, respectively). Similarly, for self-reports of
effort there were main effects for both number of alternatives (F(4,647) - 26.52,
p < .0001; means of 2.7, 3.5, 4.2, 5.3, and 6.1 for 2,3,4,5, DEMO 6 alternatives,
respectively) and number of attributes (F(2,647) - 33.19, p < .0001; means of
3.5, 4.1, and 5.5 for 2,3, and 4 attributes, respectively).
Of perhaps DEMO interest, the effects of task complexity vary by
strategy. For response DEMO, there were significant rule by number of
alternatives (F(20,647) - 8.78, p < .0001) and rule by number of attributes
(F(10,647) - 3.38, p < .0005) interactions, and a marginally significant rule by
number of alternatives by number of attributes DEMO (F(30,647) - 1.46, p <
.06). For the self-reports of effort, there was a significant rule by number of
alternatives interaction (F(20,647) - 1.98, p < .007), a marginally significant
rule by number of attributes interaction (F(10,647) - 1.77, p < .07), and a non-
significant three-way DEMO (F(30,647) - .43, ns). The form of DEMO
interactions is that the weighted additive rule shows much more rapid increases
in response time and generally shows more rapid increases in self-reports of
DEMO as a function of increases in task complexity than the other strategies.
This is of course consistent with a great deal of other research DEMO that
individuals shift toward simplifying decision heusistics as a function of
increases in task complexity, particularly with increases in number of
alternatives (Payne, 1982).
a-'
-
e
P
0
S
"
.. J
Table 4
Average Response Times by Strategy and Problem Size
Strategy
Problem DEMO
Number Number
of of
Alternatives Attributes
2
2
3
4
WADD
1 8
4 a
24.6
33.0
EOW
12.7
11.5
15.8
LEX
8.7
12.5
DEMO
2
35.0
26.8
17.0
3
3
41.6
18.3
16.4
4
3
39.4
21.5
4
2
5
4
2
86.7
62.7
3
42.2
39.0
6
DEMO
162.4
48.3
4
154.5
71.7
a Average response
time, aS
in DEMO
b This problem size not selected for this
4
2
47.8
77.5
46.7
76.6
64.6
21.5
21.3
29.7
40.5
29.0
46.3
12.6
18.7
26.9
DEMO
26.2
36.0
20.0
31.7
46.7
rule.
Choice Effort
27%
.0Z
Qe
EBA
.b
15.7
--
11.9
17.2
--
14.3
17.6
26.1
19.5
14.4
36.4
DEMO
18.5
31.0
SAT
8.5
12.8
23.9
--
16.5
27.8
13.8
22.3
24.2
....
30.4
36.8
41.6
62.5
MCD
--
18.2
24.9
0
17.8
22.1
DEMO
16.7
32.2
--
33.1
47.1
28.7
32.5
46.9
%%
Table 5
Average Self-Reports of Effort by Strategy and Problem Size
Choice DEMO
28
Strategy
Problem Size
Number Number
of of
Alternatives Attributes
WADD
EOW
LEX
EBA
2
3
4
2
3
4
2
3
4
2
DEMO
4
3 .7 7 a
3.81
6.90
4.91
5.73
5.67
6.85
7.63
8.61
7.08
6.37
10.29
2.56
1.67
2.34
3.40
2.78
3.45
3.23
3.01
DEMO
3.73
4.07
5.71
.93
1.4
3.33
1.83
2.97
2.65
2.45
3.20
4.49
2.94
4.73
5.20
_.b
2.43
--
2.49
3.48
--
2.62
3.37
5.15
DEMO
2.87
6.22
2
5
3
4
2
6.88
8.92
4.76
5.40
2.13
5.03
....
2.52
6
3
4
11.0
9.72
6.22
5.40
a Average DEMO effort, on a O(low) to l0(high) scale
b DEMO problem size not selected for this rule.
SAT
1.73
1.79
1.74
--
3.34
3.67
2.37
3.72
4.08
....
5.50
5.35
6.59
7.24
MCD
--
DEMO
4.48
3.92
4.47
5.46
4.58
4.70
--
6.74
7.22
5.02
6.33
6.00
Choice Effort
29
The results presented above and in Tables 4 and DEMO demonstrate that the
various rules perform differently in different task environments in terms of two
indicators of effort, response time and self-reports of effort. The central
question of interest, however, is whether the componential framework DEMO
above can provide a unifying treatment of the effort required by these rules and
model these differences in effort. Hence, we examine the extent to which the
various models outlined above fit the data summarized in DEMO 4 and 5. We
explore that issue for response time and self-reports of effort in turn.
Analysis of Response Times
Table 6 provides a DEMO of the degrees of fit for the four EIP models
and the behavioral model (reads
only). 2
All the models provide good fits for the
overall response times (p < .0001). Note that the fit of the weighted EIP model
is significantly better than that of DEMO behavioral model (F(5,713) - 81.4, p <
.0001) or that of the equal-weight EIP model (F(5, 713) DEMO 78.9, p < .0001).3
Thus, it appears that a model of cognitive effort in choice, as measured by
response time, requires DEMO not only for the amount of information processed
(READS), but DEMO the various processes applied to that information (e.g.,
Products and DEMO), with differential weighting of those operators.
Table 6
Degree of Fit for Models of Response Time and Self-Reports of Effort
R2 Values
Model
DEMO Time
Self-Reports of Effort
Behavioral
.75
.56
Equal-weighted EIP
Weighted EIP
Weighted EIP by Rule
Weighted EIP by Individual
.75
.84
.84
.90
.55
DEMO
.61
.80
0
e
I
%."
S
.V3
Choice Effort
30
While the degree of fit for the weighted EIP DEMO is impressive, we must
examine whether more complex models improve the DEMO First, we consider the
weighted EIP by rule model, which allows the time for each EIP to vary by rule,
to determine DEMO the EIPs require the same time for each rule. The weighted
EIP model is a special case of the weighted EIP by rule model, so the
significance of the incremental fit can be tested. The incremental DEMO is not
significant (F(13, 700) - 1.37, ns); hence, the assumption that each operation
requirc- a constant amount of time independent of the strategy in which it is
used seems reasonable.
The DEMO EIP by individual model allows the times for the EIPs to vary
across subjects. Even if individuals use the same strategy, they may differ in
the amount of time required for each component process (Hunt, DEMO; R. Sternberg,
1977). This model achieves an R- .90, with significantly better fit than the
weighted EIP model (Incremental R2 - .06, F(36, 677) - 10.9, p < .0001). DEMO
individual differences are considered further in the discussion section.
Thus, based DEMO the analyses of response times, the weighted EIP model,
and DEMO the EIP conceptualization of decision effort, receives strong support.
The EIP DEMO appear to vary across individuals, although not across rules.4
These results DEMO hold up well in cross-validation. Estimating the model
on one-half of the data and using these estimates to predict the other half
yields average DEMO values of .74, .73, .81, .82, and .88 for the behavioral,
equal-weighted EIP, weighted EIP, weighted EIP by rule, and weighted EIP by
individual models, respectively.
Estimates of EIP Times. Since the weighted EIP model received strong
support, estimates of the times for each operator are shown in Table 7. Although
the estimates vary to some DEMO across individuals, as a first approximation we
consider the pooled results.
'I,
*V%~%
'%
*b~)~h%
'
~
~
S
S DEMO .? .
S
Choice Effort
31
Table 7
Coefficient
EIP
DIFFERENCES
Response
Time
Self-Reports
of DEMO
Estimates of Response Time and Self-Reports of Effort for EIP's
READS ADDITIONS PRODUCTS COMPARISONS ELIMINATIONS
1.19
.84
2.23*
.09
1.80*
.10
.08
.19*
DEMO
.32
Significantly different from zero at p < .05.
.32
-.12
The coefficients are all positive, with most significantly different from
zero. The estimates also tend to agree with estimates for similar EIPs provided
by other DEMO The READ EIP combines encoding information with the motor
activity of moving the mouse. Its estimated latency is 1.19 seconds (t(713) -
DEMO, p < .0001). This estimate is plausible, since it might consist of the
movement of the mouse, estimated to be in the range of .2 - .8 seconds by
Johnson, Payne, Schkade, and Bettman (1986), and an eye fixation, estimated to
require a DEMO of .2 seconds (Russo, 1978). ADDITIONS and SUBTRACTIONS both
take less than one second, with estimates of .84 (t(713) - 4.54, p < .0001) and
.32 (t(713) - .98, n.s.) respectively. These values are not significantly
different (t(713) - 1.03, n.s.) and are consistent with those provided by
Dansereau (1969), Groen and Parkman (1972), and others (see Chase, 1978, Table 3,
p. 76). Our estimate for the PRODUCT EIP, 2.23 seconds (t(713) - 10.36, p <
.0001), is larger than that commonly reported in the literature.
The time for DEMO is very short, .08 seconds (t(713) - .22, n.s.),
and that for ELIMINATIONS, 1.80 seconds (t(713) - DEMO, p < .01), is relatively
long. This may reflect the DEMO of COMPARES and ELIMINATIONS.
Choice Effort
32
In sum, based both upon its degree of fit and the generally plausible time
estimates for the EIPs, the proposed weighted EIP model receives impressive
support when response times are used as an DEMO of effort. The next set of
results examines the performance of the various models when self-reports of
effort form the indicator of effort.
Analysis DEMO Self-Reports of Effort
There are several reasons why self-reports of effort are interesting as a
second indicator of decision effort. First, self-reported effort might tap
different aspects of strategy execution effort and might not be closely DEMO
to decision latency. As Kahneman (1973) observed, two different mental DEMO may
take similar amounts of time, but one might be seen DEMO much more effortful than
the other. This speculation receives some support in our data: the overall
correlation between time and the self-reported effort index is .29. Secondly,
while the analysis of latency helps validate the DEMO EIP conceptualization
of effort, self-perceptions of effort may also be important DEMO understanding why
decision-makers avoid certain strategies. However, several cogent arguments for
DEMO in the use of self-reported measures of effort should also be noted.
Foremost among these is the possibility that subjects cannot accurately report
demands DEMO cognitive resources (Gopher and Donchin, In press), or that such
reports do not allow comparisons across tasks which make widely differing
demands.
DEMO Fit. From the results shown in Table 6, it can be DEMO that the
absolute levels of fit are lower than for the response latencies, but are still
highly significant (p < .0001).6 The DEMO EIP model again provides
significantly greater fit than the behavioral (F(DEMO, 713) - 10.0, p < .0001) or
equal-weighted EIP (DEMO(5, 713) - 13.0, p < .0001) models.
Y.7
*
0
S
0
U'
%
A,
'.
Choice Effort
33
The weighted EIP model of subjective effort can also DEMO compared to more
complex models. The weighted EIP by rule model shows a small, but statistically
significant increase in fit (incremental R- .02, F(13,700) - 2.6, p < .002).
The weighted DEMO by individual model shows a substantial increase in fit
(incremental R DEMO - .21, F(36,677) - 20.1, p < .0001)DEMO
Hence, the results essentially replicate those for response times. The
weighted DEMO model provides the best explanation of decision-makers' self reports
of the DEMO associated with each decision problem, and the effort estimates
appear to DEMO across individuals, but only slightly across rules.
Cross-validation of these results DEMO also encouraging. Average R 2 values
of .53, .54, .58, DEMO, and .78 are obtained for the behavioral, equal-weighted
EIP, weighted DEMO, weighted EIP by rule, and weighted EIP by individual models,
respectively.
7
Estimates of EIP Effort. Estimates of the subjective effort associated
DEMO each EIP from the weighted EIP model pooled across subjects are given in
Table 7. These estimates represent the increase in reported effort per DEMO on
the sum of two 0-10 scales. The largest estimate is for the ELIMINATION
operator, .32. However, the high intercorrelation between ELIMINATIONS and
DEMO (.85) must temper any interpretation of this coefficient and the
small (.04) coefficient for COMPARISONS. The PRODUCT operator, as might be
expected, is seen as fairly effortful, with a coefficient of .19, while the
coefficients for READS and ADDITIONS are also significantly positive.8
Discussion
The DEMO of effort plays a major role in attempts to understand the
contingent use of processing strategies. An approach to measuring the effort
associated with DEMO decision strategies is propused in this study, using a
set of DEMO operators (i.e., READS, ADDITIONS, COMPARISONS, PRODUCTS,
C
.4'
Choice Effort
DIFFERENCES, and ELIMINATIONS) as a common "language" for DEMO decision
strategies. This set of operators is used to generate a metric of the effort
required to execute a decision strategy in terms of DEMO number of EIPs involved.
The empirical results yielded strong support for this proposed componential
approach to strategy effort. A model of effort based upon DEMO EIP counts
(the weighted EIP model) provided good fits for response times and self-reports
of effort, two different measures of decision effort. In addition to this
absolute level of fit, the weighted EIP model also was statistically superior to
a behavioral model using only reads and to DEMO equal-weight EIP model for each of
the two indicators of effort.
The estimates of time taken for each EIP were mostly plausible and in DEMO
with prior research, hence providing additional confidence in the approach. We
DEMO examined the potential generalizability of our results to a broader range of
cognitive tasks. Specifically, estimates of the times taken for various EIPs
drawn from studies of other information processing tasks (see Johnson and Payne,
1985, p. 406 for the specific values of these estimates) were DEMO as weights to
produce an analogue to the weighted EIP model for response times.9 That is, the
values drawn from the literature for the time for each EIP were used as
coefficients of the EIP counts DEMO produce a predicted response time. This model
produced an R2 value of .81, only slightly below that of the weighted EIP model.
The performance of this model provides encouraging evidence that the componential
approach may generalize DEMO a variety of cognitive tasks. In addition, the
closeness of the DEMO estimates for individual EIPs obtained from our study of
decision making to those derived from other cognitive tasks, noted above,
provides support for the generalizability of our experimental procedure.
In general, the weights for various EIPs nbtained in our study were
essentially the same regardless of the DEMO strategy used. Hence, the
0
%
S
A.-.
-
'.A
-A
*were
|
%
-V.51'
Choice Effort
35
original assumptions of serial DEMO and independence of EIP duration across
rules and problem sizes made by Johnson and Payne (1985) receive encouraging
support in this research. Taken DEMO, these results imply that a small
number of simple operators can DEMO viewed as the futdamental components from which
decision rules are constructed (DEMO, 1979; Bettman and Park, 1980).
However, the results do suggest significant individual differences in the
effort associated with individual EIPs. For DEMO, for some individuals
computational operators were relatively more difficult than comparisons. DEMO
others, this difference was not present. This suggests the possibility that
DEMO Llay choose different rules in part because different component EIPs
may be relatively more or less difficult or effortful across individuals. In
addition, although the evidence for a model of effort based upon EIP counts is
DEMO, the findings presented thus far are all limited to a situation DEMO
which individuals were required to use various strategies which had been
prescribed for them. Suppose, however, that parameters characterizing individual
subjects' performance on this constrained choice task (e.g., average EIP times)
could be DEMO to those subjects' behaviors in a choice task where subjects
free DEMO use whatever strategy they wished. This would provide suggestive
evidence for the proposed componential approach.
As an exploratory analysis, we related the average times spent by subjects
on arithmetic operations (ADDITIONS and PRODUCTS) in the DEMO choice tasks
to the processing patterns used by those subjects in the unconstrained choices
they made in the final experimental session. In particular, processing patterns
of three subjects for whom arithmetic operators were relatively more expensive
(took more time) were compared to those of the four subjects for whom such
operators took less time. The subjects for whom the arithmetic DEMO were
relatively more expensive showed significantly greater variability in their
fl
* --
at.
-
6
..
a
'.,
*
Choice Effort
36
information acquisition across attributes and alternatives on the twelve
unconstrained DEMO This result is conpistent with use of more heuristic,
non-compensatory processes rather than use of computationally expensive
compensatory strategies such as weighted adding (Payne, 1976). Showing that
performance on the constrained task can be related to processing in unconstrained
choice situations offers suggestive support for the DEMO approach, although the
small sample size precludes strong conclusions.
Another contribution DEMO the study is more methodological. The MOUSELAB
decision-monitoring software and hardware worked exceptionally well in providing
detailed data about the decision task. The ability DEMO monitor the sequence of
acquisitions, measure latencies, and in general maintain experimental control.•
over the choice task makes this system potentially very valuable DEMO a variety of
research issues in decision making and other areas of cognition.
The attainment of experimental control, necessary to predict the operators
used and implement the proposed EIP models of effort, is not without costs. In
the constrained decision task, subjects do not select strategies; rather, they
apply given rules. Hence, the task eliminates many difficult problems normally
faced by individuals making decisions. Subjects did not have to select or
DEMO a strategy, and the sequence of operations was specified. Thus, they
did not have to engage in possibly effortful control processes determining what
DEMO do next. In addition, by providing all of the weights, cutoffs, and ratings,
the need for potentially difficult valuation processes was eliminated. Finally,
some of the timing estimates are undoubtedly affected by the DEMO apparatus
used (i.e., the matrix display and the mouse). These restrictions may be less
worrisome, however, given the analyses of the "free choice" task. Since the
timing estimates derived from the constrained choices predict aspects of the
processing patterns in the free choices, our confidence in the procedures used is
%a'
A
0
0
%
0
DEMO
a,*
Choice Effort
37
increased. However, further research relaxing these restrictions on processing
flexibility would be desirable.
A second set of caveats is that although DEMO approach which breaks down
decision strategies into more detailed components seems to be strongly supported
as an approach to measuring decision effort, we have focused on a particular
level of detail in taking such an approach. DEMO example, one could model
multiplications in terms of underlying arithmetic operations (e.g., Dansereau,
1969) or anchoring and adjustment (Lopes, 1982)DEMO In addition, one could extend
our models to include EIPs that DEMO the transfer of information to long-term
memory and various mental "bookkeeping" operations.
Modeling cognitive effort at the level of EIPs allows us to DEMO how the
effort associated with various strategies might vary as a function of differences
in task environments. In particular, such variation in effort across task
environments could be predicted by computer simulation of the performance of
DEMO heuristics in such enviror'ren!s. As an example of this approach,
Payne, Bettman, and Johnson (In press) used both computer simulation DEMO process
tracing experiments to examine the joint effects of effort and accuracy in the
adaptive use of decision processes. A Monte-Carlo simulation study utilized DEMO
proposed measure of effort based on EIP counts, along with various DEMO of
accuracy, to identify heuristic choice strategies that approximated the accuracy
DEMO normative procedures tnd required substantially less effort. No single
heuristic, however, did well across all task environments. Thus, a decision
maker striving to maintain a high level of accuracy with a minimum of effort
would DEMO to use a variety of heuristics.
Payne, Bettman, and Johnson (DEMO press) then tested the degree of
correspondence between the efficient processing DEMO for a given decision
problem identified by the simulations and the actual information processing
0*
-%
2.
6:7-
%
%
r/d
S
DEMO
0
V
N
S
I
e
0
1
0
Choice Effort
38
behavior exhibited by people. People were shown to be DEMO adaptive in their
responses to changes in the nature of the alternatives available to them, and to
the presence or absence of time pressure. The results for actual decision
behavior tended to validate the patterns expected DEMO the basis of the simulation
estimates. Of particular interest was the finding that people were sensitive to
changes in decision context that impact the DEMO accuracy of heuristics as
well as affecting relative effort.
Taken together, DEMO present results, plus those reported in Payne, Bettman,
and Johnson (In press), support the hypothesis that decision-makers choose
strategies as a function of a strategy's demand for mental resources, i.e., the
DEMO required to use a strategy, and the strategy's ability to DEMO an
accurate response. However, it is important to recognize that a DEMO/benefit
viewpoint of strategy selection does not rule out the possibility of other
factors impacting strategy usage. For example, there is growing evidence that
justifiability may influence the choice of processing strategy (e.g., Tetlock &
Kim, 1987). In addition, more perceptual factors such as the DEMO frame
(Tversky & Kahneman, 1981) may also influence strategy use. DEMO, the
present study, along with others, shows how measures of DEMO effort can be
predictive of strategy use.
Finally, the approach to DEMO cognitive effort developed in this paper
may also have applied value. For example, recently it has been suggested that
the use of nutritional information in the supermarket by consumers might be
improved by decreasing the effort DEMO associated with processing that
information (Russo, Staelin, Nolan, Russell, DEMO Metcalf, 1986). The methodology
developed in this paper could be DEMO to test the impact of different information
displays on the use of a preferred decision strategy. A related area of
Choice Effort
39
decision aids (Keen & Scott-
application would be the design of computer-based
Morton, 1978; Kleinmuntz & Schkade, 1988).
Choice Effort
40
References
Abelson, R.P., & Levi, A. (1985)DEMO Decision Making and Decision Theory. In
G. Lindzey and E. Aronson (DEMO), The Handbook of Social Psychology. Vol.l.
New York: Random House.
DEMO, L. R., & Mitchell, T. R. (1978). A contingency model for the selection of
decision strategies. Academy of Management Review, 3, 439-449.
Bettman, J. R. (1979). An information processing theory of consumer choice.
Reading, MA: Addison-Wesley.
Bettman, J. R., & Park, C. W. (1980). Effects of prior knowledge, experience,
and DEMO of the choice process on consumer decision processes: A protocol
analysis. DEMO of Consumer Research, 7, 234-248.
Bettman, J. R., & Zins, M. (1979). Information format and choice task effects in
decision DEMO Journal of Consumer Research, 6, 141-153.
Card, S. K., Moran, T. P., & Newell, A. (1983). The psychology of DEMO
interaction. Hillsdale, NJ: Lawrence Erlbaum.
Carpenter, P. A., & Just, M. A. (1975). Sentence comprehension: A
psycholinguistic processing model of verification. Psychological Review,
82, 45-73.
Chase, W. G. (1978). Elementary information processes. In W. K. Estes (Ed.),
Handbook of DEMO and Cognitive Processes, Volume 5. Hillsdale, NJ:
Lawrence Erlbaum.
Dansereau, D. F. (1969). An information processing model of mental
multiplication. DEMO dissertation, Carnegie-Mellon University.
Ghiselli, E., Campbell, J.P., & Zedeck, S. (1981). Measurement theory for the
behavioral sciences. San Francisco: DEMO Freeman.
Gopher, D., & Donchin, E. (In press). Workload: An examination of the concept.
In K. Boff & L. Kaufman (DEMO), Handbook of Perception and Human
Performance. New York: John Wiley.
DEMO
•
,
-
-
. o
.
. , ..
,
,.
', '
'
"
-
-
-
.
I0
FO ME
.-
-
-
-d
-4 -
Choice Effort
41
Performance. DEMO York: John Wiley.
Groen, G. J., & Parkman, J. M. (1972). A chronometric analysis of simple
addition. Psychological Review, L9, 329-343.
Hammond, K. R. (1986). A theoretically based review of DEMO and research in
judgment and decision making. Report 260, Center for DEMO on Judgment
and Policy, Institute of Cognitive Science, University of Colorado.
Hogarth, R.M. (1987). Judgement and Choice, Second Edition. New York:
John Wiley.
Huber, 0. (1980). The influence of some DEMO variables on cognitive operations
in an information-processing decision model. Acta Psychologica, DEMO, 187-
196.
Hunt, E. B. (1978). Mechanics of verbal DEMO Psychological Review, 85, 109-
130.
Johnson, E. J. (1979). Deciding how to decide: The effort of making a decision.
Unpublished manuscript, University of Chicago.
Johnson, E. J., & Payne, J.W. (1985). Effort and accuracy in choice. Management
Science, 31, 394-414.
Johnson, E. J., Payne, J. W., Schkade, D. A., & Bettman, J. R. (1986).
Monitoring information processing and decisions: The mouselab system.
Unpublished manuscript, Fuqua School of Business, Duke University.
Kahneman, D. (1973). Attention and effort. Englewood Cliffs, NJ: Prentice-
Hall.
Keen, P. G. W., & Scott-Morton, M. S. (1978). Decision DEMO systems: An
organizational perspective. Reading: Mass.: Addison-Wesley.
-
•
,
P
*
*.%*...% "
*
S q
*
Choice Effort
42
Klayman, J. (1985). DEMO's decision strategies and their adaptation to task
characteristics. Organizational Behavior and Human Decision Processes 35,
179-201.
Kleinmuntz, D., & Schkade, D.A. (1988). The Cognitive implications of
information displays in computer-supported decision making. Working Paper
2010-88, Alfred P. Sloan School of Management, Massachussetts Institute DEMO
Technology.
Kmenta, J. (1986). Elements of econometrics, Second Edition. DEMO York:
Macmillan.
Lopes, L. L. (1982). Toward a procedural theory of judgment. Unpublished
manuscript, University of Wisconsin.
Maule, A. J. (1985). Cognitive approaches to decision making. In G. Wright
(Ed.), Behavioral Decision Making. New York: Plenum.
Navon, D., & Gopher, DEMO (1979). On the economy of the human processing system.
Psychological DEMO, 86, 214-255.
Neter, J., & Wasserman, W. (1974). applied linear statistical models. Homewood,
Ill.: Richard D. Irwin.
Newell, DEMO, & Simon, H. A. (1972). Human problem solving. Englewood DEMO, NJ:
Prentice-Hall.
Paquette, L., & Kida, T. (1988)DEMO The effect of decision strategy and task
complexity on decision performance. Organizational Behavior and Human
Decision Processes, 41 128-142.
_"
Payne, J. DEMO (1976). Task complexity and contingent processing in human decision
making: An information search and protocol analysis. Organizational
Behavior and Human Performance, 16, 366-387.
Payne, J. W. (1982). Contingent decision behavior. Psychological Bulletin, 92,
382-402.
.
•
S
0
1
Choice Effort
43
Payne, J. W., Bettman, J. R., & DEMO, E. J. (In press). Adaptive strategy
selection in decision making. Journal of Experimental Psychology:
Learning, Memory. and Cognition.
Russo, J. DEMO (1978). Eye fixations can save the world: Critical evaluation and
comparison between eye fixations and other information processing
methodologies. In H. K. DEMO (Ed.), Advances in Consumer Research, Volume
Ann Arbor, Michigan: Association for Consumer Research.
Russo, J. E., & Dosher, B. A. (1983). Strategies for multiattribute binary
choice. Journal of Experimental Psychology: DEMO, Memory, and
Cognition, 9, 676-696.
Russo, J. E., Staelin, R., Nolan, C. A., Russell, G. J., & Metcalf, B. L.
(1986). Nutrition information in the supermarket. Journal of Consumer
Research, 13, 48-70.
Shugan, S. M. (1980). The cost DEMO thinking. Journal of Consumer Research, 7,
99-111.
Siegler, R. (DEMO). Strategy choice procedures and the development of
multiplication skill. Unpublished manuscript, Carnegie-Mellon University.
Sternberg, R. (1977). Component processes in analogical reasoning.
Psychological Review, 84, 353-378.
Tversky, A., & Kahneman, D. (DEMO). The Framing of Decisions and the Psychology
of Choice. Science, DEMO, 453-458.
Wickens, C. D. (1984). Engineering psychology and human DEMO Columbus,
Ohio: Charles E. Merrill.
Wright, P. L. (1975)DEMO Consumer choice strategies: Simplifying vs. optimizing.
Journal of Marketing Research, 11, 60-67.
4
%
-- I
6
pr i. '
Authors' Notes
Choice Effort
44
The research reported in this paper was supported by a contract from the
Engineering Psychology Program of the Office DEMO Naval Research. The order of
authorship is arbitrary. Each author contributed equally to all phases of this
project. Requests for reprints should be sent DEMO James R. Bettman, Center for
Decision Studies, Fuqua School of Business, Duke University, Durham, North
Carolina 27706.
.4'.
.4.
4-v.
"DEMO
'4. '
Choice Effort
45
Footnotes
'The behavioral model and the equal-weight EIP model are special cases of
(or nested within) the weighted EIP model. DEMO, the additional fit provided by
the weighted EIP model over each DEMO these two simpler models can be tested
statistically (Neter and Wasserman, 1974, p. 89). Similarly, the weighted EIP
model is nested DEMO the weighted EIP by rule and weighted EIP by individual
models.
2Although not the subject of interest in this paper, for completeness a
model using as independent variables the number of alternatives, the number of
attributes, their product, and a dummy variable for each rule was estimated. DEMO
R2 values for response time and self-reported effort were .65 and .57,
respectively.
3 The degrees of freedom for the numerator in these DEMO represent
the difference between the use of six EIP variables for the weighted EIP model
and one variable for the behavioral and equal-weight EIP DEMO The degrees of
freedom for the denominator reflect the total trials and the total number of
variables used for the weighted EIP model (Neter and Wasserman, 1974, p. 89).
4 The models were also DEMO using the response time data disaggregated
to the level of individual acquisitions, the total time spent on each
alternative, and the total time DEMO on each attribute. The relative fits of
the various models essentially replicate those of the aggregate results, although
the absolute levels of fit are of course lower for the more disaggregate
analyses.
5 There may be DEMO distinction between anticipated effort and experienced
effort, with the former being DEMO effort a strategy is predicted to require for
solving a problem and the latter reflecting the effort actually used. In the
current study, we focus on experienced effort. While strategy selection may be a
function of DEMO effort, we argue that a major basis for estimating
WML Mr
DEMO
-
-
0
•
S
0
.7
h
i
'A T)L XF
Choice Effort
46
function of anticipated effort, we argue that a major basis for estimating
anticipated effort is experienced effort on previous decision tasks. Hence,
analysis of experienced DEMO can potentially lead to insights into the bases
for anticipated effort. The relationship between anticipated and experienced
effort is an important topic for study, but it is beyond the scope of the current
investigation.
6 1f DEMO assume that the two self-report measures of effort and difficulty
which we combined to form the self-reported effort index are both fallible.-
measures of DEMO, the intercorrelation between them (r-.85) serves as a
baseline to DEMO the reliability of this index. Correcting for this
unreliability would provide an estimated R of .83 for the weighted EIP model for
effort (Ghiselli, Campbell, and Zedeck, 1981, p. 290.)
7 To further DEMO the robustness of the results, models for both response
times and DEMO of effort were run which added variables for the number of
alternatives, the number of attributes, and dummy variables for rules to the
DEMO EIP by individual model. While these additional variables produced
significant increases in fit (F(11,666) -3.30, p < .0002 and F(11,666) - 4.04, p
< .0001 for response time and effort DEMO), the increases in R 2 are very
small (.005 and DEMO for response time and effort, respectively).
8 Although errors are DEMO indirectly related to decision effort, for the
sake of completeness logistic DEMO were run using the behavioral, equal-
weighted EIP, and weighted EIP models with a 0-1 dependent variable representing
whether or not an error DEMO made on a particular choice problem. The pseudo-R2
values from this analysis were .62, .63, and .64 for the behavioral, equal-
weighted EIP, and weighted EIP models, respectively. The weighted EIP model.
performed better DEMO the behavioral model (p < .05) but not better than the
equal-weighted EIP model (p - .105).
9 The weighted EIP model for times was the only model. considered, since the
,
S
DEMO
'p.
.
%
Choice Eftokt
9 The weighted EIP model for times was the only DEMO considered, since the
estimates from the literature only refer to times DEMO do not account for
individual differences or differences across rules.
,
DEMO
..
.-
a,
I--
.-!
-o ,
.o
S{1g42fwefx}