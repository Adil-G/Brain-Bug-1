<-----Page 0----->Developing Trust with Technology: An Exploratory Study
Greg Elofson
Goizueta School of Business School
Emory University
1300 Clifton Rd. #418
Atlanta, Georgia 30322
Greg_Elofson@bus.emory.edu
(404) 727-1183

Developing Trust with Technology: An Exploratory Study

Abstract
While technology has been often held responsible for engendering
mistrust, little has been done to establish the role of information

<-----Page 1----->technology in creating trust in organizations. This paper explores the
phenomena of trust-building through Intelligent Agents. Beginning with
an overview on the meanings and measures of trust, the effects of trust
in a variety of settings are examined. Further, a series of exploratory
studies illustrate the positive effect of a intelligent agents in
generating trust in an organizational setting.

Introduction
Trust is threaded through our interpersonal relationships, our group
interactions, contracts and culture. The positive effects of trust on
social dilemmas has been shown in a number of
instances[1][23][34][35][38]. And, trust has been given a critical role
in labor-management relations[9][37], problem solving[6][40],
organizational performance[15], organizational communication[32],
prosocial behavior[33], and accepting feedback[9].
Trust has been credited with being the basic feature of all social
situations that demand cooperation and interdependence[17].
Furthermore, trust has been acknowledged in economic and organization
theory as the most efficient mechanism for governing
transactions[2][3][28]. From the sociological viewpoint, trust is
ìessential for stable social relationshipsî[5](p 64) and vital for
cooperation in society[29].
Clearly, the reach and effect of trust in the affairs of individuals
and organizations is largely pervasive. Nevertheless, trust is somewhat
illusive, difficult to define, difficult to create, and difficult to
measure.
To gather a better understanding of trust, and the relationship
information technology may have with trust, this paper covers several
areas. First, a number of perspectives on the meaning of trust are
examined and then synthesized to form a working definition of trust.
Second, a general overview of the role information technology may play
in the promotion of trust in organizations is given. Following this, a
more focused examination of the relationship of a particular class of
technologies, intelligent agents, to the promotion of trust between
superiors and subordinates is discussed. And, finally, a series of
exploratory studies are described that lend empirical support to the
notion that intelligent agents can play a role in the development of
trust in organizations.
Defining Trust

<-----Page 2----->The academic literature on trust provides a variety of meanings and
perspectives. For example, Garfinkel[13] characterizes trust as a
necessary taken-for-granted condition for social interaction: there
must always be an ìet ceteraî assumption where every agreement has
unspoken but understood qualifications, assumptions, and provisions for
future actions (247-248). Here, trust provides a foundation for
understanding and interpretation. But it appears to have cultural
bounds, inasmuch as the nature of the unspoken may vary across
individuals of different backgrounds. Trust, for Garfinkel, is
essential but brittle; easily broken by misunderstandings that
naturally arise in the growing number of exchanges between individuals
of different cultures.
Luhmann[21] notes that trust begins where knowledge ends: trust
provides a basis for dealing with uncertain, complex, and threatening
images of the future. The implications here are several. Trust becomes
a solution to cognitive dissonance, faintly coerced through an
inability to sort out uncertainty. Trust, here, is a reliance on some
number of certainties in turbulent conditions - despite conditions
largely unknown, other individuals, whose actions affect oneís own
welfare, can be counted on to act in a predictable and presumably
benevolent fashion.
Barber[4] is somewhat less circumspect in defining trust;
characterizing it as the expectations involving a general moral order
and specific norms of competence and responsibility. For Barber, in the
act of trusting, we make the belief that an associate will act in
accordance with a well understood conduct of behavior and we are
capable of saying just what that behavior entails. The implication is
that the ìet ceteraî assumptions described by Garfinkel are, perhaps,
better understood - but no less brittle.
Lewis and Weigert[20] define trust as ìobservations that indicate that
members of a system act according to and are secure in the expected
futures constituted by the presence of each other for their symbolic
representations.î They, too, characterize trust in terms of actions
that conform to expectations. They also implicitly address the
developmental nature of trust, noting that it is the outcome of
observations.
Some definitions of trust, however, are without an explicit set of
expectations. Rempel and Holmes[30] suggest that trust is simply ìthe
degree of confidence that you feel when you think about a
relationship.î This concept of trust has a strongly subjective bent
that elludes substantiation through any process of matching
expectations to outcomes.

<-----Page 3----->Like many other definitions of trust, Zaltman and Moorman[39] define
trust through prediction that is value free: ìan interpersonal or
interorganizational state that reflects the extent to which the parties
can predict one anotherís behavior; can depend on one another when it
counts; and have faith that the other will continue to act in a
responsive manner despite an uncertain future.î This definition does
not address what we often assume to be characteristic of trust - that
those expectations are largely about outcomes that are in common with
our own interests.
Giffin [14] includes trustís implicit goal directed characteristic in
providing an alternative definition: ìreliance upon the characteristics
of an object, or the occurrence of an event, or the behavior of a
person in order to achieve a desired but uncertain objective in a risky
situation.î Further, she cited the following elements as essential to
describing a trusting person:
1. A person is relying on something.
2. This something relied upon may be an object, an event, or a person.
3. Something is risked by the trusting person.
4. The trusting person hopes to achieve some goal by taking this risk.
5. The desired goal is not perceived as certain.
6. The trusting person has some degree of confidence in the object of
his trust.
Using these definitions, a composite definition of trust is suggested:
trust is the outcome of observations leading to the belief that the
actions of another may be relied upon, without explicit guarantee, to
achieve a goal in a risky situation. Whether or not the exact nature of
those actions can be enumerated is unspecified in this definition of
trust. The expected actions may be known or unknown.
Given, trust is valuable and can be loosely defined. We know trust when
it is apparent and recognize that it is essential for many activities
and valuable for almost all relationships. Trust is, however, fragile.
Moreover, trust is the outcome of observations: it takes time to
engender trust. Itís existence necessarily requires continued
performance and adherence to the expectations placed on the trusted
individual. Trust, then, must be both created and maintained.
Butler[7] points out that trust is related to situational variables: 1)
dimensions of organizational climate such as whether information is
shared or withheld, whether disagreements are encouraged or buried, and
whether managers share or keep power, 2) type of job, including skills
and technology required, 3) layout of the work area, 4) reward system
and whether it encourages competition or cooperation, 5) the existence
of mutual goals, and 6) a common threat. These six variables relate to

<-----Page 4----->several of the components of trust:
1. the ability to make observations
2. unspoken agreements and assumptions
3. goal achievement under risk.
For example, the figure below illustrates the relationship Butlerís
variables and the components of trust. Here it is shown that sharing
information, provided that the information is understood, enables the
individual to make observations. Also, permitting disagreements
provides the opportunity for understanding the underlying assumptions
another is using, resolving misunderstandings of issues falling in
Garfinkelís ìet ceteraî category.
Figure 1. Relationships between Variables and Components of Trust
Also, in the figure are other relevant relationships between the
variables and components that further illustrate, though not
exhaustively, the rationale for situation variables affecting creation
and maintenance of trust. Some of these variables, however, do not
readily apply to the trust components; whether managers share or keep
power and layout of work area.
To establish a relationship between the trust components and the
willingness to share power, additional focus must be placed on the
creation and maintenance of trust. Butler suggests that trust may be
created simply through the display of trusting behavior[7]. So, when
the manager behaves as if there is an unspoken agreement that he or she
the shares the same goal as the subordinate (through the distribution
of his or her power), the subordinate may infer that the manager has
ìet ceteraî assumptions and/or goals similar to his or her own.
Therefore, the manager who shares power exhibits trusting behavior, and
in so doing engenders trust in the subordinate.
While trusting behavior tends to create trust, behavior that may be
perceived as distrustful has, not surprisingly, the opposite effect.
Related to Butlerís argument for the value of exhibiting trusting
behavior, Zand[40] summarized a possible negative effect of information
exchange on trust in the following passage:
One who does not trust others will conceal or distort relevant
information, and avoid stating or will disguise facts, ideas,
conclusions, and feelings he believes will increase his exposure to
others, so that the information he provides will be low in accuracy,
comprehensiveness, and timeliness; and therefore have a low congruence
with reality...When others encounter this behavior, initially they will
hesitate to reveal information, reject influence, and evade control.
When others fail to disclose information that may be used to make
observations, not only is the creation and maintenance of trust

<-----Page 5----->inhibited, but a message is sent that the particular individual
responsible for the lack of disclosure may have very different goals
from the individual evaluating them.
That these findings are evident in organizational environments is
suggested by March, who noted the following: ìDecision-makers discount
much of the information that is generated. Not all information is
ignored, however, and inferences are made. Decision-makers learn not to
trust overly clever people, and smart people learn not to be overly
clever.î[12].
The Role of IT in Trust
Overall, deviation from the components of trust will negatively affect
its creation and maintenance. Actions that prevent either the ability
to make observations, the understanding of anotherís assumptions, or
the sharing of goals reduce the likelihood of developing trust.
Conversely, sharing information and power, familiarity with anotherís
tasks, encouragement of cooperation, however, do encourage the possible
creation and maintence of trust.
Information technology may play a vital role in the promotion of trust
in organizations. Until now, the role of IT in the creation and
maintenance of trust has not only been overlooked, but information
technology has long been understood as an inhibitor of trust. For
example, Zuboff[41] explains that managers perceive operator mistrust
as an expression of resistance to technology (p. 89). And, Muir[25]
suggests that ìa decision aid, no matter how sophisticated or
intelligent ...may be rejected by a decision-maker who does not trust
it, and so its potential benefits to system performance will be lost.î
An inventory of several kinds of information technologies and how they
can support the creation and maintenance of trust, however, provide the
beginnings of an understanding of their trust promoting role:
Group Decision Support Systems[26] - These systems encourage the
sharing of information, cooperation, and assumption surfacing
Decision Support Systems[36] - These systems encourage repeatability of
decision method and the facility for reproducing the decision method
used
Knowledge Based Systems[16] - These systems include the development of
codified heuristics, used in decision making, that may be examined and
tested
Organizational Decision Support Systems[19] - These systems include

<-----Page 6----->organization-wide executive information systems that allow for the
evaluation of activities that may reinforce an executives belief in
goal congruence
Delegation Technologies[10] - These systems provide for the cognitive
re-apportionment of decision making activities, and in doing so, create
a decision audit trail that can be used to make observations.
These information technologies should have the capability, if properly
managed, of enhancing an organizationís ability to create and maintain
trust. The goal of this paper is, however, not to exhaustively treat
the possible trust promoting characteristics of each of these types of
technologies. Rather, this paper will provide a concentrated
examination of the trust promoting characteristics of intelligent
agents.
Intelligent Agents
Intelligent agents provide a mechanism for the re-apportionment of
cognitive responsibilities to machine actors. They acquire the
heuristics decision makers use over time, and assume responsibility for
a growing number of decision-making activities over time and
proportional to the tasks given to them. Also, the heuristics that the
intelligent agents capture may be useful for promoting trust in the
individual responsible for their development.
Consider, for example, the business task of environmental scanning,
where a senior manager may rely on the assessments of scanning unit
personnel for making decisions. The process is severely hampered by a
lack of trust in the scanning unit personnel[11]. And interleaving
intelligent agents between the senior manager and the scanner may
provide the necessary additional information, the ability to make
observations, to enhance the senior managerís trust in the scanner.
Under these circumstances, intelliigent agents would be used by the
subordinate manager responsible for interpreting information about the
organizationís external environment. With the passage of time, the
decision heuristics that this manager used to evaluate threats and
opportunities in the business environment are re-apportioned to the
intelligent agents, making the knowedge available to the rest of the
organization as well as allowing the initelligent agents to have a
greater role in the scanning process.
These intelligent agents might also serve as a promoter of trust,
because the additional information provided by them, information that
supplements the scannerís reports to senior management, includes the
intelligent agent generated cognitive rules that a scanner uses in

<-----Page 7----->interpreting the environment. The rules, the additional cognitive
information generated by the intelligent agents, may be added to
scanning reports. And, judging from the set of experimental evaluations
that follow, the rules significantly increase trust in the individuals
providing those reports.
A more detailed discussion of a intelligent agents and their role in
the process of scanning the business environment, and whether or not
they have the capability of creating and maintaining trust, is the
subject of the rest of this paper.
Intelligent Agent System Used to Test Trust
In the empirical study that follows, I used the an existing intelligent
agent system developed in 1990, that was designed for supporting the
applied task of environmental scanning [10]. During environmental
scanning activities, managers and analysts combine their efforts in
monitoring and searching the environment. For example, given the most
current goals of the organization, the managers (experts in some aspect
of the external environment such as political events, regulatory
measures, competitor financial status, etc.) decide upon monitoring
sets of qualitative indicators that might provide insight into various
threats and opportunities to the organization. Once the indicators are
chosen, the managers request estimates from the analysts of the
indicators' values. The analyst has the role of locating and
interpreting information that will shed light on the disposition of the
indicators in question. So, in a typical information request, the
manager generates a list of information requirements and sends them via
electronic-mail to the analyst . The analyst finds the answer, and
sends them back. After this, the manager classifies the information.
The figure below illustrates the information exchanged in this
relationship:

Figure 2. Flow of Information in Environmental Scanning
These intelligent agents provide an active channel of communication
between analysts and managers, where their tasks are structured as
separate sequential interleaved processes. For example, when an
organization monitors the political climate of a foreign country, the
attributes a manager may considers the following: pro-regime and
anti-regime sense of relative deprivation; pro-regime and anti-regime
belief in violence; coercive force available to both pro-regime and
anti-regime actors; and institutional support for both pro-regime and
anti-regime actors. In seeking attributes for these values, the manager
initiates a request for information by passing a structured message to

<-----Page 8----->a particular intelligent agent. The initial message would state the
following:
a) attributes for which the manager requires values and the name of the
intelligent agent - which corresponds to the threat or opportunity
being monitored
b) an explanation or elucidation of the attributes to better clarify
the nature of each being requested
c) scaling information specifying the values which are acceptable as
answers to the attribute request.
This structured message is delivered by the intelligent agent to the
analyst, and the questions about the attribute values, together with
the explanations and scaling information, are given by the intelligent
agent to the analyst. To each of the attribute questions the analyst
responds with a value corresponding to one of the scaled values
provided by the intelligent agent. Additionally, the analyst may
provide a written explanation of why he chose a particular scaled value.
Once this is done, the intelligent agent carries its new information
back to the manager. And, upon returning to the manager, the
intelligent agent displays the answers to the specified questions and
asks (in effect), ìWhat does it mean?î To this, the manager may ask
for an explanation of one of the analyst's answers, or give the
intelligent agent an assessment, a classification, of the information
provided. With the answer, the intelligent agent forms an initial
concept - a concept that is represented as a collection of
attribute-value pairs and a classification.
The figure below shows the rules induced by this system in a series of
eight separate political analyses performed on archival data that
described the Polish conflict of the early 80ís[10].

Figure 3. Rules Generated by Intelligent Agent
These concepts, corresponding to somewhat likely, highly likely, and
likely chances of political turmoil, represent the cognitive heuristics
a managerís has used in diagnosing problems. But whether making these
heuristics available to a senior manager will help in the either the
creation or maintenance of trust remains to be seen, and is the subject
of the next section.

<-----Page 9----->Empirical Evaluation of Trust
In evaluating the effect of decision heuristics on principal trust,
several exploratory evaluations were conducted. Specifically, four
tests were run to evaluate the effects of intelligent-agent generated
decision heuristics on trust, and consequently, whether there would be
an increased propensity for trusting a decision-maker who provided
those decision heuristics to the intelligent agents. The first of these
evaluations was conducted to establish whether the computer generated
information provided by an intelligent agent would positively impact
trust. Following these positive results, a second test was run to
determine whether the presence of only decision heuristics (without the
subjects knowing they were computer generated and without the guarantee
of additional information) would have a positive effect on trust, where
the subjects were professional managers. The third test was run to
examine what kinds of order effects in the paired t-test were present.
And the fourth test was conducted to determine whether arguably
irrelevant (placebo) information affected trust.
Within these experiments, a collection of constraints guided the design
of each. These considerations required that: 1) like the principals,
the subjects in the experiment should not have high familiarity with
the scanning activity of interest, 2) also, to avoid confounding the
value of the decision heuristics with subject opinion, an area of
unfamiliarity be chosen, 3) to avoid the possibility of
semantic/presentation effects, the evaluations presented by the two
experts under scrutiny be randomized, and 4) the heuristics presented
to the subjects be generated from the above mentioned intelligent agent
sytem. Given these guiding considerations, the design of each
experiment is described.

Test 1
Fifteen students from a graduate level evening course in expert systems
were instructed to read a brief statement informing them of their need
to have relevant environmental scanning information as part of their
business responsibilities:
You have the responsibility of developing Eagle Corpís overseas
business, and one of its concerns is in negotiating and winning a
contract with the Buhmar Company, Poland's largest manufacturer of
tractors. The negotiation is over a contract of several million dollars
to supply parts for Buhmar's new line of tractors. You have the
responsibility for closing contract negotiations with Buhmar. Moreover,

<-----Page 10----->you improve your companyís position the longer the negotiations take.
But, if political violence breaks out in Poland, you stand to lose
income.
You have two political analysts working for you (A and B). They are
located in different divisions within Eagle Corp., and have recently
sent this week's update to you about political tensions in Poland.
Please read them and fill out the accompanying questionnaire.
The students were told that two environmental scanning professionals
would be providing them with the needed information. Next, they were
then given an environmental assessment by the first professional:
Analyst Aís conclusion:
Given the current political tensions in Poland and the growing division
between the government and the citizens, I believe that political
turmoil is highly likely - and that further destablizing activities
could result in an outbreak of violence. The fact that economic
conditions have lately deteriorated for the workers, who ar faced with
the military power controlled by the Central Committee, makes the
problem worse.
They were then asked to answer a series of Likert-type questions (the
McCroskey Trust Scale [22]) about their trust in that individualís work
(figure 3) (A = 5 = Strongly Agree, B = 4 = Agree, C = 3 = Neutral, D =
2 = Disagree, E = 1 = Strongly Disagree). Next, the students were given
a very similar assessment by the second professional and the rules
generated by the machine induction algorithm (figure 4) about the topic
in question:

Analyst Bís Conclusion:
Political Turmoil is highly likely at this time. The increase in
tensions between the workers and government could quickly lead to an
outbreak of violence unless the workerís current grievances are quickly
heard and acted on by the government. This problem is being exacerbated
by the Polish army and police remaining very loyal to the Central
Committee.
Additionally, they were told that the rules that accompanied the second
opinion were generated by a machine learning algorithm that monitored
the activities of the second professional, and that supportive

<-----Page 11----->information for each of the variables in the rules was available to
them. They were read an example of the kind of information about a
particular variable that could be expected, and were then asked to fill
out a duplicate, Likert-type questionnaire regarding their trust in the
competence of the second professional.
Finally, they were given five final questions regarding their trust and
willingness to hire either one or the other professional: (as it turned
out, questions 4 and 5 might have been unclear - read as though the
subjects were being asked if they could choose A or B over any analyst
in the world.)
1. I trust the results given by analyst A more than analyst B: A
B C D E
2. I trust the results given by analyst B more than analyst A: A
B C D E
3. If you prefer the results of one analyst over another, please state
as specifically as possible why:
4. If I could have only one analyst working for me, I would choose
analyst A: A B C D E
5. If I could have only one analyst working for me, I would choose
analyst B: A B C D E

Analyst B: Rationales used for Political Evaluation
Condition 1: Highly Likely Political Turmoil
IF the Pro-Regime Actors' Belief in Violence is Strong
and the Anti-Regime Actors' Belief in Violence is Somewhat Strong
and the Institutional Support for Pro-Regime Actors is Moderate
and the Institutional Support for Anti-Regime Actors is Strong
Then a condition of political turmoil being highly likely is indicated.
To support this conclusion, the following must be verified:
Pro-Regime Actor's Relative Deprivation is Low
Anti-Regime Actor's Relative Deprivation is High
Coercive Force Available to Pro-Regime Actors is Strong
Coercive Force Available to Anti-Regime Actors is Not Strong

<-----Page 12----->Condition 2: Likely Political Turmoil
IF Pro-Regime Actors' Belief in Violence is Moderate
and Institutional Support for Pro-Regime Actors is Low
and Institutional Support for Anti-Regime Actors is Strong
then a condition of political turmoil being Likely is indicated.
to support this conclusion, the following must be verified:
Pro-Regime Actors' Relative Deprivation is Low
Anti-Regime Actors' Relative Deprivation is High
Anti-Regime Actors' Belief in Violence is Moderate
Coercive Force Available to Pro-Regime Actors is Strong
Coercive Force Available to Anti-Regime Actors is Not Strong
Condition 3: Political Turmoil only Somewhat Likely
IF Pro-Regime Actors' Belief in Violence is Moderate
and Pro-Regime Actors' Institutional Support is Low
then a condition of political turmoil being somewhat likely is
indicated.
to support this conclusion, the following must be verified:
Coercive Force Available to Anti-Regime Actors is Not Strong
Pro-Regime Actors' Relative Deprivation is Low
Anti-Regime Actors' Relative Deprivation is High
Anti-Regime Actors' Belief in Violence is High
Coercive Force Available to Pro-Regime Actors is Strong

Figure 4. Intelligent Agent Generated Rules Provided for Test Subjects
Test 2
The second test was run to determine several effects. First, while the
results of the initial test were very positive, we wanted to determine
whether professional executives would trust the political analysis of
the expert providing the decision heuristics more than the other
expert. Additionally, we wanted to determine whether trust was still
increased when the cognitive information was presented as is, without
attributing its origin to an AI based program, and without the promise

<-----Page 13----->of additional supporting data - as in the first test.
Twenty-three professional managers participated in this test. Their
average age was 37 years and they had an average of 15 years of work
experience. They were given the same description of responsibilities as
were the students in the first test. First, they reviewed the analysis
by the expert without additional heuristics. Second, they reviewed the
second expert's analysis with the heuristics.
As a precaution, the written opinions of the two experts were
randomized, so that the written opinion of A on one subjects's report
appeared as the written opinion of B on another's. Moreover, no mention
of any AI-based support system was made in regard to the decision
heuristics. The heuristics were presented simply as the way in which an
opinion was formed. Also, no mention of the availability of any
supporting information was made in reference to the heuristics.
Test 3
A third test was conducted to determine the presence of any order
effects in the presentation of the experts' findings. Fourteen students
from a graduate level evening course in systems analysis participated
in the experiment. The conditions of the experiment were identical to
test 2, except that the expert's opinion that was accompanied by
decision heuristics was presented first.
Test 4
A fourth test was run to determine whether an information ìplaceboî
effect could be found. Because the effects of the decision heuristics
were positively related to trust in the previous experiments, the
possibility of this effect being related to just ìmore informationî was
investigated.
Seventeen students from a graduate evening course in expert systems
served as subjects for this test. Also, the test was conducted in the
same order and with the same information as test 3, except that the
heuristics were replaced by the following information:
A recent newspaper article summarized Poland's past changes by
reporting that, originally, Communist rule was opposed by most Poles.
But the Communists used police power and other methods to crush
resistance. Communist controlled elections in 1947 gave them a large
majority in the new legislature. By 1948, Communist rule was firmly
established.

<-----Page 14----->During the late 1940ís, the USSR gained increasing influence over the
Polish government. In 1949, a USSR military officer, Konstantin
Rokossovsky, was made Polandís defense minister. And, Polish Communists
suspected of disloyalty to the USSR were removed from power.
In 1970, strikes and riots broke out in Gdansk and other cities.
Thousands of Poles demanded better living conditions and economic and
political reforms. After several days of riots, Gomulka resigned and
Edward Gierek became the Communist Party leader.
The answers to the two sets of 22 McCroskey trust scale questions were
analyzed as a paired t-test. Means for each of the responses regarding
A and B, together with p-values for the two-tailed test, are reported.
Also, the five remaining questions were evaluated. The first two
regarding relative trust in A and B, as well as the fourth and fifth
questions regarding hiring A or B, were evaluated as unpaired t-tests.
The means for these values, as well as the p-values for the two-tailed
tests, are also reported here. The written answers for the third
questions were completed only occasionally and will be discussed below.
Discussion
Test 1
Figure 5 presents the data of the initial test. The evidence suggests
that the effects of the data generated by the delegation technology
approach had a significant impact on the trust the subjects had in B
over A. The questions being least affected by the intervention of the
AI-based data were questions 13, 15, and 16.These questions focused on
the professionalís status in society, as well as on general authority
and experience. In discussing these questions with the subjects after
the experiment was administered, they expressed their belief that the
person would not have had the job unless they were qualified. So it
appears that they answered those particular questions from an a priori
perspective that was not significantly influenced by the available
information.
Concerning the final set of questions, the results of which are shown
in figure 6, the AI-based intervention appears to have had a positive
significant impact on both trust and hiring preferences. The answers of
thirteen subjects were used in this evaluation, instead of fifteen, as
two of the subjects did not respond to the final five questions. Also,
while many of the respondents did not provide the written answer
requested by the third of these questions, some did respond. For
example, the reasons they gave for trusting the analysis accompanied by

<-----Page 15----->intelligent-agent generated information included the following:
1) ì A conclusion on a matter should be based on an established,
consistent, procedural or methodological evaluation process,î
2) ìAnalyst B not only states his conclusions, but also gives reasons
for his statements. You can see and feel the flow in thoughts in
analyst B,î
3) ìAnalyst B had reasons for his result. Also, of equal importance,
he/she could give facts to backup the rankings of attributes,î
4) ìIn analyst B's report he was able to back his information with a
given set of rules. I, as a manager, like to be able to look at the
analyst's reasoning and critique it for possible error,î
5) ìI would show A the work done by B and tell him to make his work
look like that,î
6) ìThe facts and rules leading to analyst B's conclusions are covered
in more detail and they appear to indicate more thorough research.
However, A's line of reasoning is more clearly presented.î
Not all of the subjects, however, preferred the work of the
intelligent-agent supported analyst B. Of the written explanations,
there was one who trusted A over B and had the following to say:
ìI prefer short reports. I hate to read through extra material just to
get the answers to my questions. I trust analyst A to do the job. I
trust my judgement. I don't need a bunch of explanations, just the
answer to my question.î
Figure 5 - Results from Test 1

Figure 6 - Test 1 Comparisons for Trust and Hiring Preferences
Test 2
Figure 7 represents the data of the test run with professional managers
as subjects. While the evidence suggests that the effects of the
AI-based data had a significant impact on trust, it had a less positive
impact, as measured by the 22 questions of the trust scale, than it had
on the initial group. Factors that may have contributed to this
difference include issues of presentation and the promise of extra

<-----Page 16----->information. Concerning presentation, OíKeefe [27] showed that
improvements in style increase perceived prestige. Thus, a terse set of
rules would correspond to lower perceived prestige in the case of the
professional managers, as opposed to the initial group, because they
were not told the information was computer generated and could
conceivably be expecting more eloquence in the presentation. The
initial group would possibly expect a terse style of presentation as
they were informed of this prior to the onset of the experiment.
The absence of the promise of additional backup information may also
have contributed to the smaller impact of the intelligent-agent
generated data. In the initial group, they were read an example of some
of the explanatory information that supported Bís rules, and told that
additional information was available. In the case of the professional
managers, the availability of additional information was neither
suggested nor implied. They received only the heuristics that
accompanied Bís analysis - nothing else. There was no information
available about Poland. There was only data about the decision-makerís
method of analysis. And looking back to the written answers of the
initial group in test 1, to the remarks numbered ì3î and ì6,î the
combination of rules and facts were given as reasons for the greater
trust in B.
Like the first test, greater statistical significance was found when
measuring attitudes about the analystís intelligence and ability to
provide an analysis of the current problem. Less significance was
detected concerning the a priori characteristics of the analyst (ie.
questions 13, 14, 16, 18-22).

Figure 7 - Results from Test 2

Figure 8 - Trust and Hiring Preferences from Test 2
Overall, the results of the final set of questions regarding trust and
hiring preferences showed the effects of AI-based intervention to be
strongly significant (figure 8). The number of respondents for the
trust preference was 19, and the hiring preference was 20, as not all
of the subjects filled out this part of the questionnaire. Very few
filled out the written answer to question 3, but consistent with the
presented data, most of the answers affirmed the positive effects of
the cognitive data:

<-----Page 17----->ìAnalyst B demonstrated more systematic thinkingî
ìAnalyst B conveyed messages with depth and knowledge of subjectî
ìI prefer B. Identification of logical factors gives credibilityî
One of the professional managers, however, preferred Aís analysis and
explained with the following:
ìA gives the impression that he has an understanding of the situation.
Analyst B gives the impression that he just understands the mechanisms
involved but not necessarily the actual conditions.î
Finally, one professional manager strongly disagreed with trusting
either analyst, writing: ìI donít like either of them.î

Figure 9 - Results from Test 3

Figure 10 - Trust and Hiring Preferences for Test 3

Test 3
The third test was run to detect the possible effects of the order in
which the analyses were presented. Thus, the analysis accompanied by
the intelligent-agent based cognitive data was presented before the
analysis without the cognitive data. Other than this difference, the
test was run just as test 2. The 14 respondents were graduate and
undergraduate students in an evening systems analysis class. The
results of their responses are shown in figure 9. The evidence suggests
that, when the order of presentation was reversed, the effects of the
data generated by the knowledge processor still had a significant
impact on the trust the subjects had in A over B, to roughly the same
degree (and slightly weaker) as for the subjects in test 2.
Overall, the summary questions at the end of the experiment provided
strong evidence that the subjects trusted and preferred to hire the
analyst whose information was accompanied by the decision heuristics
(figure 10). Finally, the few remarks that were made to clarify why the

<-----Page 18----->analysis of one was preferred to another were consistent with the
remarks of the other previous test.
Test 4
The fourth test was conducted to determine whether the presence of a
greater quantity of information was responsible for increased trust in
analysts. The accompanying information detailed above was presented
with the findings of the first analyst, A. The results of the test are
shown in figure 11. These results show that the arguably irrelevant
information not only had no positive impact on trust, but rather on
several dimensions (questions 5, 11, 15, 16, 21) detracted from trust.
Overall, based on the responses to the final five summary questions
(figure 12), we conclude that the ìplaceboî information had no impact
on either trust preferences or hiring preferences. The few remarks made
concerning whether one analyst should be trusted over another include
the following:
ìB was more explicit and clear about the reason for the turmoilî
ìB was more specific in terms of pointing out the population involved
(working people)î
Figure 11 - Test Results from Test 4

Figure 12 - Trust and Hiring Preferences for Test 4
Conclusion
Trust has many facets and many influences. In sociological and economic
realms, the influence of trust is rather pervasive and typically
positive. The breadth of our notions of trust, however, combined with
its elusiveness and resistance to value measurements, may have
contributed to the tentativeness with which researchers have regarded
it.
Given the outcomes of the empirical studies discussed in this paper, it
is important to separate them from being a simple re-validation of the
widely held information economics viewpoint on information. That is,
from the information economics viewpoint, decision-makers generally
prefer more information until its marginal cost exceeds its incremental
value. If the experimental results were viewed as being a simple
re-iteration of this premise, a salient point would be overlooked.

<-----Page 19----->These experiments examined how information pertaining to the
problem-solver, and not the problem, affects trust. In studiess two and
three, the subjects in the experiment were not responding to direct
information about Poland and its potential for political violence. They
were responding to cognitive data that explained how the scanner
thought about the problem. There was no information about Poland in
those heuristics. The evidence suggesting that this cognitive data
engendered trust in the scanners was derived from the availability of
how the scanners thought. Simply giving additional information about
Poland, as in experiment 4, did not engender trust in the scanner.
Still, an alternative scenario for creating this greater trust, other
than the intelligent agents, should be addressed. That is, why not
simply have the scanners report their cognitive information without the
intervention of intelligent-agent based knowledge acquisition methods?
There is a straightforward reason for this. First, the literature
explaining the extreme difficulty experts have with specifying their
heuristics is voluminous. Simply put, experts are generally very
unsuccessful at stating cognitive information. The examples used in the
experiment, where both analysts gave a written evaluation of the
political situation in Poland, displayed a genuine sense of reasoning
without appearing to be deliberately ìcognitive.î Any attempt to
present the written findings of the analysts as a set of heuristics
would have gone directly against research in knowledge acquisition.
Finally, besides promoting trust, information technology may have a
role for promoting trust through such factors as reliability, dynamism,
an communicator intentions. The solutions may reach to problems
dominated by control, such as the agency problem. They may also
contribute to solutions of the ìcommons problem,î [24] as well as
information sharing.
References
[1] Alcock, J. E., and Mansell, D. ìPredisposition and behavior in a
collective dilemma,î Journal of Conflict Resolution, 21, 1977, 443-457.
[2] Arrow, K., ìPolitical and Economic Evaluation of Social Effects and
Externalities.î In J.Margolis (Ed.), The Analysis of Public Output. New
York, National Bureau of Economic Research, Columbia University Press,
1970.
[3] Arrow, K., The Limits of Organization. New York, Norton. 1974.

<-----Page 20----->[4] Barber, B., The Logic and Limits of Trust, New-Jersey, Rutgers
University Press, 1983.
[5] Blau, P.M., Exchange and Power in Social Life, New York, Wiley,
1964.
[6] Boss, R.W., ìTrust and Managerial Problem Solving Revisited,î Group
and Organizational Studies, 3(3), 1978, 331-342
[7] Butler, J.K., Jr., ìReciprocity of Trust between Professionals and
Their Secretaries,î Psychological Reports, 53, 1983, 411-416.
[8] Deutsch, M.A., ìTrust and Suspicion,î Journal of Conflict
Resolution, 1958, 2, 265-279.
[9] Earley, P.C. ìTrust, Perceived Importance of Praise and Criticism,
and Work Performance: An Examination of Feedback in the United States
and England,î Journal of Management, 12, 4, 1986. 457-473.
[10] Elofson, G. S., and Konsynski, B. R., ìDelegation Technologies:
Environmental Scanning with Intelligent Agents,î Journal of Management
Information Systems, Vol 4, No 2, Summer 1991.
[11] Elofson. G.S., ìAn AI-Based Monitoring Technology for Management
Control,î Proceeding of the 26rd Hawaii International Conference on
Systems Sciences, January 1993.
[12] Feldman, M.S., and March, J.G., ìInformation as Signal and
Symbol,î Administrative Science Quarterly, 26, 1981, 171-186.
[13] Garfinkel, H., ìStudies in the Routine Grounds of Everyday
Activities,î Social Problems, 1964. 225-250.
[14] Giffin, K., ìThe contribution of studies of source credibility to
a theory of interpersonal trust in the communication process,î
Psychological Bulletin, vol 68, no 2. 1967. 104 -120.
[15] Hart, K.M., Capps, H.R., Cangemi, J.P., Caillouet, L.M.,
ìExploring Organizational Trust and its Multiple Dimensions: A Case
Study of General Motors,î Organization Development Journal, Summer,
1986. 31-39.
[16] Hays-Roth, F., D.A. Waterman, and D. Lenat, eds., Building Expert
Systems, Addison-Wesley, Reading, Mass., 1983.
[17] Johnson-George, C. and Swap, W.C., ìMeasurement of Specific
Interpersonal Trust: Construction and Validation of a Scale to Assess

<-----Page 21----->Trust in a Specific Other,î Journal of Personality and Social
Psychology, Vol. 43, No.6, 1982, 1306-1317.
[18] Lebowitz, M., ìExperiments with Incremental Concept Formation:
UNIMEMî Machine Learning, Vol 2, No 2, September 1987.
[19] Lee, R.M., A.M. McCosh, and P. Migliarese, eds. Organizational
Decision SUpport Systems, North-Holland, Amsterdam, The Netherlands,
1988.
[20] Lewis, D. and Weigert, A., ìSocial Atomism, Holism, and Trust,î
Sociological Quarterly, 1985, 455-471.
[21] Luhmann, N., Trust and Power, New York, John Wiley, 1979.
[22] McCroskey, J.C., ìScales for the Measurement of Ethos,î Speech
Monographs, 33, 1966, 65-72.
[23] Messick, D.M., Wilke, H., Brewer, M.B., Kramer, R.M., Zemke, P.E.,
Lui, L., ìIndividual Adaptations and structural change as a solution to
social dilemmas,î Journal of Personality and Social Psychology, 44,
1983, 294-309.
[24] Moore, S.F., Shaffer, L.S., Pollak, E.L., and Taylor-Lemcke, P.,
ìThe effects of interpersonal Trust on Prior Commons Problem Experience
on Commons Management,î The Journal of Social Psychology, 127(1), 1987,
19-29.
[25] Muir, B.M., ìTrust between Humans and Machines, and the Design of
Decision Aids,î International Journal of Man-Machine Studies, 27, 1987,
527-539.
[26] Nunamaker, J.F., L. Applegate, and B.R.Konsynski, ìComputer Aided
Deliberation: Model Management and Group Deliberation Support,
Operations Research, Nov.-Dec, 1988.
[27] OíKeefe, B.J. and S. A. McCornack, ìMessage Design Logic and
Message Goal Structure: Effects on Perception of Message Quality in
Regulative Communication Situations,î Human Communication Research,
Vol.. 14, No. 1, Fall 1987.
[28] Ouchi, W.G., ìMarkets, Bureaucracies, and Clans,î Administrative
Science Quarterly, 25, 1980, 129-141.
[29] Parsons, T., The Social System, Glencoe, IL, Free Press, 1951.

<-----Page 22----->[30] Rempel, J.K., Holmes, J.G., and Zanna, M.P., ìTrust in Close
Relationships,î Journal of Personality and Social Psychology, 49, 1985,
95-112.
[31] Rempel, J.K., and Holmes, J.G., ìHow do I Trust Thee?î Psychology
Today, February, 1986, 28-34.
[32] Roberts, K.H. and OíReilly, C.A., III. ìFailures in Upward
Communication in Organizations: Three Possible Culprits,î Academy of
Management Journal, 17, 1974, 205-215.
[33] Rotter, J.B., Interpersonal Trust, Trustworthiness, and
Gullibility,î American Psychologist, Vol. 35, No 1, January 1980, 1-7.
[34] Samuelson, C.D., Messick, D.M., Rutte, C.G., Wilke, H.,
ìIndividual and Structural Solutions to resource dilemmas in two
cultures.î Journal of Personality and Social Psychology, 47, 1984,
94-104.
[35] Sato, Kaori, ìTrust and Group Size in a Social Dilemma,î Japanese
Psychological Research, 30, 2, 1988, 88-93.
[36] Sprague, R.H., ìA Framework for Research on Decision Support
Systems,î MIS Quarterly, Vol. 4, No. 4, Dec 1980, pp1-26.
[37] Taylor, R.G., Jr., ìThe Role of Trust in Labor-Management
Relations,î Organization Development Journal, Summer 1989. 85-89.
[38] Yamagishi, T., ìThe Provision of a Sanctioning System as a Public
Good,î Journal of Personality and Social Psychology,î 51, 1986, 110-116.
[39] Zaltman, G. and Moorman, C., ìThe Importance of Personal Trust in
the Use of Research,î Journal of Advertising Research,
October-November, 1988. 16-24.
[40] Zand, D.E., ìTrust and Managerial Problem Solving,î Administration
Science Quarterly, 17, 1972, 229-239.
[41] Zuboff, S., ìIn the Age of the Smart Machine: The Future of Work
and Power,î Basic Books, New York, 1984.
[42] Zucker, Lynne G., ìProduction of Trust: Institutional Sources of
Economic Structure, 1840-1920,î Research in Organizational Behavior,
Vol. 8, p 53-111. 1986

