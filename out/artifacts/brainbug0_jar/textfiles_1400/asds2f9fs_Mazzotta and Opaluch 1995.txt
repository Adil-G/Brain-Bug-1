<-----Page 0----->Decision Making When Choices Are Complex: A Test of Heiner's Hypothesis
Author(s): Marisa J. Mazzotta and James J. Opaluch
Source: Land Economics, Vol. 71, No. 4 (Nov., 1995), pp. 500-515
Published by: University of Wisconsin Press
Stable URL: http://www.jstor.org/stable/3146714
Accessed: 02/06/2010 21:16
Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at
http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless
you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you
may use content in the JSTOR archive only for your personal, non-commercial use.
Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
http://www.jstor.org/action/showPublisher?publisherCode=uwisc.
Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed
page of such transmission.
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

University of Wisconsin Press is collaborating with JSTOR to digitize, preserve and extend access to Land
Economics.

http://www.jstor.org

<-----Page 1----->Decision
Heiner's

Making When Choices
Hypothesis

Are Complex:

A Test

of

MarisaJ. Mazzottaand JamesJ. Opaluch
ABSTRACT.ThispaperexploresHeiner'shypothesis concerninga gap betweenthe cognitiveability
of decisionmakersand the difficultyof decisions
(theC-D gap). Wediscussthe implicationsof decision heuristics
for coefficientestimateswhenuncertaintyisfaced bydecisionmakers,wherethelevelof
varieswithcomplexity.
Statisticalanalyuncertainty
sis stronglysupportsthepresenceof a C-D gap and
providesevidencesupportingthe use of decision
heuristics.The resultsof both directand indirect
methodssuggestthatmixeddecisionstrategiesmay
be used. We also find that complexityeffectscan
have importantimplicationsfor welfareanalysis.

(JELQ20)

I. INTRODUCTION
When faced with complex or unfamiliar
choices, individuals frequently appear to
employsimplifieddecision rules,whichhave
lowerrequirementsfor informationprocessing (Heiner 1983; Tverskyand Kahneman
1979; Simon 1955). Numerous empirical
studieshave providedevidenceof "bounded
rationality,"or systematicviolationsof utility theory(e.g., Schoemaker1982),and many
theorieshavebeen proposedto explainthese
violations.
Decision heuristics have been the focus
of considerableresearch,particularlywithin
the psychologyliterature.Some researchers
have focused on attempts to explain the
mentalprocesses,or frameworks,that result
in selection of a particulardecisionstrategy,
and others have consideredthe roles of task
and context effects in decision strategyselection (Payne 1982;Kahnemanand Tversky
1979;Tversky,Sattath,and Slovic 1988).
Within the economics literature,Heiner
(1983) proposes a theory of strategy selection, based on the idea that complex decision environmentsresult in a gap between
the competence,or cognitive ability,of the
decision maker and the difficultyof the decision (the "C-D gap"). Beyond the C-D
gap, attempts to optimize become unreli-

able, in that they mayproduceless desirable
results than simpler decision strategies.
Thus, although optimizing strategies are
more flexible, the complexityof a decision
may precludethe use of all availableinformation.Heiner constructsa "reliabilitycondition,"which establishes a rule for use of
particulardecisionstrategies.This condition
implies that when optimizingstrategies are
not sufficientlyreliableto producea benefit
from increased choice flexibility,they are
excluded from the set of strategies to be
considered.
Heiner states that "whengenuine uncertainty exists, allowing greater flexibilityto
react to more informationor administera
more complexrepertoireof actions will not
necessarily enhance an agent's performance"(1983,563). Under these conditions,
individualschoose to restrictthe choice set
of responses or the informationto be processed by using simplified decision rules.
Heiner arguesthat "it is not the absence of
a C-D gap, but rather its presence which
conditions regularityin behavior"(p. 563).
Hence, Heiner hypothesizes that uncertainty may result in behavior that is more
systematicand predictable,rather than behaviorthat is less consistent.
Although the use of decision heuristics
may lead to more predictablebehavior, if
such behavior is not consistent with Neoclassical optimization,there may be important implications for both normative and
positive applicationsof economic analysis.
In termsof positiveeconomicanalyses,some
decision hueristicswill imply that behavior

The authorsare, respectively,Ph.D. candidateand
professorin the Departmentof ResourceEconomics,
Universityof Rhode Island.We greatlyappreciatethe
assistanceof PenpornJanekarnkifand PiyaliTalukdar
with the debriefingexperiment.Fundingfor this paper
was providedby the Rhode IslandAgriculturalExperiment Station(AES #2710).
Land Economics * November 1995 * 71 (4): 500-15

<-----Page 2----->71(4)

Mazzottaand Opaluch:Heiner'sHypothesis

is inconsistentwith optimizationof a Neoclassicalutilityfunction(e.g., Kahnemanand
Tversky1979; Opaluchand Segerson 1989).
For example, if individuals use a lexicographicdecision heuristic,the behaviorwill
not be consistent with optimization of a
continuousutilityfunction.
Even when decision heuristicsare consistent with maximizationof a Neoclassical
utility function, the relevance of normative
economic analyses may be brought into
question if decision makers use decision
hueristics.It is often arguedthat it does not
matter whether consumers actually maximize utility, so long as observablebehavior
is consistent with maximizingsome utility
function. Although positive analyses can
proceed in such a case, normativeNeoclassical analyses lose their policy relevance.
For example, application of Hausman's
(1981) approachwill not provide a correct
measure of compensatingvariation if individuals do not maximizeutility. Similarly,a
competitive equilibrium will not achieve
Pareto efficiency unless consumers select
the bundle of goods that maximizestheir
level of satisfaction.
Hence, it can be important to identify
when decision heuristics are used and to
identify the mechanisms used to make
choices. This paper develops an empirical
test for the C-D gap, identifiesthe level of
complexity that evokes complexity effects
caused by the C-D gap, and attempts to
identify decision strategies employed beyond the C-D gap.
Both direct and indirectmethods may be
used to attempt to identify decision processes (Svenson 1979). One direct approach
is the use of verbalprotocols,where respondents are asked to describethe process that
they employ to arriveat decisions.This can
be done by using concurrentverbal protocols to get decision makers to verbalize
thought processeswhile they make choices,
or by debriefingdecision makers after the
fact, using retrospective verbal protocols
(see, e.g., Ericssonand Simon 1984;Nisbett
and Wilson 1977). In contrast, indirect approachesemploy structuralanalysisto infer
decisionprocessesfrom observedchoices by

501

statisticallytesting different models of behavioron observedchoices.
The direct verbal protocol approachhas
the advantage of being more straightforward and less susceptibleto difficultiesthat
may confound indirect attempts to identify
different forms of behavior, particularly
when mixed strategies are used. However,
the use of verbalprotocolshas at least four
possible shortcomings.First, it may be difficult to get individualsto describe their
decision-makingprocess, as there is some
evidence that individualsare not able to
accurately describe this process (Ericsson
and Simon 1984;Nisbett and Wilson 1977).
Second, there may not be an opportunityto
communicatedirectly with respondents,as
with a mail surveyor when using secondary
data. Third, debriefingor verbal protocols
are time consuming,and thus are generally
only used on a portion of respondents.
Fourth, it may be difficult to analyze the
resulting qualitativedata and to generalize
the results.Hence, it is importantto explore
both the direct and indirect approachesto
identifyingunderlyingdecision processes.
The primaryfocus of this paperis on the
indirect approach,using statistical analysis
of data from a paired comparisonssurvey.
Complexityis defined in terms of the number of attributesthat differ across the two
alternativesin the comparison.We develop
models for behavior under different assumptionsregardingdecision makingunder
complexity. These alternative models are
shown to have differentimplicationsfor the
behaviorof estimatedparametersin simple
versus complexchoices. We carryout statistical tests in an attemptto determinewhich
of the models, if any, are consistent with
actual behavior.
First, we attempt to identify when decisions become complex-Heiner's C-D gap
-and test for the use of decision heuristics
by examiningdata regardingactual choices
that were made undervaryinglevels of complexity. The latter test attempts to identify
whether any particularheuristicdominates,
so that behavior may become more predictable, as Heiner hypothesized.Third,we
examinesome implicationsof complexityfor
welfare analysis.

<-----Page 3----->Land Economics

502

We also comparethe resultsof the statistical tests for heuristicsto insightsobtained
from debriefingof surveyrespondents.This
allowsus to comparethe informationthat is
obtained from an indirect, statistical approach applied after the fact with that obtained from the direct approachwhere verbal protocolsare used as part of the survey
process. The methodologythat we develop
can be used to determinewhethercomplexity is an issue for a particulardata set, and if
so at what point. This can be particularly
useful for testing data obtainedfrom a contingent valuation survey in cases where
complexitymay be of concern.
The paper is organized as follows. Section II extends the standarddiscrete choice
model of decision making to include complexity effects and examines the implications of alternativeforms of decision simplification for the discrete choice model.
Section III describesthe applicationof this
generalized model to data from a paired
comparisonssurvey for landfill siting and
presentsresultsof empiricaltests for consistency with Heiner's hypotheses.Section IV
presentsthe summaryand conclusions.
II. THEORETICALFRAMEWORK
The discrete choice model (McFadden
1973)extendsthe Neoclassicaltheoryof behavior to an important class of decisions,
those where the individualchooses amonga
fixednumberof alternatives.However,when
faced with complexcomparisonsof alternatives with many attributes,individualsmay
employ a simplifieddecision strategythat is
inconsistentwith full optimization,since full
optimizationrequirescompletebalancingof
all attributesin the decision process. This
section brieflyreviewsthe standardneoclassical discretechoice model,then extendsthe
model to account for complexityof choice.
This section also examinesthe implications
of simplifieddecision strategiesfor the discrete choice model.
The Neoclassical Discrete Choice Model

The Neoclassicaltheoryof discretechoice
is based on the randomutilitymodel, which

November 1995

assumes that individualsmake choices to
maximize utility, but that the researcher
cannot explain choice behavior precisely,
due to potential observationaldeficiencies
(McFadden 1973; Manski 1977; Maddala
1983; Ben-Akiva and Lerman 1985). Thus,
the researchermust treat the total utilityof
an alternative as a random variable composed of a measurableor "observable"component and an unobservable component.
This randomutilitymay be expressedas
=
UnA= V(ZA, SA) + E(Z[A,SA1) VnA

[1]

where UA is the level of utility obtained
when alternativeA is selected by individual
n, V(-) is the measurablecomponentof utilis the unobservablecomponent of
ity,
E(.)and z and S are vectors of attributes
utility,
of the alternativeand attributesof the individual,respectively.The probabilitythat individual n will choose alternative A over
alternativeB is given by
Pr(A) = Pr(UnA 2 UnB)
= Pr ( VnA+
= P(

E•

- En

n

VB + EB)

2 VB -VnA)

= Pr(rqn ? dVn)

[2]

anddVn= (VB _
whereIn =
E•)
the randomVnA).
to estimate
In order(Enutility
model, it is necessaryto make assumptions
about the functional form of the utility
function and about the probabilitydistribution of the disturbances.The utilityfunction
is usually assumed to be linear in the parameters, but need not be linear in the
attributes.If the disturbancesare assumed
to have a Weibull distribution,their difference will be logisticallydistributed,and the
binarylogit model results.The differencein
unobservablecomponentsof utility is r =
functionis
EA - EB, and its distribution
1
z) = (1 + ez/)
F(•q >

[3]

where 4, the alternative scale parameter

<-----Page 4----->71(4)

Mazzottaand Opaluch: Heiner'sHypothesis

(Hastings and Peacock 1975), is equal to
crv/Tr, and c is the standarderror of -l.
The assumptionof identicallydistributed
disturbances,together with the fact that a
change in the scale of the utility function
will preservechoice probabilities,allowsthe
utility scale to be fixedby choosinga convenient variance for the distributionof the
disturbances(Ben-Akivaand Lerman1985).
Hence, the utility function can be scaled by
an arbitraryfactor without affecting the
choice probabilities,and the scale can be
chosen in order to provide a convenient
the varianceof 9q.
value for
"2, functioncan be
The utility
approximated
as a polynomialin the explanatoryvariables
by takinga Taylorseries approximationof a
given order. For example, a first-orderapproximationwould linearizethe utilityfunction, resultingin a binary logit model with
choice probability

503

are certainabout their preferences,but that
the researchermustview choices as stochastic due to observationaldeficiencies.However, for manychoices, an additionalsource
of uncertaintyarises.When faced with complex decisions,individualsmay be uncertain
aboutthe utilityobtainedfromtheir choices.
This could happen if the individualis unfamiliarwith the commodity,if there is uncertainty with respect to some attributes,or if
the choice environmentis sufficientlycomplex that the individualdoes not fully comprehend the implications of a particular
choice. Thus, uncertainty about a choice
may arise not only from uncertaintyabout
possible outcomes,but as a result of a psychologicalstate of uncertaintyabout preferences on the part of the decision maker.In
this paper we are concernedwith this psychological uncertaintyrather than the uncertaintyassociatedwith riskychoices.
A choice among alternatives could become complex if there are many alternaPr(A) =1 1 + exp[--(,'/4)(xA
-_ B)]
tives, or if the alternativesdiffer in terms of
a large numberof attributes.In such cases,
1
individualsmay be unable to perform the
complex balancing of all attributes across
1 + exp[-y(xA _ xB)]
alternativesthat is implied by utility maxiwhere 3 is a vector of coefficients of the
mization,and may instead resort to various
means of simplifyingthe choice. For the
and
of
are
vectors
xA
utility function,
xB
attributesof the two alternatives,and y is a
purposes of this paper, because our data
vector of each utility coefficient dividedby consistsof choices between two alternatives,
complexityis assumedto be a functiononly
•, the alternativescale parameter,which is
of the numberof attributesthat varyacross
to
the
variance
of
The
coproportional
lq.
efficients, Pi, are interpretedas derivatives the alternatives.
of the utility function with respect to each
Uncertainty about preferences on the
explanatoryvariable,or the marginalutility part of respondentsimplies that lq,the difof the attribute. For linear-in-parameters ference in unobservablecomponentsof utilmodels, 4 and P cannot be estimatedsepa- ity, is composedof two elements,where the
first is knownby the respondentbut not by
rately,but are estimatedas a ratio, y =P/=
(Cameron 1988). Because choice probabili- the researcherand the second is stochastic
for both the respondentand the researcher.
ties are independent of the scale of the
utility function, the choice of scale is arbi- Thus, this second element is a randomvariable whose variance depends on the comtrary, and the utility function is scaled so
that 4 = 1, which is equivalent to setting plexity of the decision. Consequently,the
the variance of the difference in distur- varianceof the utility differencefor the ith
choice madeby individualn is uo2(ci),and is
bances equal to rr2/3.
a functionof the complexityof the ith choice
and
the
Discrete
Choice
Model
Complexity
(ci) and perhapsthe cognitiveabilityof the
decision maker.
Discrete choice models are generally
As long as the variance of the utility
based on the assumptionthat respondents difference is constant,its value is unimpor-

<-----Page 5----->504

LandEconomics

tant, since the model is based on an ordinal
measure of utility, which implies that the
scale of the utility function is arbitrary.
However, if the variance differs systematically across decisions,-l will not be identically distributed across observations,resulting in statistically inconsistent logit
estimates (Greene 1990). In the discussion
below, we assume that heteroskedasticity
arises from variationsin the complexityof
choices, defined in terms of the numberof
attributesthat differ acrossalternatives.
A generalizedlogit frameworkcan be set
up that allowsfor heteroskedasticityrelated
to differentlevels of complexity.This model
implies that the likelihoodfunction is composed of m factors,where each factoris the
product of the likelihood functions for all
observationswith a givenvariance.The likelihood functionis
L= i
g=

1

-

Ln

[51

(nI=Ag))

where I(g) is the set of decisions at complexity level g, Ln is the likelihood of the
nth observationand the bracketedexpression is the individualsub-likelihoodfunction
for choices at each complexitylevel. Separate parameterscan be estimated for each
complexitylevel, based on the model given
in equation [4], in order to test hypotheses
concerningthe type of decisionstrategythat
is used by survey respondents. The estiare ratios of the
mated parameters,
y•(g),function,
coefficientsof the utility
Pi(g), to
the scaledvariances,4(g), where i indicates
the attribute.Note that 4(g) is proportional
to the variance of -q, and thus may differ
acrosschoices of differingcomplexity,but is
constant across attributes within a given
choice occasion.
If individualsexperience no uncertainty
and fully comprehendall trade-offsimplied
by choices for any level of decisioncomplexity, then their choices will reflect perfect
cognition-the absence of Heiner's C-D
gap. In this case, individualsare able to
balance all attributes, so that there is no
complexity effect. Thus, their behavior
shouldbe consistentwith a Neoclassicalop-

November1995

timizationstrategy,which implies a strictly
quantitativeand simultaneousevaluationof
all attributes.However,in some cases individuals may consider and balance all attributesand trade-offsbut, due to cognitive
limitations, they are less able to provide
consistent choices as questions become
complex. We call this "Neoclassical complexity,"wherebyindividualsattemptto fully
optimize,but make errors due to the complexityof choices that they face.
In other cases complexitymight lead individualsto simplifytheir choices by using
decision heuristics.A varietyof possible decision strategieshave been discussedin the
literature (see, e.g., Svenson 1979). Within
the context of this paper,decisionstake the
form of choices between two alternatives,
where each alternativeis described by six
attributes.Therefore,the decisionheuristics
consideredare those that simplifyconsideration of and comparisonsamong attributes.
The strategies considered here might also
be relevantto choices amongmore than two
alternatives,but in the case of multi-alternative choices, strategies aimed at eliminating a subset of alternatives might also
become important,and would need to be
considered.
In attemptingto specify possible simplificationstrategies,we hypothesizedtwo general forms of simplificationbased on potential violations of Neoclassical optimization.
The two general types of simplificationconsidered are qualitative strategies and sequentialstrategies.These two dimensionsof
simplificationmightbe combinedin various
ways and degrees to producedifferentdecision strategies.Increaseddecision complexity might result in changesin decision strategy from the Neoclassicalstrategies,where
individualsperformstrictlyquantitativeand
simultaneousevaluationsof all attributes,to
strategies that rely on qualitative and/or
sequentialevaluationsof attributes.
An example of a qualitative strategy is
enumeration,or a comparisonof alternatives by adding up the number of favored
attributesfor each alternativeand selecting
the alternativewith the largest number of
favoredattributes.This has been referredto
in the psychologyliteratureas the "majority

<-----Page 6----->Mazzottaand Opaluch:Heiner'sHypothesis

71(4)

of confirmingdimensions"strategy (Russo
and Dosher 1983).An exampleof a sequential strategyis lexicographicordering,where
the individualchooses based on a hierarchy
of attributes, so that an alternative is selected based on the most preferred attribute. If the alternativesare identical in
terms of this attribute,the individualwould
proceed in order throughthe hierarchy,selecting an alternative based on the most
favoredattributethat differs across alternatives.
The model that we present is applied to
data from a paired comparisonssurvey of
choices among alternativesites for locating
landfills, where each site is described in
terms of its attributes. The survey is describedin some detail in Section III.
andTesting
Modeling
for Complexity
Effects
A two-partapproachwas used to test for
the existence and form of complexityeffects
in the survey responses. First, we constructedtests for the existenceof complexity
effects-what Heiner (1983) terms the C-D
gap-based on the modifieddiscrete choice
model described above (equations [4]-[61).
These tests are based on statisticalcomparisons of sub-likelihoodfunctionswith separate coefficient estimates for each level of
complexity.This set of tests also may provide informationregardingthe general form
that complexityeffects might take, by comparingratios of the estimatedparameters:
-Yi(g)
yi(h)

pi(g)/dp(g)
Ph)/(h)

where the
are estimated
g
•i's different parameters,
and h represent
complexitylevels,
and i representsthe attribute.Because each
estimated parameter,
is equal to the
•i(g), divided
utility coefficient, Pi(g),
by the
scaled variance, 4(g), if either Pi or 4 is
constant, tests can be performed using a
ratio of two y,'s. If the varianceis constant,
y1(g)/Yi(h) will be equal to the ratio of
utility coefficients, and if the utility coefficients (13P's)
are constant,it will be a ratio of
two 4's, which are proportionalto the vari-

505

ance of the difference in random components of utility,-l. Each of the hypothesized
decision strategiesresults in differentimplications for these ratios, as discussedbelow.
Second, we comparedindirectand direct
methods of testing for specific decision
strategies.We used indirect statisticaltests
to compare alternative models, based on
hypothesizeddecision strategies, and compared the results of these tests to a direct
debriefingexperiment.The basis for each of
the tests is describedin more detail in the
followingsections.
Implications
for the ModifiedDiscrete
ChoiceModel

The first set of tests for complexityeffects are summarizedin Figure 1. If individuals are able to make complete and accurate choices for all levels of complexity,such
that there is no complexityeffect, then their
behavior reflects perfect cognition and full
neoclassical optimization.In this case, the
C-D gap is not significant,and the coefficients and variances as described in the
model presented in equations [4]-[6], are
equal across levels of complexity,(-i(g)=
and their ratios,y,(g)/iy(h), are
y,(h) =
all equal•i),
to one. This implies that the parameter estimates, y,(g), for the sub-likelihood functionsrepresentingdifferentlevels
of complexitydo not differsignificantlyfrom
those for a model that pools observations
for all complexitylevels.
FIGURE1
SUMMARYOF TESTSFOR COMPLEXITY
yi(g) =

3i(g)/4)(g)
4 = scaledvariance
1 ==utilitycoefficient,
h =
levels
i

attribute; g,

Neoclassical
Optimization
(Perfect

complexity

yi(g) = yi(h) = Ti
for all complexity levels
yi(g)/y,(h) = 1

Cognition)
Neoclassical
Complexity

4(h) > 4(g)
Iyi(g)l> Ii(h)l
for h morecomplexthan g
yli(g)/yi(h) = yj(g)/Yl(h)

Simplification
Strategies

Pi(g) * i3(h)
yli(g)/yi(h) * y(g)/Y1(h)

<-----Page 7----->LandEconomics

506

If uncertainty for the respondent increases with complexity,then behaviormay
be consistentwith the Neoclassicalcomplexity model, where the variance of the error
term increases with complexity.Because (
is proportionalto the varianceof the error
term, it is the same for all attributeswithin
a choice occasion,but varies across choices
of differing levels of complexity.This implies that the utility coefficients show no
systematicvariation for different levels of
complexity, (pi(g) = Pi(h) = pi where h

representsa higherlevel of complexitythan
g), but the variance for more complex
choices is greaterthan that for less complex
choices, (4(h) > 4(g)). Thus, the estimated
parametersfor less complex questions are
greater in absolute value than those for
> Iyi(h)l),
more complex questions,
in equal proporbut the parameterschange(I•i(g)l
tions acrossall attributes.That is,
( )
-i /d- g
(g)/(h)
i/(h)
pj/(g)

(g )
/-1
1/(h)

h)-= yj(g)/1j(h),

[7]

where i and j are differentattributes.In the
limit, as questions become infinitely complex, decisionmakersbecome unable to discriminate among alternatives, so that all
alternativesbecome equallylikely, the variance increases, and the estimated parameters tend to zero.
If respondentsadopt simplifieddecision
strategiesas choices become complex,then
the revealed utility coefficients,Pi(g), may
change with complexity.For example, if a
lexicographicstrategyis adopted,the utility
coefficients(Pi(g)) for less "important"coefficients would decline as questions become complex. Alternatively,if responses
are consistent with enumeration, then
weights become equal as complexity increases and the coefficientswill tend to converge with complexity.In either case, the
estimated parameters, yi(g), will vary with
complexity, and their ratios will not be equal
across attributes.

November1995

The linear logit model implies that the
revealed dollar value of an attribute is a
function of the ratio of the estimatedcoefficient of that attributeto the coefficienton
the monetarypayment(see, e.g., Hanemann
1984).Thus, for both the Neoclassicaloptimization and the Neoclassical complexity
modelsmonetaryvalueswill not changewith
complexity,since the ratios of the coefficients do not vary with complexity.However, if respondentsadoptsimplifyingstrategies as questionsbecome complex,then the
relative values of the coefficients may
change, which could result in apparent
changes in revealed values. Logically,one
should carry out welfare analysisusing observationswhere respondentsare most capable of revealingtheir values, prior to the
point where the Heiner C-D gap occurs.
In summary,both forms of Neoclassical
behavior imply that the relative values of
coefficientson different attributesare constant as questionsbecome more complex.In
contrast, the adoption of simplifiedstrategies with increasingcomplexityimplies that
the relative values of coefficients change
with complexity.The way that coefficients
change depends on the strategyused. However, if mixedstrategiesare used, there may
not be a systematicpattern of changes in
coefficients.Thus, tests of parameterratios
can tell us whetherthe Neoclassicalhypothesis is apparentlyviolated, but may not allow us to determinea unique form of simplificationused by respondents.
III. APPLICATION OF THE MODEL
Descriptionof the Survey

The model describedin the previoussection was appliedto data collected in Rhode
Islandfor a landfillsitingstudy(see Opaluch
et al. 1993).The purpose of the surveywas
to obtain public input into landfill siting
decisions and to develop a method of ranking alternative landfill sites in terms of
social preferences regarding the potential
impacts to natural resources and local communities.

<-----Page 8----->71(4)

Mazzottaand Opaluch:Heiner'sHypothesis

The landfill siting survey was administered in person to 1,107 Rhode Island residents in the spring of 1990, and used a
paired comparisons approach. The survey
presented respondentswith two hypothetical sites, described in terms of their attributes, and respondents were asked to
choose the site that they preferred for the
landfill.Results of focus groupsessions indicated that respondents could not easily
manage more than six characteristicsin a
single comparison,which necessitated the
divisionof the surveyinto two separatesections. Each respondentcompleted a survey
booklet containing 11 paired comparisons,
consistingof 6 site comparisonsand 5 location comparisons.A site is defined as the
500-acre area including the actual landfill,
and a location is defined as the foursquare-mile area surroundingthe landfill
site. For the purposes of this paper, tests
were conductedusing only the data for sites,
as the location attributes were predominantly qualitativevariables,so that testing
for qualitativestrategieswould be difficult.
A set of 123 distinct pairwise comparisons
of sites were developed, using Addelman's
procedure for incomplete factorial design
(Addelman 1962a, 1962b; Addelman and
Kempthorne1961).These were presentedin
28 different surveybooklets, with about 40
replicationsof each booklet.
Examplesof site comparisonsare shown
in Figure 2. Respondentswere required to
choose one of the two sites by placing a
sticker labeled "landfill"on the site that
they preferred for the landfill. Each site
comparisondepicted two 500-acresites, describedby the amountof woods, marsh,and
farmlandat the site; the presence of highversus low-qualitygroundwater;normalversus unique wildlife habitat;and the annual
cost to each Rhode Islandhousehold.In the
least complexsurveyquestions,only two out
of six attributesdiffer across the two sites,
while in the most complex questions,all six
attributesdiffer. Figure 2(a) illustratesone
of the least complexsite comparisons,where
site A has 50 more acres of woods than site
B, while site B has 50 more acres of farmland than site A. Hence, in choosing be-

507

tween the sites, the decision maker need
only determinewhether woods or farmland
are more importantto preserve.Figure 2(b)
showsone of the most complexsite comparisons, where all attributes differ between
the two alternatives.Here a decision maker
would need to considera far more complex
comparisonamong all six attributes,where
site A has more farmland,has high-quality
groundwater,and is more costly, while site
B has more marsh, more woods and has
unique wildlifehabitat.
The surveybooklets were constructedso
that the questions appear in order of increasing complexity. This was critical for
providingrespondentswith experience answeringrelativelyeasy questionsbefore facing more difficult questions. However, this
also implies that some of the complexity
effect maybe maskedby learning,thus making it more difficult to identify complexity
effects. In addition, to the extent that this
orderingprovideda learningprocess, complexityeffects might occur at a higher complexity level compared to a case where
learningwas not facilitated.
IndirectTestsfor Complexity
Effects

For the purposesof this paper, complexity is defined in terms of the number of
attributes that differ in a comparison.In
some comparisonsonly two attributesdiffer,
while in others all six attributesdiffer.Thus,
using this definitionof complexity,there are
five distinctcomplexitylevels.The estimated
parametersare based on the model:
pA(g)=

1
PA

1 + exp -

[y(g)(xA

[8]
- xB)]

where Py(g)is the vector of estimated parameters for a given complexitylevel, defined in equation [5], and (xA - xB) is the
vector of differencesin attributelevels between alternativeA and alternativeB. The
estimated parameters, yi(g), are ratios of
the coefficients of the utility function, iP(g),

to the scaled variances,4(g), where i indicates the attribute.

<-----Page 9----->508

Land Economics

November 1995

(s (strongly
gly
preferA
for the

(strongly
prefer B
for the

landflll)

landfil)

5

5

4

4

.

I

.I

....

A- : ?:.
.................

............o...........
:;: ~: : ~: :

:~:~~:~::~::~:::::~:~:'

::.

r~; :- ?:.

.

.

. .

..

?~

~~

?: ?
..~:::::::::
......?:-

?::''...

I

.

.

Low
GroundwaterLow
riutdwter
Guality
Ouaisty
..

landfilo

........
Cost to EachR.I.Hous.ho.d.

$360 per year
lndfill)~$.

? :~

.....:

?~-:

?-:

.

'.
oostto ach R.toseol:.
7
360 oer

(a) Least Complex Site Comparison

(strongly
prefer A
for the
leandill)

(strongly
prefer B
for the
landfllO

~

~

6 C~\\\\\\\\\\\\\\\\\\~~n~~n~~
n

4

En\VP

\\\\\\\V\\\\\\\\\\m~-

- - ?- -?-~p~V)

I

I

4\\\,C\\\?,,\\\\\\\\

\\\\\\\n\\u~

i~i~

3:::x:ilj~j:;i::iiii-i

..................
.......:?::?:
.............................
?::
?-::
?????
??::
??::
::::::::::::::::::~:~

......
............:::
...........??::....???::

I

,\ \ \ \

30Ac ofMarsh

100 ofMarshr
Acme

100 Acresof Farmland

No Farmland

MoMost
st
(b(b)
)
Comparison
SSite
ite
Complex
Comparison
Compl
ex

FIGURE 2
EXAMPLE SITE COMPARISONS

Parameterestimates (y,(g)) and asymp-

totic t-statistics are given in Table 1 for the
five levels of complexity and for the full data
set. All parameters have the expected sign
and, using a two-tailed test, most are significant at the .10 level, with many being sig-

nificant at higher levels. Note that the
parameters on marsh and farmland are interpreted as their additional importance over
an acre of woodland, which was treated as a
residual for acreage not allocated to farmland or marsh for each 500-acre site. Thus,

<-----Page 10----->Mazzottaand Opaluch:Heiner'sHypothesis

71(4)

509

TABLE 1
ESTIMATED
PARAMETERS
ANDAsYMroTICT-STATISTICS
ComplexityLevel: Level2
Numberof
Observations
Marsh
Farmland
Groundwater

Wildlife
Cost

Level3

Level4

Level5

Level6 Full Data Set

422

1,300

1,528

1,666

762

5,678

-0.0088
(- 1.44)

-0.0013
(- 1.69)

-0.0009
(- 1.42)

-0.0002
(-0.33)

-0.0001
(-0.10)

-0.0006
(-2.00)

-0.0076
(- 5.62)

-0.0070
(-6.79)

-0.0037
(- 5.83)

-0.0028
(- 5.25)

-0.0033
(- 2.68)

-0.0039
(- 13.00)

- 2.2936

- 1.7176

- 1.2951

- 1.2950

- 1.0125

(-4.12)

(-11.59)

(-10.09)

(-15.93)

(-9.45)

-0.4379
(-0.37)

- 0.3539
(- 1.39)

-0.5870
(-6.71)

-0.6154
(-7.70)

- 0.5325 -0.5692
(-4.69)
(- 12.68)

- 1.3354

(-27.48)

-0.0010

-0.0037

- 0.0025

- 0.0022

- 0.0015

(-0.30)

(-6.23)

(-9.61)

(-8.81)

(-2.67)

(-24.00)

**

**

**

**

**

- 0.0024

*= significant at .10 level; ** = significant at > .01 level.

the fact that several of the estimated coefficients for marshare not statisticallysignificantindicatesthat the value of marshis not
statistically different from that for woodlands. The parameterestimates for the different complexitylevels are graphedin Figure 3.
The first set of tests checks for consistency with the perfect cognition,Neoclassical optimizationhypothesis,where no complexity effects occur. If respondentsbehave
accordingto Neoclassical optimization,the
estimated parameterswill not differ significantly across levels of complexity.The null
hypothesis of no change across complexity
levels is
Ho:

y(2) = y(3) = y(4) = y(5) = y(6)

[9]

and the likelihoodratio test statisticfor the
joint test is
6

-2

L(y) -

, L(y(g))

[10]

g=2

where L(y) is the log likelihood value for
the pooled data set, and L(y(g)) is the log
of the sub-likelihoodfunction for the g th
level of complexity.This is distributedchi-

square with degrees of freedom equal to
EK(g) - K, where K is the numberof coefficients on attributes in the comparison.
Test results are shown in Table 2. The null
hypothesis that the coefficients do not
change over different levels of complexity
can be rejected at better than the 0.0001
significancelevel. This implies that the results are inconsistentwith the hypothesisof
perfect cognition, that the C-D gap exists,
and that respondentsappearto exhibitsome
form of boundedrationality.
A second joint test was done for differences in all parametersbetween each pair
of complexitylevels in an attemptto identify
how the parameters change as questions
become more complex.This set of tests can
be used to identifythe level of complexityat
which the C-D gap occurs. The joint likelihood ratio test describedabove was used to
carryout these tests. The test statisticsare
presented in Table 3. The results indicate
modest but statisticallyinsignificantdifferences between levels 2 and 3 (significantat
the .14 level), very significant differences
between levels 3 and 4 (significantat the
.003 level), and no significant differences
between levels 4 and 5 or levels 5 and 6.
This suggests that question complexityappears to become an importantissue when

<-----Page 11----->510

LandEconomics

November1995
TABLE 3

(Absolute
Values)

r LIKELIHOOD
RATIOTESTSOFPAIRSOF
JOINT
COMPLEXITY
LEVELS

2.5
(-*-GrondwaterWildi6
2

Levels2 and3 vs. Pooled Modelfor Levels2 + 3

1.5
1

0.5
0

2

3

4
ComplexityLevel

5

6

- 2L(y-(2))
- 2L(y-(3))
Sum for levels 2 and 3

472.048
1,539.767
2,011.815

- 2L(y-(2 + 3))

2,020.09

X2Test Statistic= 8.275with 5 degreesof freedom

Significance level = .14

Levels3 and 4 vs. Pooled Modelfor Levels3 + 4

0.01

- 2L(y-(3))
- 2L(y-(4))

1,539.767
1,780.534

0.008

Sumfor levels 3 and4

3,320.301

0.006

- 2L(y-(3 + 4))

3,338.013

0.004

X2Test Statistic= 17.712with5 degreesof freedom
Significancelevel = .003
Levels4 and5 vs. Pooled Modelfor Levels4 + 5

0.002
2

3

4
ComplexityLevel

5

6

FIGURE 3
PARAMETERS
ESTIMATED
(Absolute Values)

- 2L(y-(4))
- 2L(y-(5))

1,780.534
1,814.133

Sumfor levels4 and5

3,594.667

- 2L(y-(4 + 5))

3,597.507

=
X2Test Statistic =2.84with5 degreesof freedom

Significance level

.73

Levels5 and 6 vs. PooledModelfor Levels5 + 6
- 2L(y-(5))
- 2L(y-(6))

TABLE 2
RATIOTESTOFSEPARATE
JoINTLIKELIHOOD
LEVELSVS.POOLED
MODEL
COMPLEXITY
Log-LikelihoodValuesfor SeparateComplexityLevels
-

2L(y-(2))
2L(y-(3))
2L(y-(4))
2L(y(5))
2L(y-(6))

Sumfor all complexitylevels
- 2L(y-)for PooledModel

472.048
1,539.767
1,780.534
1,814.133
783.496

6,389.978

6,451.109
X2Test Statistic= 61.13with20 degreesof freedom
Significancelevel > .0001

alternatives differ by four or more attributes. This is consistent with Heiner's hypothesis that "changes in environmental
variables [e.g., increasing complexity] may
shift the reliability... of an action; ... If this
happens rule-governed behavior will switch

1,814.133
783.496

Sumfor levels5 and 6

2,597.629

- 2L(y-(5 + 6))

2,602.933

=
X2Test Statistic =5.304with5 degreesof freedom
Significance level

.38

from allowing to severely restricting that
action. Thus, a relatively sudden 'switching'
between different behavior patterns may occur" (1983, 582).
Asymptotic t-tests were used to test for
changes in individual estimated parameters,
yi(g), between complexity levels. The null
and alternative hypotheses are
H0:

yi(g) = yi(h)

HA: Iyi(g)I > IYi(h)I]

11]

<-----Page 12----->Mazzottaand Opaluch:Heiner'sHypothesis

71(4)

where h represents a higher level of complexitythan g. The resultsof these tests are
presented in Table 4, and show the same
pattern that was observed in the joint test
(Table 3). All of the test statistics that are
significantat the .10 level or better are of
negative sign, as would be expected if the
variance,4(g), increaseswith complexity,or
if the utility coefficients,p3(g), decrease in
absolute value. This trend providessupport
for either the Neoclassical complexityhypothesis or a simplificationhypothesis.
A possibleexplanationfor these resultsis
that respondents may have little difficulty
answeringthe easier questions, where two
or three attributesvary, but complexityeffects become importantwhen four or more
attributesvary. Beyond this point there appears to be no furthersignificantchange in
coefficients. This suggests that individuals
may use some sort of decision heuristic,
such that decision makingdoes not become
more complex beyond that point. It should
be emphasized,however,that the estimated
parametersare all of the appropriatesign,
and are generally statistically significant,
even at the highest levels of complexity.
This suggeststhat, althoughthere is a complexity effect, respondentsare able to provide reasoned choices to the surveyeven at
the highest levels of complexity.Thus, their
responsesmaybe consistentwith some form
of simplification,but the results do not suggest that respondentswere totally confused
and unable to providesensible responses.

TABLE4
T-STATISTICSFOR TESTSOF EQUALITY
BETWEENPARAMETERS

Complexity
Levels:

2 vs. 3

3 vs. 4

4 vs. 5

5 vs. 6

Marsh

- 1.220

- 0.401

- 0.795

- 0.038

Farmland

-0.381

- 2.679

- 1.100

0.358

0.100

- 2.155

-0.001

- 2.101

- 0.070
0.840

0.863
- 1.867

0.240
-0.956

- 0.597
-1.08

Groundwater
Wildlife
Cost

* = .25
*** = .05
level; **** = .25
significance level;
level; ****** = .005 level.

511

A logical second step in testingwould be
to attempt to determine whether coefficients diverge,as would be expected if lexicographicstrategies dominate,or converge,
as would be expected if enumerativestrategies dominate.However,two problemsprevented us from using these tests. First, the
two effects oppose each other so that, in the
aggregate, effects might not be clear. Second, comparisonsare only meaningful for
attributes that are measured in the same
units. This would only allow for two unambiguous comparisons-farmland versus
marsh, and groundwater quality versus
wildlife habitat. Thus, there would be little
basis for drawing conclusions from these
tests.
Therefore, the second stage in testing
was to compare results of alternativemodels, rather than to test for whether coefficients convergeor diverge.We constructed
a set of models to represent different hypothesized decision strategies. Unfortunately, one of our hypothesizedstrategies,
the lexicographic strategy, could not be
tested due to deficiencies in the data set.
We used nonnested methods (see Horowitz
1983 and Ben-Akivaand Lerman1985, 171)
to attemptto determinewhether one of the
other strategies was dominant. However,
these tests were inconclusive,suggestingthat
mixed strategiesare used by individuals,or
that different strategies are used by different respondents. Given space limitations,
details of these tests are not reportedhere.
Finally,we examinedthe implicationsof
complexityfor calculatingwelfare effects, in
the form of monetaryvalues of attributes.
In this case, we compared the estimated
attribute values using data for complexity
level 3 versus those obtainedfor complexity
level 4, since this is where the complexity
effect appearsto become important.Table 5
shows the estimated values at these two
complexitylevels. As can be seen, most attribute values are fairly similar at the two
complexitylevels, but the estimatedvalue of
wildlife habitat is approximately2.5 times
higher at level 4 as comparedto level 3. In
this case the evidence indicates that estimated dollar values are generallyhigher at
complexitylevel 4 than at complexitylevel 3.
This could suggest the use of a decision

<-----Page 13----->512

LandEconomics

TABLE5
ESTIMATEDATTRIBUTEVALUES FOR
COMPLEXITYLEVELS3 AND 4

Attribute

Marsh(perAcre)a
Farm(perAcre)a
Groundwater
WildlifeHabitat

Estimated Value At
Complexity Level
4
3

$0.35
$1.89
$464.22
$95.65

$0.36
$1.50
$520.12
$235.74

a Incrementalvalue per acre above base case (woodland).

heuristicwhere the wildlifehabitatbecomes
more importantrelative to other attributes
for more complexchoices.
Summaryof IndirectTestResults

The clearest result that is evident from
the indirecttests is that some form of complexity effects occur, and the majority of
these effects occur at complexity level 4.
These complexityeffects also result in different welfare estimates for more complex
choices. There was also some evidence that
simplifieddecisionstrategieswere employed
beyond level 4, as decisions appeared to
become no more complexat levels 5 and 6.
This suggests that the C-D gap becomes
significant for comparisonsof alternatives
with more than three attributes,and is consistentwith Heiner's"switching"hypothesis.
However,these results do not appear to
confirmHeiner's (1983) hypothesisthat behaviorbecomes more predictablewhen simplified decision strategies are employed.
Heiner's predictabilityhypothesis may be
appropriatein a case where particulardecision rules are well established.For example,
a businessor other organizationmightspecify such rules for its employeesor members,
in which case behavior would be predictable.However,in a surveythat presents
unfamiliarchoices, or in other cases where
establishedrules do not exist, behaviormay
become less predictable,since it is difficult
to predict which decision heuristic will be
appliedin a given situation.

November1995

TheDirectApproach:Experimental
Evidence

for Simplification
A set of experimentsusing retrospective
verbal protocolswas conductedon a small
sample of 18 people in orderto see whether
it is possible for surveyrespondentsto verbalize their decision strategy,and to compare the direct and indirect approaches.
First, a shortened version of the landfill
survey,containingonlysite comparisons,was
administeredto each individualin the same
way that the surveywas originallyadministered. After completingthe survey,each respondent was asked a series of questions,
which included questions about their general impressionsof the survey,whetherthey
found the survey to be easy or difficultto
answer,and whether all questionswere answered in the same way. Then respondents
were asked to return to each question in
turn and explain how they made their
choices. Because of the small sample size
and the preliminarynature of this experiment, it is difficultto drawany conclusions,
but some generalobservationsmaybe made.
First, it appearedthat people have definite hierarchiesof attributes,but it proved
very difficultto determinewhether this led
to sequentialdecision strategies.For example, one person never chose a site with
unique wildlife habitat, but when questioned further,stated that he might give up
unique wildlife habitat if the cost exceeded
$1,000,or possiblyif the other site had "lots
of' marshand woods. Severalof the respondents stated that they completelyignoredan
attributeor attributesthat they did not consider to be important.
Most respondents in the experiment
stated that they made their selections by
firstlookingat each attributeindividuallyto
see if it differedbetween Site A and Site B.
Second, they excluded attributes that did
not differ from consideration, and sometimes also excluded an attribute or attributes that they did not consider important. Several respondents also stated that
they did not consider attribute differences
that they consideredto be "small."Finally,

<-----Page 14----->71(4)

Mazzottaand Opaluch:Heiner'sHypothesis

they made their selection by balancingthe
remainingattributedifferences.
These types of responses appear to indicate that a mixed strategyis used, and that
the strategy depends on elimination of attributes or differences not considered
"large" enough or "important"enough to
consider. Thus, this combines elements of
both sequential and qualitative simplification with Neoclassical optimizationin one
choice. This would explainthe mixedresults
of testing presentedabove, and the fact that
strategiesappear to switch after complexity
level 3. When only two or three attributes
differ, balancing appears to be possible.
When more than three attributes differ,
people may simplifyby eliminatingone or
more attributes from consideration, and
then balancing the remaining attributes.
However,it would be difficultto distinguish
balancing from lexicographicor enumeration strategieswhen a small number of attributes are being compared.For example,
when only two attributesdiffer,a sequential
and quantitativestrategywouldbe the same
as a balancingstrategy.
Based on this debriefing experiment, it
appears that the direct approach-asking
people how they make choices-can provide
important insights. However, as discussed
above, this approach may not always be
possible to carry out, interpretationof the
qualitativeresults obtainedcan be problematic, and there are questions regarding
whetherpeople can accuratelydescribetheir
mental processes. Nevertheless, the direct
approachcan be useful, particularlyin designing surveys,by providingthe researcher
with informationthat might allow the indirect approachto be used more fruitfully.
IV. CONCLUSIONS
This paperhas providedsome initialsteps
for identifyingthe effects of complexityon
selection of decision strategies.We explore
a range of issues related to Heiner's (1983)
hypotheses about the existence of a gap

between the cognitive ability of the decision
maker and the difficulty of the decision,
termed the C-D gap. First, we extend the
standard discrete choice model to encom-

513

pass the case where the decisionmakerfaces
uncertain alternatives,and where the level
of uncertaintyvaries over choice occasions.
We then discussthe implicationsof various
modes of decision makingfor estimatedcoefficientsof discretechoice models.
The statistical analysis stronglysupports
a complexity effect, thereby rejecting the
hypothesisof perfect cognitionor full Neoclassicaloptimization.The empiricalresults
provide strong supportfor a complexityeffect occurringwhen the alternativesdiffer
by four or more attributes, and are thus
consistent with Heiner's hypothesizedC-D
gap and with his switchinghypothesis.The
results also suggest that, rather than one
strategy being dominant, mixed strategies
may be used by individualsor that different
strategies are used by different individuals
in the sample.A direct method in the form
of a debriefingexperiment,althoughbased
on a small sample,also supportsthe idea of
mixed strategiesbeing used by respondents.
This impliesthat Heiner'shypothesisthat
choices become more predictable when
heuristicsare used is not supportedin this
case. That is, it may become more difficult
to predict behavior in cases where fixed
decision rules are not firmlyestablishedbecause it can be difficultto identifyand predict which decision rule will be employedin
any particularcase. Thus, Heiner'shypothesis regardingincreased predictabilitywhen
simplifiedrules are employedwill not tend
to hold when decision makers do not have
the previousexperiencenecessaryto establish decision rules.
Because of the difficultiesencounteredin
tryingto identify decision strategiesin secondary data when there is no single dominant rule, an importantarea for future research is to develop methods that are more
effective in identifying decision heuristics.
This methodologicaldevelopmentshouldinvolve a studythat uses both direct and indirect approaches.In the context of surveys,
one important objective would be to attempt to develop a means of askingpeople
how they make their choices as part of the
actual survey.
The next important step is to develop a
more general theory of revealed preferences

<-----Page 15----->514

LandEconomics

that includes behavior based on decision
heuristics.This would allow us to infer preferences fromobservablebehaviorwhen simplified decision strategies are used, to the
extent that preferencesare revealed by decision heuristics. For example, a lexicographicdecisionrule impliesthat the choice
reveals the most preferredattributedifference, rather than the most preferredalternative. This informationcan potentiallybe
used to identify the relative importanceof
attributes. However, the appropriate discrete choice model takes a different form
when the preferred attribute difference is
identifiedthan that based on selecting the
preferredalternative.
Given that our results suggestthat mixed
strategiesappearto be important,one would
want to develop mixed estimationmethods,
based on separate models of behavior for
each differentdecision strategy.The models
would then be integratedin a mannersimilar to the switching regressions approach
(see, e.g., Goldfeldand Quandt1973).Using
this approach, the appropriatebehavioral
rule is first inferred for each observation.
Observationsare then used to estimate the
parametersof each of the behavioralmodels. Finally,preferencesfor the "representative" individualare estimatedusing the observationsfrom all models. An approachof
this sort allows one to explicitly consider
decision heuristicswithin a frameworkthat
quantifies preferences. This provides a
means for integratingpsychologicalmodels
of decision makingwith economic analysis
in a way that enrichesboth disciplines.

November1995

Ben-Akiva,
Moshe,andStevenR. Lerman.1985.

DiscreteChoiceAnalysis:Theoryand Applications to TravelDemand.Cambridge:The MIT
Press.
Cameron,TrudyAnn. 1988. "A New Paradigm
for ValuingNon-MarketGoods Using Referendum Data: MaximumLikelihood Estimation by CensoredLogistic Regression."Journal of EnvironmentalEconomicsManagement
15 (Sept.):355-79.
Ericsson,K. A., and H. A. Simon. 1984.Protocol
Analysis:VerbalReportsas Data. Cambridge:

MITPress.

Greene, WilliamH. 1990. EconometricAnalysis.

NewYork:Macmillan
Publishing
Company.
Goldfield,StephenM., andRichardE. Quandt.
1973. "A Markov Model for SwitchingRegressions."Journalof Econometrics1 (Mar.):
3-16.
Hanemann,W. Michael.1984. "WelfareEvaluations in Contingent Valuation Experiments
with Discrete Responses."AmericanJournal
Economics66 (Aug.):332-41.
of Agricultural
Hastings, N. A. J., and J. B. Peacock. 1975.
StatisticalDistributions.
New York:John Wiley
and Sons.
Hausman,JerryA. 1981."ExactConsumer'sSurplus and Deadweight Loss." AmericanEconomicReview71 (Sept.):662-76.
Heiner, Ronald A. 1983. "The Origin of Predictable Behavior."AmericanEconomicReview 73 (Sept.):560-95.

of
Horowitz,Joel. 1983."Statistical
Comparison

Non-Nested Probabalistic Discrete Choice
Models." TransportationScience 17 (Aug.):
319-50.
Kahneman, Daniel, and Amos Tversky. 1979.
"Prospect Theory: An Analysis of Decision
Under Risk."Econometrica47 (Mar.):263-91.
and
Maddala, G. S . 1983. Limited-Dependent
QualitativeVariablesin Econometrics.Cam-

Press.
University
bridge:Cambridge

Manski, C. 1977. "The Structure of Random
UtilityModels."TheoryandDecision8:229-54.
References
McFadden,D. 1973."ConditionalLogitAnalysis
of QualitativeChoice Behavior."In Frontiers
Addelman, J. 1962a. "OrthogonalMain-Effect
of Econometrics,ed. P. Zarembka.New York:
Plans for Asymmetrical Factorial ExperiAcademicPress.
4:21-46.
ments."Technometrics
Nisbett, RichardE., and TimothyDeCampWil. 1962b. "Symmetricaland Asymmetrical
son. 1977."TellingMoreThanWe CanKnow:
Fractional Factorial Plans." Technometrics
Verbal Reports on Mental Processes."Psy4:47-58.
chologicalReview84 (May):231-59.
Addelman,J., and 0. Kempthorne.1961. "OrOpaluch,JamesJ., and KathleenSegerson.1989.
"RationalRoots of 'Irrational'Behavior:New
thogonalMain-EffectPlans."U.S. Air Force,
Office of Aerospace Research, Aeronautical
Theories of Economic Decision-Making."
ResearchLaboratory,ARL TechnicalReport
NortheasternJournalof Agriculturaland Re79.
sourceEconomics18 (Oct.):81-95.

<-----Page 16----->71(4)

Mazzottaand Opaluch:Heiner'sHypothesis

515

dence and Limitations."Journalof Economic
Opaluch,James J., Stephen K. Swallow,Thomas
Literature20 (June):529-63.
Weaver,ChristopherW. Wessells,and Dennis
Wichelns. 1993. "Evaluating Impacts from
Simon,HerbertA. 1955."A BehavioralModel of
Rational Choice." QuarterlyJournalof EcoNoxious Facilities: Including Public Prefernomics69:174-83.
ences in CurrentSitingMechanisms."Journal
Economicsand Management Svenson,Ola. 1979."ProcessDescriptionsof Deof Environmental
24 (1):41-59.
cision Making."Organizational
Behaviorand
HumanPerformance23:86-112.
Payne, John W. 1982. "ContingentDecision Behavior."PsychologicalBulletin92 (2):382-402. Tversky, Amos, and Daniel Kahneman. 1974.
Russo, J. Edward, and BarbaraAnne Dosher.
"Judgement Under Uncertainty: Heuristics
and Biases."Science185:99-118.
1983. "Strategies for MultiattributeBinary
Choice." Journalof ExperimentalPsychology: Tversky,Amos, ShmuelSattath,and Paul Slovic.
1988. "ContingentWeighting in Judgement
Learning,Memory,and Cognition9 (4):676-96.
and Choice." Psychological Review 95
Schoemaker,Paul J. H. 1982. "The Expected
(3):371-84.
Utility Model: Its Variants, Purposes, Evi-

