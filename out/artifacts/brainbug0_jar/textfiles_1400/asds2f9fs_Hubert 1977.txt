<-----Page 0----->Psychological Bulletin
1977. Vol. 84, No. 2, 289-297

Kappa Revisited
Lawrence Hubert
University of Wisconsin—Madison
The relationship between matching models discussed in combinatorial probability theory and the measurement of nominal scale response agreement is reviewed. An emphasis is placed on indexing response agreement between two
raters using the Kappa statistic developed by Cohen, Fleiss, Light, and others.
Appropriate mean and variance expressions are provided, and their use is illustrated with a numerical example.
During the last decade several papers and
books have appeared in the psychological
literature that suggest procedures for measuring nominal scale response agreement (e.g.,
Everitt, 1968; Fleiss, 1973; Fleiss, Cohen, &
Everitt, 1969; Light, 1971). Most of these
contributions discuss an index of agreement
originally introduced by Cohen (1960) called
kappa (K), or alternatively, extend the basic
rationale for Cohen's measure to more general
situations (e.g., to multiple raters, conditional
agreement, etc.). This paper does not attempt
to develop any variations on K for use in a new
application, but instead tries to make explicit
some of the elementary statistical models that
have been advanced for purposes of significance testing and the construction of confidence intervals. Unfortunately, depending
upon the recency of the references used by a
researcher, there is some ambiguity as to what
procedures are appropriate, and at present the
applied researcher has no single source of
information available that develops the elementary statistical alternatives in any depth.
The topic of nominal scale response agreement is extremely broad and is capable of
being developed in many different directions.
To keep the present paper within reasonable
bounds, however, only a few of the more
elementary problems generally encountered
by an applied researcher are actually addressed.
Furthermore, a major emphasis is placed on the
bivariate case of two raters and on statistical
Requests for reprints should be sent to Lawrence
Hubert, Department of Educational Psychology,
University of Wisconsin, 1025 West Johnson Street,
Madison, Wisconsin 53706.

results that have been available in the combinatorial chance literature for many years. Apparently, the relationships have never been
pointed out between the problems of indexing
nominal scale response agreement and what is
called matching in combinatorics. This later
area of probability theory immediately provides a rigorous theoretical framework for
many of the inference techniques encountered
in using K.
Kappa
The basic paradigm of interest in measuring
nominal scale response agreement can be developed in the form of a T X T contingency
table that categorizes the responses of two
raters to the same set of n objects (see Table 1).
In Table 1 it is assumed that:
1.) Raters 1 and 2 both assign n objects to
one of T nominal (i.e., unordered) categories
Ai, At, ..., AT.
2.) »,•/ is the number of objects placed in
category A< by Rater 1 and in category Aj by
Rater 2.
3.) »(. is the number of responses in row i,
—n.j is the number of responses in column j,
and ».. = n is the total number of responses
given by each rater.
As a simple numerical example, which will be
continued throughout the discussion, Table 2
presents a 3 X 3 contingency table using 200
subjects assessed by 2 raters. This particular
illustration is taken from Fleiss, Cohen, and
Everitt (1969).
Since the agreements between the two raters
are defined by those entries along the main

289

<-----Page 1----->290

LAWRENCE HUBERT

Table 1

Illustrative T X T Contingency Table for 2
Raters and n Objects
Rater 2

Rater 1

A1

Ai

••••

AT

totals

HIT

n\.

AT

HTT

nT.

Column
totals

n.T

n.. ( = »)

Ai

left-to-right diagonal of the contingency table,
T
the sum Ro = 53 «« could be considered a
i-l

raw form of an agreement statistic. In particular, if P0 = Ro/n is the observed proportion
of agreement and PK = Re/n is the (as yet
undefined) expected proportion of agreement
under an assumption that Raters 1 and 2 are
operating "independently," then by definition,
K = (Po ~ Pe)/(l ~ P.).

Given this general form of the index K, a
problem still remains in defining reasonable
statistical models that generate significance
tests for K under an independence hypothesis,
or possibly, confidence intervals that are
appropriate when an independence hypothesis
does not necessarily hold.
The expected number of agreements under
the two major models to be discussed initially
r
will be given by Re = (53 »<.».<)/»• This ext-l

pected number of agreements Re is an exact
expression for one of the models considered ; in
the second, Re provides a natural estimate. It
should be noted at the outset, however, that
any bounded statistic, which thus has a finite
expectation, can be used in a *-type index, say
k. Specifically, if Q is the statistic under
consideration, then

LQ -

(0 - E(Q)l

For example, if Q = R0, then Cohen's measure
of agreement is obtained. Within a slightly
different context, if Q is the number of repetitions in a free-recall paradigm, then k is
equivalent to the adjusted ratio of clustering

(Hubert & Levin, 1976),
generalization to ic proves
tending the notion of rater
the most basic applications

and so on. This
important in exagreement beyond
(Fleiss, 1971).

Significance Tests for K Under an Independence
Assumption Using R0 Directly
In testing a hypothesis that the observed
value of K does not differ significantly from
zero, it is possible to deal directly with the raw
index R0. In other words, if the observed
proportion of agreement is too large or too
small in relation to what could be expected
under an independence assumption, then by
implication K will be assumed to differ significantly from zero as well. It should be recognized, however, that this use of R0 in significance testing is different from considering R0
as an actual measure of agreement.
At this point, the choice of a model to
characterize the independence assumption becomes crucial and has to be dealt with carefully. As an analogy that should be familiar
to many applied researchers, a similar problem
of choosing models appears in testing a sample
correlation coefficient against zero. In this
instance, the models that are usually considered
are based on either the parametric assumptions
of a bivariate normal distribution in which the
population parameter p is set equal to zero
(Hays, 1973), or possibly, a conditional nonparametric strategy using all randomizations
of one sample (Bradley, 1968). In a related
manner, two reasonable models can be defined
for use with R0. The first possibility given
below is based on randomization, whereas the
second requires a set of unknown parameters,
Table 2
Illustrative 3 X 3 Contingency Table for 2
Raters and 200 Subjects
Rater 2

T_J._I_

Rater 1

Al

Ai

A,

(across)

Ai
A3

106
22
2

10
28
12

4
10
6

120
60
20

Totals
(down)

130

SO

20

200

A!

<-----Page 2----->KAPPA REVISITED

and therefore, even though no normality
assumption is necessary, could be considered a
parametric alternative.
Matching Model
One of the simplest probabilistic models for
evaluating R0 has been available in various
forms for several hundred years and has been
discussed under the general topic of matching
distributions in probability theory (Barton,
1958). Using the historical context as a
motivation, we are given two decks of n
cards each containing T different types of
elements. The first deck is composed of a,cards of type i, 1 < i < T, and the second
deck is composed of bi cards of the same type
i, 1 < i < T. One deck is laid out randomly
in some order from, say, positions 1 to n;
in the same manner the cards from the second
deck are also randomly placed in order from
positions 1 to n. A match occurs when two cards
of type i, one from each deck, appear in the
same position. In other words, if a< = »*.,
bi = n.i, 1 < i < T, then R0 is merely the
total number of matches.
The distributional properties of R0 under the
matching model, assuming the compositions of
the two decks remain fixed, are very wellknown (e.g., David & Barton, 1962), and may
be summarized as follows:
1. EM(J?o) = (l/») £ rn.
i-l

2

£
i-l

T

- » £ nt.n.i(m. + n. + (£
t-l

i-l

2. If the marginal frequencies are all
relatively small but T is large in relation to n,
then a Poisson distribution with parameter
EM(.KO) provides a reasonable approximation
to the distribution of R0. Alternatively, if T
is small in relation to « but the marginal frequencies are all relatively large, then a normal
distribution with parameters EM(^?O) and
VinCRo) gives a reasonable approximation.
Even more accurate correspondences than
those suggested above may be obtained with

291

Pearson Type I curves or Charlier Type B
functions.
Although Poisson limits are possible for many
of the statistics discussed in the sections to
follow, the conditions for an adequate normal
approximation are the most common in behavioral science applications. Consequently,
only large sample tests based on the normal
distribution will be considered.
If, for notational convenience, we define
P.i = n.i/n, and Pi. = »,-./», then the mean
and variance of R0 under the matching model
become

n[n/(n i-l

- £ Pi.p.i(pt. + p.i) + (£ Pi.p.tn
i-l

i-l

These formulae will prove useful in later
comparisons.
Rollback's Model
Again, the convenient interpretation of
matching cards is employed but now the compositions of both decks for fixed n are assumed
to follow separate (and independent) multinomial distributions. Specifically, for the given
categories Ai, A*, ..., AT, the composition
of the first deck is governed by a multinomial
distribution with parameters iri, ITS, . . . , irr,
and for the second deck by a multinomial
distribution with parameters ir\, ..., TTT',
T
T
where £ *•< = £ IT/ = 1. The number of
1-1
1-1
matches, R0, under Kullback's (1939) model
has the very simple and well-known form of a
binomial random variable with parameters n
T
and £ *&{. Thus,
i-l
T

EK(R0)

VK(R0)

n£ -

= »(

<-----Page 3----->292

LAWRENCE HUBERT

and the standard approximating distributions
for the binomial may be employed. Moreover,
estimating these two moments under Kullback's model in the conventional manner by
replacing TT,- by Pi. and IT/ by P.,, we have

statistic K could be employed as well. For the
matching model the necessary moments are
obtained very simply since, if K = (P0 — Pe)/
(1 — Pe), where
= (1/w2) E

T

VK(R0) = «(

E

where P., and Pi. are defined as previously.
Although EK(R0) under Kullback's model
and EM(KO) for the matching model are the
same that is, Re = EM(-R0) = EK^O) = 95,
the appropriate variance terms differ. For
example, using the data of Table 2 as an example, VicCRo) = 49.87500 and VM(-R0) =
34.14573. Consequently, since R0 = 149, a z
score of 7.701 would be obtained for the
matching model, and a z score of 6.372 would
be obtained for Kullback's model. In either
case, however, it is obvious that K could be
considered different from zero at a very small
significance level.
It is important to note that the difference
between the two variance terms Vu(R0) and
does not disappear as n increases. In
particular, if we treat n/(n — 1) as 1 in the
expression for VM(^O), then

= E Pt.P.i,
i—l

then all quantities in K are fixed except for P0.
Therefore, by using the simple rules for finding
moments of linear combinations of random
variables (Hays, 1973),
EM 00 = 0,
VM« = [!/(« -

-E

x [E Pi.P.i - E Pi.P.t(Pi. + P.i)
r
i-l

For Kullback's model, however, if Pe is still
T
defined as (1/w2) E nt.n.i, then Pe is also a
1=1
random variable and can contribute in a
nontrivial way to the appropriate variance
term. Specifically, for Kullback's model.

7*

- E P^

VM(*o) = VK(&>) + «[2(E Pi.P.#
i—l

T

- £ Pi.p.i(pi. + p.<)].

T

V2
— V
P- P -(P-i. 41
^ JTi.r.\\f
^ iP .%}-^ 4~ CV
v,^ P-t »>•*P »t/• ) _!•
i-l
»-i

v-l

For some special cases (e.g., when all row and
column marginals are the same) the term on
the far right will be zero. This term is never
strictly positive, however; consequently,
VK(RO) is generally greater than Vu(Ro)- In
other words, Kullback's model provides a
significance test that in most instances is conservative with respect to the corresponding
significance test under the matching model.
Significance Tests for K Under an Independence
Assumption Using K
Instead of testing the independence assumption using the raw index R0, the complete

Clearly, the difference between VM(K) and
VK(«) is trivial and disappears if we treat the
multiplicative factor of (!/(» — 1)) in VM(K)
as \/n. The expression for VK(K) was first
given in the psychological literature by Fleiss,
Cohen, and Everitt (1969), and is relatively
simple to obtain. Its derivation requires a very
general result from large sample theory, dealing
with asymptotic variances for statistics defined
as functions of frequencies (see Fisher, 1925;
Fleiss, Cohen, & Everitt, 1969; Goodman &
Kruskal, 1972; Rao, 1973).
For our numerical example, K = .4286. Thus,
using the matching model we obtain VM(«) =

<-----Page 4----->KAPPA REVISITED

.003097, and using Kullback's model, VK(K) =
.003082. The z score statistic for the matching
model is 7.701, the same as that obtained for
R0; however, for Kullback's model the z score
is now increased to a value of 7.720. Both
values would imply rejection of the independence assumption at a very small level of
significance.
With this background in mind several
general statements can be made regarding
significance testing for K under the various
independence models. First of all, if the multiplicative factor of !/(« — 1) in the expression
for VM(RO) under the matching model is
treated as 1/w, then the variances of K under
both the matching model and Kullback's
model are the same; thus, the derived z score
statistics will be identical. Furthermore, using
R0 directly along with the matching model
will also produce this same z score statistic.
On the other hand, if Kullback's model is
chosen for testing R0, then the calculated z
score statistic will generally be smaller than the
previous common value.
It is interesting to note that the last z score
discussed above (i.e., the one using Kullback's
model and R0) will be identical to the z score
originally suggested by Cohen (1960) for testing K itself. In particular, Cohen asserts that
the variance of K is

293

and the same type of strategy can be employed
for related significance tests. As one specific
case that can be used as an illustration, suppose we are interested only in agreement for
those responses assigned to Category A< by
Rater 1. Following Light (1971), a conditional
index, called KP,., can be defined as follows:

= C(P»/P.'.) - P,-]/[i - P.,-],
where n.^./n is considered to be the expected
frequency for the entry in row i and column i,
and P«= Hu/n. Obviously, the crucial quantity
in KP( is ««, and thus we can use either n»
per se or the complete statistic KP( to test a
hypothesis that KP,. does not differ significantly
from zero. Again, either the matching model or
Kullback's model will provide an appropriate
set of moments for a large sample significance
test.
If only the quantity na is considered, then
under the matching model we have
EM(««) = nPi.P.i,
- P.-.P.^Pi. + P.i) + (P,.P,-)2].
Using KP( itself,

£ P.tPt./[n(l - £ P..-P,-.)]
i-l

f-1

= Pe/[>(l - P.)],
which, as Fleiss, Cohen, and Everitt (1969)
point out, illogically assumes that Kullback's
model holds and that the marginal frequencies
are fixed. Nevertheless, Cohen's strategy indirectly produces a "legitimate" z score statistic, which can be justified in terms of
Kullback's model and a direct evaluation of R0.

EM(W.) = 0,
VM(KP,)= [!/(» - l)][PVPi.]
X [(1 - P,,)/(l - P.,-)].
As an example from Table 2, if we conditionalize on Rater 1 responding within Category A2, then the following quantities are easily
calculated :
«22 = 28,

Conditional Measures of Agreement Under
an Independence Assumption
Although the previous discussion considered
the complete contingency table in defining
response agreement, a natural conditional
index can be developed in a similar manner,

EM(w22) =15,
VM(«22) = 7.914573,
KPi

= .2889,

,) = .003908,
z = 4.621 (for both w22 and «ps).

<-----Page 5----->294

LAWRENCE HUBERT

Since «,-,- is generated by a binomial process
for Kullback's model, the following expressions
can be obtained:
EK(»«) = wPi.P.i,
ii) = «Pi.P.i(l - Pi.P.i),
) = 0,

,) = [1/wJP.i/P,,]

procedure is not equivalent, in terms of a final
0 score, to testing «« directly under Kullback's
model. Also, there is no conservative relationship between Light's suggestion and the other
strategies, since the relevant variance terms
differ by a multiplicative factor that may be
either less than or greater than one. In particular, under Light's scheme, the variance of
KP, is assumed to be [l/w][P.,-/(l — P.i)],
and under Kullback's model,

x [d - p,-.)/a - P.*)].
In conclusion, it appears that Light's variance
This last variance formula requires the expression has no statistical justification, and
general large sample variance technique men- therefore should not be used.
tioned earlier in deriving VK(K) under KullAlternative Models for K Under an
back's model (see Rao, 1973). Numerically,
Independence Assumption
using the sample example as we did for the
Although
Kullback's model and the matchmatching model, the following quantities are
ing
model
seem
to be the most natural strateobtained :
gies for testing the significance of R0 or K, other
possibilities do exist. For purposes of a comparison, one such scheme, which appears
= 15,
especially relevant for an application discussed
VK"(«M) = 13.875000,
by Light (1971), will be mentioned briefly.
z = 3.490 (for «22),
Specifically, the general T X 7" contingency
table (Table 1) now represents the categorizaVK^PS) = .003889,
tion of a single object by In raters, where the
raters can be divided into two separate groups
z = 4.633 (for KP,).
and paired (e.g., brothers and sisters matched
It is obvious that formulas for condi- within the same family). Indexing agreement
tionalizing on column i rather than on row i in this context is really an attempt to measure
could be developed in an analogous fashion the consistency in categorization caused by the
merely by interchanging the roles of P,-. and pairing of the In raters; each diagonal entry
P.i in the various formulae. Furthermore, if na in the contingency table corresponds to the
we wish to conditionalize on more than one number of, say, brother-sister combinations
row or column, then the same procedures will that place the single object into the same
apply using a collapsed contingency table in Category A<.
Using Levene's (1949) matching model as a
which the appropriate rows (or columns) are
characterization
of independence, the 2n
grouped together into a single response class.
Again, several interesting conclusions can be observers are randomly split into two groups
obtained regarding the four possible variance and then randomly paired. The number of
terms and associated z score test statistics. If matches R0 in this case is approximately
the factor of (!/(» - 1)) is treated as (1/n) normal, with the following moments for large
under the matching model, then exactly the samples :
T
same z scores will be generated using KullEL(JZo) = » E qf,
back's model with «p,. or the matching model
i-l
with either KP< or »«. However, Light (1971)
suggests a variance term of [1/»][P.</(1 — P.i)]
VL(«.) = »[(£ <?i2)2 + L ?i2 - 2 £ ?<'],
for KP(, which supposedly extends Cohen's
t—l
i— 1
i— 1
(1960) original logic to the conditional agreement measure KP{. Unfortunately, Light's where qf = (».< + «<.)/2», 1 < i < T.

<-----Page 6----->KAPPA REVISITED

Since under Levene's model the expectation
term is no longer Re, the definition for a /c-type
index requires the more general form of a K
statistic:

- (p. -t«i£ <?<2)/a -»»i£ <?<2).
Although the row and column marginals vary
under Levene's model, the term qf does not,
1 < i < T. Consequently, for the k function
of the frequencies given above, the variance
T

is (!/[«(! - L ?.-2)])2VL(.Ro). It is just as
i-l

convenient, however, to deal with Levene's
model in the same way that K was indirectly
tested against zero using R0 and Kullback's
model. Specifically, if our example were considered appropriate for Levene's model, then

295

intervals for a population analogue of K. Almost
no information is available about the necessary
non-null matching distributions, and what
results have been discovered appear to be
rather inappropriate for our context (see
David & Barton, 1962). As a way of illustrating
what is necessary to develop a theory of confidence intervals, a brief discussion of a
parametric multinomial approach is presented
in this section.
Specifically, it is assumed that a multinomial
distribution is responsible for generating the
TXT contingency table for the two raters,
where T# is the probability that Rater 1 will
assign a particular object to Class A { and Rater
2 will assign the same object to Class Aj.
an<

Obviously,

i Kullback's

matching model discussed previously is
equivalent to assuming that n-y = iti.ir.j,
1 < i, j < T. As a population analogue of K
or KP( we can define

EL(KO) = 95.25,
VL(£>) = 34.237813,
z = 7.648.
Again, a rejection of "random" agreement
could be made at a very small significance level.
It is important to recognize, however, that for
some data sets the use of different models
could conceivably lead to different results.
Although Levene's model is somewhat contrary to the inference schemes used in psychology, an almost identical result for EL(^O)
has been used in measuring nominal scale
response agreement in sociology for some time
(see Krippendorff, 1970; Scott, 1957). In fact,
even though no variance term has been available to applied researchers up to now, Levene's
notion may be generally more popular in the
social sciences than either of the other two
matching concepts presented earlier.

If we assume K( POP ) and KP,.(POP) are estimated by
K and Kpt, respectively, and also use the general
large sample procedure for finding the variance
of a statistic defined as a function of frequencies, we obtain the estimated large
sample variances given below (the expression
for V(K) is also available in Fleiss, Cohen, &
Everitt, 1969) :'
VOO = [!/»][!/ (1 - Pe)]4
X {£ P«[(l - Pe)

(l-Po)2E

i-li-l

p«(P. < +Py.) 1

-(PJ>e-2Pe+Po)2},

Confidence Intervals for K or KP,. When an
Independence Assumption Does Not
Necessarily Hold
Although the emphasis in this paper is on
matching models, the matching concept itself
is of marginal interest in developing confidence

1
Some recent unpublished work by Fleiss and
Cicchetti on the non-null distribution of weighted «
indicates that the formula for V(K) may lead to biased
confidence intervals unless the sample size is very large.
In comparing two independent estimates o f « , however,
this formula appears adequate even for small samples.

<-----Page 7----->296

LAWRENCE HUBERT

X [Pi. - P«]{ (1 X [P«.IP.<(P«. - Pa)

Further, by employing these variance terms
in the usual way and assuming that a normal
approximation is adequate, the following
(1 — a) confidence intervals are obtained:

If Ro denotes the number of agreements
under one of the possible concepts given above,
the appropriate expectation and variance terms
can be found immediately in the combinatorial
chance literature (see David & Barton, 1962).
For example, using DeMoivre's definition and
letting w/"1 denote the marginal frequency for
Rater g and Class At, 1 < i < T, \ < g < G,
then the following moments can be given :

= » £ n PS*,
i-1 k—1

Kp,.±2 a / A/V(<oO,

where z a /2 is the upper a/2 percentage point
of a standard normal distribution. Continuing
our previous numerical example, the following
95% confidence intervals would be found for
K and KPV using V(/c) = .002885 and V((cp2) =
.005071:
K: (.324; .534),
KP,: (.150;.428).
Other Generalizations
Many generalizations of the Kappa index
are possible, including the important extension to the case of multiple raters. In general,
however, ambiguities always arise when contingency tables are defined by more than two
factors, and consequently, it is natural to
expect similar difficulties in developing procedures to assess agreement among more than
two raters. For instance, given G raters and
the natural extension of our previous TXT
contingency table to a Gfold contingency table,
there are at least three separate concepts of
agreement that could be defined:
1. DeMoivre's definition of agreement: An
agreement occurs if and only if all raters agree
on the categorization of an object.
2. Target rater definition of agreement:
Assuming that the first rater is identified as
unique in some way (e.g., as providing the
"true" categorization or standard), an agreement occurs if and only if another rater places
an object in the same category as the first
rater.
3. Pairwise definition of agreement: An
agreement occurs if and only if two raters
categorize an object consistently.

VCR0) = »{(G-

i-1 k-1

££

-£

k'-l

i-l 4=1

where P,-<<" = n^/n, \<i<T,\<g<G,
and factors of !/(» — 1) are treated as 1/w.
For the case of a target pack, all raters are
independent; thus, if factors !/(« — 1) are
again treated as 1/w, the moments below are
immediate :

t-2 i-1

L {£ p<

Light (1971) discusses the target pack application and provides the same variance term but
through a much more complicated formula.
Finally, using the third definition, if agreements are summed over all pairs of raters and
factors of !/(» — 1) are treated as 1/w, we
obtain

£ ££
k'-k+l k-1 i-1

V(R0) = n £

£ [£ P/*>P<<»'>

k'-k+l k-1

i-1

x (i - P<<*> - Pi (t/)

T

(£Pi

<-----Page 8----->KAPPA REVISITED

This application is also discussed by Light
(1971), but without the benefit of an expression
for the variance. Obviously, the last two variance expressions are very similar in form and
differ only in the initial summations. The key
to this relationship involves the independence
of pairs of raters; in both cases, the variance
of a sum of agreements can be expressed as a
simple sum of the variances associated with
each separate pair of raters.
For all three possible definitions of multiple
rater agreement, a kappa-type index can be
defined as
IR0 - E(JZo)]/[max(JZ.) - £(£„)],
where max (R0) varies depending upon the
matching definition used, that is, for DeMoivre's definition, max (J?0) = n; for the
target pack, max (R0) = (G — 1)»; and for the
pairwise definition, max (R0) = ( 9 )»• Moreover, since all marginals are fixed under the
matching model, the variance of the index is
just l/[max (Ro) — E(J?0)]2 times the original
variance for .Rein addition to the topics briefly sketched
above, there are numerous other extensions,
which could also be pursued. Most of these
generalizations, however, appear rather specialized and thus will not be developed explicitly.
For example, for a multiple rater problem, a
multinomial model could be assumed, and
confidence intervals could be placed on a
population analogue of the appropriate k index.
Moreover, for the target rater example, an
alternative model that assumes fixed marginals
for the target rater but "multinomial" marginals for the other G — 1 raters could be
proposed, and confidence intervals could be
developed in a related manner (see Goodman
& Kruskal, 1972, for other applications of this
notion). Finally, weighted k indices could be
proposed for all cases mentioned in this paper
that would allow disagreements to contribute
in some differential way to an overall agreement measure (e.g., Cohen, 1968).

297

References
Barton, D. E. The matching distributions: Poisson
limiting forms and derived methods of approximation.
Journal of the Royal Statistical Society B, 1958, 20,
73-92.
Bradley, J. V. Distribution-free statistical tests. New
York: Prentice-Hall, 1968.
Cohen, J. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 1960,
20, 37-46.
Cohen, J. Weighted kappa: Nominal scale agreement
with provision for scaled disagreement or partial
credit. Psychological Bulletin, 1968, 70, 213-220.
David, F. N., & Barton, D. E. Combinatorial chance.
New York: Hafner, 1962.
Everitt, B. S. Moments of the statistics kappa and
weighted kappa. British Journal of Mathematical and
Statistical Psychology, 1968, 21, 97-103.
Fisher, R. A. Statistical methods for research workers.
Edinburgh, Scotland: Oliver & Boyd, 1925.
Fleiss, J. L. Measuring nominal scale agreement among
many raters. Psychological Bulletin, 1971,76,378-382.
Fleiss, J. L. Statistical methods for rates and proportions.
New York: Wiley, 1973.
Fleiss, J. L., Cohen, J., & Everitt, B. S. Large sample
standard errors of kappa and weighted kappa.
Psychological Bulletin, 1969, 72, 323-327.
Goodman, L. A., & Kruskal, W, H. Measures of association for cross classifications, IV: Simplification of
asymptotic variances. Journal of the American
Statistical Association, 1972, 67, 415-421.
Hays, W. L. Statistics for social scientists. New York:
Holt, Rinehart & Winston, 1973.
Hubert, L., & Levin, J. A general statistical framework
for assessing categorical clustering in free recall.
Psychological Bulletin, 1976, 81, 1072-1080.
Krippendorff, K. Bivariate agreement coefficients for
reliability of data. In E. F. Bortatta (Ed.), Socio^
logical Methodology 1970. San Francisco: Jossey-Bass,
1970.
Kullback, S. Note on a matching problem. Annals of
Mathematical Statistics, 1939, 10, 77-80.
Levene, H. On a matching problem arising in genetics.
Annals of Mathematical Statistics, 1949, 20, 91-94.
Light, R. J. Measures of agreement for qualitative
data: Some generalizations and alternatives. Psychological Bulletin, 1971, 76, 365-377.
Rao, C. R. Linear statistical inference and its applications. New York: Wiley, 1973.
Scott, W. A. Reliability of content analysis: The case
of nominal scale coding. Public Opinion Quarterly,
1955, li>, 321-325.

Received November 6, 1975

