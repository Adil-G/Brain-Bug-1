<-----Page 0----->PSYCHOLOGICAL BULLETIN
Vol. 51, No. 4, 1954

THE THEORY OF DECISION MAKING 1
WARD EDWARDS
The Johns Hopkins University

economists call it, the theory of consumer's choice) has become exceedingly elaborate, mathematical, and
voluminous. This literature is almost
unknown to psychologists, in spite of
sporadic pleas in both psychological
(40, 84, 103, 104) and economic
(101, 102, 123, 128, 199, 202) literature for greater communication between the disciplines.
The purpose of this paper is to review this theoretical literature, and
also the rapidly increasing number of
psychological experiments (performed
by both psychologists and economists) that are relevant to it. The
review will be divided into five sections: the theory of riskless choices,
the application of the theory of riskless choices to welfare economics, the
theory of risky choices, transitivity in
decision making, and the theory of
games and of statistical decision
functions. Since this literature is unfamiliar and relatively inaccessible to
most psychologists, and since I could
not find any thorough bibliography
on the theory of choice in the eco1
This work was supported by Contract nomic literature, this paper includes
N5ori-166, Task Order I, between the Office
of Naval Research and The Johns Hopkins a rather extensive bibliography of the
University. This is Report No. 166-1-182, literature since 1930.

Many social scientists other than
psychologists try to account for the
behavior of individuals. Economists
and a few psychologists have produced a large body of theory and a
few experiments that deal with individual decision making. The kind of
decision making with which this body
of theory deals is as follows: given
two states, A and B, into either one of
which an individual may put himself,
the individual chooses A in preference to B (or vice versa). For instance, a child standing in front of a
candy counter may be considering
two states. In state A the child has
$0.25 and no candy. In state B the
child has $0.15 and a ten-cent candy
bar. The economic theory of decision
making is a theory about how to predict such decisions.
Economic theorists have been concerned with this problem since the
days of Jeremy Bentham (17481832). In recent years the development of the economic theory of consumer's decision making (or, as the

Project Designation No. NR 145-089, under
that contract. I am grateful to the Department of Political Economy, The Johns Hopkins University, for providing me with an
office adjacent to the Economics Library
while I was writing this paper. M. Allais,
M. M. Flood, N. Georgescu-Roegen, K. O.
May, A. Papandreou, L. J. Savage, and especially C. H. Coombs have kindly made
much unpublished material available to me.
A number of psychologists, economists, and
mathematicians have given me excellent, but
sometimes unheeded, criticism. Especially
helpful were C. Christ, C. H. Coombs, F.
Mosteller, and L. J. Savage.

380

THE THEORY OF RISKLESS CHOICES"
Economic man. The method of
those theorists who have been con2
No complete review of this literature is
available. Kauder (105, 106) has reviewed the
very early history of utility theory. Stigler
(180) and Viner (194) have reviewed the
literature up to approximately 1930. Samuelson's book (164) contains an illuminating
mathematical exposition of some of the content of this theory. Allen (6) explains the concept of indifference curves. Schultz (172) re-

<-----Page 1----->THEORY OF DECISION MAKING

cerned with the theory of decision
making is essentially an armchair
method. They make assumptions,
and from these assumptions they deduce theorems which presumably can
be tested, though it sometimes seems
unlikely that the testing will ever
occur. The most important set of
assumptions made in the theory of
riskless choices may be summarized
by saying that it is assumed that the
person who makes any decision to
which the theory is applied is an
economic man.
What is an economic man like? He
has three properties, (a) He is completely informed. (6) He is infinitely
sensitive, (c) He is rational.
Complete information. Economic
man is assumed to know not only
what all the courses of action open to
him are, but also what the outcome of
any action will be. Later on, in the
sections on the theory of risky choices
and on the theory of games, this assumption will be relaxed somewhat.
(For the results of attempts to introduce the possibility of learning
into this picture, see 51, 77.)
Infinite sensitivity. In most of the
older work on choice, it is assumed
that the alternatives available to an
individual are continuous, infinitely
divisible functions, that prices are
infinitely divisible, and that economic
man is infinitely sensitive. The only
purpose of these assumptions is to
make the functions that they lead to,
views the developments up to but not including the Hicks-Allen revolution from the point
of view of demand theory. Hicks's book (87)
is a complete and detailed exposition of most
of the mathematical and economic content of
the theory up to 1939. Samuelson (167) has
reviewed the integrability problem and the revealed preference approach. And Wold (204,
205, 206) has summed up the mathematical
content of the whole field for anyone who is
comfortably at home with axiom systems and
differential equations.

381

continuous and differentiable. Stone
(182) has recently shown that they
can be abandoned with no serious
changes in the theory of choice.
Rationality. The crucial fact about
economic man is that he is rational.
This means two things: He can
weakly order the states into which he
can get, and he makes his choices so
as to maximize something.
Two things are required in order
for economic man to be able to put all
available states into a weak ordering.
First, given any two states into which
he can get, A and B, he must always
be able to tell either that he prefers
A to B, or that he prefers B to A, or
that he is indifferent between them.
If preference is operationally defined
as choice, then it seems unthinkable
that this requirement can ever be
empirically violated. The second
requirement for weak ordering, a
more severe one, is that all preferences must be transitive. If economic
man prefers A to B and B to C, then
he prefers A to C. Similarly, if he is
indifferent between A and B and
between B and C, then he is indifferent between A and C. It is not
obvious that transitivity will always
hold for human choices, and experiments designed to find out whether
or not it does will be described in the
section on testing transitivity.
The second requirement of rationality, and in some ways the more
important one, is that economic man
must make his choices in such a way
as to maximize something. This is
the central principle of the theory of
choice. In the theory of riskless
choices, economic man has usually
been assumed to maximize utility. In
the theory of risky choices, he is assumed to maximize expected utility.
In the literature on statistical decision making and the theory of
games, various other fundamental

<-----Page 2----->382

WARD EDWARDS

principles of decision making are
considered, but they are all maximization principles of one sort or another.
The fundamental content of the
notion of maximization is that economic man always chooses the best
alternative from among those open
to him, as he sees it. In more technical language, the fact that economic
man prefers A to B implies and is
implied by the fact that A is higher
than B in the weakly ordered set
mentioned above. (Some theories introduce probabilities into the above
statement, so that if A is higher than
B in the weak ordering, then economic man is more likely to choose A
than B, but not certain to choose A.)
This notion of maximization is
mathematically useful, since it makes
it possible for a theory to specify a
unique point or a unique subset of
points among those available to the
decider. It seems to me psychologically unobjectionable. So many different kinds of functions can be maximized that almost any point actually
available in an experimental situation
can be regarded as a maximum of
some sort. Assumptions about maximization only become specific, and
therefore possibly wrong, when they
specify what is being maximized.
There has, incidentally, been almost no discussion of the possibility
that the two parts of the concept of
rationality might conflict. It is conceivable, for example, that it might
be costly in effort (and therefore in
negative utility) to maintain a weakly
ordered preference field. Under such
circumstances, would it be "rational"
to have such a field?
It is easy for a psychologist to point
out that an economic man who has
the properties discussed above is very
unlike a real man. In fact, it is so
easy to point this out that psycholo-

gists have tended to reject out of
hand the theories that result from
these assumptions. This isn't fair.
Surely the assumptions contained in
Hullian behavior theory (91) or in
the Estes (60) or Bush-Mosteller
(36, 37) learning theories are no more
realistic than these. The most useful
thing to do with a theory is not to
criticize its assumptions but rather
to test its theorems. If the theorems
fit the data, then the theory has at
least heuristic merit. Of course, one
trivial theorem deducible from the
assumptions embodied in the concept
of economic man is that in any
specific case of choice these assumptions will be satisfied. For instance,
if economic man is a model for real
men, then real men should always
exhibit transitivity of real choices.
Transitivity is an assumption, but it
is directly testable. So are the other
properties of economic man as a
model for real men.
Economists themselves are somewhat distrustful of economic man
(119, 156), and we will see in subsequent sections the results of a number of attempts to relax these assumptions.
Early utility maximization theory.
The school of philosopher-economists
started by Jeremy Bentham and
popularized by James Mill and others
held that the goal of human action is
to seek pleasure and avoid pain.
Every object or action may be considered from the point of view of
pleasure- or pain-giving properties.
These properties are called the utility
of the object, and pleasure is given
by positive utility and pain by negative utility. The goal of action, then,
is to seek the maximum utility. This
simple hedonism of the future is
easily translated into a theory of
choice. People choose the alternative,
from among those open to them, that

<-----Page 3----->THEORY OP DECISION

leads to the greatest excess of positive
over negative utility. This notion of
utility maximization is the essence of
the utility theory of choice. It will
reappear in various forms throughout this paper. (Bohnert [30] discusses the logical structure of the
utility concept.)
This theory of choice was embodied
in the formal economic analyses of all
the early great names in economics.
In the hands of Jevons, Walras, and
Menger it reached increasingly sophisticated mathematical expression
and it was embodied in the thinking
of Marshall, who published the first
edition of his great Principles of
Economics in 1890, and revised it at
intervals for more than 30 years
thereafter (137).
The use to which utility theory was
put by these theorists was to establish the nature of the demand for
various goods. On the assumption
that the utility of any good is a
monotonically increasing negatively
accelerated function of the amount of
that good, it is easy to show that the
amounts of most goods which a consumer will buy are decreasing functions of price, functions which are
precisely specified once the shapes of
the utility curves are known. This is
the result the economists needed and
is, of course, a testable theorem. (For
more on this, see 87, 159.)
Complexities arise in this theory
when the relations between the
utilities of different goods are considered. Jevons, Walras, Menger,
and even Marshall had assumed that
the utilities of different commodities
can be combined into a total utility
by simple addition; this amounts to
assuming that the utilities of different
goods are independent (in spite of
the fact that Marshall elsewhere discussed the notions of competing
goods, like soap and detergents, and

MAKING

383

completing goods, like right and left
shoes, which obviously do not have
independent utilities). Edgeworth
(53), who was concerned with such
nonindependent utilities, pointed out
that total utility was not necessarily
an additive function of the utilities
attributable to separate commodities.
In the process he introduced the notion of indifference curves, and thus
began the gradual destruction of the
classical utility theory. We shall return to this point shortly.
Although the forces of parsimony
have gradually resulted in the elimination of the classical concept of
utility from the economic theory of
riskless choices, there have been a
few attempts to use essentially the
classical theory in an empirical way.
Fisher (63) and Frisch (75) have developed methods of measuring marginal utility (the change in utility [u]
with an infinitesimal change in
amount possessed [Q], i.e., du/dQ)
from market data, by making assumptions about the interpersonal similarity of consumer tastes. Recently
Morgan (141) has used several variants of these techniques, has discussed mathematical and logical flaws
in them, and has concluded on the
basis of his empirical results that the
techniques require too unrealistic
assumptions to be workable. The
crux of the problem is that, for these
techniques to be useful, the commodities used must be independent
(rather than competing or completing), and the broad commodity classifications necessary for adequate
market data are not independent.
Samuelson (164) has shown that the
assumption of independent utilities,
while it does guatantee interval scale
utility measures, puts unwarrantably
severe restrictions on the nature of
the resulting demand function. Elsewhere Samuelson (158) presented,

<-----Page 4----->384

WARD EDWARDS

primarily as a logical and mathematical exercise, a method of measuring marginal utility by assuming
some time-discount function. Since
no reasonable grounds can be found
for assuming one such function rather
than another, this procedure holds no
promise of empirical success. Marshall suggested (in his notion of "consumer's surplus") a method of utility
measurement that turns out to be
dependent on the assumption of constant marginal utility of money, and
which is therefore quite unworkable.
Marshall's prestige led to extensive
discussion and debunking of this
notion (e.g., 28), but little positive
comes out of this literature. Thurstone (186) is currently attempting
to determine utility functions for
commodities experimentally, but has
reported no results as yet.
Indifference curves. Edgeworth's
introduction of the notion of indifference curves to deal with the
utilities of nonindependent goods was
mentioned above. An indifference
curve is, in Edgeworth's formulation, a constant-utility curve. Suppose that we consider apples and
bananas, and suppose that you get

the same amount of utility from
10-apples-and-l-banana as you do
from 6-apples-and-4-bananas. Then
these are two points on an indifference curve, and of course there are
an infinite number of other points on
the same curve. Naturally, this is not
the only indifference curve you may
have between apples and bananas. It
may also be true that you are indifferent between 13-apples-and-5bananas and 5-apples-and-15-bananas. These are two points on another,
higher indifference curve. A whole
family of such curves is called an indifference map. Figure 1 presents
such a map. One particularly useful
kind of indifference map has amounts
of a commodity on one axis and
amounts of money on the other.
Money is a commodity, too.
The notion of an indifference map
can be derived, as Edge worth derived
it, from the notion of measurable
utility. But it does not have to be.
Pareto (146, see also 151) was seriously concerned about the assumption that utility was measurable up
to a linear transformation. He felt
that people could tell whether they
preferred to be in state A or state B,
but could not tell how much they
25
preferred one state over the other. In
other words, he hypothesized a utility
function measurable only on an ordi_J 20
nal scale. Let us follow the usual
0_
QL
economic language, and call utility
< 15
measured on an ordinal scale ordinal
utility, and utility measured on an
interval
scale, cardinal utility. It is
10
meaningless to speak of the slope, or
Ld
marginal utility, of an ordinal utility
CO
function; such a function cannot be
differentiated. However, Pareto saw
that the same conclusions which had
0
5
10
15
20
25 been drawn from marginal utilities
could be drawn from indifference
NUMBER OF BANANAS
curves. An indifference map can be
FIG. 1. A HYPOTHETICAL INDIFFERENCE MAP drawn simply by finding all the com-

<-----Page 5----->THEORY OF DECISION

binations of the goods involved
among which the person is indifferent. Pareto's formulation assumes
that higher indifference curves have
greater utility, but does not need to
specify how much greater that utility
is.
It turns out to be possible to deduce from indifference curves all of
the theorems that were originally deduced from cardinal utility measures.
This banishing of cardinal utility was
furthered considerably by splendid
mathematical papers by Johnson
(97) and Slutsky (177). (In modern
economic theory, it is customary to
think of an w-dimensional commodity
space, and of indifference hyperplanes in that space, each such hyperplane having, of course, n— 1 dimensions. In order to avoid unsatisfactory
preference structures, it is necessary
to assume that consumers always
have a complete weak ordering for all
commodity bundles, or points in commodity space. Georgescu-Roegen
[76], Wold [204, 205, 206, 208],
Houthakker [90], and Samuelson
[167] have discussed this problem.)
Pareto was not entirely consistent
in his discussion of ordinal utility.
Although he abandoned the assumption that its exact value could be
known, he continued to talk about
the sign of the marginal utility coefficient, which assumed that some
knowledge about the utility function
other than purely ordinal knowledge
was available. He also committed
other inconsistencies. So Hicks and
Allen (88), in 1934, were led to their
classic paper in which they attempted
to purge the theory of choice of its
last introspective elements. They
adopted the conventional economic
view about indifference curves as determined from a sort of imaginary
questionnaire, and proceeded to derive all of the usual conclusions about

MAKING

385

consumer demand with no reference
to the notion of even ordinal utility
(though of course the notion of an
ordinal scale of preferences was still
embodied in their derivation of indifference curves). This paper was
for economics something like the behaviorist revolution in psychology.
Lange (116), stimulated by Hicks
and Allen, pointed out another inconsistency in Pareto. Pareto had assumed that if a person considered
four states, A, B, C, and D, he could
judge whether the difference between
the utilities of A and B was greater
than, equal to, or less than the difference between the utilities of C and D.
Lange pointed out that if such a
comparison was possible for any A,
B, C, and D, then utility was cardinally measurable. Since it seems
introspectively obvious that such
comparisons can be made, this paper
provoked a flood of protest and comment (7, 22, 117, 147, 209). Nevertheless, in spite of all the comment,
and even in spite of skepticism by a
distinguished economist as late as
1953 (153), Lange is surely right.
Psychologists should know this at
once; such comparisons are the basis
of the psychophysical Method of
Equal Sense Distances, from which
an interval scale is derived. (Samuelson [162] has pointed out a very interesting qualification.
Not only
must such judgments of difference be
possible, but they must also be transitive in order to define an interval
scale.) But since such judgments of
differences did not seem to be necessary for the development of consumer
demand theory, Lange's paper did
not force the reinstatement of cardinal utility.
Indeed, the pendulum swung
further in the behavioristic direction.
Samuelson developed a new analytic
foundation for the theory of con-

<-----Page 6----->386

WARD EDWARDS

sumer behavior, the essence of which
is that indifference curves and hence
the entire structure of the theory of
consumer choice can be derived
simply from observation of choices
among alternative groups of purchases available to a consumer (160,
161). This approach has been extensively developed by Samuelson
(164, 165, 167, 169) and others (50,
90, 125, 126). The essence of the idea
is that each choice defines a point
and a slope in commodity space.
Mathematical approximation methods make it possible to combine a
whole family of such slopes into an
indifference hyperplane. A family of
such hyperplanes forms an indifference "map."
In a distinguished but inaccessible
series of articles, Wold (204, 205, 206;
see also 208 for a summary presentation) has presented the mathematical
content of the Pareto, Hicks and Allen, and revealed preference (Samuelson) approaches, as well as Cassel's
demand function approach, and has
shown that if the assumption about
complete weak ordering of bundles of
commodities which was discussed
above is made, then all these approaches are mathematically equivalent.
Nostalgia for cardinal utility. The
crucial reason for abandoning cardinal utility was the argument of the
ordinalists that indifference curve
analysis in its various forms could do
everything that cardinal utility could
do, with fewer assumptions. So far
as the theory of riskless choice is concerned, this is so. But this is only an
argument for parsimony, and parsimony is not always welcome. There
was a series of people who, for one
reason or another, wanted to reinstate cardinal utility, or at least
marginal utility. There were several
mathematically invalid attempts to

show that marginal utility could be
defined even in an ordinal-utility
universe (23, 24, 163; 25, 114).
Knight (110), in 1944, argued extensively for cardinal utility; he
based his arguments in part on introspective considerations and in part
on an examination of psychophysical
scaling procedures. He stimulated a
number of replies (29, 42; 111). Recently Robertson (154) pleaded for
the reinstatement of cardinal utility
in the interests of welfare economics
(this point will be discussed again
below). But in general the indifference curve approach, in its various
forms, has firmly established itself as
the structure of the theory of riskless
choice.
Experiments on indifference curves.
Attempts to measure marginal utility
from market data were discussed
above. There have been three experimental attempts to measure indifference curves. Schultz, who pioneered
in deriving statistical demand curves,
interested his colleague at the University of Chicago, the psychologist
Thurstone, in the problem of indifference curves. Thurstone (185)
performed a very simple experiment.
He gave one subject a series of combinations of hats and overcoats, and
required the subject to judge whether
he preferred each combination to a
standard. For instance, the subject
judged whether he preferred eight
hats and eight overcoats to fifteen
hats and three overcoats. The same
procedure was repeated for hats and
shoes, and for shoes and overcoats.
The data were fitted with indifference
curves derived from the assumptions
that utility curves fitted Fechner's
Law and that the utilities of the
various objects were independent.
Thurstone says that Fechner's Law
fitted the data better than the other
possible functions he considered, but

<-----Page 7----->THEORY OF DECISION

presents no evidence for this assertion. The crux of the experiment
was the attempt to predict the indifference curves between shoes and
overcoats from the other indifference
curves. This was done by using the
other two indifference curves to infer
utility functions for shoes and for
overcoats separately, and then using
these two utility functions to predict
the total utility of various amounts
of shoes and overcoats jointly. The
prediction worked rather well. The
judgments of the one subject used are
extraordinarily orderly; there is very
little of the inconsistency and variability that others working in this
area have found. Thurstone says,
"The subject . . . was entirely naive
as regards the psychophysical problem involved and had no knowledge
whatever of the nature of the curves
that we expected to find" (18S, p.
154). He adds, "I selected as subject
a research assistant in my laboratory
who knew nothing about psychophysics.
Her work was largely
clerical in nature. She had a very
even disposition, and I instructed her
to take an even motivational attitude
on the successive occasions . . . I was
surprised at the consistency of the
judgments that I obtained, but I am
pretty sure that they were the result
of careful instruction to assume a uniform motivational attitude."3 From
the economist's point of view, the
main criticism of this experiment is
that it involved imaginary rather
than real transactions (200).
The second experimental measurement of indifference curves is reported
by the economists Rousseas and Hart
(157). They required large numbers
of students to rank sets of three combinations of different amounts of ba8
Thurstone, L. L. Personal communication,
December 7, 1953.

MAKING

387

con and eggs. By assuming that all
students had the same indifference
curves, they were able to derive a composite indifference map for bacon and
eggs. No mathematical assumptions
were necessary, and the indifference
map is not given mathematical form.
Some judgments were partly or completely inconsistent with the final map,
but not too many. The only conclusion which this experiment justifies is
that it is possible to derive such a
composite indifference map.
The final attempt to measure an
indifference curve is a very recent one
by the psychologists Coombs and
Milholland (49). The indifference
curve involved is one between risk
and value of an object, and so will be
discussed below in the section on the
theory of risky decisions. It is mentioned here because the same methods (which show only that the indifference curve is convex to the
origin, and so perhaps should not be
called measurement) could equally
well be applied to the determination
of indifference curves in riskless
situations.
Mention should be made of the
extensive economic work on statistical demand curves. For some reason
the most distinguished statistical demand curve derivers feel it necessary
to give an account of consumer's
choice theory as a preliminary to the
derivation of their empirical demand
curves. The result is that the two
best books in the area (172, 182) are
each divided into two parts; the first
is a general discussion of the theory
of consumer's choice and the second
a quite unrelated report of statistical
economic work. Stigler (179) has
given good reasons why the statistical
demand curves are so little related to
the demand curves of economic
theory, and Wallis and Friedman
(200) argue plausibly that this state

<-----Page 8----->388

WARD EDWARDS

of affairs is inevitable. At any rate,
there seems to be little prospect of
using large-scale economic data to fill
in the empirical content of the theory
of individual decision making.
Psychological comments. There are
several commonplace observations
that are likely to occur to psychologists as soon as they try to apply the
theory of riskless choices to actual
experimental work. The first is that
human beings are neither perfectly
consistent nor perfectly sensitive.
This means that indifference curves
are likely to be observable as indifference regions, or as probability
distributions of choice around a
central locus. It would be easy to
assume that each indifference curve
represents the modal value of a normal sensitivity curve, and that choices
should have statistical properties
predictable from that hypothesis as
the amounts of the commodities
(locations in product space) are
changed. This implies that the definition of indifference between two
collections of commodities should be
that each collection is preferred over
the other 50 per cent of the time.
Such a definition has been proposed
by an economist (108), and used in
experimental work by psychologists
(142). Of course, SO per cent choice
has been a standard psychological
definition of indifference since the
days of Fechner.
Incidentally, failure on the part of
an economist to understand that a
just noticeable difference (j.n.d.) is a
statistical concept has led him to
argue that the indifference relation is
intransitive, that is, that if A is indifferent to B and B is indifferent to
C, then A need not be indifferent to C
(8, 9, 10). He argues that if A and B
are less than one j.n.d. apart, then A
will be indifferent to B; the same of
course is true of B and C; but A and

C may be more than one j.n.d. apart,
and so one may be preferred to the
other. This argument is, of course,
wrong. If A has slightly more utility
than B, then the individual will
choose A in preference to B slightly
more than SO per cent of the time,
even though A and B are less than
one j.n.d. apart in utility. The 50 per
cent point is in theory a precisely
defined point, not a region. It may in
fact be difficult to determine because
of inconsistencies in judgments and
because of changes in taste with time.
The second psychological observation is that it seems impossible even
to dream of getting experimentally
an indifference map in w-dimensional
space where n is greater than 3. Even
the case of w = 3 presents formidable
experimental problems. This is less
important to the psychologist who
wants to use the theory of choice to
rationalize experimental data than
to the economist who wants to derive a theory of general static equilibrium.
Experiments like Thurstone's (185)
involve so many assumptions that it
is difficult to know what their empirical meaning might be if these assumptions were not made. Presumably,
the best thing to do with such experiments is to consider them as tests
of the assumption with the least face
validity. Thurstone was willing to
assume utility maximization and independence of the commodities involved (incidentally, his choice of
commodities seems singularly unfortunate for justifying an assumption of independent utilities), and so
used his data to construct a utility
function. Of course, if only ordinal
utility is assumed, then experimental
indifference curves cannot be used
this way. In fact, in an ordinalutility universe neither of the principal assumptions made by Thurstone

<-----Page 9----->THEORY OF DECISION

can be tested by means of experimental indifference curves. So the
assumption of cardinal utility, though
not necessary, seems to lead to considerably more specific uses for experimental data.
At any rate, from the experimental
point of view the most interesting
question is: What is the observed
shape of indifference curves between
independent commodities? This question awaits an experimental answer.
The notion of utility is very similar
to the Lewinian notion of valence
(120, 121).
Lewin conceives of
valence as the attractiveness of an
object or activity to a person (121).
Thus, psychologists might consider
the experimental study of utilities to
be the experimental study of valences,
and therefore an attempt at quantifying parts of the Lewinian theoretical
schema.
APPLICATION OF THE THEORY OF
RISKLESS CHOICES TO WELFARE ECONOMICS4
The classical utility theorists assumed the existence of interpersonally
comparable cardinal utility. They
were thus able to find a simple answer to the question of how to determine the best economic policy:
That economic policy is best which
results in the maximum total utility,
summed over all members of the
economy.
The abandonment of interpersonal
comparability makes this answer useless. A sum is meaningless if the
units being summed are of varying
sizes and there is no way of reducing
them to some common size. This
4
The discussion of welfare economics given
in this paper is exceedingly sketchy. For a
picture of what the complexities of modern
welfare economics are really like (see 11, 13,
14, 86, 118, 124, 127, 139, 140, 148, 154, 155,
166, 174).

MAKING

389

point has not been universally recognized, and certain economists (e.g.,
82, 154) still defend cardinal (but not
interpersonally comparable) utility
on grounds of its necessity for welfare economics.
Pareto's principle. The abandonment of interpersonal comparability
and then of cardinal utility produced
a search for some other principle to
justify economic policy.
Pareto
(146), who first abandoned cardinal
utility, provided a partial solution.
He suggested that a change should
be considered desirable if it left
everyone at least as well off as he
was before, and made at least one
person better off.
Compensation principle. Pareto's
principle is fine as far as it goes, but
it obviously does not go very far.
The economic decisions which can be
made on so simple a principle are few
and insignificant. So welfare economics languished until Kaldor (98)
proposed the compensation principle. This principle is that if it is
possible for those who gain from an
economic change to compensate the
losers for their losses and still have
something left over from their gains,
then the change is desirable. Of
course, if the compensation is actually
paid, then this is simply a case of
Pareto's principle.
But Kaldor asserted that the
compensation need not actually be
made; all that was necessary was
that it could be made. The fact that
it could be made, according to
Kaldor, is evidence that the change
produces an excess of good over harm,
and so is desirable. Scitovsky (173)
observed an inconsistency in Kaldor's
position: Some cases could arise in
which, when a change from A to B
has been made because of Kaldor's
criterion, then a change back from B
to A would also satisfy Kaldor's

<-----Page 10----->390

WARD

criterion. It is customary, therefore,
to assume that changes which meet
the original Kaldor criterion are only
desirable if the reverse change does
not also meet the Kaldor criterion.
It has gradually become obvious
that the Kaldor-Scitovsky criterion
does not solve the problem of welfare
economics (see e.g., 18, 99). It assumes that the unpaid compensation
does as much good to the person who
gains it as it would if it were paid to
the people who lost by the change.
For instance, suppose that an industrialist can earn $10,000 a year
more from his plant by using a new
machine, but that the introduction of
the machine throws two people irretrievably out of work. If the salary
of each worker prior to the change
was $4,000 a year, then the industrialist could compensate the
workers and still make a profit. But
if he does not compensate the workers, then the added satisfaction he
gets from his extra $10,000 may be
much less than the misery he produces in his two workers. This example only illustrates the principle;
it does not make much sense in these
days of progressive income taxes, unemployment compensation, high employment, and strong unions.
Social welfare functions. From here
on the subject of welfare economics
gets too complicated and too remote
from psychology to merit extensive
exploration in this paper. The line
that it has taken is the assumption
of a social welfare function (21), a
function which combines individual
utilities in a way which satisfies
Pareto's principle but is otherwise
undefined. In spite of its lack of
definition, it is possible to draw
certain conclusions from such a function (see e.g., 164). However, Arrow
(14) has recently shown that a social
welfare function that meets certain

EDWARDS

very reasonable requirements about
being sensitive in some way to the
wishes of all the people affected,
etc., cannot in general be found in
the absence of interpersonally comparable utilities (see also 89).
Psychological comment. Some economists are willing to accept the
fact that they are inexorably committed to making moral judgments
when they recommend economic
policies (e.g., 152, 153). Others still
long for the impersonal amorality of a
utility measure (e.g., 154). However
desirable interpersonally comparable
cardinal utility may be, it seems
Utopian to hope that any experimental procedure will ever give information about individual utilities
that could be of any practical use in
guiding large-scale economic policy.

THE THEORY OF RISKY CHOICES"
Risk and uncertainty. Economists
and statisticians distinguish between
6

Strotz (183) and Alchian (1) present nontechnical and sparkling expositions of the von
Neumann and Morgenstern utility measurement proposals. Georgescu-Roegen (78) critically discusses various axiom systems so as to
bring some of the assumptions underlying this
kind of cardinal utility into clear focus. Allais
(3) reviews some of these ideas in the course of
criticizing them, Arrow (12, 14) reviews parts
of the field.
There is a large psychological literature on
one kind of risky decision making, the kind
which results when psychologists use partial
reinforcement. This literature has been reviewed by Jenkins and Stanley (96). Recently
a number of experimenters, including Jarrett
(95), Flood (69, 70), Bilodeau (27), and myself (56) have been performing experiments on
human subjects who are required to choose
repetitively between two or more alternatives,
each of which has a probability of reward
greater than zero and less than one. The problems raised by these experiments are too complicated and too far removed from conventional utility theory to be dealt with in this
paper. This line of experimentation may eventually provide the link which ties together
utility theory and reinforcement theory.

<-----Page 11----->THEORY OF DECISION MAKING

risk and uncertainty. There does not
seem to be any general agreement
about which concept should be associated with which word, but the
following definitions make the most
important distinctions.
Almost everyone would agree that
when I toss a coin the probability
that I will get a head is .5. A proposition about the future to which a number can be attached, a number that
represents the likelihood that the
proposition is true, may be called a
first-order risk. What the rules are for
attaching such numbers is a much
debated question, which will be
avoided in this paper.
Some propositions may depend on
more than one probability distribution. For instance, I may decide that
if I get a tail, I will put the coin back
in my pocket, whereas if I get a head,
I will toss it again. Now, the probability of the proposition "I will get
a head on my second toss" is a function of two probability distributions,
the distribution corresponding to the
first toss and that corresponding to
the second toss. This might be called
a second-order risk. Similarly, risks of
any order may be constructed. It is a
mathematical characteristic of all
higher-order risks that they may be
compounded into first-order risks by
means of the usual theorems for compounding probabilities. (Some economists have argued against this procedure [83], essentially on the grounds
that you may have more information
by the time the second risk comes
around. Such problems can best be
dealt with by means of von Neumann
and Morgenstern's [197] concept of
strategy, which is discussed below.
They become in general problems of
uncertainty, rather than risk.)
Some propositions about the future
exist to which no generally accepted
probabilities can be attached. What

391

is the probability that the following
proposition is true: Immediately
after finishing this paper, you will
drink a glass of beer? Surely it is
neither impossible nor certain, so it
ought to have a probability between
zero and one, but it is impossible for
you or me to find out what that probability might be, or even to set up
generally acceptable rules about how
to find out. Such propositions are
considered cases of uncertainty, rather
than of risk. This section deals only
with the subject of first-order risks.
The subject of uncertainty will arise
again in connection with the theory
of games.
Expected utility maximization. The
traditional mathematical notion for
dealing with games of chance (and so
with risky decisions) is the notion
that choices should be made so as to
maximize expected value. The expected value of a bet is found by
multiplying the value of each possible
outcome by its probability of occurrence and summing these products across all possible outcomes. In
symbols:
where p stands for probability, $
stands for the value of an outcome,
and pi+p*+ • • • +£n = l.
The assumption that people actually behave the way this mathematical notion says they should is
contradicted by observable behavior
in many risky situations. People are
willing to buy insurance, even though
the person who sells the insurance
makes a profit. People are willing to
buy lottery tickets, even though the
lottery makes a profit. Consideration
of the problem of insurance and of the
St. Petersburg paradox led Daniel
Bernoulli, an eighteenth century
mathematician, to propose that they
could be resolved by assuming that

<-----Page 12----->392

WARD EDWARDS

people act so as to maximize expected
utility, rather than expected value
(26). (He also assumed that utility
followed a function that more than a
century later was proposed by Fechner for subjective magnitudes in
general and is now called Fechner's
Law.) This was the first use of the
notion of expected utility.
The literature on risky decision
making prior to 1944 consists primarily of the St. Petersburg paradox
and other gambling and probability
literature in mathematics, some literary discussion in economics (e.g., 109,
187), one economic paper on lotteries
(189), and the early literature of the
theory of games (31, 32, 33, 34, 195),
which did not use the notion of
utility. The modern period in the
study of risky decision making began
with the publication in 1944 of von
Neumann and Morgenstern's monumental book Theory of Games and
Economic Behavior (196, see also
197), which we will discuss more fully
later. Von Neumann and Morgenstern pointed out that the usual assumption that economic man can
always say whether he prefers one
state to another or is indifferent between them needs only to be slightly
modified in order to imply cardinal
utility. The modification consists of
adding that economic man can also
completely order probability combinations of states. Thus, suppose
that an economic man is indifferent
between the certainty of $7.00 and a
50-50 chance of gaining $10.00 or
nothing. We can assume that his
indifference between these two prospects means that they have the same
utility for him. We may define the
utility of $0.00 as zero utiles (the
usual name for the unit of utility, just
as sone is the name for the unit of
auditory loudness), and the utility
of $10.00 as 10 utiles?, These two

arbitrary definitions correspond to
defining the two undefined constants
which are permissible since cardinal
utility is measured only up to a linear
transformation. Then we may calculate the utility of $7.00 by using
the concept of expected utility as follows:
17(17.00) = .5 £7($10.00) +.5 E7($0.00)
= .5(10)+.5(0) = 5.
Thus we have determined the cardinal utility of $7.00 and found that it
is 5 utiles. By varying the probabilities and by using the already found
utilities it is possible to discover the
utility of any other amount of money,
using only the two permissible arbitrary definitions. It is even more
convenient if instead of +$10.00,
— $10.00 or some other loss is used as
one of the arbitrary utilities.
A variety of implications is embodied in this apparently simple notion. In the attempt to examine and
exhibit clearly what these implications are, a number of axiom systems,
differing from von Neumann and
Morgenstern's but leading to the
same result, have been developed
(73, 74, 85, 135, 136, 171). This
paper will not attempt to go into
the complex discussions (e.g., 130,
131, 168, 207) of these various alternative axiom systems. One recent
discussion of them (78) has concluded, on reasonable grounds, that
the original von Neumann and Morgenstern set of axioms is still the best.
It is profitable, however, to examine what the meaning of this notion is from the empirical point of
view if it is right. First, it means that
risky propositions can be ordered
in desirability, just as riskless ones
can. Second, it means that the concept of expected utility is behaviorally meaningful. Finally, it means
choices among risky alternatives

<-----Page 13----->THEORY OF DECISION

are made in such a way that they
maximize expected utility.
If this model is to be used to predict actual choices, what could go
wrong with it? It might be that the
probabilities by which the utilities
are multiplied should not be the objective probabilities; in other words, a
decider's estimate of the subjective
importance of a probability may not
be the same as the numerical value of
that probability. It might be that
the method of combination of probabilities and values should not be
simple multiplication. It might be
that the method of combination of
the probability-value products should
not be simple addition. It might be
that the process of gambling has
some positive or negative utility of
its own. It might be that the whole
approach is wrong, that people just
do not behave as if they were trying
to maximize expected utility. We
shall examine some of these possibilities in greater detail below.
Economic implications of maximizing expected utility.
The utilitymeasurement notions of von Neumann and Morgenstern were enthusiastically welcomed by many
economists (e.g., 73, 193), though a
few (e.g., 19) were at least temporarily (20) unconvinced. The most
interesting economic use of them was
proposed by Friedman and Savage
(73), who were concerned with the
question of why the same person who
buys insurance (with a negative expected money value), and therefore is
willing to pay in order not to take
risks, will also buy lottery tickets
(also with a negative expected money
value) in which he pays in order to
take risks. They suggested that these
facts could be reconciled by a doubly
inflected utility curve for money, like
that in Fig. 2. If / represents the
person's current income, then he is

MAKING

393

clearly willing to accept "fair" insurance (i.e., insurance with zero expected money value) because the
serious loss against which he is insuring would have a lower expected
utility than the certain loss of the
insurance premium. (Negatively accelerated total utility curves, like
that from the origin to /, are what
you get when marginal utility decreases; thus, decreasing marginal

UJ

DOLLARS
FIG. 2. HYPOTHETICAL UTILITY CURVE FOR
MONEY, PROPOSED BY FRIEDMAN AND SAVAGE

utility is consistent with the avoidance of risks.) The person would also
be willing to buy lottery tickets, since
the expected utility of the lottery
ticket is greater than the certain loss
of the cost of the ticket, because of
the rapid increase in the height of the
utility function. Other considerations make it necessary that the
utility curve turn down again. Note
that this discussion assumes that
gambling has no inherent utility.
Markowitz (132) suggested an important modification in this hypothesis.
He suggested that the
origin of a person's utility curve for
money be taken as his customary

<-----Page 14----->394

WARD EDWARDS

financial status, and that on both
sides of the origin the curve be assumed first concave and then convex.
If the person's customary state of
wealth changes, then the shape of his
utility curve will thus remain generally the same with respect to where
he now is, and so his risk-taking behavior will remain pretty much the
same instead of changing with every
change of wealth as in the FriedmanSavage formulation.
Criticism of the expected-utility
maximization theory. It is fairly easy
to construct examples of behavior
that violate the von NeumannMorgenstern axioms (for a particularly ingenious example, see 183). It
is especially easy to do so when the
amounts of money involved are very
large, or when the probabilities or
probability differences involved are
extremely small. Allais (5) has constructed a questionnaire full of items
of this type. For an economist interested in using these axioms as a
basis for a completely general theory
of risky choice, these examples may
be significant. But psychological interest in this model is more modest.
The psychologically important question is: Can such a model be used to
account for simple experimental examples of risky decisions?
Of course a utility function derived
by von Neumann-Morgenstern means
is not necessarily the same as a classical utility function (74, 203; see also
82).
Experiment on the von NeumannMorgenstern model. A number of experiments on risky decision making
have been performed. Only the first
of them, by Mosteller and Nogee
(142), has been in the simple framework of the model described above.
All the rest have in some way or
another centered on the concept of
probabilities effective for behavior

which differ in some way from the
objective probabilities, as well as on
utilities different from the objective
values of the objects involved.
Mosteller and Nogee (142) carried
out the first experiment to apply the
von Neumann-Morgenstern model.
They presented Harvard undergraduates and National Guardsmen with
bets stated in terms of rolls at poker
dice, which each subject could accept
or refuse. Each bet gave a "hand"
at poker dice. If the subject could
beat the hand, he won an amount
stated in the bet. If not, he lost a
nickel. Subjects played with $1.00,
which they were given at the beginning of each experimental session.
They were run together in groups of
five; but each decided and rolled the
poker dice for himself. Subjects were
provided with a table in which the
mathematically fair bets were shown,
so that a subject could immediately
tell by referring to the table whether
a given bet was fair, or better or
worse than fair.
In the data analysis, the first step
was the determination of "indifference offers." For each probability
used and for each player, the amount
of money was found for which that
player would accept the bet SO per
cent of the time. Thus equality was
defined as SO per cent choice, as it
is likely to be in all psychological experiments of this sort. Then the
utility of $0.00 was defined as 0
utiles, and the utility of losing a
nickel was defined as — 1 utile. With
these definitions and the probabilities
involved, it was easy to calculate the
utility corresponding to the amount
of money involved in the indifference
offer. It turned out that, in genera),
the Harvard undergraduates had
diminishing marginal utilities, while
the National Guardsmen had increasing marginal utilities.

<-----Page 15----->THEORY OF DECISION MAKING

395

The utilities thus calculated were periment. Consequently, their conused in predicting the results of more clusion that the amount of money
complex bets. It is hard to evaluate possessed by the subjects was not
the success of these predictions. At seriously important can only be true
any rate, an auxiliary paired- if their utility curves are utilitycomparisons experiment showed that for-w-more dollars curves and if the
the hypothesis that subjects maxi- shapes of such curves are not affected
mized expected utility predicted by changes in the number of dollars
choices better than the hypothesis on hand. This discussion exhibits a
that subjects maximized expected type of problem which must always
money value.
arise in utility measurement and
The utility curve that Mosteller which is new in psychological scaling.
and Nogee derive is different from The effects of previous judgments on
the one Friedman and Savage (73) present judgments are a familiar
were talking about. Suppose that a story in psychophysics, but they are
subject's utility curve were of the usually assumed to be contaminating
Friedman-Savage type, as in Fig1. 2, influences that can be minimized or
and that he had enough money to put eliminated by proper experimental
him at point P. If he now wins or design. In utility scaling, the fundaloses a bet, then he is moved to a mental idea of a utility scale is such
different location on the indifference that the whole structure of a subject's
curve, say Q. (Note that the amounts choices should be altered as a result
of money involved are much smaller of each previous choice (if the choices
than in the original Friedman-Savage are real ones involving money gaina
use of this curve.) However, the con- or losses). The Markowitz solution
struction of a Mosteller-Nogee utility to this problem is the most practical
curve assumes that the individual is one available at present, and that
always at the same point on his solution is not entirely satisfactory
utility curve, namely the origin. This since all it does is to assume that
means that the curve is really of the people's utilities for money operate
Markowitz (132) type discussed in such a way that the problem does
above, instead of the Friedman- not really exist. This assumption is
Savage type. The curve is not really plausible for money, but it geta
a curve of utility of money in general, rapidly less plausible when other
but rather it is a curve of the utility- commodities with a less continuous
for-w-more dollars. Even so, it must character are considered instead.
be assumed further that as the total
Probability preferences. In a series
amount of money possessed by the of recent experiments (55, 57, 58,
subject changes during the experi- 59), the writer has shown that subjects,
ment, the utility-for-«-more dollars when they bet, prefer some probabilcurve does not change. Mosteller and ities to others (57), and that these
Nogee argue, on the basis of detailed preferences cannot be accounted for
examination of some of their data, by utility considerations (59). All
that the amount of money possessed the experiments were basically of the
by the subjects did not seriously same design. Subjects were required
influence their choices. The utility to choose between pairs of bets accurves they reported showed chang- cording to the method of paired coming marginal utility within the parisons. The bets were of three
amounts of money usdd in their ex- kinds: positive expected value, nega-

<-----Page 16----->396

WARD EDWARDS

tive expected value, and zero expected value. The two members of
each pair of bets had the same expected value, so that there was never
(in the main experiment [57, 59]) any
objective reason to expect that choosing one bet would be more desirable
than choosing the other.
Subjects made their choices under
three conditions: just imagining they
were betting; betting for worthless
chips; and betting for real money.
They paid any losses from their own
funds, but they were run in extra
sessions after the main experiment to
bring their winnings up to $1.00 per
hour.
The results showed that two factors were most important in determining choices: general preferences or
dislikes for risk-taking, and specific
preferences among probabilities. An
example of the first kind of factor is
that subjects strongly preferred low
probabilities of losing large amounts
of money to high probabilities of
losing small amounts of money—they
just didn't like to lose. It also turned
out that on positive expected value
bets, they were more willing to accept
long shots when playing for real
money than when just imagining or
playing for worthless chips. An example of the second kind of factor
is that they consistently preferred
bets involving a 4/8 probability of
winning to all others, and consistently
avoided bets involving a 6/8 probability of winning. These preferences
were reversed for negative expected
value bets.
These results were independent of
the amounts of money involved in
the bets, so long as the condition of
constant expected value was maintained (59). When pairs of bets which
differed from one another in expected
value were used, the choices were a
compromise between maximizing ex-

pected amount of money and betting
at the preferred probabilities (58).
An attempt was made to construct
individual utility curves adequate to
account for the results of several subjects. For this purpose, the utility of
$0.30 was defined as 30 utiles, and it
was assumed that subjects cannot
discriminate utility differences smaller than half a utile. Under these assumptions, no individual utility curves
consistent with the data could be
drawn. Various minor experiments
showed that these results were reliable and not due to various possible
artifacts (59). No attempt was made
to generate a mathematical model of
probability preferences.
The existence of probability preferences means that the simple von
Neumann-Morgenstern method of
utility measurement cannot succeed.
Choices between bets will be determined not only by the amounts of
money involved, but also by the
preferences the subjects have among
the probabilities involved. Only an
experimental procedure which holds
one of these variables constant, or
otherwise allows for it, can hope to
measure the other. Thus my experiments cannot be regarded as a way
of measuring probability preferences;
they show only that such preferences
exist.
It may nevertheless be possible to
get an interval scale of the utility of
money from gambling experiments by
designing an experiment which measures utility and probability preferences simultaneously. Such experiments are likely to be complicated
and difficult to run, but they can be
designed.
Subjective probability.
First, a
clarification of terms is necessary.
The phrase subjective probability has
been used in two ways: as a name
for a school of thought about the

<-----Page 17----->THEORY OF DECISION MAKING

logical basis of mathematical probability (51, 52, 80) and as a name for
a transformation on the scale of
mathematical probabilities which is
somehow related to behavior. Only
the latter usage is intended here. The
clearest distinction between these
two notions arises from consideration of what happens when an objective probability can be denned (e.g.,
in a game of craps). If the subjective
probability is assumed to be different
from the objective probability, then
the concept is being used in its second, or psychological, sense. Other
terms with the same meaning have
also been used: personal probability,
psychological probability, expectation (a poor term because of the
danger of confusion with expected
value).
(For a more elaborate
treatment of concepts in this area,
see 192.)
In 1948, prior to the Mosteller and
Nogee experiment, Preston and
Baratta (149) used essentially similar
logic and a somewhat similar experiment to measure subjective probabilities instead of subjective values.
They required subjects to bid competitively for the privilege of taking
a bet. All bids were in play money,
and the data consisted of the winning
bids. If each winning bid can be considered to represent a value of play
money such that the winning bidder
is indifferent between it and the bet
he is bidding for, and if it is further
assumed that utilities are identical
with the money value of the play
money and that all players have the
same subjective probabilities, then
these data can be used to construct a
subjective probability scale. Preston
and Baratta constructed such a
scale. The subjects, according to the
scale, overestimate low probabilities
and underestimate high ones, with an
indifference point (where subjective

397

equals objective probability) at about
0.2. Griffith (81) found somewhat
similar results in an analysis of
parimutuel betting at race tracks, as
did Attneave (17) in a guessing game,
and Sprowls (178) in an analysis of
various lotteries. The Mosteller and
Nogee data (142) can, of course, be
analyzed for subjective probabilities
instead of subjective values. Mosteller and Nogee performed such an
analysis and said that their results
were in general agreement with
Preston and Baratta's. However,
Mosteller and Nogee found no indifference point for their Harvard
students, whereas the National
Guardsmen had an indifference point
at about 0.5. They are not able to
reconcile these differences in results.
The notion of subjective probability has some serious logical difficulties.
The scale of objective probability is
bounded by 0 and 1. Should a subjective probability scale be similarly
bounded, or not? If not, then many
different subjective probabilities will
correspond to the objective probabilities 0 and 1 (unless some transformation is used so that 0 and 1 objective probabilities correspond to
infinite subjective probabilities, which
seems unlikely). Considerations of
the addition theorem to be discussed
in a moment have occasionally led
people to think of a subjective
probability scale bounded at 0 but
not at 1. This is surely arbitrary.
The concept of absolute certainty is
neither more nor less indeterminate
than is the concept of absolute impossibility.
Even more drastic logical problems
arise in connection with the addition
theorem. If the objective probability
of event A is P, and that of A not
occurring is Q, then P+Q=1. Should
this rule hold for subjective probabilities? Intuitively it seems neces-

<-----Page 18----->398

WARD EDWARDS

sary that if we know the subjective
probability of A, we ought to be able
to figure out the subjective probability of not-^4, and the only reasonable rule for figuring it out is subtraction of the subjective probability
of A from that of complete certainty.
But the acceptance of this addition
theorem for subjective probabilities
plus the idea of bounded subjective
probabilities means that the subjective probability scale must be identical with the objective probability
scale. Only for a subjective probability scale identical with the objective probability scale will the
subjective probabilities of a collection of events, one of which must
happen, add up to 1. In the special
case where only two events, A and
not-A, are considered, a subjective
probability scale like SI or S2 in
Fig. 3 would meet the requirements
of additivity, and this fact has led to
some speculation about such scales,
particularly about 51. But such
scales do not meet the additivity requirements when more than two
events are considered.
One way of avoiding these diffiH
_J

52.

CO
<

CO

o
OL
Q.

Si
0.5

L.J

O

Ld
CO

D
CO
0

0.5

OBJECTIVE PROBABILITY
FIG. 3. HYPOTHETICAL SUBJECTIVE PROBABILITY CURVES

I

culties is to stop thinking about a
scale of subjective probabilities and,
instead, to think of a weighting
function applied to the scale of objective probabilities which weights these
objective probabilities according to
their ability to control behavior. Presumably, I was studying this ability
in my experiments on probability
preferences (55, 57, 58, 59). There is
no reason why such weighted probabilities should add up to 1 or should
obey any other simple combinatory
principle.
Views and experiments which combine utility and subjective probability.
The philosopher Ramsey published
in 1926 (reprinted in 150) an essay
on the subjective foundations of the
theory of probability; this contained
an axiom system in which both utility
and subjective probability appeared.
He used 0.5 subjective probability as
a reference point from which to determine utilities, and then used these
utilities to determine other subjective probabilities.
Apparently,
economists did not discover Ramsey's
essay until after von Neumann and
Morgenstern's book aroused interest
in the subject. The only other formal
axiom system in which both utility
and subjective probability play a
part is one proposed by Savage
(171), which is concerned with uncertainty, rather than risk, and uses
the concept of subjective probability
in its theory-of-probability sense.
The most extensive and important
experimental work in the whole field
of decision making under risk and
uncertainty is now being carried out
by Coombs and his associates at the
University of Michigan. Coombs's
thinking about utility and subjective
probability is an outgrowth of his
thinking about psychological scaling
in general. (For a discussion of his
views, see 43, 44, 45, 46, 47.) The

<-----Page 19----->THEORY OF DECISION MAKING

essence of his work is the attempt to
measure both utility and subjective
probability on an ordered metric
scale. An ordered metric scale has all
the properties of an ordinal scale,
and, in addition, the distances between some or all of the stimuli can.
be rank ordered. Coombs has developed various experimental procedures for obtaining such information about the spacings of stimuli.
In the most important article on
utility and subjective probability to
come out of the Coombs approach,
Coombs and Beardslee (48) present
an analysis of gambling decisions involving three independent variables:
utility for prize, utility for stake, and
subjective probability. All three are
assumed measurable only up to an
ordered metric, although it is assumed that the psychological probability of losing the stake is one minus
the psychological probability of
winning the prize, an assumption that
limits the permissible underlying
psychological probability functions
to shapes like those in Fig. 3. An
elaborate graphic analysis of the indifference surfaces in this threedimensional space is given, containing far too many interesting relationships to summarize here. An experiment based on this model was designed. Coombs is reluctant to use
sums of money as the valuable objects in his experiments because of
the danger that subjects will respond
to the numerical value of the amount
of dollars rather than to the psychological value. Therefore he used
various desirable objects (e.g., a
radio) as stimuli, and measured their
utility by the techniques he has developed to obtain ordered metric
scales. He used simple numerical
statements of probability as the
probability stimuli, and assumed that
subjective probability was equal to

399

objective probability. The subject
from whose judgments the ordered
metric utility measurement was constructed was then presented with
imaginary bets involving these objects and probabilities, and it turned
out that she almost always chose the
one with the higher expected utility.
This experiment is significant only
as an illustration of the application
of the method; the conclusion that
subjects attempt to maximize expected utility cannot very comfortably be generalized to other subjects
and to real choices without better
evidence.
Coombs and Milholland (49) did a
much more elaborate experiment in
which they established ordered metric
scales, both for the utilities of a collection of objects and for the subjective probabilities of a collection of
statements (e.g., Robin Roberts will
win 20 games next year). Statements
and objects were combined into
"bets," and the two subjects for
whom the ordered metric scales had
been established were asked to make
judgments about which bet they
would most, and which they would
least, prefer from among various
triads of bets. These judgments were
examined to discover whether or not
they demonstrated the existence of
at least one convex indifference curve
between utility and subjective probability (the requirements for demonstrating the convexity of an indifference curve by means of ordered
metric judgments are fairly easy to
state). A number of cases consistent
with a convex indifference curve were
found, but a retest of the ordered
metric data revealed changes which
eliminated all of the cases consistent
with a convex indifference curve for
one subject, and all but one case for
the other. It is not possible to make
a statistical test of whether or not

<-----Page 20----->400

WARD EDWARDS

that one case might have come about
by chance. No evidence was found
for the existence of concave indifference curves, which are certainly inconsistent with the theory of risky
decisions. This experiment is a fine
example of the strength and weakness of the Coombs approach. It
makes almost no assumptions, takes
very little for granted, and avoids
the concept of error of judgment; as
a result, much of the potential information in the data is unused and
rarely can any strong conclusions be
drawn.
A most disturbing possibility is
raised by experiments by Marks (133)
and Irwin (94) which suggest that the
shape of the subjective probability
function is influenced by the utilities
involved in the bets. If utilities and
subjective probabilities are not independent, then there is no hope of predicting risky decisions unless their
law of combination is known, and it
seems very difficult to design an experiment to discover that law of combination. However, the main differences that Marks and Irwin found
were between probabilities attached
to desirable and undesirable alternatives. It is perfectly possible that
there is one subjective probability
function for bets with positive expected values and a different one for
bets with negative expected values,
just as the negative branch of the
Markowitz utility function is likely
to be different from the positive
branch. The results of my probability preference experiments showed
very great differences between the
probability preference patterns for
positive and for negative expected value bets (57), but little difference
between probability preferences at
different expected-value levels so
long as zero expected value was not
crossed (59). This evidence supports

the idea that perhaps only two subjective probability functions are necessary.
Santa Monica Seminar. In the
summer of 1952 at Santa Monica,
California, a group of scientists conferred on problems of decision making. They met in a two-month seminar sponsored by the University of
Michigan and the Office of Naval
Research. The dittoed reports of
these meetings are a gold mine of
ideas for the student of this problem.
Some of the work done at this seminar is now being prepared for a book
on Decision Processes edited by R. M.
Thrall, C. H. Coombs, and R. L.
Davis, of the University of Michigan.
Several minor exploratory experiments were done at this seminar.
Vail (190) did an experiment in which
he gave four children the choice of
which side of various bets they
wanted to be on. On the assumption
of linear utilities, he was able to compute subjective probabilities for these
children. The same children, however, were used as subjects for a
number of other experiments; so,
when Vail later tried them out on
some other bets, he found that they
consistently chose the bet with the
highest probability of winning, regardless of the amounts of money involved. When 50-50 bets were involved, one subject consistently chose
the bet with the lowest expected
value. No generalizable conclusions
can be drawn from these experiments.
Kaplan and Radner (100) tried out
a questionnaire somewhat like
Coombs's method of measuring subjective probability. Subjects were
asked to assign numbers to various
statements. The numbers could be
anything from 0 to 100 and were to
represent the likelihood that the
statement was true. The hypotheses
to be tested were: (a) for sets of ex-

<-----Page 21----->THEORY OF DECISION

haustive and mutually exclusive
statements in which the numbers assigned (estimates of degree of belief)
were nearly equal, the sums of these
numbers over a set would increase
with the number of alternatives (because low probabilities would be overestimated) ; (b) for sets with the same
numbers of alternatives, those with
one high number assigned would have
a lower set sum than those with no
high numbers. The first prediction
was verified; the second was not.
Any judgments of this sort are so
much more likely to be made on the
basis of number preferences and
similar variables than on subjective
probabilities that they offer very
little hope as a method of measuring
subjective probabilities.
Variance preferences. Allais (2, 3,
4) and Georgescu-Roegen (78) have
argued that it is not enough to apply
a transform on objective value and on
objective probability in order to predict risky decisions from expected
utility (see also 188); it is also necessary to take into account at least the
variance, and possibly the higher
moments, of the utility distribution.
There are instances in which this
argument seems convincing. You
would probably prefer the certainty
of a million dollars to a 50-50 chance
of getting either four million or nothing. I do not think that this preference is due to the fact that the expected utility of the 50-50 bet is less
than the utility of one million dollars
to you, although this is possible. A
more likely explanation is simply
that the variances of the two propositions are different. Evidence in
favor of this is the fact that if you
knew you would be offered this choice
20 times in succession, you would
probably take the 50-50 bet each
time. Allais (5) has constructed a
number of more sophisticated exam-

MAKING

401

ples of this type. However, from a
simple-minded psychological point of
view, these examples are irrelevant.
It is enough if the theory of choice
can predict choices involving familiar
amounts of money and familiar
probability differences—choices such
as those which people are accustomed
to making. It may be necessary for
economic theory that the theory of
choice be universal and exceptionless,
but experimental psychologists need
not be so ambitious. This is fortunate, because the introduction of the
variance and higher moments of the
utility distribution makes the problem of applying the theory experimentally seem totally insoluble. It is
difficult enough to derive reasonable
methods of measuring utility alone
from risky choices; when it also becomes necessary to measure subjective probability and to take the
higher moments of the utility distribution into account, the problem
seems hopeless. Allais apparently
hopes to defeat this problem by using
psychophysical methods to measure
utility (and presumably subjective
probability also). This is essentially
what Coombs has done, but Coombs
has recognized that such procedures
are unlikely to yield satisfactory
interval scales. The dollar scale of
the value of money is so thoroughly
taught to us that it seems almost impossible to devise a psychophysical
situation in which subjects would
judge the utility, rather than the dollar value, of dollars. They might
judge the utility of other valuable
objects, but since dollars are the
usual measure of value, such judgments would be less useful, and even
these judgments would be likely to be
contaminated by the dollar values of
the objects. I would get more utility
from a new electric shaver than I
would from a new washing machine,

<-----Page 22----->402

WARD EDWARDS

but because of my knowledge of the
relative money values of these objects, I would certainly choose the
washing machine if given a choice
between them. Somewhat similar
arguments can be applied against
using psychophysical methods to
measure subjective probability. A
final point is that, since these subjective scales are to be used to predict
choices, it would be best if they could
be derived from similar choices.
Other approaches. Shackle (175)
has proposed a theory of decision
making under risk and uncertainty.
This theory is unique in that it does
not assume any kind of maximizing
behavior. For every possible outcome of a decision made in a risky or
uncertain situation, Shackle assumes
that there is a degree of potential
surprise that this, rather than some
other, outcome would occur. Every
outcome-potential surprise pair is
ranked in accordance with its ability
to stimulate the mind (stimulation increases with increasing outcome and
decreases with increasing potential
surprise). The highest-ranking positive outcome-potential surprise pair
and the highest-ranking negative pair
are found, and these two possibilities
alone determine what the individual
will do. Semi-mathematical methods
are used to predict the outcome of
consideration of possible lines of action. Although attempts have been
made to relate it to Wald's minimax
principle for statistical decision functions (see below), the fact remains
that most critics of the Shackle point
of view have judged it to be either too
vague to be useful, or, if specified in
detail, too conducive to patently absurd predictiona (e.g., 201).
Shackle's point of view was developed primarily to deal with unique
choices—choices which can be made
only once. Allais (3) has similarly

criticized conventional utility theory's attack on this problem. Since
the usual frequency theory of probability conceives of the probability as
the limit of the outcomes of a large
number of similar trials, it is questionable that notions which use probability in the ordinary sense (like the
notion of maximizing expected utility) are applicable to unique choices.
However, this seems to be an experimental problem. If notions which use
ordinary probability are incapable of
predicting actual unique choices,
then it will be necessary to seek other
theoretical tools. But so long as a
generally acceptable probability can
be defined (e.g., as in the unique toss
of a coin), it is not necessary to assume a priori that theories based on
conventional probabilities will be inadequate. When no generally acceptable probability can be defined,
then the problem becomes very different.
Cartwright and Festinger (38, 41)
have proposed a theory about the
time it takes to make decisions which
is in some ways similar to those discussed in this section. The main difference is that they add the concept
of restraining forces, and that they
conceive of all subjective magnitudes
as fluctuating randomly around a
mean value. From this they deduce
various propositions about decision
times and the degree of certainty
which subjects will feel about their
decisions, and apparently these propositions work out experimentally
pretty well (38, 3Q, 61, 62). The
Lewinian theoretical orientation
seems to lead to this kind of model;
Lewin, Dembo, Festinger, and Sears
(122) present a formally similar
theory about level of aspiration. Of
course, the notion of utility is very
similar to the Lewinian notion of
valence.

<-----Page 23----->THEORY OF DECISION

Landahl (115) has presented a
mathematical model for risk-taking
behavior based on the conceptual
neurology of the mathematical biophysics school.
Psychological comments. The area
of risky decision making is full of
fascinating experimental problems.
Of these, the development of a satisfactory scale of utility of money and
of subjective probability must come
first, since the theory of risky decision making is based on these notions. The criterion for satisfactoriness of these scales must be that they
successfully predict choices other
than those from which they were derived. To be really satisfactory, it is
desirable that they should predict
choices in a wide variety of differing
situations.
Unlike the subjective
scales usually found in psychophysics, it is likely that these scales will
differ widely from person to person,
so a new determination of each scale
must be made for each new subject.
It can only be hoped that the scales
do not change in time to any serious
degree; if they do, then they are
useless.
Once scales of utility and subjective probability are available, then
many interesting questions arise.
What about the addition theorem for
subjective probabilities? Does gambling itself have utility, and how
much? To what extent can these subjective scales be changed by learning?
To what degree do people differ, and
can these differences be correlated
with environmental, historical, or
personality differences? Finally, psychologists might be able to shed light
on the complex economic problem of
interacting utilities of different goods.
The area of risky decision making,
like the area of the theory of games,
tends to encourage in those interested in it the custom of carrying out

MAKING

403

small pilot experiments on their sons,
laboratory assistants, or secretaries.
Such experiments are too seldom
adequately controlled, and are almost never used as a basis for largerscale, well-designed experiments.
Whether an ill-designed and haphazardly executed little experiment is
better than no experiment at all is
questionable. The results of such
pilot experiments too often are picked
up and written into the literature
without adequate warning about the
conditions under which they were
performed and the consequent limitations on the significance of the results.
THE TRANSITIVITY OF CHOICES
In the section on riskless choices
this paper presented a definition of
economic man. The most important
part of this definition can be summed
up by saying that economic man is
rational. The concept of rationality
involves two parts: that of a weak
ordering of preferences, and that of
choosing so as to maximize something. Of these concepts, the one
which seems most dubious is the one
of a weakly ordered preference field.
This is dubious because it implies
that choices are transitive; that is, if
A is preferred to B, and B is preferred
to C, then A is preferred to C.
Two economists have designed experiments specifically intended to
test the transitivity of choices. Papandreou performed an elaborate and
splendidly controlled experiment
(145) designed to discover whether or
not intransitivities occurred in imagined-choice situations.
He prepared triplets of hypothetical bundles of admissions to plays, athletic
contests, concerts, etc., and required
his subjects to choose between pairs
of bundles. Each bundle consisted of
a total of four admissions to two
events, e.g., 3 plays and 1 tennis

<-----Page 24----->404

WARD EDWARDS

tournament. In the main experiment, each bundle is compared with
two others involving the same kinds
of events, but in the better designed
auxiliary experiment, a total of six
different events are used, so that each
bundle has no events in common with
the other two bundles in its triplet.
Since there are three bundles in each
triplet, there are three choices between pairs for each triplet, and
these choices may, or may not, be
transitive. The subjects were permitted to say that they were indifferent between two bundles; consequently there were 27 possible configurations of choices, of which only
13 satisfied the transitivity axiom.
In the main experiment, 5 per cent
of the triplets of judgments were
intransitive; in the auxiliary experiment, only 4 per cent. Papandreou
develops a stochastic model for
choices under such conditions; the
results are certaihly consistent with
the amount of intransitivity permitted by his model. Papandreou
concludes that at least for his specific
experimental conditions, transitivity
does exist.
May (138), using different kinds of
stimuli in a less elaborate experiment,
comes up with results less consistent
with transitivity. May required a
classroom group to make pairwise
choices between three marriage partners who were identified only by
saying how intelligent, good looking,
and rich they were. Judgments of
indifference were not permitted. The
results were that 27 per cent of the
subjects gave intransitive triads of
choices. May suggests, very plausibly, that intransitive choices may be
expected to occur whenever more
than one dimension exists in the
stimuli along which subjects may
order their preferences. However,
May would probably have gotten

fewer intransitivities if he had permitted the indifference judgment. If
subjects are really indifferent among
all three of the elements of a triad of
objects, but are required to choose
between them in pairs and do so by
chance, then they will choose intransitively one-fourth of the time.
Papandreou's stochastic model gives
one theory about what happens
when preferences diverge just slightly
from indifference, but presumably a
more detailed model can be worked
out. Papandreou's model permits
only three states: prefer A to B,
prefer B to A, and indifferent. It
ought to be possible to base a model
for such situations on the cumulative
normal curve, and thus to permit any
degree of preference. For every combination of degrees of preference,
such a model would predict the frequency of intransitive choices.
In the paired comparisons among
bets (57) described in the section on
risky choices, quite elaborate intransitivities could and did occur.
However, it is easy to show that any
intransitivity involving four or more
objects in a paired comparisons
judgment situation will necessarily
produce at least one intransitivity involving three objects. Consequently,
the intransitive triplet or circular
triad is the best unit of analysis for
intransitivities in these more complicated judgment situations.
I
counted the frequency of occurrence
of circular triads and found that they
regularly occurred about 20 per cent
of the total number of times they
could occur. (Of course, no indifference judgments could be permitted.)
The experiment fulfills May's criterion for the occurrence of intransitivities, since both probability and
amount of money were present in
each bet, and subjects could be expected to take both into account

<-----Page 25----->THEORY OF DECISION MAKING

when making choices. It might be
supposed that the difference between
the imaginary choices of the Papandreou and May experiments and the
real choices in my experiment would
lead to differences in the frequency of
occurrence of intransitivities, but
there were no substantial differences
in my experiment between the frequencies of occurrence in the justimagining sessions and in the real
gambling sessions, and what differences there were, were in the direction
of greater transitivity when really
gambling. These facts should facilitate further experiments on this problem.
In one sense, transitivity can never
be violated. A minimum of three
choices is required to demonstrate
intransitivity. Since these choices
will necessarily be made in sequence,
it can always be argued that the person may have changed his tastes between the first choice and the third.
However, unless the assumption of
constancy of tastes over the period of
experimentation is made, no experiments on choice can ever be meaningful, and the whole theory of choice
becomes empty (see 184 for a similar
situation). So this quibble can be rejected at once.
Utility maximization will not work
except with a transitive preference
field. Consequently, if the models
discussed in this paper are to predict
experimental data, it is necessary
that intransitivities in these data be
infrequent enough to be considered
as errors. However, from a slightly
different point of view (54) the occurrence or nonoccurrence of transitive
choice patterns is an experimental
phenomenon, and presumably a lawful one. May has suggested what
that law is: Intransitivities occur
when there are conflicting stimulus
dimensions along which to judge.

405

This notion could certainly be tested
and made more specific by appropriate experiments.
A final contribution in a related,
but different, area is Vail's stochastic
utility model (191). Vail assumes
that choices are dependent on utilities that oscillate in a random manner around a mean value. From this
assumption plus a few other reasonable ones, he deduces that if the
over-all preference is 1>2>3, and if
1 is preferred to 2 more than 2 is
preferred to 3, then the frequencies of
occurrence of the six possible transitive orderings should be ordered as
follows: 123>132>213>312>231
>321. This result is certainly easy
to test experimentally, and sounds
plausible.
THE THEORY OF GAMES AND OF
DECISION FUNCTIONS'
This section will not go into the
theory of games or into the intimately
related subject of statistical decision
functions at all thoroughly. These
are mathematical subjects of a highly
6
Marschak (134), Hurwicz (92), Neisser
(143), Stone (181), and Kaysen (107) published reviews of The Theory of Games and
Economic Behavior which present the fundamental ideas in much simpler language than
the original source. Marschak works out in
detail the possible solutions of a complicated
three-person bargaining game, and thereby
illustrates the general nature of a solution. The
two volumes of Contributions to the Theory of
Games (112, 113), plus McKinsey's book on
the subject (129), provide an excellent bibliography of the mathematical literature. McKinsey's book is an exposition of the fundamental
concepts, intended as a textbook, which is
simpler than von Neumann and Morgenstern
and pursues certain topics further. Wald's
book (198) is, of course, the classical work on
statistical decision functions. Dross's book
(35) presents the fundamental ideas about
statistical decision functions more simply, and
with a somewhat different emphasis. Girshick
and Blackwell's book (79) is expected to be a
very useful presentation of the field.

<-----Page 26----->406

WARD EDWARDS

technical sort, with few statements
which lend themselves to experimental test. Rather, the purpose of this
section is to show how these subjects
relate to what has gone before, to give
a brief summary of the contents of
Theory of Games and Economic Behavior by von Neumann and Morgenstern (197), and to describe a few experiments in the area of game playing
—experiments which are stimulated
by the theory of games although not
directly relevant to it.
The theory of games. The theory of
games probably originated in the
work of Borel (31, 32, 33, 34; see also
71, 72) in the 1920's. In 1928, von
Neumann (195), working independently of Borel, published the first
proof of the fundamental theorem in
the theory, a theorem that Borel had
not believed to be generally true.
However, the subject did not become
important until 1944, when von
Neumann and Morgenstern published their epoch-making book (196).
(A second edition, with an appendix
on cardinal utility measurement,
came out in 1947 [197].) Their purpose was to analyze mathematically a
very general class of problems, which
might be called problems of strategy.
Consider a game of tic-tac-toe. You
know at any moment in the game
what the moves available to your opponent are, but you do not know
which one he will choose. The only
information you have is that his
choice will not, in general, be completely random; he will make a move
which is designed in some way to increase his chance of winning and diminish yours. Thus the situation is
one of uncertainty rather than risk.
Your goals are similar to your opponent's. Your problem is: what
strategy should you adopt? The
theory of games offers no practical
help in developing strategies, but it
does offer rules about how to choose

among them. In the case of tic-tactoe, these rules are trivial, since
either player can force a draw. But
in more complicated games of strategy, these rules may be useful. In
particular, the theory of games may
be helpful in analyzing proper strategy in games having random elements, like the shuffling of cards, or
the throwing of dice. It should be
noted that the concept of a game is an
exceedingly general concept. A scientist in his laboratory may be considered to be playing a game against
Nature. (Note, however, that we
cannot expect Nature to try to defeat
the scientist.) Negotiators in a labor
dispute are playing a game against
one another. Any situation in which
money (or some valuable equivalent)
may be gained as the result of a
proper choice of strategy can be considered as a game.
To talk about game theory, a few
technical terms are necessary. A
strategy is a set of personal rules for
playing the game. For each possible
first move on your part, your opponent will have a possible set of responses. For each possible response
by your opponent, you will have a set
of responses, and so on through the
game. A strategy is a list which specifies what your move will be for every
conceivable previous set of moves of
the particular game you are playing.
Needless to say, only for the simplest
games (e.g., matching pennies) does
this concept of strategy have any
empirical meaning.
Associated with strategies are imputations. An imputation is a set of
payments made as a result of a game,
one to each player. In general, different imputations will be associated
with different sets of strategies, but
for any given set of strategies there
may be more than one imputation
(in games involving coalitions).
Imputation X is said to dominate

<-----Page 27----->THEORY OF DECISION MAKING

imputation F if one or more of the
players has separately greater gains
(or smaller losses) in X than in F and
can, by acting together (in the case of
more than*one player), enforce the
occurrence of X, or of some other imputation at least as good. The relationship of domination is not transitive.
A solution is a set of imputations,
none of which dominates another,
such that every imputation outside
the solution is dominated by at least
one imputation within the solution.
Von Neumann and Morgenstern assert that the task of the theory of
games is to find solutions. For any
game, there may be one or more than
one. One bad feature of the theory of
games is that it frequently gives a
large, or even infinite, number of solutions for a game.
The above definitions make clear
that the only determiner of behavior
in games, according to this theory, is
the amounts of money which may be
won or lost, or the expected amounts
in games with random elements. The
fun of playing, if any, is irrelevant.
The minimax loss principle. The
notions of domination and of solution
imply a new fundamental rule for
decision making—a rule sharply different from the rule of maximizing
utility or expected utility with which
this paper has been concerned up to
this section. This rule is the rule of
minimizing the maximum loss, or,
more briefly, minimax loss. In other
words, the rule is to consider, for each
possible strategy that you could
adopt, what the worst possible outcome is, and then to select that strategy which would have the least illeffects if the worst possible outcome
happened. Another way of putting
the same idea is to call it the principle
of maximizing the minimum gain, or
maximin gain. This rule makes considerable sense in two-person games

407

when you consider that the other
player is out to get you, and so will
do his best to make the worst possible
outcome for you occur. If this rule is
expressed geometrically, it asserts
that the point you should seek is a
saddle-point, like the highest point in
a mountain pass (the best rule for
crossing mountains is to minimize the
maximum height, so explorers seek
out such saddle-points).
Before we go any further, we need
a few more definitions. Games may
be among any number of players, but
the simplest game is a two-person
game, and it is this kind of game
which has been most extensively and
most successfully analyzed. Fundamentally, two kinds of payoff arrangements are possible. The simplest and most common is the one in
which one player wins what the other
player loses, or, more generally, the
one for which the sum of all the payments made as a result of the game is
zero. This is called a zero-sum game.
In nonzero-sum games, analytical
complexities arise. These can be diminished by assuming the existence
of a fictitious extra player, who wins
or loses enough to bring the sum of
payments back to zero. Such a fictitious player cannot be assumed to
have a strategy and cannot, of course,
interact with any of the other players.
In zero-sum two-person games,
what will happen? Each player, according to the theory, should pick his
minimax strategy. But will this result in a stable solution? Not always.
Sometimes the surface representing
the possible outcomes of the game
does not have a saddle-point. In this
case, if player A chooses his minimax
strategy, then player B will have an
incentive not to use his own minimax
strategy, because having found out
his opponent's strategy, he can gain
more by some other strategy. Thus
the game has no solution.

<-----Page 28----->408

WARD EDWARDS

Various resolutions of this problem
are possible. Von Neumann and
Morgenstern chose to introduce the
notion of a mixed strategy, which is a
probability distribution of two or
more pure strategies. The fundamental theorem of the theory of games is
that if both players in a zero-sum
two-person game adopt mixed strategies which minimize the maximum
expected loss, then the game will always have a saddle-point. Thus each
person will get, in the long run, his
expected loss, and will have no incentive to change his behavior even
if he should discover what his opponent's mixed strategy is. Since A is
already getting the minimum possible
under the strategy he chose, any
change in strategy by B will only increase A's payoff, and therefore cause
B to gain less or lose more than he
would by his own minimax strategy.
The same is true of B.
Games involving more than two
people introduce a new element—the
possibility that two or more players
will cooperate to beat the rest. Such
a cooperative agreement is called a
coalition, and it frequently involves
side-payments among members of the
coalition. The method of analysis for
three-or-more-person games is to consider all possible coalitions and to
solve the game for each coalition on
the principles of a two-person game.
This works fairly well for three-person games, but gets more complicated
and less satisfactory for still more
people.
This is the end of this exposition of
the content of von Neumann and
Morgenstern's book. It is of course
impossible to condense a tremendous
and difficult book into one page. The
major points to be emphasized are
these: the theory of games is not a
model of how people actually play
games (some game theorists will dis-

agree with this), nor is it likely to be
of any practical use in telling you
how to play a complicated game; the
crux of the theory of games is the
principle of choosing tfre strategy
which minimizes the maximum expected financial loss; and the theory
defines a solution of a game as a set
of imputations which satisfies this
principle for all players.
Assumptions, In their book von
Neumann and Morgenstern say "We
have . . . assumed that [utility] is
numerical . . . substitutable and unrestrictedly transferable between the
various players." (197, p. 604.) Game
theorists disagree about what this
and other similar sentences mean.
One likely interpretation is that they
assume utility to be linear with the
physical value of money involved in
a game and to be interpersonally
comparable. The linear utility curves
seem to be necessary for solving twoperson games; the interpersonal comparability is used for the extension to
n persons. Attempts are being made
to develop solutions free of these assumptions (176).
Statistical decision functions. Von
Neumann (195) first used the minimax principle in his first publication
on game theory in 1928. Neyman
and Pearson mentioned its applicability to statistical decision problems in 1933 (144). Wald (198), who
prior to his recent death was the
central figure in the statistical decision-function literature, first seriously
applied the minimax principle to statistical problems in 1939. Apparently, all these uses of the principle
were completely independent of one
another.
After Theory of Games and Economic Behavior appeared in 1944,
Wald (198) reformulated the problem
of statistical decision making as one
of playing a game against Nature,

<-----Page 29----->THEORY OF DECISION

The statistician must decide, on the
basis of observations which cost
something to make, between policies,
each of which has a possible gain or
loss. In some cases, all of these gains
and losses and the cost of observing
can be exactly calculated, as in industrial quality control. In other
cases, as in theoretical research, it is
necessary to make some assumption
about the cost of being wrong and the
gain of being right. At any rate, when
they are put in this form, it is obvious
that the ingredients of the problem of
statistical decision making have a
gamelike sound. Wald applied the
minimax principle to them in a way
essentially identical with game theory.
A very frequent criticism of the
minimax approach to games against
Nature is that Nature is not hostile,
as is the opponent in a two-person
game. Nature will not, in general,
use a minimax strategy. For this
reason, other principles of decision
making have been suggested. The
simple principle of maximizing expected utility (which is the essence of
the Bayes's theorem [15, 198] solution
of the problem) is not always applicable because, even though Nature is
not hostile, she does not offer any
way of assigning a probability to each
possible outcome. In other words,
statistical decision making is a problem of uncertainty, rather than of
risk. Savage has suggested the principle of minimaxing regret, where regret is defined as the difference between the maximum which can be
gained under any strategy given a
certain state of the world and the
amount gained under the strategy
adopted. Savage believes (170, also
personal communication) that neither
von Neumann and Morgenstern nor
Wald actually intended to propose
the principle of minimaxing loss; they

MAKING

409

confined their discussions to cases in
which the concepts of minimax loss
and minimax regret amount to the
same thing. Other suggested principles are: maximizing the maximum
expected gain, and maximizing some
weighted average of the maximum
and minimum expected gains (93).
None of these principles commands
general acceptance; each can be
made to show peculiar consequences
under some conditions (see 170).
Experimental games. The concepts
of the theory of games suggest a new
field of experimentation: How do
people behave in game situations?
Such experimentation would center
on the development of strategies, particularly mixed strategies, and, in
three-or-more-person games, on the
development of coalitions and on the
bargaining process. You should* remember that the theory of games
does not offer a mathematical model
predicting the outcomes of such
games (except in a few special cases);
all it does is offer useful concepts and
language for talking about them, and
predict that certain outcomes will
not occur.
A few minor experiments of this
kind have been conducted by Flood,
a mathematician, while he was at
Rand Corporation. He usually used
colleagues, many of whom were experts in game theory, and secretaries
as subjects. The general design of
his experiments was that a group of
subjects were shown a group of desirable objects on a table, and told
that they, as a group, could have the
first object they removed from the
table, and that they should decide
among themselves which object to
choose and how to allocate it. In the
first experiment (64) the allocation
problem did not arise because enough
duplicate objects were provided so
that each subject could have one of

<-----Page 30----->410

WARD EDWARDS

the kind of object the group selected.
The subjects were Harvard undergraduates, and the final selection was
made by negotiation and voting. In
the second experiment (65), in which
the subjects were colleagues and secretaries, a long negotiation process
eliminated some of the objects, but a
time limit forced a selection by lot
from among the rest. Further negotiations to solve the allocation problem
were terminated by a secretary, who
snatched the object, announced that
it was hers, and then tried to sell it.
No one was willing to buy, so the experiment terminated. Other experiments (66, 67) showed that coalitions
sometimes form, that a sophisticated
subject could blackmail the group for
an extra side-payment by threatening
to change his vote, and that the
larcenous secretary, having succeeded
once, had to be physically restrained
in subsequent sessions to prevent
more larceny. The general conclusion
suggested by all these experiments is
that even experts on game theory are
less rational and more conventional
than game theory might lead experimenters to expect.
Psychological comments. The most
nutritive research problems in this
area seem to be the social problems of
how bargaining takes place. Flood's
experiments left bargainers free and
used physical objects, whose utilities
probably vary widely from subject to
subject, as stimuli to bargain over.
This is naturalistic, but produces
data too complex and too nonnumerical for easy analysis. A simpler situation in which the possible communications from one bargainer to another are limited (perhaps by means
of an artificial vocabulary), in which
the subjects do not see one another,
and in which the object bargained
over is simple, preferably being
merely a sum of money, would be

better. Physical isolation of one subject from another would make it possible to match each subject against a
standard bargainer, the experimenter
or a stooge, who bargains by a fixed
set of rules that are unknown to the
subject. Flood (personal communication) is conducting experiments of
this sort. For three-or-more-person
games, Asch's (16) technique of using
a group consisting of only one real
subject and all the rest stooges might
well be used. It would be interesting,
for instance, to see how the probability of a coalition between two players
changes as the number and power of
players united against them increase.
The theory of games is the area
among those described in this paper
in which the uncontrolled and casually planned "pilot experiment" is
most likely to occur. Such experiments are at least as dangerous here
as they are in the area of risky decision making. Flood's results suggest that it is especially important to
use naive subjects and to use them
only once, unless the effects of expertness and experience are the major
concern of the experiment.
SUMMARY
For a long time, economists and
others have been developing mathematical theories about how people
make choices among desirable alternatives. These theories center on the
notion of the subjective value, or
utility, of the alternatives among
which the decider must choose. They
assume that people behave rationally,
that is, that they have transitive
preferences and that they choose in
such a way as to maximize utility or
expected utility.
The traditional theory of riskless
choices, a straightforward theory of
utility maximization, was challenged
by the demonstration that the mathe-

<-----Page 31----->THEORY OF DECISION

matical tool of indifference curves
made it possible to account for riskless choices without assuming that
utility could be measured on an interval scale. The theory of riskless
choices predicted from indifference
curves has been worked out in detail.
Experimental determination of indifference curves is possible, and has
been attempted. But utility measured on an interval scale is necessary
(though not sufficient) for welfare
economics.
Attention was turned to risky
choices by von Neumann and Morgenstern's demonstration that complete weak ordering of risky choices
implies the existence of utility measurable on an interval scale. Hosteller
and Nogee experimentally determined utility curves for money from
gambling decisions, and used them to
predict other gambling decisions.
Edwards demonstrated the existence
of preferences among probabilities in
gambling situations, which complicates the experimental measurement
of utility. Coombs developed a model

MAKING

411

for utility and subjective probability
measured on an ordered metric scale,
and did some experiments to test implications of the model.
Economists have become worried
about the assumption that choices
are transitive. Experiments have
shown that intransitive patterns of
choice do occur, and so stochastic
models have been developed which
permit occasional intransitivities.
The theory of games presents an
elaborate mathematical analysis of
the problem of choosing from among
alternative strategies in games of
strategy. This paper summarizes the
main concepts of this analysis. The
theory of games has stimulated interest in experimental games, and a
few bargaining experiments which
can be thought of in game-theoretical
terms have been performed.
All these topics represent a new
and rich field for psychologists, in
which a theoretical structure has already been elaborately worked out
and in which many experiments need
to be performed.

REFERENCES
6. ALLEN, R. G. D. The nature of indiffer1. ALCHIAN, A, The meaning of utility
ence curves. Rev. econ. Stud., 1933, 1,
measurement. Amer. econ. Rev., 1953,
110-121.
43, 26-50.
7. ALLEN, R. G. D. A note on the determi2. ALLAIS, M. Fondements d'une thebrie
positive des choix comportant un
nateness of the utility function. Rev.
risque et critique des postulats et
econ. Stud., 1934, 2, 155-158.
8. ARMSTRONG, W. E. The determinateness
axiomes de 1'ecole americaine. Colof the utility function. Econ. J., 1939,
logue Internationale du Centre Na49, 453-467.
tional de la Recherche scientifique, 1952,
9. ARMSTRONG, W. E. Uncertainty and the
No. 36.
utility function. Econ. J., 1948, 58,
3. ALLAIS, M. Le comportement de 1'hom1-10.
rae ratlonnel devant le risque: critique
10. ARMSTRONG, W. E. A note on the theory
des postulats et axiomes de 1'ecole
americaine. Econometrica, 1953, 21,
of consumer's behavior. Ox}, econ.
Pap., 1950, 2, 119-122.
503-546.
11. ARMSTRONG, W. E, Utility and the
4. ALLAIS, M. L'Extension des theories de
theory of welfare. Ox}, econ. Pap.,
1'equilibre economique gen6ral et du
1951, 3, 259-271.
rendement social au cas du risque.
12. ARROW, K. J. Alternative approaches to
Econometrica, 1953, 21, 269-290.
the theory of choice in risk-taking
5. ALLAIS, M. La psychologic de 1'homme
situations. Econometrica, 1951, 19,
rationnel devant le risque: La theorie
404-437.
et I'exp6rience. J, soc. Statist,, Paris,
13. ARROW, K. J. An extension of the basic
1953, 94, 47-73.

<-----Page 32----->412

14.
15.

16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.

27.
28.
29.
30.

WARD EDWARDS
theorems of classical welfare economics. In J. Neyman (Ed.), Proceedings
of the second Berkeley symposium on
mathematical statistics and probability.
Berkeley: Univ. of Calif. Press, 1951.
Pp. S07-S32.
ARROW, K. J. Social choice and individual values. New York: Wiley, 1951.
ARROW, K. J., BLACKWELL, p., &
GIRSHICK, M. A. Bayes and minimax
solutions of sequential decision problems. Econometrica, 1949, 17, 213-244.
ASCH, S. E. Social psychology. New
York: Prentice-Hall, 1952.
ATTNEAVE, F. Psychological probability
as a function of experienced frequency.
/. exp. Psychol, 1953, 46, 81-86.
BAUMOL, W. J. Community indifference.
Rev. econ. Stud., 1946, 14, 44-48.
BAUMOL, W. J. The Neumann-Morgenstern utility index—an ordinalist
view. /. polit. Econ., 1951, 59, 61-66.
BAUMOL, W. J. Discussion. Amer. econ.
Rev. Suppl, 1953, 43, 415-416.
BERGSON (BURK), A. Reformulation of
certain aspects of welfare economics.
Quart. J. Econ., 1938, 52, 310-334.
BERNARDELLI, H. Note on the determinateness of the utility function. Rev.
econ. Stud., 1934, 2, 69-75.
BERNARDELLI, H. The end of marginal
utility theory? Economica, 1938, 5,
192-212.
BERNARDELLI, H.
A reply to Mr.
Samuelson's note. Economica, 1939, 6,
88-89.
BERNARDELLI, H. A rehabilitation of the
classical theory of marginal utility.
Economica, 1952, 19, 254-268.
BERNOULLI, D.
Specimen theoriae
novae de mensura sortis. Comentarii
Academiae Scientiarum Imperiales Petropolitanae, 1738, 5, 175-192. (Trans,
by L. Sommer in Econometrica, 1954,
22, 23-36.)
BILODEAU, E. A. Statistical versus Intuitive confidence. Amer. J. Psychol.,
1952, 65, 271-277.
BISHOP, R. L. Consumer's surplus and
cardinal utility. Quart. J. Econ., 1943,
57, 421-449.
BISHOP, R. L. Professor Knight and the
theory of demand. /. polit. Econ.,
1946, 54, 141-169.
BOHNERT, H. G. The logical structure of
the utility concept. In R. M. Thrall,
C. H. Coombs, & R. L. Davis (Eds.),
Decision Processes. New York: Wiley,
in press.

31. BOREL, E. La thforie du jeu et les equations integrates & noyau symfitrique.
C. R. Acad. Set., Paris, 1921, 173,
1304-1308. (Trans, by L. J. Savage in
Econometrica, 1953, 21, 97-100.)
32. BOREL, E. Sur les jeux oil interviennent
1'hasard et I'habilitfi des joueurs. In
E. Borel, Theorie des probability.
Paris: Librairie Scientifique, J. Hermann, 1924. Pp. 204-224. (Trans, by
L. J. Savage in Econometrica, 1953, 21,
101-115.)
33. BOREL, E. Algebre et calcul des probabilitfe. C. R. Acad. Sci., Paris, 1927,
184, 52-53. (Trans, by L. J. Savage in
Econometrica, 1953,21, 116-117.)
34. BOREL, E. Traitt du calcul des probabilites et de ses applications, applications
des jeux de hasard. Vol. IV, No. 2.
Paris: Gauthier-Villars, 1938.
35. BROSS, I. Design for decision. New
York: Macmillan, 1953.
36. BUSH, R. R., & MOSTELLER, F. A
mathematical model for simple learning. Psychol. Rev., 1951, 58, 313-323.
37. BUSH, R. R., & MOSTELLER, F. A model
for stimulus generalization and discrimination. Psychol. Rev., 1951, 58,
413-423.
38. CARTWRIGHT, D. Decision-time in relation to differentiation of the phenomenal field. Psychol. Rev., 1941, 48, 425442.
39. CARTWRIGHT, D. The relation of decision-time to the categories of response. Amer. J. Psychol., 1941, 54,
174-196.
40. CARTWRIGHT, D. Survey research: psychological economics. In J. G. Miller
(Ed.), Experiments in social process.
New York: McGraw-Hill, 1950. Pp.
47-64.
41. CARTWRIGHT, D., & FESTINGER, L. A
quantitative theory of decision. Psychol. Rev., 1943, 50, 595-621.
42. CLARK, J. M. Realism and relevance in
the theory of demand. J. polit. Econ.,
1946, 54, 347-353.
43. COOMBS, C. H. Psychological scaling
without a unit of measurement. Psychol. Rev., 1950, 57, 145-158.
44. COOMBS, C. H. Mathematical models in
psychological scaling.
/. Amer.
statist. Ass., 1951, 46, 480-489.
45. COOMBS, C. H. A theory of psychological scaling. Bull. Engng Res. Inst.
Univer. Mich., 1952, No. 34.
46. COOMBS, C. H. Theory and methods of
social measurement. In L. Festinger &
D. Katz (Eds.), Research methods in

<-----Page 33----->THEORY OF DECISION
the behavioral sciences. New York:
Dryden, 1953. Pp. 471-S3S.
47. COOMBS, C. H. A method for the study
of interstimulus similarity. Psychometrika, in press.

63.

48. COOMBS, C. H., & BEARDSLEE, D. C.

49.

50.

51.
52.

53.
54.
55.

56.

57.
58.

59.
60.
61.

Decision making under uncertainty.
In R. M. Thrall, C. H. Coombs, &
R. L. Davis (Eds.), Decision processes.
New York: Wiley, in press.
COOMBS, C. H., & MILHOLLAND, J. E.
Testing the "rationality" of an individual's decision making under uncertainty. Psychometrika, in press.
CORLETT, W. J., & NEWMAN, P. K. A
note on revealed preference and the
transitivity conditions. Rev. econ.
Stud., 1952, 20, 156-158.
DE FINETTI, B. La prevision: ses lois
logiques, ses sources subjectives. Ann.
Inst. Poincare, 1937, 7, 1-68.
DE FINETTI, B. Recent suggestions for
the reconciliation of theories of probability. In J. Neyman (Ed.), Proceedings of the second Berkeley symposium
on mathematical statistics and probability. Berkeley: Univer. of Calif.
Press, 1951.
EDGEWORTH, F. Y.
Mathematical
psychics. London: Kegan Paul, 1881.
EDWARDS, W. Discussion. Econometrica,
1953, 21, 477. (Abstract)
EDWARDS, W. Experiments on economic
decision-making in gambling situations. Econometrica, 1953, 21, 349350. (Abstract)
EDWARDS, W. Information, repetition,
and reinforcement as determiners of
two-alternative decisions.
Amer.
Psychologist, 1953, 8, 345. (Abstract)
EDWARDS, W. Probability-preferences
in gambling. Amer. J. Psychol., 1953,
66, 349-364.
EDWARDS, W. Probability preferences
among bets with differing expected
values. Amer. J. Psychol., 1954, 67,
56-67.
EDWARDS, W. The reliability of probability preferences. Amer. J. Psychol.,
1954, 67, 68-95.
ESTES, W. K. Toward a statistical
theory of learning. Psychol. Rev.,
1950, 57, 94-107.
FESTINGER, L. Studies in decision: I.
Decision-time, relative frequency of
judgment and subjective confidence
as related to physical stimulus differences. J. exp. Psychol., 1943, 32, 291306.

62. FES,TINGE;R, L. Studies in decision: II.

64.
65.
66.
67.
68.

69.
70.
71.

72.

73.

MAKING

413

An empirical test of a quantitative
theory of decision. J. exp. Psychol.,
1943,32,411^23.
FISHER, I. A statistical method for
measuring "marginal utility" and
testing the justice of a progressive income tax. In J. Hollander (Ed.),
Economic essays contributed in honor of
John Bates Clark. New York: Macmillan, 1927. Pp. 157-193.
FLOOD, M. M. A preference experiment.
Rand Corp. Memo., November 1951,
No. P-256.
FLOOD, M. M. A preference experiment
(Series 2, Trial 1). Rand Corp. Memo.,
December 1951, No. P-258.
FLOOD, M. M. A preference experiment
(Series 2, Trials 2, 3, 4). Rand Corp.
Memo., January 1952, No. P-263.
FLOOD, M. M. A preference experiment
(Series 3). Unpublished memorandum,
Rand Corporation. February 25, 1952.
FLOOD, M. M.
Some experimental
games. Rand Corp. Memo., March
1952, No. RM-789-1. (Revised June
1952.)
FLOOD, M. M. Testing organization
theories. Rand Corp. Memo., November 1952, No. P-312.
FLOOD, M. M. An experimental multiple-choice situation.
Rand Corp.
Memo., November 1952, No, P-313.
FR£CHET, M. Emile Borel, initiator of
the theory of psychological games and
its application. Econometrica, 1953,
21, 95-96.
PRICKET, M.,& VON NEUMANN, J. Commentary on the three notes of Emile
Borel. Economelrica, 1953, 21, 118126.
FRIEDMAN, M., & SAVAGE, L. J. The
utility analysis of choices involving
risk. J. polit. Econ., 1948, 56, 279-304.
(Reprinted with minor changes in
G. J. Stigler & K. E. Boulding [Eds.],
Readings in price theory. Chicago:
Richard D. Irwin, 1952. Pp. 5796.)

74. FRIEDMAN, M., & SAVAGE, L. J.

The

expected-utility hypothesis and the
measurability of utility. /. polit.
Econ., 1952, 60, 463-475.
75. FRISCH, R. New methods of measuring
marginal utility. In R. Frisch, Beitrage
zur okonomischen'Theorie. Tubingen:
Mohr, 1932.
76. GEORGESCU-ROEGEN, N.
The pure
theory of consumer's behavior. Quart.
J. Econ., 1936, 50, 545-593.
77- GE.QRGESCU-RQEGEN, N- The theory of

<-----Page 34----->414

78.

79.
80.
81.
82.

83.

84.
85.

86.
87.
88.
89.
90.
91.
92.

93.
94.

WARD EDWARDS
choice and the constancy of economic
laws. Quart. J. Econ., 1950, 64, 125138.
GEORGESCU-ROEGEN, N. Utility, expectations, measurability, prediction.
Paper read at Econometric Soc.,
Kingston, September, 1953.
GIRSHICK, M. A., & BLACKWELL, D.
Theory of games and statistical decisions. New York: Wiley, 1954.
GOOD, I. J. Probability and the weighing
of evidence. London: Griffin, 1950.
GRIFFITH, R. M. Odds adjustments by
American horse-race bettors. Amer.
J. Psychol., 1949, 62, 290-294.
HARSANYI, J. C. Cardinal utility in welfare economics and in the theory of
risk-taking. /. polit. Econ., 1953, 61,
434-435.
HART, A. G. Risk, uncertainty, and the
unprofitability of compounding probabilities. In 0. Lange, F. Mclntyre,
& T. O. Yntema (Eds.), Studies in
mathematical economics and econometrics. Chicago: Univer. of Chicago
Press, 1942. Pp. 110-118.
HAYES, S, P., JR. Some psychological
problems of economics. Psychol. Bull.,
1950, 47, 289-330.
HERSTEIN, I. N., & MILNOR, J. An
axiomatic approach to measurable
utility. Econometrica, 1953, 21, 291297.
HICKS, J. R. The foundations of welfare
economics. Econ. J., 1939, 49, 696712.
HICKS, J. R. Value and capital. Oxford:
Clarendon Press, 1939.
HICKS, J. R., & ALLEN, R. G. D. A reconsideration of the theory of value.
Economica, 1934, 14, 52-76, 196-219.
HILDRETH, C. Alternative conditions for
social orderings. Econometrica, 1953,
21, 81-94.
HOUTHAKKER, H. S. Revealed preference and the utility function. Economica, 1950, 17, 159-174.
HULL, C. L. Principles of behavior, an
introduction to behavior theory. New
York: D. Appleton-Century, 1943.
HURWICZ, L. The theory of economic behavior. Amer. econ. Rev., 1945, 35,
909-925. (Reprinted in G. J. Stigler &
K. E. Boulding [Eds.], Readings in
price theory. Chicago: Richard D.
Irwin, 1952. Pp. 505-526.)
HURWICZ, L. What has happened to the
theory of games? Amer. econ. Rev.
Suppl., 1953, 43, 398^05.
IRWIN, F. W. Stated expectations as

95.
96.

97.
98.
99.
100.

101.
102.
103.
104.
105.

106.
107.
108.
109.
110.
111.
112.

functions of probability and desirability of outcomes. /. Pers., 1953, 21,
329-335.
JARRETT, JACQUELINE M. Strategies in
risk-taking situations. Unpublished
Ph.D. thesis, Harvard Univer., 1951.
JENKINS, W. O., & STANLEY, J. C., JR.
Partial reinforcement: a review and
critique. Psychol. Bull., 1950, 47, 193234.
JOHNSON, W. E. The pure theory of
utility curves. Econ. J., 1913, 23, 483513.
KALDOR, N. Welfare propositions and
inter-personal comparisons of utility.
Econ. J., 1939, 49, 549-552.
KALDOR, N. A comment. Rev. econ.
Stud., 1946, 14, 49.
KAPLAN, A., & RADNER, R. A questionnaire approach to subjective probability—some experimental results.
Working Memorandum 41, Santa
Monica Conference on Decision Problems, August 15, 1952.
KATONA, G. Psychological analysis of
business decisions and expectations.
Amer. econ. Rev., 1946, 36, 44-62.
KATONA, G. Contributions of psychological data to economic analysis. /.
Amer. statist. Ass., 1947, 42, 449-459.
KATONA, G. Psychological analysis of
economic behavior. New York: McGraw-Hill, 1951.
KATONA, G. Rational behavior and economic behavior. Psychol. Rev., 1953,
60, 307-318.
KAUDER, E. Genesis of the marginal
utility theory from Aristotle to the
end of the eighteenth century. Econ.
J., 1953, 63, 638-650.
KAUDER, E. The retarded acceptance of
the marginal utility theory. Quart. J.
Econ., 1953, 67, 564-575.
KAYSEN, C. A revolution in economic
theory? Rev. econ. Stud., 1946, 14, 115.
KENNEDY, C. The common sense of indifference curves. Oxf. econ. Pap.,
1950, 2, 123-131.
KNIGHT, F. H. Risk, uncertainty, and
profit.
Boston: Houghton Mifflin,
1921.
KNIGHT, F. H. Realism and relevance
in the theory of demand. /. polit.
Econ., 1944, 52, 289-318.
KNIGHT, F. H.
Comment on Mr.
Bishop's article. /. polit. Econ., 1946,
54, 170-176.
KUHN, H. W., & TUCKER, A. W. (Eds.)
Contributions to the theory of games.

<-----Page 35----->THEORY OF DECISION

113.

114.
115.

116.
117.
118.
119.
120.
121.

122.

123.
124.
125.
126.
127.
128.
129.

Vol. I. Ann. Math. Stud., No. 24.
Princeton: Princeton Univer. Press,
1950.
KUHN, H. W., & TUCKER, A. W. (Eds.)
Contributions to the theory of games.
Vol. II. Ann. Math. Stud., No. 28.
Princeton: Princeton Univer. Press,
1953.
LANCASTER, K. A refutation of Mr.
Bernardelli. Economica, 1953, 20,
259-262.
LANDAHL, H. D. A neurobiophysical interpretation of certain aspects of the
problem of risks. Bull. Math. Biophysics, 1951, 13, 323-335.
LANGE, O. The determinateness of the
utility function. Rev. econ. Stud., 1933,
1, 218-225.
LANGE, O. Note on the determinateness
of the utility function. Rev. econ.
Stud., 1934, 2, 75-77.
LANGE, O. The foundations of welfare
economics. Econometrica, 1942, 10,
215-228.
LANGE, O. The scope and methods of
economics. Rev. econ. Stud., 1945, 13,
19-32.
LEWIN, K. Principles of topological psychology. New York: McGraw-Hill,
1936.
LEWIN, K. Behavior and development
as a function of the total situation.
In L. Carmichael (Ed.), Manual of
child psychology. New York: Wiley,
1946. Pp. 791-844.
LEWIN, K., DEMBO, TAMARA, FESTINGER,
L., & SEARS, PAULINE S. Level of
aspiration. In J. McV. Hunt (Ed.),
Personality and the behavior disorders.
Vol. I. New York: Ronald, 1944. Pp.
333-378.
LEWISOHN, S. A. Psychology in economics. Polit. Sci. Quart., 1938, 53,
233-238.
LITTLE, I. M. D. The foundations of
welfare economics. Oxf. econ. Pap.,
1949, 1, 227-246.
LITTLE, I. M. D. A reformulation of the
theory of consumer's behavior. Oxf.
econ. Pap., 1949, 1, 90-99.
LITTLE, I. M. D. The theory of consumer's behavior—a comment. Oxf.
econ. Pap., 1950, 2, 132-135.
LITTLE, I. M. D. Social choice and individual values. /. polit. Econ., 1952,
60, 422H132.
MACFIE, A. L. Choice in psychology and
as economic assumption. Econ. J.,
1953, 63, 352-367.
McKiNSEY, J. C. C. Introduction to the

130.
131.

132.
133.

134.

135.

136.

137.
138.
139.
140.
141.
142.
143.
144.

145.

146.

MAKING

415

theory of games. New York: McGrawHill, 1952.
MALINVAUD, E. Note on von NeumannMorgenstern's strong independence
axiom. Econometrica, 1952, 20, 679.
MANNE, A. S. The strong independence
assumption—gasolene blends and
probability mixtures. Econometrica,
1952, 20, 665-669.
MARKOWITZ, H. The utility of wealth.
J. polit. Econ., 1952, 60, 151-158.
MARKS, ROSE W. The effect of probability, desirability, and "privilege"
on the stated expectations of children.
J. Pers., 1951, 19, 332-351.
MARSCHAK, J. Neumann's and Morgenstern's new approach to static economics. J. polit. Econ., 1946, 54, 97115.
MARSCHAK, J. Rational behavior, uncertain prospects, and measurable
utility. Econometrica, 1950, 18, 111141.
MARSCHAK, J. Why "should" statisticians and businessmen maximize
"moral expectation"? In J. Neyman
(Ed.), Proceedings of the second
Berkeley symposium on mathematical
statistics and probability. Berkeley:
Univer. of Calif. Press, 1951. Pp. 493506.
MARSHALL, A. Principles of economics.
(8th Ed.) New York: Macmillan, 1948.
MAY, K. O. Transitivity, utility, and
aggregation in preference patterns.
Econometrica, 1954, 22, 1-13.
MELVILLE, L. G. Economic welfare.
Econ. J., 1939, 49, 552-553.
MISHAN, E. J. The principle of compensation reconsidered. J. polit. Econ.,
1952,60,312-322.
MORGAN, J. N. Can we measure the
marginal utility of money? Econometrica, 1945, 13, 129-152.
MOSTELLER, F., & NoGEE, P. An experimental measurement of utility. /.
polit. Econ., 1951, 59, 371-404.
NEISSER, H. The strategy of expecting
the worst. Soc. Res., 1952, 19, 346363.
NEYMAN, J., & PEARSON, E. S. The testing of statistical hypotheses in relation to probability a priori. Proc.
Cambr. phil. Soc., 1933, 29, 492-510.
PAPANDREOU, A. G. An experimental
test of an axiom in the theory of
choice. Econometrica, 1953, 21, 477.
(Abstract)
PARETO, V. Manuale di economic, politica, con una introduzione utta scienta

<-----Page 36----->416
147.
148.
149.

150.

151.
152.
153.
154.
155.
156.

157.

158.
159.
160.
161.

162.

163.
164.

WARD EDWARDS
sociale. Milan, Italy: Societa Editrice
Libraria, 1906.
PHELPS-BROWN, E. H. Note on the determinateness of the utility function.
Rev. econ. Stud., 1934, 2, 66-69.
PIGOU, A. C. Some aspects of welfare
economics. Amer. econ. Rev., 1951, 41,
287-302.
PRESTON, M. G., & BARATTA, P. An
experimental study of the auctionvalue of an uncertain outcome. Amer.
J. Psychol., 1948, 61, 183-193.
RAMSEY, F. P. Truth and probability.
In F. P. Ramsey, The foundations of
mathematics and other logical essays.
New York: Harcourt Brace, 1931.
RICCI, U. Pareto and pure economics.
Rev. econ. Stud., 1933, 1, 3-21.
ROBBINS, L. Interpersonal comparisons
of utility: a comment. Econ. J., 1938,
48, 635-641.
ROBBINS, L. Robertson on utility and
scope. Economica, 1953, 20, 99-111.
ROBERTSON, D. H. Utility and all that
and other essays. London: George
Allen & Unwin, 1952.
ROTHENBERG, J. Conditions for a social
welfare function. /. polit. Econ., 1953,
61, 389-405.
ROTHSCHILD, K. W. The meaning of
rationality: a note on Professor Lange's
article. Rev. econ. Stud., 1946, 14, 5052.
ROUSSEAS, S. W., & HART, A. G. Experimental verification of a composite
indifference map. J. polit. Econ., 1951,
59,288-318.
SAMUELSON, P. A. A note on measurement of utility. Rev. econ. Stud., 1937,
4,155-161.
SAMUELSON, P. A. Empirical implications of utility analysis. Econometrica,
1938, 6, 344-356.
SAMUELSON, P. A. A note on the pure
theory of consumer's behavior. Economica, 1938, S, 61-71.
SAMUELSON, P. A. A note on the pure
theory of consumer's behavior. An
addendum. Economica, 1938, S, 353354.
SAMUELSON, P. A. The numerical representations of ordered classifications
and the concept of utility. Rev. econ.
Stud., 1938, 6, 65-70.
SAMUELSON, P. A. The end of marginal
utility: a note on Dr. Bernardelli's
article. Economica, 1939, 6, 86-87.
SAMUELSON, P. A. Foundations of economic analysis. Cambridge, Mass.:
Harvard Univer. Press, 1947.

165. SAMUELSON, P. A. Consumption theory
in terms of revealed preference. Economica, 1948, 15, 243-253.
166. SAMUELSON, P. A. Evaluation of real
national income. Oxf. econ. Pap.,
1950, 2, 1-29.
167. SAMUELSON, P. A. The problem of integrability in utility theory. Economica, 1950, 17, 355-385.
168. SAMUELSON, P. A. Probability, utility,
and the independence axiom. Econometrica, 1952, 20, 670-678.
169. SAMUELSON, P. A. Consumption theorems in terms of overcompensation
rather than indifference comparisons.
Economica, 1953, 20, 1-9.
170. SAVAGE, L. J. The theory of statistical
decision. J. Amer. statist. Ass., 1951,
46, 55-67.
171. SAVAGE, L. J. An axiomatic theory of
reasonable behavior in the face of uncertainty. Unpublished manuscript,
Statistical Research Center, Univer.
of Chicago, No. SRC-21222S14.
172. SCHULTZ, H. The theory and measurement of demand. Chicago: Univer. of
Chicago Press, 1938.
173. SCITOVSKV, T. A note on welfare propositions in economics. Rev. econ. Stud.,
1941, 9, 77-88.
174. SCITOVSKY, T. The state of welfare economics. Amer. econ. Rev., 1951, 41,
303-315.
175. SHACKLE, G. L. S. Expectations in economics. Cambridge, Eng.: Cambridge
Univer. Press, 1949.
176. SHAPLEY, L. S., & SHUBIK, M. Solutions
of n-person games with ordinal utilities. Econometrica, 1953, 21, 348-349.
(Abstract)
177. SLUTSKY, E. E. Sulla teoria del bilancio
del consumatore, Giornale degli economisti, 1915, SI, 1-26. (Trans, by O.
Ragusa and reprinted in G. J. Stigler
& K. E. Boulding [Eds.], Readings in
price theory. Chicago: Richard D.
Irwin, 1952. Pp. 27-56.)
178. SPROWLS, R. C. Psychological-mathematical probability in relationships of
lottery gambles. Amer. J. Psychol.,
1953, 66, 126-130.
179, STIGLER, G. J. The limitations of statistical demand curves. J. Amer.
statist. Ass., 1939, 34, 469-481.
180, STIGLER, G. J. The development of
utility theory. /. polit. Econ., 1950,
58, 307-327, 373-396.
181, STONE, J. R. N. The theory of games.
Econ. J., 1948, 58, 185-201.
182. STONE, R. (J. R. N.) The role of measure-

<-----Page 37----->THEORY OF DECISION

183.
184.

185.
186.
187.
188.
189.
190.

191.

192.

193.
194.
195.
196.

ment in economics. Cambridge, Eng.:
Cambridge Univer. Press, 1951.
STROTZ, R. H. Cardinal utility. Amer.
econ. Rev. SuppL, 1953, 43, 384-405.
SWEEZY, A. R. The interpretation of
subjective value theory in the writings
of the Austrian economists. Rev. econ.
Stud., 1933, 1, 176-185.
THURSTONE, L. L. The indifference
function. /. soc. Psychol., 1931, 2,
139-167.
THURSTONE, L. L. The measurement of
values. Psychol. Rev., 1954, 61, 47-58.
TINTNER, G. The theory of choice under
subjective risk and uncertainty. Econometrica, 1941, 9, 298-304.
TINTNER, G. A contribution to the nonstatic theory of choice. Quart. J.
Econ., 1942, 56, 274-306.
T6RNQVIST, L. On the economic theory
of lottery-gambles. Skand. AktuarTidskr., 1945, 28, 228-246.
VAIL, S. V. Expectations, degrees of belief, psychological probabilities. Unpublished manuscript Univer. of Michigan, Seminar on the Application of
Mathematics to the Social Sciences,
October 23, 1952.
VAIL, S. V. A stochastic model of utilities. Unpublished manuscript, No. 24,
Univer. of Michigan, Seminar on the
Applications of Mathematics to the
Social Sciences, April 23, 1953.
VAIL, S. V. Alternative calculi of subjective probabilities. In R. M. Thrall,
C. H. Coombs, & R. L. Davis (Eds.),
Decision processes. New York: Wiley,
in press.
VICKREY, W. S. Measuring marginal
utility by reactions to risk. Econometrica, 1945, 13, 319-333.
VINER, J. The utility concept in value
theory and its critics. /. polit. Econ.,
1925, 33, 369-387, 638-659.
VON NEUMANN, J. Zur Theorie der
Gesellschaftsspiele. Math. Ann., 1928,
100, 295-320.
VON NEUMANN, J., & MORGENSTERN, O.

197.

198.
199.
200.

201.
202.
203.
204.
205.
206.
207.
208.
209.

MAKING

417

Theory of games and economic behavior.
(1st Ed.) Princeton: Princeton Univer.
Press, 1944.
VON NEUMANN, J., & MORGENSTERN, O.
Theory of games and economic behavior. (2nd Ed.) Princeton: Princeton Univer. Press, 1947.
WALD, A. Statistical decision functions.
New York: Wiley, 1950.
WALKER, K. F. The psychological assumptions of economics. Econ. Rec.,
1946, 22, 66-82.
WALLIS, W. A., & FRIEDMAN, M. The
empirical derivation of indifference
functions. In O. Lange, F. Mclntyre,
& T. O. Yntema, (Eds.), Studies in
mathematical economics and econometrics. Chicago: Univer. of Chicago
Press, 1942.
WECKSTEIN, R. S. On the use of the
theory of probability in economics.
Rev. econ. Stud., 1953, 20, 191-198.
WEISSKOPF, W. A. Psychological aspects of economic thought. /. polit.
Econ., 1949, 57, 304-314.
WELDON, J. C. A note on measures of
utility. Canad. J. Econ. polit. Sci.,
1950, 16, 227-233.
WOLD, H. A synthesis of pure demand
analysis. Part I. Skand. Aktuar~
Tidskr., 1943, 26, 85-118.
WOLD, H. A synthesis of pure demand
analysis. Part II. Skand. AktuarTidskr., 1943, 26, 220-263.
WOLD, H. A synthesis of pure demand
analysis. Part III. Skand. AktuarTidskr., 1944, 27, 69-120.
WOLD, H. Ordinal preferences or cardinal utility? Econometrica, 1952, 20,
661-664.
WOLD, H., & JUR^EN, L. Demand analysis. New York: Wiley, 1953.
ZEUTHEN, F. On the determinateness of
the utility function. Rev. econ. Stud.,
1937, 4, 236-239.

Received for early publication A pril 8, 1954.

