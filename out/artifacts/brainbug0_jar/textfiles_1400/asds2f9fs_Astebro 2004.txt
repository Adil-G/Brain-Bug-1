<-----Page 0----->314

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 51, NO. 3, AUGUST 2004

Key Success Factors for Technological Entrepreneurs’
R&D Projects
Thomas Åstebro

Abstract—The impact of 36 innovation, technology, and market
characteristics on the probability that early stage R&D projects
will reach the market is examined. Analysis is based on data from
561 R&D projects developed by technological entrepreneurs. Four
characteristics stand out as most predictive: expected profitability,
technological opportunity, development risk, and appropriability
conditions. These predict future commercial success well with an
out-of-sample accuracy of 80.9%. This model performs better than
R&D managers’ and venture capital’s (VC’s) forecasts of success
and has the potential to be used as a screening tool for early stage
R&D investment reviews by, for example, VCs. Implications for
both research and practice are discussed.
Index Terms—Early stage R&D, probability of commercialization, project evaluation, project screening, technological
entrepreneurs.

I. INTRODUCTION

R

ESEARCHERS disagree on how to construct appropriate
R&D project evaluation models [1]. The most common
method of project evaluation is discounted cash flow (DCF), followed by various scoring approaches [2]. Others have proposed
evaluation models based on operations research (OR) principles
[3]. All these approaches have both benefits and shortcomings.
Scoring models may suffer from vaguely measured variables
and from the difficulty of combining both financial and nonfinancial variables on a common scale. OR models have been
found to be too complex to be of much practical use [2], [3].
DCF requires detailed information in situations where information may be very vague and have problems taking into account
information that is difficult to quantify.
As OR approaches were rejected and DCF became a known
entity, R&D project management research in the 1980s focused
on using statistical approaches to investigate “key success
factors” that would explain R&D projects’ success. Such statistical analysis could be used to build calibrated project scoring
models (for example [4]). However, in a review of this research,
Balachandra and Friar [1] conclude that researchers have
been unable to converge on any key success factors. Research

Manuscript received February 1, 2003; revised December 1, 2003. Review
of this manuscript was arranged by Department Editor A. S. Bean and Editor-in-Chief G. F. Farris. This work was supported in part by the Natural Sciences and Engineering Research Council of Canada and the Social Sciences
and Humanities Research Council of Canada’s joint program in Management
of Technological Change, in part by the Canadian Imperial Bank of Commerce,
and in part by the Canadian Innovation Centre.
The author is with the Joseph L. Rotman School of Management, University
of Toronto, Toronto, ON M5S3E6 Canada (e-mail: tastebro@utm.utoronto.ca).
Digital Object Identifier 10.1109/TEM.2004.830863

subsequently shifted gears toward investigating the innovation
process [5], without ever resolving what key characteristics
explain R&D projects’ success.
Recent literature has, therefore, been criticized for an undue
focus on the process of project selection while not addressing
the decision criteria necessary to select projects. Cooper [6]
summarizes recent research as addressing “how to do projects
right, without knowing which are the right projects.” Murphy
and Kumar [7] are critical as well, stating that “improving the
development process downstream, while neglecting upstream
choices may be a fruitless exercise.”
This paper revisits the question regarding the key success factors for screening early stage R&D projects. It addresses many
of the methodological problems that have plagued past research
and focus on a specific under-researched decision-making context: screening of early-stage R&D projects by technological
entrepreneurs. The scope is limited to examining the likelihood
that projects will reach the market. This is an important intermediate stage for achieving financial returns and includes both
technical success as well as acceptance in the marketplace.1 The
561 investigated R&D projects are typically of an incremental
nature with relatively small investments [9].
The research contributions are twofold: 1) this is the first
statistical analysis on the effect of key success factors on the
commercial success for early stage R&D projects generated
outside of established organizations and 2) to the author’s
knowledge this is the first large-scale study of key success
factors, where three methods biases are avoided: hindsight bias,
common method variance bias, and low statistical power.
The results do not support Balachandra and Friar’s [1]
suggestion that technology factors are less important for incremental and low-tech projects. The degree to which an invention
represents a clear improvement over previous products and the
degree to which there is technological uncertainty associated
with further development are two important success determinants for these projects. The key success factors for screening
technological entrepreneurs’ R&D projects are, therefore,
not much different than those that have been researched as
important for screening R&D projects within established
organizations.
Researchers can replicate this study’s useful design. The
critical ingredients are: 1) a large sample of projects; 2) project
screening criteria measured at an early stage; 3) data on key
management interventions; and 4) a clear outcome variable.
Single-firm studies of large numbers of historical project
screenings within large firms or venture capital firms where
1Technical

and commercial success are sometimes analyzed separately [8].

0018-9391/04$20.00 © 2004 IEEE

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

<-----Page 1----->ÅSTEBRO: KEY SUCCESS FACTORS FOR TECHNOLOGICAL ENTREPRENEURS’ R&D PROJECTS

records have been kept are likely to be better sources for obtaining good data than the previous dominant research design
of cross-firm post-outcome mail surveys. The use of subjective
evaluations on criteria at the screening stage seems in this
context to be of less concern.
II. R&D PROJECT EVALUATION RESEARCH
There has been a great deal of statistical research on the determinants of R&D project performance using various analytical
approaches. Reviews can be found in [1] and [10]–[14]. Rather
than rereviewing this extensive literature, I provide a few examples and then proceed to draw conclusions based on the excellent
meta-analysis by [1].
In the most comprehensive study to date, Cooper [4] tried to
predict 102 project successes and 93 failures. A factor analysis
on 48 variables was conducted to generate a smaller and more
manageable subset of predictors. Thirteen factors were identified and they explained 69.3% of the variance of the original 48
variables. A total of seven of the 13 factors were significantly
related to perceived project success at least at the 0.10 level.
These were (in decreasing order of significance): product superiority and uniqueness, project/company resource compatibility,
market need/growth/size, economic disadvantage to consumer,
newness to firm, technological resource compatibility and, fiof 0.42 and
nally, market competitiveness. The model had an
an overall prediction accuracy of 84.1%, and performed well in
a naïve split-sample test.
However, performing a meta-analysis of 60 articles, Balachandra and Friar [1] found contradictory results and little
stability of success factors across studies: there seems to be
no clear agreement even on the direction of influence of the
specific factors analyzed, let alone their significance, if any.
But when aggregating up to a sufficiently generic level of
analysis researchers seem to agree that the following classes of
variables are important: market, technology, environment, and
organizational [1], [10], [11], [13], [14], with disagreement as
to the importance of additional classes.
A similar lack of convergence is reached in the literature
on espoused decision criteria used by venture capitalists. Zopounidis [15] summarizes this literature with two conclusions.
The first is that the criterion of the management team is considered predominant in all the studies concerning decisions in
venture investment and the second is the great diversity of evaluation criteria and their relative importance (ranking of criteria)
from one study to the other” [15, p. 63].
The value of using statistically derived models for R&D
decision-making has nevertheless been clearly illustrated by
Zacharakis and Meyer [16] who investigated the ability of VCs
to accurately assess the future success of a venture seeking
investment. The 51 interviewed practicing U.S. VCs had on
average over ten years of VC experience and over 22 years of
work experience and focused on seed and early stage deals.
Zacharakis and Meyer [16] conducted an experiment where
the VCs received several pieces of information about 25 actual
investments that had subsequently achieved either success or
failure. Approximately 57% of the investments were in the
seed and early stages. The VCs were requested to evaluate the

315

ventures as they would during the initial screening stage. Their
predictions were then compared with the actual outcomes and
the percentage of correctly classified outcomes was computed.
This study show VCs to have a low ability to correctly forecast the outcomes of ventures—at best the VCs had a classification accuracy of approximately 40%. Perplexingly, the more
information about the venture that was provided to the VCs the
less able they were at correctly predicting outcomes. When information about the track record of the team and competition
was included the classification accuracy reduced to 31% and
when additional information about the team and product was
introduced the classification accuracy declined to 17%. These
results indicate that VCs are rather poor at making investment
decisions and that more information makes them more confused
and less accurate evaluators.
Zacharakis and Meyer then compared the ability of the VCs to
the forecasting accuracy of a simple statistical model that used
the same information as the VCs to make predictions. The classification accuracy of the statistical models ranged from 40% to
60%, always clearly surpassing the judgments made by the VCs.
These preliminary results are very encouraging. They indicate
that substantial improvements are possible in the screening stage
of investment decisions by using statistical models.
To create a reliable statistical model for R&D decision
support one should avoid previous methodological mistakes.
Balachandra and Friar [1] identify four major sources of weakness in previous research on R&D key success factors, namely
quality of data, the definition of a new product, factor selection
and definition, and measurement of factors.
Most studies on the determinants or factors influencing the
success of R&D projects have been conducted by simultaneously collecting information on independent and dependent
variables after the projects have been completed (for example
[4], [10], [17], and [18]). These studies suffer from both
common method variance bias [19] and hindsight bias [20],
thereby degrading data quality.
Another serious problem in previous work is the relatively
sparse amount of observations in relation to the number of predictors that have been used. Cliff [21] suggests: “With 40 or
so variables, a group of 150 persons is about the minimum, although 500 is preferable” [21, p. 339]. Previous studies all fail to
reach the preferable sample size and only [4] reaches the absolute minimum, as defined by [21]. This failure creates instability
of results in test-retests and partly explains the lack of convergence on a stable set of “success factors.”
Balachandra and Friar [1], in addition, report confusion about
the measurement and interpretation of the dependent variable:
project “success” due to the use of subjective scales and vague
(or complete absence) of a definition to guide respondents.
In all previous studies, the predictive information is collected
on perceptual scales. The respondent is asked to make a judgment as to the importance of a particular variable, for example,
the importance of “market size” on a scale with tick-marks,
rather than providing objective data on the size of the market.
This procedure is somewhat defendable due to the difficulty of
collecting objective data on a large set of variables through a
mail survey. Nevertheless, several well-known methodological
problems arise. What does, for example, a “7” on a ten-point

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

<-----Page 2----->316

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 51, NO. 3, AUGUST 2004

scale mean? What will the estimated coefficient for the variables mean? Tests of interrater reliability can only provide comfort in our belief that higher scores are indeed higher than lower
scores, not on the meaning of particular scores. Nevertheless,
[22], among others, shows that expert judgmental scores on predictors are quite useful in predicting outcomes, even in very
difficult decision situations. In fact, statistical models based on
judgmental scores perform better then experts’ judgmental predictions based on the same underlying judgmental scores [23].
Armed with this knowledge one can claim that collecting judgmental data on various potential predictors from well-informed
individuals is useful, but asking these individuals about their
overall judgment of the outcome (e.g., project success) is less
useful.
A final issue identified by Balachandra and Friar [1] is the
wide variety of R&D projects and new product development
projects studied. Quite possibly there are no generic success
factors as they might depend on the stage of the review and
other conditions. Balachandra and Friar [1], therefore, suggest
three contextual variables that condition which variables that
are important: the nature of innovation (incremental versus
radical), market (existing versus new), and technology (high
versus low).
In summary, future studies should: 1) avoid common method
and hindsight bias; 2) use larger data-sets than those used before; 3) define the outcome variable more precisely and avoid
expert judgment of it; 4) either limit or condition for the mix of
projects included; and 5) use subjective assessments of predictors with caution.

III. SAMPLE SELECTION AND DATA
A. Sampling Strategy
The Inventors’ Assessment Program (IAP) at the Canadian
Innovation Centre (CIC) was used as the main source for R&D
project data. The CIC represents an organization independent
of the R&D projects they evaluate and, therefore, is not likely
to suffer from bias of judgment associated with being directly
involved in the projects.
The IAP collects data on invention and product-market characteristics at a very early stage of the projects’ development,
thus allowing this study to avoid hindsight bias. Data on predictors are collected on average two years before the successful
projects reach the market. To further illustrate the early stage of
the review, the average accumulated out-of-pocket R&D expenditures at the time of evaluation are Cdn $9,853 (1995 value).
The average accumulated out-of-pocket R&D expenditures for
those reaching the market are on the other hand Cdn $54,513.
More data on the characteristics of these projects are available
in [9]. Over 14 000 R&D projects have been evaluated since the
inception of the IAP program in 1976.
The IAP strictly evaluates R&D projects undertaken by technological inventors/entrepreneurs conducting efforts outside the
confines of established organizations. As will be described, the
sample consists of a rather homogenous set of projects. This
sample is unique. There has been little analysis of the characteristics of technological entrepreneurs’ R&D projects and, to

the author’s knowledge there has not been any analysis of these
projects’ key success criteria.2
To have a project evaluated, the inventor/entrepreneur fills out
a questionnaire and pays a fee. The fee was Cdn. $262 in 1995
(about U.S. $185). In comparison with models of R&D project’s
success in established organizations [4], this review does not
consider organizational factors since, in an overwhelming majority of cases, there is not yet an organization to evaluate. The
IAP analyst–typically an engineer–compares the project with
other similar projects using data provided by the potential entrepreneur and searches various on-line databases such as patent
and trademark databases. The analyst, then, subjectively rates
the project on 37 criteria and reports these back together with
an overall recommendation.3 The ratings on 37 criteria serve as
the independent variables in this study. They are collected prior
to and independent of observing project outcomes, thus completely avoiding common method variance bias [29] and hindsight bias [20]. The reliability of the criteria range from 0.84 to
0.96 [30].
B. Data
The sample frame was all projects reviewed from the start of
the IAP until and including 1993. Projects reviewed after 1993
were excluded to ensure that outcomes were observable in 1996
when outcome data were collected. A random sample of projects
were selected from each year between 1976 and 1993. Using a
CD-ROM of Canadian residential addresses, 1826 records were
updated with current addresses. This number represents 21% of
the sample frame.
The “total survey design” method was followed to collect outcome data [31]. This involved several rounds of pretests of the
survey instrument and detailed reviews of the instrument by analysts at the IAP. The telephone survey method was chosen for its
ability to generate high response rates and for greater control of
the data collection process [32]. Individuals recorded as responsible for the projects were first mailed a letter informing that a
call would be placed. Telephone calls were made primarily in
evenings to residential telephone numbers during an eight-week
period in the spring of 1996. Among the 1826 sampled records,
1465 were from subjects who could be reached and asked to participate in the survey. We obtained 1095 responses, representing
an adjusted response rate of 75%.4
2For recent research on technological entrepreneurs/inventors, see [9] and
[24]–[26].
3Thirty-three of these criteria were developed by G. Udell at the Oregon Innovation Center in 1974 as critical for venture success [27], [28], and were used
at Waterloo from the start in 1976. In 1989, the CIC introduced a revised list
with four more criteria.
4A concern was that there would not be enough successes to estimate meaningful models. A subset of projects where analysts at the CIC had information
indicating that the invention might have reached the market were, therefore, included. CIC analysts had obtained this information through various sources such
as newspaper clippings. Seventy-five additional observations were included this
way. The addition of the choice-based observations did not change the distribution of observations across the overall ratings for the full 1976–1993 sample
( = 6:39, d:f : = 4, n.s.). Neither did the addition of the choice-based observations change the distribution of observations across ratings for the 1989–1993
subsample ( = 6:34, d:f : = 4, n.s.). Given that there are no changes in the
underlying distribution of data with the addition of the choice-based observations, there will be no bias in regression parameter estimates, only a change in
the constant [33, pp. 90–91].

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

<-----Page 3----->ÅSTEBRO: KEY SUCCESS FACTORS FOR TECHNOLOGICAL ENTREPRENEURS’ R&D PROJECTS

The projects in this sample typically represent rather modest
technological improvements. A plurality of the projects are consumer oriented (47%), most for household and general consumer use (28%), followed by sports and leisure applications
(15%). A list of successful inventions reviewed by the IAP includes a new milk container design, an impact absorbent material sewn into the back of a T-shirt for hockey players, a meat
tenderness tester, and a toilet tissue holder. However, there is
also a reasonable fraction of “high-tech” (6%), and industrial
equipment (6%) inventions [34], thus representing a broad cross
section of industrial applications. The same list includes an industrial-strength crusher of recycled cans, a new method for repairing worn feed rolls in sawmills, a reusable plug to insert in
wooden hydroelectric poles after testing for rot, and a computerized and mechanically integrated tree harvester. The concepts
submitted to the IAP for review are at their stage of inception;
none has reached the marketplace, and they range in development efforts from brief sketches to working prototypes. Some
are in the process of patenting, very few have already patented
their invention. It is fairly obvious from reading their descriptions that the inventions are not radical. Since radial inventions
appear very seldom [35] and incremental innovations represent
the majority of all innovations [36] this sample is not an uncommon sample of inventions.
An overwhelming majority of respondents are male (89%)
and from the Province of Ontario (72%). A number of tests
were conducted to establish that the variation in sampling and
response proportions across the years of submissions, provinces
in Canada, gender, and rating were random: that is, no selection
biases were detected (for details contact the author).
There can be many definitions of “project success” [37]. In
this study, the dependent variable was defined in the phone
survey as “Did you ever start to sell NAME or a later revised
]. The
or improved version of this invention?” [
definition employed here is clear and easy to replicate across
studies, does not depend on a subjective evaluation, and is a
necessary but not a sufficient condition for financial success.
A change in both the scaling of variables and variable composition in July 1989 unfortunately necessitated the deletion of
records prior to this date.5 The resulting analysis data set for
the period 1989–1993 consisted of 561 projects containing 499
failures and 62 successes, large enough to comply with standard
requirements for multivariate analysis [21].
The 37 criteria were graded on a three-point scale by the IAP,
i.e., A (Acceptable, which means that the criteria appears to be
favorable or satisfactory), B (Borderline, meaning the criteria
rated as B needs to be improved or strengthened), and C (Critical
5The change in period covered and the inclusion of the choice-based sample
did not have a significant effect on the distribution of the probability of success.
A comparison of the distributions of both the absolute number of successes and
the proportion of successes across the ratings in the 1976–1993 sample and the
1989–1993 sample produced no significant differences ( = 7:78, d:f : = 4,
n.s. and  = 4:13, d:f : = 4, n.s., respectively). Similar comparisons between
the 1989–1993 random sample and the 1989–1993 sample augmented with the
choice-based observations produced no differences ( = 1:18, d:f : = 4, n.s.
and  = 4:07, d:f : = 4, n.s., respectively.) However, the reduction in the
sample to cover only 1989–1993 resulted in a marginally significant change in
the distribution of total number of observations across the overall ratings ( =
8:94, d:f : = 4, p < 0:10). The change reflects the increased learning at the
IAP.

317

weakness, meaning it may be necessary to discontinue the effort
to commercialize the project.) For the purpose of statistical analysis the scores on the underlying criteria were converted into
,
, and
numerical data according to the following:
. All variables are scaled so that a higher value is thought
to represent a higher technical and/or commercial value. Positive coefficient estimates are, therefore, expected.
Because the analysts used a fixed format for entering
data on the criteria (a sheet where all criteria were listed)
there were few missing observations. Nevertheless, lack of
data on approximately 43% of the observations for one criteria (“Service: Will this innovation require less servicing
or less costly servicing than alternatives?”) resulted in that
criteria being eliminated from further analysis. There were
108 missing observations among the remaining 20 196 cells
observations) representing 0.53%. These
( variables
missing data were imputed to variables’ mean value.
While this work is not geared toward testing a priori hypotheses, it is nevertheless useful to examine whether the 36
criteria that will be analyzed have face validity for R&D project
evaluation.
Three criteria relate to the potential technological improvement of the invention. In addition, the outstanding development
cost, R&D uncertainty, (technological) resource availability,
tooling, and production costs are assessed. These variables
might be grouped under the rubric “technological opportunity”
[36]. Three criteria examine potential external constraints
such as safety concerns, environmental impacts, and legality.
Seven criteria consider various measures of demand, while five
additional criteria are the most well-known innovation characteristics impacting the diffusion of innovations [38]. Price
is included, as well as two measures of current and potential
competition. Three additional cost measures follow, while one
criterion examines system integration issues. Appropriability
conditions [36] and various investment criteria (e.g., expected
return, payback time) are also considered. The variables have
face value. Except for organizational aspects, they cover the
main groups of variables identified by [1] and are particularly
detailed on technology and demand characteristics. All of the
36 criteria are covered in one way or another in the review by
Balachandra and Friar [1].
IV. RESULTS
To determine the key success factors of R&D project
commercialization a logistic maximum-likelihood model for
a binary outcome is estimated,6 starting with the exclusion
of all variables and where the most significant variables are
successively included and for each inclusion the model is
reestimated (so-called forward selection).7 A -value of 0.05 is
6Three alternative link functions were explored: logit, probit, and gompit. All
three generated qualitatively similar results.
7I experimented with other procedures and selection criteria to judge the robustness of results. The variables selected for inclusion were identical using
a backward selection procedure with the exception of V23 (function), which
was replaced by V2 (functional performance). V23 and V2 are practically interchangeable in their definitions and have almost identical coefficients. The model
with V23 was selected by tossing a coin. A forward selection with a p-level of
0.10 did not cause more variables to be included. The four key success factors
are, therefore, robust to the choice of the cutoff point for inclusion.

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

<-----Page 4----->318

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 51, NO. 3, AUGUST 2004

used to determine the inclusion of variables. That is, the final
model only contains predictors that are significant at the 5%
level. The point of using this method instead of retaining the
full model is that the resulting model is easier to understand
and relate to for a practicing R&D manager. The method is also
consistent with the research tradition in this field (for example,
[4]). Finally, if the model is to be used in practice it is important
that it is robust and not overly sensitive to the data on which it
was estimated. The full model will likely fare less well in such
robustness tests as it takes into account the effect of a multitude
of mainly insignificant variables. The research method applied
here is geared toward generating a practically useful tool for
R&D project evaluation rather than to test a priori hypotheses.
Following the forward selection process four variables remain. These are, in decreasing order of statistical significance:
40 (expected profitability), 33 (development risk), 35 (IP
protection), and 23 (function provided), with the following parameter estimates ( -values in parenthesis):

where
the odds ratio of success.8 The model obtained a
. This model correctly predicts 444 out of the
psuedo
561 projects (79.1%). The model correctly predicts 398 of the
499 failures (79.7%) and predicts 46 of 62 successes correctly
(74.2%). Interestingly, the model is about equally adequate at
predicting successes as it is predicting failures while the number
of successes is an order of magnitude less.
The size of the odds-ratio for 40 “expected profitability” is
the largest. A one-unit increase in the variable “expected profitability” from “B”–borderline to “A–acceptable, means a 3.1
unit increase in the odds-ratio. The interpretation of this result is straightforward: “A”-rated inventions on “expected profitability” have odds 3.1 times higher to reach the market than
“B”-rated inventions on “expected profitability.” All four variables come close to or surpass an odds-ratio of two, indicating
not just significant but also substantial effect sizes.
I further test the model’s accuracy using an out-of-sample forward test. The data-set with 561 observations is split in two,
the first containing 383 projects that were evaluated between
1989 and 1992, and the second containing 178 projects evaluated during 1993. A model is estimated on the 1989–1992 data
set. I then analyze how well the coefficient estimates from that
data predict the outcomes of projects evaluated during 1993.
This test is more demanding than a random split-sample test
(used by Cooper [4]), since it requires the model to be consistent over time.
As expected, the model on the subset of data from 1989 to
1992 is somewhat different from the model for the overall data
set. This model included only three significant variables: the
two previous “expected profitability” and “function” and the
additional “trend of demand.” Most of the prediction power is,
8The “odds-ratio” is defined as follows. The odds of an event are calculated
as the number (or probability) of events divided by the number (or probability)
of nonevents. In this particular data set the mean odds of success are 62=499 =
0:124. The odds of failure are, therefore, 8:1. The odds ratio is calculated by
dividing the odds in the treated or exposed group by the odds in the control
group.

however, generated by “expected profitability” and “function.”
As this model is optimized on a subset of data, it is likely to
have lower classification accuracy for the hold-out sample than
what the model for the complete data set would have. Nevertheless, the model correctly classified 80.9% of all out-of-sample
projects, with correctly classifying 60.9% of the successes and
83.7% of the failures. In this test, the ability to correctly classify successes has deteriorated, whereas the ability to correctly
classify failures has improved providing a minute increase in the
classification accuracy. Clearly, the model has merit as a classifier when applied to new data.
The recommendation by the IAP is likely to have an impact on
whether the inventor continues or not [39]. There are two potential effects. The first is that the advice may shift the baseline odds
of commercialization by increasing the odds that a good project
is pursued and decrease the odds that a poor project is pursued.
This effect has no impact on coefficient estimates. Åstebro and
Chen [39] examine the magnitude of this “treatment” effect in
detail and find that it shifts the baseline probability of success
up (for positive advise) or down (for negative advise) on average
about two percent. The second potential effect is that the evaluation by the IAP may bias the relationship between the key success criteria and outcomes, for example, by having an inventor
improve on certain key weaknesses singled out by the IAP. This
problem is difficult to address with the available data and generally requires complicated econometric models that often rests
on untestable assumptions (see, for example, [40]). To the extent possible, I consider this issue using reduced-form models.
To investigate the potential bias in coefficient estimates due
to the “treatment,” I first examine the correlation between the
underlying 36 criteria and the analysts’ overall rating of the
project, and compare the results with those obtained from the
previously estimated statistical model predicting commercial
success. If analysts use the appropriate predictors of commercial success as identified by the statistical model, it is difficult
to claim that they do not use the appropriate key success factors
when making their judgments.
To address the above comparison an ordinal logistic regression model is fitted using the overall recommendation as the
outcome variable. The recommendation has five values that
represents the advise given to the inventor and is ordered from
“most promising” to “least promising” (for details, see [41]).
Using a -value of 0.05 to determine inclusion of predictors,
the resulting model has a pseudo
of 0.38 and contains 11 of
the possible 36 explanatory variables.9 With variables identical
in predicting commercial success and analysts’ overall ratings
italicized, the variables predicting analysts’ recommendations
are, “expected profitability,” “IP protection,” “technical feasibility,” “development risk,” “potential sales,” “distribution,”
“size of investment,” “function,” “appearance,” “legality,” and
“duration of demand.” This test shows that experts at the
IAP pay significant attention to the variables indicated by the
statistical model as most important in predicting commercial
success. However, they also take into account other information
when making their overall recommendation.
9The same variables appear using either backward or forward variable
selection.

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

<-----Page 5----->ÅSTEBRO: KEY SUCCESS FACTORS FOR TECHNOLOGICAL ENTREPRENEURS’ R&D PROJECTS

A regression is then performed where the predicted value of
the overall recommendation is inserted in the logistic regression
for predicting commercial success. The predicted value is generated from the previously estimated ordinal logit. If the experts
are using the key success factors appropriately, one would expect that there would be no remaining explanatory power of the
key success factors once the predicted overall recommendation
is included. To (statistically) identify the first-stage equation in
the second stage, I include four dummy variables, one each for
the years 1990, 1991, 1992, and 1993, as well as a dummy variable for the location of the inventor, where the value unity is
assigned if the inventor is located in the Province of Ontario
and zero, otherwise. These are likely to be independent of the
key success factors and are reasonable choices for identification
purposes.
When including the predicted overall recommendation in the
equation determining future commercial success and using a
variable inclusion criterion of 0.05 there are no other variables
included. The predicted overall recommendation generates a
of 0.24, which is three percentage points greater than
pseudo
the model with the four success criteria. The former and latter regressions together suggest that the overall recommendation provides similar type of information as the four key success factors.
While I cannot parametrically investigate the potential degree
of bias that is produced by running a reduced-form single-stage
model of the key success criteria, the above-performed analysis
at least shows that the key success criteria are indeed important predictors of the commercial success of these projects. The
out-of-sample test of the model also confirms this conclusion.
It should be noted that previous research on key success
factors has never investigated the problem of biased coefficient
estimates due to (management or other) intervention, although
such effects are likely to be rampant. Consider, for example,
Cooper’s study [4]. Project managers were asked to fill out a
questionnaire for one successful and one unsuccessful project
ex post of project completion. The success and failure of these
projects are a function not only of the characteristics at the
start of the projects but also of the management behavior and
resource allocations throughout the projects’ duration. Projects
that may at an early stage be considered to be most promising
are likely to receive more managerial attention and various
resources leading to a potential self-fulfilling prophecy. Any
statistical analysis of project success must take such effects
into account since the model of interest is that which indicates
the likelihood of success before additional resources have been
allocated, not after.
V. DISCUSSION AND CONCLUSION
Recent literature on managing the innovation process has
been criticized for an undue focus on the process of R&D
project selection, while not addressing the decision criteria
necessary to select appropriate projects [6], [7]. I revisit the
fundamental question regarding the key success factors, specifically for early stage R&D projects conducted by technological
entrepreneurs.
I examine the impact of a comprehensive set of 36 innovation and product-market characteristics on the likelihood that an

319

R&D project conducted by a technological entrepreneur will be
commercialized. These characteristics include several dimensions of technological opportunity, competition, legal and societal conditions, and nine dimensions characterizing user need
and market demand. The examined characteristics, while reasonably comprehensive in their coverage of technical, market,
and commercial conditions, do not include any organizational
dimensions.
The research contributions are twofold: 1) this is the first
statistical analysis on the effect of key success factors on the
commercial success for early stage R&D projects generated
outside of established organizations and 2) to the author’s
knowledge this is the first large-scale study of key success
factors where three methods biases are avoided: hindsight bias,
common method variance bias, and low statistical power.
A statistical model based on four underlying characteristics
as assessed by project-independent analysts correctly predicts
80.9% of all outcomes in terms of reaching the market (or not)
in out-of-sample tests. The model is approximately equally able
to correctly classify both successful and unsuccessful ventures.
This is an unexpected result as it implies that the model is able
to detect and appropriately use information in a highly multivariate setting where there is a low signal-to-noise ratio and data
is highly uncertain. Its prediction accuracy is higher than R&D
department managers’ ability to predict the technical success
of their own R&D projects, which was estimated by Mansfield
[42] to be 66%. The model is further at least twice as accurate
as seasoned VCs in the U.S. who have a maximum prediction
accuracy of 40% for seed and early stage ventures [16].
The estimated model performs seemingly worse than that derived by Cooper [4], which had a within-sample prediction accuracy of 84.1%, but Cooper’s study suffers from four problems
which may inflate results. These are the following.
1) There is likely common method variance bias as data on
independent and dependent variables were collected at the
same time, from the same person and after the fact.
2) There is likely hindsight bias as subjects were asked to select one successful and one unsuccessful project and then
to provide data that would explain outcomes. In such situations, subjects are likely to provided inflated causation
of outcomes [20].
3) There is potential for biased coefficient estimates due to
(management or other) interventions in Cooper’s study.
That is, while one is interested in how to assess the
impact of project characteristics on future success at the
screening stage, Cooper actually estimated the impact
of project success after management interventions. To
control for the effect of interventions it does not help
to merely ask subjects to recall project characteristics at
project origination. (It is also unlikely that such recalls
will be accurate.) What is needed is to: a) estimate the effects of early-stage characteristics on project outcomes in
the hypothetical absence of interventions and b) add the
marginal effect of management and other interventions
on outcomes. This paper showed that estimating such
counterfactual models are possible and that in this case
the intervention had relatively little effect.

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

<-----Page 6----->320

IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT, VOL. 51, NO. 3, AUGUST 2004

4) It is also not clear how well Cooper’s model performs
when used in actual decision situations as his test was
within-sample. The out-of-sample test used in this paper
is more critical than Cooper’s split-sample test and illustrates that the model is likely to perform well for other
data sets than that which it was estimated on.
One should, however, be clear of the limitations of this study.
The statistical model applies to what it has been calibrated
on: screening for market success of seed and early stage R&D
projects conducted by technological entrepreneurs. The data
cover R&D projects with, typically, high uncertainty, low
probability of success, and small development costs. Further,
the projects are primarily “low-tech” and incremental. Nevertheless, the obtained statistics implies a great improvement
over current practice in the screening of seed and early stage
R&D investment proposals.
The four key criteria: expected profitability, technological
opportunity (function), technological uncertainty (development
risk), and appropriability conditions (IP protection) contain
no great surprises as past research has already identified these
characteristics (among 68 others). What is interesting is that
three variables provide additional explanatory power over and
beyond the “overall” estimate of expected returns. There might
be two explanations that are not mutually exclusive. First,
analysts’ at the IAP fail to incorporate all available information
in their estimate of expected returns through cognitive biases.
Second, analysts consider some variables to be predictive of
technical and/or commercial success and consider others to be
determinants of financial returns conditional on technical and
commercial success. Further research is necessary to examine
these issues.
The results do not fit Balachandra and Friar’s [1] suggestion
that technology factors are less important for incremental and
low-tech projects since the degree to which a project represents
a clear improvement over previous products and the degree to
which there is technological uncertainty associated with further development are two out of four important determinants for
these projects.
What can other researchers learn from this study for future research designs? It is certainly possible to replicate this study’s
design using other samples. What seem to be critical ingredients
are: 1) a large sample of projects; 2) project screening criteria
measured at an early stage; 3) data on key management interventions; and 4) a clear outcome variable. Studies of historical
project screenings within single firms or venture capital firms
where records have been kept are likely to be better sources for
obtaining good data than the previous dominant research design
of cross-firm post-outcome mail surveys.
An important distinction between the type of screening model
developed here and DCF analysis for project selection is that
a screening model with ordinal subjective ratings of key success criteria is quite robust to the use of vague and nonfinancial data. It is not likely that high-quality data on the future
cash flow stream exists at the very early stage. A scoring model
that uses a simple high/medium/low assessment of the expected
return and other criteria seems sufficient to make an accurate
judgment in a first-stage screen. A second-stage screen where
data quality may be higher may use more elaborate financial

models. Another important consideration that favors a statistically calibrated scoring model is that one is much less interested
in the absolute value than the relative ranking of a project. That
is, as Wheelwright and Clark [43] illustrates, many organizations have more than enough projects to conduct and the R&D
organization is typically ineffective due to oversubscription of
resources. It is suggested that an organization should, therefore, conduct a capacity planning exercise where low-ranking
projects are eliminated irrespective of the fact that eliminated
projects may all reach a minimum ROI criterion. The application of a simple scoring model to rank order projects would in
such instances be quite effective.
Since the underlying criteria are subjectively measured there
is some uncertainty regarding the transferability of the results
to practice. The results do not inform non-IAP analysts how to
perform the subjective assessments of the four criteria. However, with some training by individuals knowledgeable about
the IAPs’ process it should nevertheless be possible to use such
a model by assessors at for example VC firms focused on seed
and early stage investments. This is an area of application reasonably related to the one investigated here. My conclusion rests
on the results by Einhorn [22] who shows that expert judgmental scores on predictors are useful in predicting outcomes
and Dawes et al. [23] who show that statistical models based on
judgmental scores perform better then experts’ judgmental predictions based on the same underlying judgmental scores. If the
IAPs’ classification accuracy is maintained by VCs using this
model it would suggest a doubling of the rate of return on VCs’
investments on seed and early stage investments due to the improvement in screening ability.
As previously suggested, it is highly plausible that superior
statistical decision-support models can be constructed on historical data in other investment decision situations. It is obvious
that the key criteria may shift across situations. The author is
currently working together with a large VC fund to create a statistical decision-support model for a later-stage fund. A statistical problem that needs to be addressed is self-selection. Those
investments that received funding are more likely to succeed
than those that did not due to the capital injection. As discussed
in this paper, there are statistical methods to control for this
effect.
ACKNOWLEDGMENT
The author would like to thank T. Cottrell, K. Dahlin, and
S. Floricel for their comments and suggestions. This is a thoroughly revised version of a paper that received the Best Paper
Award, Runner Up, Canadian Council on Small Business and
Entrepreneurship Conference, 1999, and the Best Paper Award,
TIM Division, Administrative Sciences Association of Canada
Conference, 2003.
REFERENCES
[1] R. Balachandra and J. Friar, “Factors for success in R&D projects and
new product innovation: A contextual framework,” IEEE Trans. Eng.
Manage., vol. 44, pp. 276–87, Aug. 1997.
[2] M. J. Liberatore and G. J. Titus, “The practice of management science
in R&D project management,” Manage. Sci., vol. 29, pp. 962–74, 1983.
[3] W. E. Souder, “Utility and perceived acceptability of R&D project selection models,” Manage. Sci., vol. 19, pp. 1384–1394, 1973.

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

<-----Page 7----->ÅSTEBRO: KEY SUCCESS FACTORS FOR TECHNOLOGICAL ENTREPRENEURS’ R&D PROJECTS

[4] R. G. Cooper, “An empirically derived new product project selection
model,” IEEE Trans. Eng. Manage., vol. 28, pp. 54–61, 1981.
[5] S. L. Brown and K. M. Eisenhardt, “Product development – Past research, present findings and future directions,” Acad. Manage. Rev., vol.
20, no. 2, pp. 343–378, 1995.
[6] R. G. Cooper, “Product innovation and technology strategy,” Res.
Technol. Manage., vol. 43, pp. 38–41, 2000.
[7] S. A. Murphy and V. Kumar, “The front end of new product development: A Canadian survey,” R&D Manage., vol. 27, pp. 5–15, 1997.
[8] E. Mansfield, J. Rapaport, A. Romeo, E. Villani, S. Wagner, and
F. Husic, The Production and Application of New Industrial Technology. New York: Norton, 1977.
[9] T. Åstebro, “Basic statistics on the success rate and profits for technological entrepreneurs,” Entrepreneurship: Theory and Practice, vol. 23,
pp. 41–48, 1998.
[10] G. Lilien and E. Yoon, “Determinants of new industrial product performance: A strategic reexamination of the empirical literature,” IEEE
Trans. Eng. Manage., vol. 36, pp. 3–10, 1989.
[11] M. M. Montoya-Weiss and R. Calantone, “‘Determinants of new product
performance’ a review and meta analysis,” J. Prod. Innov. Manage., vol.
11, pp. 397–417, 1994.
[12] M. Ozer, “A survey of new product evaluation models,” J. Prod. Innov.
Manage., vol. 16, pp. 77–94, 1999.
[13] D. H. Henard and D. M. Szymanski, “Why some new products are more
successful than others,” J. Marketing Res., vol. 38, pp. 362–375, 2001.
[14] J. D. Linton, S. T. Walsh, and J. Morabito, “Analysis, ranking and selection of R&D projects in a portfolio,” R&D Manage., vol. 32, no. 32, pp.
139–148, 2002.
[15] C. Zopounidis, “Venture capital modeling: Evaluation criteria for the
appraisal of investment,” The Financier ACMT, vol. 1, pp. 54–64, 1994.
[16] A. Zacharakis and D. Meyer, “The potential of actuarial decision
models: Can they improve the venture capital investment decision?,” J.
Bus. Venturing, vol. 15, pp. 323–46, 2000.
[17] M. A. Maidique and B. J. Zirger, “The new product learning cycle,” Res.
Pol., vol. 14, pp. 299–313, 1985.
[18] C. M. Yap and W. E. Souder, “Factors influencing new product success
and failure in small entrepreneurial high-technology electronics firms,”
J. Prod. Innov. Manage., vol. 11, pp. 418–432, 1994.
[19] D. T. Campbell and D. W. Fiske, “Convergent and discriminant validation by the multi-trait-multi-method matrix,” Psych. Bul., vol. 56, pp.
81–105, 1959.
[20] B. Fischhoff, “Hindsight is not equal to foresight: The effect of outcome
knowledge on judgment under uncertainty,” J. Experimental Psych.:
Hum. Perception Perform., vol. 1, no. 3, pp. 288–299, 1975.
[21] N. Cliff, Analyzing Multivariate Data. San Diego, CA: Harcourt Brace
Jovanovich, 1987.
[22] H. Einhorn, “Expert measurement and mechanical combination,” Organizational Behav. Hum. Perform., vol. 7, pp. 86–106, 1972.
[23] R. M. Dawes, D. Faust, and P. E. Meehl, “Clinical versus actuarial judgment,” Science, vol. 243, pp. 1668–74, 1989.
[24] G. Markman, D. B. Balkin, and R. A. Baron, “Inventors and new venture
formation: The effects of general self-efficacy and regretful thinking,”
Entrepreneurship: Theory and Practice, pp. 149–165, 2002.
[25] T. Åstebro, “The return to independent invention: Evidence of unrealistic optimism, risk seeking or skewness loving?,” Econ. J., vol. 113, pp.
226–239, 2003.
[26] K. Dahlin, M. Taylor, and M. Fichman, “Today’s Edisons or Hobbyists?: Technical merit and success of inventions by technological entrepreneurs,” Univ. Toronto, Toronto, ON, Canada, Working Paper, 2003.
[27] G. Udell, “Invention evaluation services: A review of the state of the art,”
J. Prod. Innov. Manage., vol. 6, pp. 157–68, 1989.
[28] G. Udell, R. Bottin, and D. Glass, “The Wal-Mart innovation network:
An experiment in stimulating American innovation,” J. Prod. Innov.
Manage., vol. 10, pp. 23–34, 1993.

321

[29] R. Rosenthal and R. Rosnow, Essentials of Behavioral Research:
Methods and Data Analysis, 2nd ed. New York: McGraw-Hill, 1991.
[30] K. G. Baker and G. S. Albaum, “Modeling new product screening decision,” J. Prod. Innov. Manage., vol. 3, no. 1, pp. 32–39, 1986.
[31] D. Dillman, Mail and Telephone Surveys: The Total Design
Method. New York: Wiley, 1978.
[32] P. J. Lavrakas, Telephone Survey Methods, 2nd ed. Newbury Park, CA:
Sage, 1993.
[33] G. S. Maddala, Limited Dependent and Qualitative Variables in Econometrics. Cambridge, U.K.: Cambridge Univ. Press, 1983.
[34] “Annual Report,” Canadian Industrial Innovation Centre, Waterloo, ON,
Canada, 1996.
[35] M. Tushman and P. Anderson, “Technological discontinuities and organizational environments,” Admin. Sci. Quart., vol. 31, pp. 439–465,
1986.
[36] W. Cohen, “Empirical studies of innovative activity,” in Handbook of
the Economics of Innovation and Technological Change, P. Stoneman,
Ed. Oxford, U.K.: Blackwell, 1995, pp. 182–264.
[37] R. G. Cooper and E. J. Kleinschmidt, “New products: What separates
winners from losers,” J. Prod. Innov. Manage., vol. 4, no. 3, pp. 169–184,
1987.
[38] E. M. Rogers, Diffusion of Innovations, 3rd ed. New York: Free Press,
1983.
[39] T. Åstebro and G. Chen, “A statistically validated method for selecting
early stage ventures,” in “What Next for Venture Capital and Private
Equity?” Conf., Toronto, ON, Canada, June 21–22, 2002.
[40] J. J. Heckman and E. J. Vytlacil, “Instrumental variables, selection
models, and tight bounds on the average treatment effect,” Nat. Bureau
of Econ. Res., Cambridge, MA, Tech. Working Paper, 2000.
[41] T. Åstebro and Y. Gerchak, “Profitable advice: The value of information
provided by Canada’s inventor’s assistance program,” Econ. Innov. New
Technol., vol. 10, no. 1, pp. 45–72, 2001.
[42] E. Mansfield, Industrial Research and Technological Innovation – An
Econometric Analysis. New York: Norton, 1968.
[43] S. C. Wheelwright and K. B. Clark, “Creating project plans to focus
product development,” Harv. Bus. Rev., vol. 70, no. 2, pp. 70–83, 1992.

Thomas Åstebro received two degrees in engineering from Chalmers University of Technology,
Göteborg, Sweden, and the Ph.D. degree from
Carnegie Mellon University, Pittsburgh, PA
He is an Associate Professor of Strategic Management at the Joseph L. Roadman School of Management, University of Toronto, Toronto, ON, Canada.
He conducts research in the economics and management of technological change and entrepreneurship
and has recently published in, among others, RAND
Journal of Economics, and the Economic Journal. He
is frequently invited by the financial industry to give speeches on how to assess
entrepreneurial ventures. He has consulted corporations such as Canadian Imperial Bank of Commerce, Bank of Montreal, FleetBoston, Skandia Insurance
Company, Nationale Nederlanden B.V., Volvo, and SKF, and institutions such as
The Royal Swedish Academy of Engineering Sciences. He has taught management of innovation in Canada, the U.S., Australia, Sweden, and Italy, received 11
international and national awards and honors, is a cofounder of three start-ups,
is the recipient of numerous research grants, has published over 20 articles, and
made over 50 conference presentations.

Authorized licensed use limited to: University of Waterloo. Downloaded on April 18, 2009 at 16:02 from IEEE Xplore. Restrictions apply.

