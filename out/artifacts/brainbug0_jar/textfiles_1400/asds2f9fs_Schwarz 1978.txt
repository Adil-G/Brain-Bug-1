<-----Page 0----->Estimating the Dimension of a Model
Author(s): Gideon Schwarz
Source: The Annals of Statistics, Vol. 6, No. 2 (Mar., 1978), pp. 461-464
Published by: Institute of Mathematical Statistics
Stable URL: http://www.jstor.org/stable/2958889 .
Accessed: 04/10/2011 21:26
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at .
http://www.jstor.org/page/info/about/policies/terms.jsp
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve and extend access to The
Annals of Statistics.

http://www.jstor.org

<-----Page 1----->The Annals of Statistics
1978,Vol. 6, No. 2, 461-464

ESTIMATING

THE DIMENSION

OF A MODEL1

BY GIDEONSCHWARZ
HebrewUniversity
The problemof selectingone of a numberof modelsof different
dimensionsis treatedby findingits Bayes solution,and evaluatingthe
leading termsof its asymptoticexpansion. These termsare a valid
large-samplecriterionbeyond the Bayesiancontext,since theydo not
dependon thea prioridistribution.

1. Introduction. Statisticiansare oftenfaced with the problem of choosing
theappropriatedimensionalityof a model thatwill fita givenset of observations.
Typical examplesof this problem are the choice of degree fora polynomial regressionand the choice of order fora multi-stepMarkov chain.
In such cases the maximumlikelihood principle invariablyleads to choosing
the highestpossible dimension. Thereforeit cannot be the rightformalization
of the intuitivenotion of choosing the "right" dimension. An extension of
the maximum likelihood principle is suggestedby Akaike [1] for the slightly
more general problem of choosing among differentmodels with different
numbersof parameters. His suggestionamounts to maximizingthe likelihood
function separately for each model j, obtaining, say, MJ(Xl, . . *, X"), and then
choosing the model for which log M,(Xl, *. *, X") - k, is largest, where k, is the

dimensionof the model. We presentan alternativeapproach to the problem.
In a model of givendimensionmaximumlikelihoodestimatorscan be obtained
as large-samplelimitsof the Bayes estimatorsfor arbitrarynowhere vanishing
a prioridistributions.
Thereforewe look for the appropriate modificationof maximumlikelihood
for our case, by studyingthe asymptoticbeavior of Bayes estimatorsunder a
special class of priors. These priors are not absolutely continuous, since they
put positiveprobabilityon some lower-dimentionalsubspaces of the parameter
space, namely the subspaces that correspond to the competingmodels. In the
large-samplelimit, the leading termof the Bayes estimatorturnsout to be just
the maximum likelihood estimator. Only in the next term somethingnew is
obtained. This was to be expected, since (as was shown in [2] and [3], albeit
for sequential testing)the leading term depends on the prior only throughits
support,while the second order term does reflectsingularitiesof the a priori
distribution. We shall arrive at the followingprocedure:
Choose the model for which log MJ(Xl,

*..,

X")

-

-k,log n is largest.

The validityof this procedure as a large-sampleversionof Bayes procedures
ReceivedAugust1976;revisedFebruary1977.
1 Writtenwhiletheauthorwas a Fellow of theInstituteforAdvancedStudieson Mt. Scopus.
AMS 1970subjectclassifications.
Primary62F99,62J99.
Keywordsandphrases.Dimension,Akaikeinformation
criterion,
asymptotics.
461

<-----Page 2----->462

GIDEON SCHWARZ

will be establishedhere for the case of independent,identicallydistributedobservations,and linear models.
2. The exact Bayes procedure. In a general parameter space, there is no
intrinsiclinear structure. We thereforeassume that observations come froma
Koopman-Darmois family, i.e., relative to some fixedmeasure on the sample
space theypossess a densityof the form

f(x, 6) = exp(6 *y(x) -b(8))l
where 6 rangesover the natural parameterspace 0, a convex subset of the KK-dimensionalstatistic. The
dimensionalEuclidean space, and y is the sufficient
competingmodels are given by sets of the formm, n 9, where each m, is a kdimensionallinear submanifoldof K-dimensionalspace.
Fittingthe asymptoticnature of the result,the a prioridistributionneed not
be known exactly. It sufficesto assume that it is of the formE Crj, wherea,
is the a prioriprobabilityof thejth model being the true one, and , the conditional a priori distributionof 0 given thejth model, has a k,-dimensionaldensitythat is bounded and locally bounded away form zero throughoutm, n 9.
This implies mutual orthogonalityof the p,, since the intersectionof two
distinctlinear manifoldseither is one of them, or has lower dimensionsthan
both.
Finally, we assume a fixedpenalty forguessingthe wrong model. (Actually,
a loss that depends on 6 and on the guess would yield the same asymptotic
results,provided the loss functionstays between two fixed positive bounds for
all wrongdecisions.) Under thisassumption,the Bayes solutionconsistsof selecting the model that is a posteriorimost probable. Via Bayes' formulathat is
equivalent to choosing thej that maximizes
S(Y, n,j) = log

Sas

exp((Y o 6 - b(6))n) dp,(6),

where the integral extends over m, n 8, and Y is the averaged y-statistic
(lln) E y(Xi).
3. Asymptotics. The asymptoticexpansion of S(y, n,j) could be obtained
fromresultsin an earlier paper [3] as a special case. We shall, however, keep
this paper self-containedby outlininga proof of the necessaryresultdirectly.
PROPOSITION. For fixedY and j, as n tendsto oo,

S(Y, n, ) = n sup (Y o 0-b())

-

ik, log n + R

wheretheremainderR = R(Y, n,j) is boundedin nforfixedY and j.
PROOF. We shall proceed in steps.
- b(6) = A - 2118_and
is
tej Lebesguemeasureon m,.
> 0, 60 is a fixedvectorin m,,

LEMMA 1. Thepropositionholds whenY o 6

i

80112where

<-----Page 3----->463

ESTIMATING THE DIMENSION OF A MODEL

Explicit evaluation of the integralyields a,(wr/n2)kj/2enA, and
sup A -

Therefore

2118-01

12=

A.

S(Y, n,j) = nA - 1k, log (n2/w) + log a,
establishesthe propositionfor this case, with R = k log (o/i) + log a,.
2. If twoboundedpositiverandomvariablesU and V agree on the set
whereeitherexceedsp for some 0 < p < sup U, then
LEMMA

log E(U1)

as n -) oo.

-

O
log E(VI)

Clearly it sufficesto show that this holds for V that vanishes where U < p.
?
and therefore
In this case 0 < Un - VI p<P,
E(V ) < E(U ) < E(V") + pf = E(Vn) (I + E(Vl))
and we only have to show log (1 + (p1/E(V1)))-O 0. Now (E(Vn))111-* sup V
(a well-knownfacton L. norms) and sup V = sup U > p yield for p/(E(V"))111
a limit strictlyless than 1, hence p"/E(VI) tendsto zero, and so does log (1 +
(p"1E(

v")))

.

For some 0 < p < eAI whereA = sup (Y o 6 - b(6)), a vector80,
exp(Y o 6 - b(6)) > p:
and somepositive21and 22' thefollowingholdswherever
LEMMA 3.

A

-

218-

o112 < (Y

o

-

b(6)) < A

-

22118

-

8o112.

As is well known, the matrixof second-orderderivativesof b(6) is the covariance matrixof y, and hence positivedefinite. ThereforeY o 6 - b(6) is strictly
convex, and is easily seen to attainits maximum. Let 60 be the point where the
maximumA is attained. The Taylor expansion of Y o 6 - b(6) around 60 now
yields the stated inequalities for some neighborhoodof 60, if 221 and 222 are
largerand smaller than all the eigenvalues of the matrixof second orderderivatives of b(6) at 60. By strictconvexity it is now easy to determinep < eA so
that it will bound exp(Y o 6 - b(6)) outside that neighborhood.
The propositionis now proved by combiningthe lemmas,and the assumption
of local boundednessof the densityfunctionof t,aon m, n e.
Qualitativelyboth our procedureand Akaike's give "a mathematicalformulationof theprincipleof parsimonyin model building." Quantitatively,since our
procedure differsfromAkaike's only in that the dimension is multiplied by
2 log n, our procedure leans more than Akaike's towards lower-dimensional
models (when thereare 8 or more observations). For large numbers of observations the proceduresdiffermarkedlyfromeach other. If the assumptionswe
made in Section 2 are accepted, Akaike's criterioncannot be asymptotically
optimal. This would contradictany proof of its optimality,but no such proof
seems to have been published,and the heuristicsof Akaike [1] and of Tong [4]
do not seem to lead to any such proof.

<-----Page 4----->464

GIDEON

SCHWARZ

REFERENCES
[1]

AKAIKE,

[3]
[4]

SCHWARZ,

H. (1974). A new look at the statistical identificationmodel. IEEE Trans. Auto.
Control19 716-723.
[2] SCHWARZ, G. (1969). A second order approximation to optimal sampling regions. Ann.

Math.Statist.40 313-315.

TONG,

G. (1971). A sequential Student test. Ann. Math. Statist. 42 1003-1009.
H. (1975). Determination of the order of a Markov chain by Akaike's information
criterion. J. Appl. Prob. 12 488-497.
DEPARTMENT
HEBREW

JERUSALEM
ISRAEL

OF STATISTICS

UNIVERSITY

