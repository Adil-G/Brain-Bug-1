<-----Page 0----->Trust Rules for Trust Dilemmas: How Decision
Makers Think and Act in the Shadow of Doubt
Roderick M. Kramer
Graduate School of Business
Stanford University
kramer roderick@gsb.stanford.edu

Abstract. Trust has long been recognized as an important antecedent
of cooperative behaviour. For example, trust facilitates the productive
exchange of informaion in collaborative relationships. Central to the decision to trust another individual in such situations is the trust dilemma:
even though recognizing the beneﬁts of trust, individuals recognize also
the prospect that their trust might be betrayed. Thus, they must decide
how much trust (or distrust) is warranted. At a psychological level, this
trust dilemma is animated by social uncertainty (uncertainty regarding
the other party’s motives, intentions and actions). Using a computer simulation methodology, this paper investigates the comparative eﬃcacy of
diﬀerent decision rules regarding trust in a simulated trust dilemma. The
results demonstrate that attributional generosity (operationalized as giving the other party the beneﬁt of the doubt) facilitates the development
and maintance of more cooperative relationships when social uncertainty
is present.

1

Trust Rules for Trust Dilemmas: How Decision Makers
Think and Act in the Shadow of Doubt

“In a broad perspective, rules consist of explicit or implicit norms, regulations,
and expectations that regulate the behavior of individuals and interactions among
them. They are a basic reality of individual and social life; individual and collective actions are organized by rules, and social relations are regulated by rules.”
– March, Schulz, & Zhou (2001, p. 5).

Imagine the following hypothetical vignette. Two researchers working on a
problem of mutual interest decide to enter into a scientiﬁc collaboration. They
each agree to share all of the ideas and empirical ﬁndings emerging from their individual research laboratories with the aim of joint publication. Imagine further
that they are working on a problem of considerable scientiﬁc and social importance (e.g., ﬁnding a vaccine for the AIDS virus). The prospects for a fruitful
collaboration between the researchers appear excellent because each brings to the
table distinctive but complementary competencies: One is a particularly gifted
theorist, the other an unusually skilled experimentalist.
R. Falcone, M. Singh, and Y.-H. Tan (Eds.): Trust in Cyber-societies, LNAI 2246, pp. 9–26, 2001.
c Springer-Verlag Berlin Heidelberg 2001


<-----Page 1----->10

Roderick M. Kramer

From the standpoint of each researcher, therefore, the collaboration represents a unique and possible invaluable opportunity. Given the importance of the
problem, it is reasonable to expect that any individual or individuals credited
with eventually solving the problem under investigation will receive considerable
acclaim, possibly even a Nobel Prize. It is an opportunity, however, that is obviously attended by some vulnerability. Neither collaborator knows a great deal
about the other. Both have worked mostly alone in the past. Thus, there is little
readily available information either can use to assess the trustworthiness of the
other as a collaborator. Moreover, the costs of misplaced trust are potentially
quite steep if one of the researchers “defects” from the collaboration in the end
stages, he or she may be able to garner the lion’s share of the credit and acclaim
for solving the problem.
The contours of this vignette, although hypothetical, are hardly unusual. In
the highly competitive world of science, the opportunities for sharing eﬀort and
insight are substantial, as are the vulnerabilities, as Watson’s surprisingly candid
account of the discovery of the molecular conﬁguration of DNA, recounted in
Double Helix, made quite evident to the general public. Such situations are but
one example of a broad class of decision problems known as trust dilemmas. In
a trust dilemma, a social decision makers hopes to reap some perceived beneﬁt
from engaging in trusting behavior with another social decision maker. Pursuit of
the opportunity, however, exposes the decision makers to the prospect that his or
her trust might be exploited and betrayed. This conjunction of opportunity and
vulnerability are the sine qua non of a trust dilemma. Because of our dependence
on, and interdependence with, other social decision makers, trust dilemmas are
an inescapable feature of social and organizational life.
At a psychological level, trust dilemmas are animated, of course, by uncertainty about the trustworthiness of the other decision maker(s) with whom the
individual is interdependent. It is not knowing the other’s character, motives,
intentions and actions that make trust desirable but risky. Social uncertainty of
this sort, indeed, is fundamental to the problem of trust. As Gambetta, (1988)
aptly observed in an early and inﬂuential analysis of this problem, “The condition of ignorance or uncertainty about other people’s behavior is central to the
notion of trust. It is related to the limits of our capacity ever to achieve a full
knowledge of others, their motives, and their responses to endogenous as well as
exogenous changes” (p. 218).
How do decision makers in trust dilemma situations cope with such uncertainty? How do they decide, for example, how much trust or distrust is appropriate when dealing with another decision makers about whom they possess
incomplete social information? To what extent do they make fairly positive presumptions about others and to what extent are they likely to entertain fairly
negative or pessimistic expectations? These are the fundamental questions I engage in this paper. To explore them, I examine some relationships between different ‘trust rules’ and their consequences within the context of ‘noisy’ (socially
uncertain) decision contexts.

<-----Page 2----->How Decision Makers Think and Act in the Shadow of Doubt

11

I approach these questions from the standpoint of one perspective on how
social decision makers make sense of and respond to the social uncertainty intrinsic to trust dilemmas. I characterize this perspective as the social auditor model.
After providing a brief overview of this model, I present some evidence for the
model and then elaborate on some of its implications.

2

The Social Auditor Model

In almost every domain of life, people rely on various kinds of rules to help them
assess problems and make decisions. For example, chess players have rules for
interpreting complex endgames and for prescribing the best moves to make in
those situations. Physicians have rules for distinguishing between patient complaints that require further investigation versus mere reassurance that nothing
serious is the matter. And taxi drivers have rules for deciding who to let into
their cab late at night and whom they should pass by. Similarly, social decision
makers possess rules for helping them form judgments and reach decisions in
interdependence dilemmas as well (see Messick & Kramer, 2001).
According to the social auditor model, individuals possess various kinds of
rules to use when trying to make sense of what to do in trust dilemma situations
(see Kramer, 1996 for a fuller elaboration of this framework). These rules include
interpretation rules (e.g., rules that help us categorize a given trust dilemma and
prescribe the sort of evidence we should look for when trying to assess another
decision maker’s trustworthiness) and action rules (rules about what behaviors
we should engage in when responding to those interpretations). To return to the
example with which I opened this chapter, we can imagine an individual involved
in a potentially fruitful cooperative relationship who has to decide how much to
cooperate with the other, given some uncertainty regarding their trustworthiness.
According to the social auditor model, people navigate through trust dilemmas using their mental models of the world. These mental models include their
social representations, which encompass everything they believe about other people, including all of their trust-related beliefs and expectations, their self representations (e.g, their beliefs about their own sophistication and competence at
judging others’ trustworthiness), and their situational taxonomies (e.g., their
beliefs about the various kinds of social situations they are likely to encounter
in their social lives). These mental models are used to help people interpret the
trust dilemmas they confront. On the basis of these interpretations, they make
choices.
These choices can be conceptualized as the action rules individuals use when
responding to trust dilemmas. Action rules thus represent decision makers’ beliefs about the “codes of [prudent] conduct” they should appeal to and employ
when trying to act in trust dilemma situations. Interpretation and action rules
are viewed in this framework as intendedly adaptive orientations. In other words,
in using them, decision makers think they will help them 1) reap the beneﬁts of
trust when one is dealing with a trustworthy other and 2) minimize the costs of
misplaced trust when one is interacting with an untrustworthy other.

<-----Page 3----->12

Roderick M. Kramer

According to the social auditor model, decision makers also monitor the
consequences of rule use in trust dilemma situations. In other words, they pay
attention to what happens to them after they have employed a given rule when
interacting with a speciﬁc other. During this post-decision “auditing” process,
they attempt to discern, for example, whether the amount of trust they have
displayed toward the other is prudent or imprudent. Was too much trust aﬀorded
or too little?
On the basis of the conclusions they reach, they are likely to change their
subsequent behavior when interacting further with the individual. The results of
the post-decision auditing process then are assumed to inform (i.e., validate or
invalidate) one’s mental model of the dilemma, resulting in possible modiﬁcation
of the rule system invoked, leading to a change in rule use, etc. The model thus
posits a cyclic, adaptive learning system of the sort described by March and his
colleagues (March, Schulz, & Zhou, 2000).
If we approach the problem of the “correct” or “optimal” level of trust or
distrust to manifest when dealing with other people in a decision making ecology in which social uncertainty is present, we can consider what properties of
an interpretation-action rule system are likely to produce the best results. For
instance, does it makes sense to be a bit “paranoid” when dealing with others
we don’t know well, making fairly pessimistic or bleak assumptions about their
willingness to behave in a trustworthy fashion. Or, all else equal, is it better
to be a bit “panglossian” in our assumptions, giving others the beneﬁt of the
doubt? In many respects, this is the dilemma faced, of course, by the two research
collaborators described at the beginning of this chapter.

3

Studying Trust Rules: “Thinking Aloud” about How
People Think and Act in the Shadow of Doubt

There are many diﬀerent ways a researcher can approach the task of trying
to discover the cognitive rules people use when dealing with trust dilemmas.
One approach, and in some respects a fairly direct method, is to simply ask
people to tell us what they think about when they respond to various kinds
of trust dilemmas. For example, to understand how negotiators assess others’
trustworthiness in a negotiation context, we can ask them to “think aloud”
about what they look for when sizing up a negotiator, and the behavioral rules
they are likely to employ on the basis of that assessment.
One advantage of this direct approach is that it enables us to learn something
about how people confronting a trust dilemma actually think. For example, we
can learn something about the kinds of rules they believe will work well or
poorly in the situation. We can also learn something about what they think other
people are likely to do or not do in the situation (i.e., the common interpretation
and action rules they expect others to use). This methodology also has the
advantage of being inductive, thereby minimizing researcher assumptions about
these important questions.

<-----Page 4----->How Decision Makers Think and Act in the Shadow of Doubt

13

Using this thinking aloud approach, I examined how experienced organizational decision makers think about professional trust dilemmas. The study
involved, in particular, interviews with 44 senior executives involved in an advanced management program (see Kramer, 2001 for a more complete description
of the study rationale, methods and results). The executives were asked to talk
about, among other things, how they would go about handling a trust dilemma
involving a potential business partner whom they did not know much about
(N.B. - the details of the vignette were structured so that they were quite similar to the dilemma described at the beginning of this chapter). They were asked
a series of questions, for example, about the kinds of things they would look for
when trying to assess the other’s trustworthiness, and also the kinds of action
rules (behaviors) they would use on the basis of that assessment.
One of the interesting patterns that emerged from this qualitative study was
that it was possible to characterize about 80% of the respondents (N = 35) as
having one of two general orientations towards other uncertainty about other
people. The ﬁrst group (N = 23), which I characterize as social optimists or
panglossians, posess fairly positive or benign views about human nature. For
these social panglossians, most people are fundamentally trustworthy most of
the time. Although social uncertainty (uncertainty about their character, motivation, and intentions) is a part of social life, it is not something that has to be
feared, but simply dealt with. Perhaps the biggest danger of such uncertainty,
from the standpoint of these individuals, is that uncertainty introduces potentially disruptive “noise” into the communication process. As one executive in
this category put it,“You have to be careful not to over-react to ambiguous information. Your assumptions about the other person can be wrong, and you can
end up ruining a good relationship”. Another individual put it this way, “You
have to wait until all the facts are in. I give other people the beneﬁt of the doubt
until I’m reasonably sure. Getting a reputation for being a distrusting person
would be fatal to my business, which runs largely on trust.”
In contrast to these social panglossians, twelve individuals were categorized
as having considerably more pessimistic and cautious views about other people
and the eﬀects of social uncertainty on their trust relations. As one put it, “I’ve
always found it’s better to play it safe than sorry. In my business, the costs of
trusting the wrong person can be pretty severe. Because of the size of the deals I
make, and the time horizon over which they play out, I need to make really good
decisions up front”. For these individuals, who I classify as social vigilants, social
uncertainty is a problem. As one put it, “I always assume that people will try
to take advantage of any loophole they ﬁnd in a deal. You have to watch people
when they think you aren’t watching them – that’s the time, in fact, you have
to be most careful”. According to these individuals, it is critical to pay attention
to others, monitoring them closely for any evidence of lack of trustworthiness.
Interestingly, amongst both populations, there was considerable conﬁdence
expressed by executives as to the accuracy and adequacy of their views about
human nature. As one executive put it, representatively, “I consider myself a
pretty good judge of human nature”. Another expressed the view, “I seldom

<-----Page 5----->14

Roderick M. Kramer

make mistakes when sizing people up”. Again, “I usually get a feeling pretty
quickly about other people – about whether they can be trusted or not . . . [that
feeling] almost always turns out to be right”. Viewed in aggregate, all of these
quotes suggests how individuals who maintain fairly panglossian views about
human trustworthiness versus those entertaining more pessimistic and paranoid
views about such matters employ diﬀerent interpretation and action rules when
responding to social uncertainty. Yet, both groups are fairly conﬁdent their rules
help them “navigate” successfully through the shoals of the trust dilemmas they
confront in life.
Another approach to studying how our social auditor thinks and acts in the
face of uncertainty–and in some respects, a more systematic approach–is to use
computer simulation tournaments in which we “pit” diﬀerent interpretation and
action rule systems against each other. Using this approach, we can discover
which cognitive rule systems are relatively robust and which tend to get people
in trouble. In other words, we can learn something about which rule systems are
good at generating mutually productive and trusting relationship and eﬀective at
minimizing exploitation, versus which rule systems tend to get people in trouble
in terms of making mistakes about who to trust and how much. Thus, by allowing
diﬀerent rule systems to interact in a “noisy” (uncertain) social environment, we
can learn something about the comparative eﬃcacy of diﬀerent rules and also
the complex and often unintended interactions among them. The study I will
brieﬂy describe next, accordingly, took this approach.
Pragmatically, we can think about these matters from the standpoint of two
decisions that the social auditor has to make in a trust dilemma situation. The
ﬁrst decision pertains to how much trust or distrust to display toward the other
person at the very beginning of their relationship. The second pertains to how
much adjustment to make in the trust or distrust displayed on the basis of
feedback, however, uncertain or “noisy”, regarding the other’s actions.
To explore these questions from the perspective of decision makers possessing relatively panglossian versus paranoid rule systems, and to probe the consequences of these diﬀerent auditing orientations, the study employed a computer
simulation approach (see Bendor, Kramer, & Stout, 1991). The advantage of a
computer simulation is that a researcher can precisely specify the nature of the
dilemma (e.g., its “payoﬀ structure”), and the consequences (costs and beneﬁts)
associated with any given choice. In other words, one can systematically evaluate
how well diﬀerent strategies for coping with trust dilemmas fare. Less obviously,
a computer simulation also provides an indirect way for trust researchers to
study people’s beliefs about trust dilemmas because, in designing a rule system
for “playing” in a computer simulation tournament, designers have to decide
how others will respond to social uncertainty or “noise” (e.g., will they try to
exploit it by “sneaking around” under it or work to keep misunderstandings from
developing?).
The computer simulation we used was similar to that employed by Axelrod
(1984) in his pioneering studies on the evolution of cooperation. Recall that, using a tournament approach, Axelrod had individuals design strategies that would

<-----Page 6----->How Decision Makers Think and Act in the Shadow of Doubt

15

play other strategies in a classic iterated and deterministic (noiseless) Prisoner’s
Dilemma Game. Because we were interested in studying how social uncertainty
aﬀects decisions, however, the procedures we used diﬀered from Axelrod’s procedures in several important ways. First and foremost, we introduced uncertainty
or “noise” into the communication process between the interdependent social
actors. In the classic Prisoner’s Dilemma, after interactants decide whether they
wish to cooperate with the other, they learn with perfect clarity whether their
partners cooperated or not. As noted earlier, in trust dilemma situations, people
suﬀer from some degree of uncertainty regarding what their partners did.
Accordingly, in order to introduce social uncertainty into our tournament,
participants’ eﬀort levels were obscured by adding or subtracting a small random
amount of “noise” to their feedback during each period of play. This noise term
was a random variable, distributed normally, with a mean of zero and a standard
deviation of eight. The term was generated by a normal transformation of a
uniform random variable and the distribution of the error term was truncated
to the interval [−100, +100]. The noise was independent across participants and
over periods. Because of this noise factor, participants in our tournament (or
more precisely, the computer programs of the strategies they submitted) could
receive feedback that their partners (strategies) had behaved either more or less
cooperatively than they actually had. This allowed us to explore how decision
makers “adjusted” to the noise (e.g., did they adopt relatively panglossian or
paranoid views about the noise?).
Adjustments to the noise were manifested in terms of how much aid or help
decision makers wanted to extend to their partners. This was operationalized as
how much eﬀort they wanted to exert on their partner’s behalf, with zero eﬀort
being the least eﬀort extended and 100 percent eﬀort being the highest possible
level of eﬀort. In this way, we tried to capture the idea that people typically
make decisions about how much trust or distrust is warranted, especially in a
new relationship, where one possesses little prior experience with the partner.
A player’s payoﬀ or beneﬁt per interaction, therefore, was equal to the other
player’s actual eﬀort level minus a cost factor of one’s own eﬀort plus a random
error term. Stated more formally,
Vt (i/j) = Ct (j, i) − αCt (i, j) + εt (j, i)

(1)

where Vt (i/j) denotes i’s payoﬀ in period t in its pairing with j, Ct (j, i) denotes
j’s eﬀort level vis-a-vis i in period t, Ct (i, j) refers to i’s choice, α the cost of
helping associated with i’s choice and εt (j, i) symbolizes the disturbance added to
j’s choice. To ensure the game meets the requirements of a Prisoner’s Dilemma,
the cost-of-helping parameter must be between zero and one; accordingly, it was
ﬁxed at 0.80. With this structure, the symmetric average maximum payoﬀ per
period was 20 and the symmetric average minimum payoﬀ was 0. However, with
noise, realized payoﬀs could fall outside this region of feasible expected payoﬀs.
In each round of play, the computer generated two normally distributed error
terms to determine the values of εt (i, j) and εt (j, i). The computer also used a
ﬁxed random number which was used to determine whether a given sample path

<-----Page 7----->16

Roderick M. Kramer

would end after the current period of play (so that the duration of a relationship
was also uncertain). Each strategy ten played all other strategies, including itself,
using the generated error terms for each period.
Entrants to the tournament were fully informed of all of the parameters
described above, including details about payoﬀs, the distribution of the random
disturbances, and the stopping probability of the game. However, they were also
instructed that they (i.e., their strategies) would not be told the realized values
of the disturbances, their partner’s true eﬀort level, or the rules deﬁning their
partner’s strategy. They were then invited to submit strategies which would be
translated into computer programs. The computer programs would be pitted
against each other in pairwise play using a round robin tournament format.
From the standpoint of thinking about coping with social uncertainty, we
can think of the social auditor’s cognitive rule systems as having to confront
two decisions. The ﬁrst decision is how much presumptive trust in the other to
manifest initially. For example, should one extend full eﬀort to the other initially
in the hope that the other will reciprocate with full eﬀort? Extending full aid
initially will help sustain a mutually productive relationship with a similarly
inclined other. On the other hand, it opens one up to early exploitation if one
happens to encounter a more predatory other. Thus, given the inherent uncertainty in the situation, it shows a high level of presumptive trust in the other.
Alternatively, should one display some level of scepticism or wariness regarding
others willingness to reciprocate our trust in them, withholding aid to them until
we see what they are willing to do? Caution provides protection against initial
exploitation, but its risks damaging a relationship with some one who be equally
willing to extend full eﬀort to us.
The second decision is how to respond to feedback regarding the others’
actions–especially when one knows that such feedback is inherently uncertain
(contaminated by noise). Stated diﬀerently, how much sensitivity should one
display to indications that the other might be reciprocating fully or less than
fully given those indications are inherently ambiguous? How reactive should one
be, for example, to the hint or suspicion that the other might not be reciprocating
fully?
To explore these issues, we sought to recruit participants who were sophisticated decision makers (i.e., individuals would be intimately familiar with theory
and research on the Prisoner’s Dilemma). Thus, the participants included leading game theorists, economists, political scientists, and social psychologists. Note
that we can view the strategies designed by these experts as, in a sense, their
cognitive rule systems that reﬂect their “projections” about others’ orientation
towards such dilemmas. In other words, they represent their orientations, as
social auditors, to the dilemma in question.
Thirteen strategies were submitted to the tournament, embodying a variety
of diﬀerent intuitions regarding how decision maker should cope with social
uncertainty. For the purposes of the present chapter, I will compare and contrast
the performance of a strategy I call VIGILANT with that of a strategy I call

<-----Page 8----->How Decision Makers Think and Act in the Shadow of Doubt

17

PANGLOSSIAN. VIGILANT, to be described ﬁrst, is our relatively paranoid
social auditor.
In terms of the decision rules embodied in this strategy, VIGILANT was nice
in the Axelrodian sense of that term (i.e., it always started out by extending
maximum eﬀort to its partner). It was therefore willing to initially assume the
possibility that it was dealing with another strategy willing to reciprocate fully.
In the parlance of the intuitive auditor model, it was willing to show some presumptive trust toward the other. However, as its name suggests, it was extremely
attentive and reactive to any signs that its partners were not reciprocating fully.
In particular, if VIGILANT detected what it believed was less than full eﬀort
from any of its partners on any trial, it retaliated by completely withdrawing
eﬀort (i.e., giving 0 points to its partner) on the next trial. The aim of this extreme reactivity was, of course, to deter further acts of perceived exploitation.
Stated in psychological terms, VIGILANT was determined to minimize the costs
associated with misplaced trust.
Because VIGILANT was determined not to be exploited, it was biased, in
one sense, toward “over assuming” the worst whenever it received a low oﬀer of
eﬀort from its partner. In other words, whenever it received feedback that the
other had not oﬀered much, it acted as if its partner had intentionally trying
to short-change it. Presuming this interpretation to be valid, VIGILANT then
retaliated. Thus, VIGILANT always puts the least charitable “spin” on any
social news it receives about others’ actions. It thus goes through life assuming
the worst with respect to others, even though there is a clear and compelling
rival explanation for such outcomes (i.e., it’s a noisy world and sometimes one
will simply have bad luck when communicating with others).
There are diﬀerent ways of dissecting VIGILANT’s performance. If we start
with Table 1, we can see how VIGILANT fared against the other players. In
particular, if we ﬁrst compare the row payoﬀs in Table 1, which reﬂect how much
help VIGILANT received on average from the other players over the course of
its interactions, to the column payoﬀs, which reﬂect how much help VIGILANT
it oﬀered to these players, we observe a powerful pattern, viz., that VIGILANT
always got back more than it gave away. In terms of beneﬁts accrued versus
those given, VIGILANT walked away a clear “winner” in every one of its close
encounters. It outperformed every strategy in terms of comparative payoﬀs. In
other words, against an array of clever players, it came out ahead of the game
in every instance. VIGILANT seems to bat a 1000.
If we think about VIGILANT’S performance from the standpoint of our
social auditor model, we would have to conclude that here is a decision maker
who is likely to be rather satisﬁed with his performance in this world. He has,
after all, come upon what seems to be a very satisfactory, even powerful set
of cognitive rule. Being vigilant appears to payoﬀ handsomely; it consistently
elicits reliably good against the full panoply of players, and avoids the prospect
of misplaced trust. No one it meets does better than it does: it walks away from
every encounter taking out from more than it put into the relationship. It is
hard to imagine such a decision maker experiencing much cognitive dissonance

<-----Page 9----->18

Roderick M. Kramer
Table 1.
NICE DRIFT. BTFT MENN. W.A. TF2T Even TFT Norm. Run. Dev. CTTF VIG.
PANGLOSSIAN

19.8

12.0

19.5

19.5

18.5

18.2

19.3 17.4

18.4

14.0 17.1

13.0

14.9

DRIFTING

26.0

16.8

23.7

23.7

18.1

20.5

17.6 17.5

19.2

12.3 12.4

6.7

0.7

BTFT

20.0

13.5

19.7

19.7

18.7

18.5

19.4 17.5

18.5

13.0 15.9

12.6

3.1

MENNONITE

20.0

13.5

19.7

19.7

18.7

18.5

19.4 17.5

18.5

12.1 15.7

12.6

3.8

Wtd. Avg.

20.8

15.7

20.4

20.4

17.9

19.3

18.4 17.0

18.7

12.5 18.0

7.3

1.7

TF2T

21.0

15.1

20.5

20.5

18.7

18.9

19.0 17.1

18.3

10.4 13.7

10.6

-0.2

Staying Even

20.2

16.2

19.9

19.9

18.3

19.0

19.3 15.4

18.0

13.4 12.1

6.2

2.2

NAIVE REALIST

21.7

16.2

21.1

21.1

17.7

19.1

15.9 14.2

15.9

13.1 11.5

6.6

1.0

7.4

-1.4

Normal

20.9

15.2

20.3

20.3

18.3

18.6

18.1 14.8

16.4

11.3 11.9

Running Avg

17.0

11.5

18.0

19.1

14.8

18.3

14.3 14.4

17.0

12.8

6.2

10.4

Deviations

21.9

11.6

20.0

20.4

17.8

16.5

12.8 12.4

15.0

7.4

14.4

7.2

1.2

CHEATING TFT

25.1

7.8

22.7

22.7

12.3

17.4

8.7

9.4

11.4

13.7 10.9

5.8

-0.4

VIGILANT

23.6

3.2

9.9

9.0

9.1

9.8

5.2

6.5

10.7

13.9

10.7

2.6

8.3

9.0

– the rule system it is employing seems, in short to be working. It is possible to
imagine that VIGILANT can even feel rather virtuous about its values, given
its view of the world. It is, after all, nice (i.e., always willing to initially extend
full aid to everyone it meets). It’s also strong and aﬃrmative (i.e., unwilling to
tolerate exploitation or abuse). Yet, although tough, it is also forgiving: it is
willing to let bygones be bygones.
Let’s turn attention now to the performance of our second strategy, called
PANGLOSSIAN, which embodies the more benign view of the world associated
with the social optimists described earlier. In terms of its rule structure, PANGLOSSIAN, like VIGILANT, was nice in that it always began a relationship
by extending maximum possible aid to its partner. However, it diﬀered from
VIGILANT in several ways. First, PANGLOSSIAN tended to be generous, returning more eﬀort on a subsequent trial than it had received from its partner
on the previous trial. PANGLOSSIAN’s generosity took the form of what might
be construed as a form of benign indiﬀerence (as long as it partner’s observed
(realized) eﬀort level exceeded 80, PANGLOSSIAN would continue to help its
partner fully, i.e., give 100% of the possible points to it. Second, although PANGLOSSIAN was provocable and would retaliate if it’s partner’s aid dipped below
80, it reverted to extending full eﬀort to its partner again, as long as the partner
satisﬁed this threshold of acceptable behavior (i.e., its observed aid level was at
least 80). Thus, it could be provoked, but was forgiving. It was thus less reactive
than VIGILANT (slower to anger) and quicker to forgive.
If we compare how well PANGLOSSIAN did against the other players, we
observe another seemingly clear and compelling pattern. As is quite evident
from inspection of the results shown in Table 1, PANGLOSSIAN lost out in
its encounters with every player in the tournament. In other words, it always
gave away to its partners more than it got back from them. In terms of its
performance, then, PANGLOSSIAN seems to be a very poor judge of others
and clearly sub-optimally adapted to the world in which it ﬁnds itself (especially
compared to VIGILANT). It appears to be far too generous, consistently overbeneﬁtting its partners.
In terms of the logic and structure of the social auditor model, note that
PANGLOSSIAN tends to give others the beneﬁt of the doubt when it comes to

<-----Page 10----->How Decision Makers Think and Act in the Shadow of Doubt

19

drawing inferences about what low extended eﬀort from the other really means.
In particular, it attributes low returns to the environmental factor (bad luck with
respect to the noise) rather than necessarily assuming it was the intention of the
other player, as does VIGILANT. Thus, as long as it is getting at least 80%
of the possible payoﬀs, it tends to under-react to unfavorable “news” from its
environment. This under-reaction seems to be rather costly, however, allowing
it to be bested not only by more suspicious or wary players such as VIGILANT,
but in fact every player in the tournament.
At ﬁrst glance, therefore, the comparison of these two social auditing strategies and rule systems seems to suggest a rather compelling conclusion: in an
uncertain world, a certain amount of suspicion and wariness – coupled with a
willingness to react strongly to even ambiguous evidence of others’ untrustworthiness – can be deemed prudent and even beneﬁcial. Certainly, VIGILANT
appears to be less likely to be taken advantage of, whereas PANGLOSSIAN
seems to turn the other cheek too often, far and for too long.
If we inspect the tournament results from a diﬀerent vantage point, however,
the story told by these tournament data turns out to be less simple and, in fact,
quite a bit more interesting. In particular, if we compare the overall performance
of the strategies (i.e., examine their total earnings in the tournament by summing
their average payoﬀs across encounters with all strategies), we see a dramatically
diﬀerent picture (see Table 2).
Table 2.

As Table 2 shows, PANGLOSSIAN emerges as the tournament winner when
we use this standard of performance. PANGLOSSIAN was the most successful
performer, earning an average per trial payoﬀ of 17.05 points. Thus, the performance of PANGLOSSIAN outshines that of VIGILANT when looked at from
this perspective. Yet, recall, VIGILANT outperformed PANGLOSSIAN in their
“face-to-face” encounter with each other, and also every other player as well,
whereas PANGLOSSIAN lost out against every player!

<-----Page 11----->20

Roderick M. Kramer

How can VIGILANT do better in every encounter with every player it meets
in its world, and nonetheless end up losing the tournament, even coming in
“dead last”, while PANGLOSSIAN gets the “short end of the stick” in every
encounter in the same world and yet ends up accumulating more resources than
any other player in the tournament? This pattern of results – at least at ﬁrst
glance – seems paradoxical. The answer to the riddle can be found, however, by
re-visiting Table 1 and inspecting the absolute pay oﬀs each strategy obtains
from the other strategies, rather than just looking at its comparative (relative)
payoﬀs. In particular, if we compare the absolute means of PANGLOSSIAN’s
payoﬀs from its encounter with each player to those garnered by VIGILANT
we see that PANGLOSSIAN walks away from each relationship with rather
respectable payoﬀs. Indeed, many times it manages to elicit almost the maximum
possible beneﬁt even allowing for the presence of noise (approaching close to the
maximal average mean of 20).
VIGILANT, in contrast, earns much less in absolute terms from these same
encounters. It may walk away the winner from each encounter in its social world
when we think only in social comparative terms (i.e., how much it did relative
to its partner), but it lives a life of impoverished relationships and diminished
returns when we look at the absolute yield it gets in those relationships. In
contrast, PANGLOSSIAN may walk away from each relationship a bit worse
oﬀ in terms of the “giving-getting” equation, but it manages to extract a lot
of absolute net gain from each encounter. Thus, in a powerful way, these ﬁndings illustrate how diﬀerent cognitive rule systems, embodying diﬀerent auditing
orientations and principles, can produce very diﬀerent experiences in life. One
auditing orientation does well from the perspective of a myopic and narrow social accounting scheme that highlights proximate payoﬀs deﬁned at the level of
local social comparisons. The other does well from the perspective of a long-term
perspective that values absolute (noncomparative) payouts from relationships.
As an additional note, it is instructive to examine how well the strategy labeled in Tables 1 and 2 as a strict reciprocator called NAÏVE REALIST fared
in this tournament. NAÏVE REALIST adjusted to the noise factor by simply returning to its partner whatever it received from them on the previous trial. Since
the average value of the noise term was 0, it assumed on average this was a reasonable stance to take toward social uncertainty. NAÏVE REALIST is equivalent
to Axelrod’s TIT-FOR-TAT). Recall that Axelrod’s results had demonstrated
the power of strict reciprocators such as TIT-FOR-TAT in a deterministic game,
where decision makers have perfect (noiseless) information about a partner’s actions. Yet, in a tournament climate clouded by uncertainty, we found that this
strategy suﬀered a sharp decline—both absolutely and relative to other players.
Indeed, NAÏVE REALIST placed a distant eighth in the ﬁnal rankings, earning
only 75% of the maximum symmetric payout. Thus, in a world of uncertainty,
strict auditing and reciprocity apparently get one in trouble.
What accounts for this degraded performance of the strict reciprocator in a
world ﬁlled with social uncertainty? Why did a strategy like PANGLOSSIAN,
with its more benign assumptions, do better than the more realistic auditors?

<-----Page 12----->How Decision Makers Think and Act in the Shadow of Doubt

21

Part of the answer to such questions can be gleaned by observing how a strict
reciprocator such as NAÏVE REALIST behaves when it plays itself in the presence of uncertainty. Because NAÏVE REALIST is nice, it starts oﬀ by extend
full eﬀort to its partner. Sooner or later, however, an unlucky (“bad”) realization
of the random error term occurs. Because NAÏVE REALIST is provocable, it
will retaliate in the next period. Its partner, being similarly provocable, returns
the compliment, leading to cycles of counterproductive mutual punishment, resulting in steadily falling levels of eﬀort. In contrast, more generous strategies
such as PANGLOSSIAN slow down this degradation by returning more than an
unbiased estimate of their partner’s eﬀort level. This generosity tends to dampen
these cycles of unintended and costly vendettas.
The surprising performance of PANGLOSSIAN, especially when contrasted
with what might seem, at ﬁrst glance to be fairly vigilant and tough auditors
such as NAÏVE REALIST and VIGILANT, merits more sustained analysis. Why
does PANGLOSSIAN’s generosity tend to work better than strict reciprocity in
a noisy setting? Stated diﬀerently, why do rules systems that favor vigilance
and strict reciprocity fare so poorly in an uncertain world? PANGLOSSIAN
beneﬁts by its not being envious of what its partner gets. As Axelrod (1984)
had noted, envy can get decision makers in trouble if it sets oﬀ cycles of mutual discontent and retaliation. By setting an absolute (nonsocial comparative)
standard for what it considers a reasonable rate of return on its relationships,
PANGLOSSIAN does not pay any attention to what its partner gets. In contrast, VIGILANT displays a great deal of concern about such comparisons – at
least, it does not wish to grant full aid to others when it suspects it is not getting
full aid back from them in return. Over the long haul, it pays a steep price for
its insistence on never getting the “short end of the stick” in its relationships.
Note also that, by under-reacting to the noise, PANGLOSSIAN accomplishes
something else – something that is important but easy to overlook. It solves the
other actor’s trust dilemma. In other words, by keeping its aid levels high, its
partner does not have to decide whether or not to punish PANGLOSSIAN.
Recall that receiving any aid from the other above 80% is coded as justifying or
warranting returning full aid back to the other. This point is important because
in much of the trust dilemma literature, the focus is often on how social actors can
solve their own trust dilemma. Little attention has been given to the importance
of solving the other person’s trust dilemma. Yet, by shifting focus in this way,
we may buy ourselves something more powerful. In a sense, we make it easier
for them to be good to us by making trusting behavior easier to justify. We
reduce their perception of vulnerability and end up reducing our own as well.
This point is especially important because of the insidious eﬀects of self-serving
and self-enhancing judgmental biases in such dilemmas (Kramer, Newton, &
Pommerenke, 1993).

<-----Page 13----->22

4

Roderick M. Kramer

How the Intuitive Social Auditor Gets into Trouble:
People’s “Naı̈ve Theories” about the Eﬃcacy
of Diﬀerent Orientations and Rule Systems

As noted earlier in this chapter, most people articulate rules they use in trust
dilemma situations quite readily. For example, when executives are asked to
think about these situations in real-organizations, they are able to enumerate
a number of general “rules of thumb” governing the building of trust and/or
protecting themselves against the prospect of misplaced trust. Moreover, they
apparently have ﬁrm ideas about how others think about social conduct and
how they are likely to behave as well. In short, people seem to easily assume
the role of both philosopher and psychologist: they ponder about both what is
right (the social decision heuristics one ought to use in a given situation) as well
as what is eﬀective (the heuristic that is likely to produce the “best” outcome
in that situation). But how well do such beliefs and intuitions correspond to
the empirical conclusions suggested by our noisy tournament? Recall that the
computer simulation study just described, we had recruited leading game theorists, psychologists, economists, and political scientists. These were very smart
people and people able to think deeply and with nuance about the strategic and
complex interactive properties of these situations. Yet, even with this elite sample, often their strategies did not perform well against even their simple clone.
Thus, even a fairly simple ﬁrst-order mental simulation of strategic interaction
might have been diﬃcult (although it is possible that they recognized that, even
though their strategies might not well against each other, they would do well
against the other strategies submitted in the tournament!).
Accordingly, to explore decision makers’ intuitions or “naı̈ve theories” about
the fairness and eﬃcacy of diﬀerent heuristics, we investigated MBA students’
beliefs about PANGLOSSIAN, VIGILANT, and NAÏVE REALIST. We gave
students enrolled in an elective course complete information about all of the
parameters for our noisy computer tournament. In fact, their information was
identical to the information given to the entrants in the original computer simulation study. We then asked them to predict how well the three strategies (NAÏVE
REALIST, PANGLOSSIAN, and VIGILANT) would do, i.e., to predict the average payoﬀ of each entry. We also asked students in a second class to predict
the relative performance or ranking of the three strategies.
As can be seen from Table 3, the results suggest that students’ intuitions
about the comparative eﬃcacy of these three strategies closely parallel the ﬁndings of Axelrod’s original tournament. They clearly expected TFT to be the most
eﬃcacious in terms of their expectations about both absolute and relative performance. VIGILANT was also viewed as a fairly eﬀective strategy. Signiﬁcantly,
PANGLOSSIAN ran a distant third in student’s expectations and predictions.
To probe further people’s intuitions about these rule systems – and how well
they help or hinder one’s success in life – I asked participants in an executive
program to write short paragraphs describing their beliefs about how strategies
would behave and perform. One participant noted about PANGLOSSIAN that,

<-----Page 14----->How Decision Makers Think and Act in the Shadow of Doubt

23

Table 3. Predicted Performance of Heuristics∗

“If you’re generous, you probably will get screwed [out of your payoﬀs] too much.
You’d be out of business in no time”. In commenting on the attractiveness of
NAÏVE REALIST, in contrast, another wrote, “The nice thing about NAÏVE
REALIST is that it keeps the playing ﬁeld level – and that’s important over the
long term”. “By far, NAÏVE REALIST is the fairest strategy”. And in thinking
about the advantages of VIGILANT, a participant stated, “In many situations
. . . you’ve got to cover your back side”. As former CEO of Intel, Andrew Grove
likes to remind his employees, “Only the paranoid survive”.

5

Implications and Conclusions

Viewed together, the results from our examination of people’s lay theories and
the results of the computer simulation indicate there is sometimes a disparity
between people’s beliefs about the eﬃcacy of diﬀerent auditing orientations and
rule systems, and their actual performance (at least in some ecologies). One
reason why people’s beliefs may be at least partly wrong may be that they
worry more about (and therefore overweight) the possibility of being exploited,
while under appreciating the mutual gains produced by generosity. Comparing
the long-term performances of PANGLOSSIAN and VIGILANT illustrates how
complex and sometimes counter-intuitive are the tradeoﬀs between short-term
and long-term results.
An important question for future research to engage is, “When are auditing
and rule systems predicated on generosity more psychologically appealing and
sustainable than those predicated on paranoia and “strict” reciprocity?” It seems
reasonable to argue, on prima facie grounds, that a number of factors contribute
to the attractiveness of generosity and lessen the perceived need for the strict
auditing and balancing-of-accounts required by VIGILANT and NAÏVE REALIST. First, a shared identity among social actors may help. Decision makers are
probably more willing to behave generously when interacting with others with

<-----Page 15----->24

Roderick M. Kramer

whom they share a social tie or bond. For example, when decision makers from
the same neighborhood meet face to face, they may trust each other more and
cooperate more reliably than they would otherwise (cf., Ostrom, 1998). Further,
common identiﬁcation with an ongong partner along some salient dimension,
such as a common group identity, may aﬀect which strategy is viewed as most
fair (Kramer, Pommerenke, & Newton 1993) and/or most eﬀective (Rothbart
& Hallmark, 1988). Research on moral exclusion (Opotow, 1990) suggests that
individuals impute higher standards of fairness to those whose group identity
they share. Consistent with Opotow’s proposition, Kramer et al. (1993) found
that increasing the salience of shared identity enhanced concerns about equality
of outcomes.
Taking the other party’s perspective may also inﬂuence individuals’ willingness to be generous in dilemma-like situations. Arriaga and Rusbult (1998) have
found that the experience of “standing in a partner’s shoes” increases constructive responses to the accomodative dilemmas they jointly experience, at least
in close personal relationships. In line with this hypothesis, it is interesting to
note that students of the Cuban Missile Crisis have pointed out the restraint
President Kennedy showed during that Crisis. When choosing to under-react to
some Soviet action that seemed to call for a Tit-for-Tat-like response, President
Kennedy tried to put himself in Khruschev’s shoes. He sought to understand how
the situation in Cuba might look to Khruschev, to grasp the kinds of political
pressures and institutional momentum driving his decisions (Garthoﬀ, 1989).
Due to the actor-observer bias, individuals are likely to construe their partner’s noisy realizations in dispositional terms, while underestimating the impact
of the noise as a situational factor. In contrast, they are likely to assume that
their partner properly understands the situational constraints and liabilities under which they labor (cf., Jervis, 1976).
In putting these results in perspective, it is important to note that it is easy
to overweight the adaptive value of panglossian world views, just as it would be
easy to underweight the functional advantages of paranoid views (cf., Kramer,
1998). The virtues of generosity emerge only in a world in which there are enough
other strategies to make generosity pay (and stinginess hurt). In nastier ecologies, where the costs of misplaced trust are steep, being very generous may
put one on the path to peril. In nastier ecologies, never walking away the loser
may be important for establishing reputational deterrence – leading to longer
cumulative gains than nicer, more generous strategies that invite exploitation
and attract predators. Thus, the wisdom of turning the other cheek depends
ultimately on the distribution of dispositions in one’s social world. Accepting
the fundamentally contingent nature of adaptive auditing and action in trust
dilemma situations may be psychologically unpleasant but necessary.

References
1. Allison, S. & Messick, D., (1990), Social decision heuristics in the use of share
resources, Journal of Behavioral Decision Making, 3, 195-204, 1990.

<-----Page 16----->How Decision Makers Think and Act in the Shadow of Doubt

25

2. Arriaga, X. B., & Rusbult, (1998), C. E., Standing in my partner’s shoes: Partner
perspective taking and reactions to accommodative dilemmas, Personality and Social
Psychology Bulletin, 24, 927-948.
3. Axelrod, R., (1980a), Eﬀective choice in the prisoner’s dilemma, Journal of Conﬂict
Resolution, 24, 3-25.
4. Axelrod, R. (1980b), More eﬀective choice in the prisoner’s dilemma, Journal of
Conﬂict Resolution, 24, 379-403.
5. Axelrod, R. (1984), The evolution of cooperation, New York: Basic Books.
6. Axelrod, R. & Dion, D. (1988), The further evolution of cooperation, Science, 242.
1385-1390.
7. Baron, J. (1996), Do no harm, In D. M. Messick & A. E. Tenburnsel (Eds.), Codes
of conduct: Behavioral research into business ethics, pp. 197-214. New York: Russell
Sage Foundation.
8. Bazerman, M. (1986), Judgment in managerial decision making. New York: Wiley.
9. Bazerman, M. H. (1994), Judgment in managerial decision making. New York:
John Wiley.
10. Bendor, J. (1987), In good times and bad: Reciprocity in an uncertain world, American Journal of Political Science, 31, 531-558.
11. Bendor, J., Kramer, R. M., & Stout, S. (1991), When in doubt: Cooperation in the
noisy prisoner’s dilemma. Journal of Conﬂict Resolution, 35, 691-719.
12. Bendor, J., Kramer, R. M., & Swistak, P. (1996), Cooperation under uncertainty
(Comment on Kollock), American Sociological Review, 61, 333-338.
13. Cialdini, R. (1993), Inﬂuence: Science and Practice, New York: Harper-Collins.
14. Garthoﬀ, R. (1989), Reﬂections on the Cuban Missle Crisis, Washington, D.C.:
The Brookings Institution.
15. Gigerenzer, G., & Too, P. M. (1999), Simple heuristics that make us smart, Oxford,
England: Oxford University Press.
16. Jervis, R. (1976), Perception and misperception in international politics, Princeton,
NJ: Princeton University Press.
17. Kramer, R. M. (1996), Divergent realities and convergent disappointments in the
hierarchic relation: The intuitive auditor at work, In R. M. Kramer and T. R. Tyler
(Eds.), Trust in organizations: Frontiers of Theory and Research. Thousand Oaks:
Sage Publications.
18. Kramer, R. M. (1998), Paranoid cognition in social systems, Personality and Social
Psychology Review, 2, 251-275.
19. Kramer, R. M. (2001), Trust and distrust as adaptive orientations in trust dilemmas, Unpublished manuscript.
20. Kramer, R. M. (1999), Trust and distrust in organizations: Emerging perspectives,
enduring questions, Annual Review of Psychology, 50, 569-598.
21. Kramer, R., Meyerson, D. & Davis, G. (1990), How much is enough? Psychological
components of ‘guns versus butter’ decisions in a security dilemma, Journal of
Personality and Social Psychology, 58, 984-993.
22. Kramer, R., Newton, E., & Pommerenke, P. (1993), Self-enhancement biases and
negotiator judgment: Eﬀects of self-esteem and mood, Organizational Behavior and
Human Decision Processes, 56, 110-133.
23. Kramer, R., Pommerenke, P., & Newton, E. (1993), The social context of negotiation: Eﬀects of social identity and accountability on negotiator judgment and
decision making, Journal of Conﬂict Resolution, 37, 633-654.
24. March, J. G., Schulz, M, & Zhou, X. (2000), The dynamics of rules: Change in
written organizational codes, Stanford, CA: Stanford University Press.

<-----Page 17----->26

Roderick M. Kramer

25. Molander, P. (1985), The optimal level of generosity in a selﬁsh, uncertain environment, Journal of Conﬂict Resolution, 29, 611-618.
26. Novak, M. & Sigmund, K. (1992), Tit-for-Tat in heterogeneous populations Nature,
355, 250-253.
27. Novak, M. & Sigmund, K. (1993), A strategy of win-stay, lose-shift that outperforms
tit-for-tat in the prisoner’s dilemma game, Nature, 364, 56-58.
28. Opotow, S. (1990), Deterring moral exclusion, Journal of Social Issues. 46, 173-182.
29. Ostrom, E. (1998), A behavioral approach to the rational choice theory of collective
action, Presidential address, American Political Science Association, 1997. American Political Science Review, 92, 1-22.
30. Ross, L. (1977), The intuitive psychologist and his shortcomings, In L. Berkowitz
(Ed.). Advances in experimental social psychology, 10. New York: Academic Press.
31. Thompson, L. (1998), The mind and heart of the negotiator, Upper Saddle River,
NJ: Prentice Hall.
32. Watson, J. (1980), The double helix: A personal account of the discovery of the
structure of DNA, (Norton Critical Edition). New York: W. W. Norton.
33. Wilson, J. Q. (1993), The moral sense, New York: Free Press.
34. Wu, J., & Axelrod, R. (1997), Coping with noise: How to cope with noise in the
iterated prisoner’s dilemma, In R. Axelrod (Ed.), The complexities of cooperation.

