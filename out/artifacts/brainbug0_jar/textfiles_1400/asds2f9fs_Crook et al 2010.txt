<-----Page 0----->Organizational Research Methods
http://orm.sagepub.com

Are We There Yet?: An Assessment of Research Design and Construct
Measurement Practices in Entrepreneurship Research
T. Russell Crook, Christopher L. Shook, M. Lane Morris and Timothy M. Madden
Organizational Research Methods 2010; 13; 192 originally published online Apr 10, 2009;
DOI: 10.1177/1094428109334368
The online version of this article can be found at:
http://orm.sagepub.com/cgi/content/abstract/13/1/192

Published by:
http://www.sagepublications.com

On behalf of:

The Research Methods Division of The Academy of Management

Additional services and information for Organizational Research Methods can be found at:
Email Alerts: http://orm.sagepub.com/cgi/alerts
Subscriptions: http://orm.sagepub.com/subscriptions
Reprints: http://www.sagepub.com/journalsReprints.nav
Permissions: http://www.sagepub.com/journalsPermissions.nav
Citations http://orm.sagepub.com/cgi/content/refs/13/1/192

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 1----->Are We There Yet?
An Assessment of Research Design and
Construct Measurement Practices in
Entrepreneurship Research

Organizational
Research Methods
Volume 13 Number 1
January 2010 192-206
# 2010 SAGE Publications
10.1177/1094428109334368
http://orm.sagepub.com
hosted at
http://online.sagepub.com

T. Russell Crook
University of Tennessee

Christopher L. Shook
Auburn University

M. Lane Morris
University of Tennessee

Timothy M. Madden
University of Tennessee

Research design is a central element of empirical research, and thus, an important
consideration for entrepreneurship researchers and anyone interested in entrepreneurshiprelated research findings. Yet, many years have past since the last thorough review of
research design and construct measurement practices. Thus, it is unknown whether there is
a gap between what is currently being done versus what needs to be done. In this article,
authors use a two-study approach involving a content analysis of published empirical
research and a survey of experts within the field to assess the current state of practices.
Their findings indicate that, in general, research design and construct measurement
practices continue to improve; however, there are some issues that still need to be resolved.
Authors lay out key implications and provide several suggestions to help resolve these issues.

T

he field of entrepreneurship and entrepreneurship-related inquiry are growing
rapidly (Dean, Shook, & Payne, 2007; Shane & Venkataraman, 2000). In the last 10
years, the Entrepreneurship Division of the Academy of Management has grown from
917 affiliated Academy of Management members to 2370—more than a 155% increase.
Although its growth trajectory has been impressive, the field has been the target of
criticism, and some scholars have claimed that the field still lacks a ‘‘respected and welldeveloped voice’’ (Busenitz et al., 2003, p. 303). At the heart of this criticism is the supposition that the field has not approached empirical research with the same rigor as other fields
(Low, 2001).
Five years ago, Busenitz et al. (2003, p. 304) claimed that the entrepreneurship field was
young and moving ‘‘through its emergent stage.’’ This stage, where research is at an early
phase of development, can be an awkward one (Low, 2001). Indeed, the emergent stage is a
time when researchers try to define a field’s boundaries via discourse and eventually build
Authors’ Note: We are grateful for the help of Paul Harvey. Please address correspondence to T. Russell Crook,
College of Business Administration, University of Tennessee, Knoxville, TN 37996; e-mail: trc@utk.edu.
192
Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 2----->Crook et al. / Research Design and Construct Measurement

193

consensus around the domain of study as well as the type of research that fits within the
scope of the domain (Busenitz et al., 2003; Summer et al., 1990). Building such consensus
is critical to exiting the emergent stage and laying a field’s foundation (Kuhn, 1962).
Within entrepreneurship, the domain of study—opportunity identification, creation, and
exploitation—has become more firmly established and has laid a solid foundation for the
field’s growth and continued development (Ireland, Webb, & Coombs, 2005; Shane & Venkataraman, 2000). While establishing the domain is clearly a critical developmental phase,
perhaps the next important step is developing a ‘‘capability to probe interesting and important issues from a solid foundation’’ (Busenitz et al., 2003, p. 303). A key way to develop
this capability is to ensure researchers are probing issues in a rigorous way. Research design
and construct measurement are central considerations when conducting rigorous empirical
research (Cohen, Cohen, West, & Aiken, 2003; Kerlinger & Lee, 2000). Construct measurement is the foundation of quality empirical research because research findings can be called
into question if the underlying constructs were not measured properly (Kerlinger & Lee,
2000). Chandler and Lyon (2001) evaluated research design and construct measurement
practices during the time period from 1989 to 1999. In their review, they found lessthan-optimal practices. More specifically, they found ‘‘much of the work done in the mainstream entrepreneurship literature remains relatively unsophisticated’’ (Chandler & Lyon,
2001, p. 110). They concluded by asserting that researchers ‘‘still have some distance to
go if [they] are to advance the field to the point where we can identify with some confidence
and make normative recommendations regarding the exact nature of the varied and complex
relationships studied under the umbrella of entrepreneurship research’’ (Chandler & Lyon,
2001, p. 112).
Several years have passed since Chandler and Lyon (2001) offered their assessment and
an agenda for improved rigor. To this end, the answer to the question of whether the field
has ‘‘gone the distance’’ or whether these unresolved issues continue to hold back efforts to
rigorously test theories and to advance the field appears important. Our study has two key
objectives. Our first objective is to assess the state of current research design and construct
measurement practices in entrepreneurship and to assess whether the field is progressing.
We did this via a two-study design. The first study involved a content analysis drawing
on Chandler and Lyon’s (2001) framework. The second study involved a survey sampling
expert entrepreneurship researchers. Thus, our assessment takes a step toward shedding
light onto whether current practices leave doubt about whether researchers have rigorously
answered research questions to date and whether current practices are holding back efforts
to accumulate knowledge within the field. Although our assessment reveals that substantial
progress is being made, there is still some distance to go. Given this, our second objective is
to offer ways for entrepreneurship researchers to continue moving forward.

Study One: Content Analysis of Empirical Studies
To assess the current state of research design and construct measurement practices, we
content-analyzed empirical entrepreneurship articles from 2000 through 2002 and 2005
through 2007. We chose these time periods because we were interested not only in assessing
current practices but also in assessing whether progress has been made. Our analysis drew

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 3----->194 Organizational Research Methods

from the same journals as Chandler and Lyon (2001). We coded all empirical articles during
these time periods from the Journal of Business Venturing (JBV-115) and Entrepreneurship
Theory and Practice (ETP-67). Then, we conducted a keyword search using the terms ‘‘new
ventures,’’ ‘‘entrepreneurship,’’ ‘‘initial public offerings,’’ and ‘‘family business’’ for articles in the other leading journals (see, Ireland, Reutzel, & Webb, 2005). This search yielded
additional empirical articles from the Academy of Management Journal (AMJ-12), Administrative Science Quarterly (ASQ-4), Journal of Management (JOM-3), Management Science (MS-13), Organization Science (OS-4), and Strategic Management Journal (SMJ20). Overall, our assessment includes 238 articles involving 242 samples.

Coding
To classify the articles in terms of research design and construct measurement practices,
we coded a calibration sample of 50 articles. After two coauthors independently coded 50
articles with an inter-rater reliability of 89%, we resolved each difference and then coded an
additional 10 articles. At that point, we achieved an inter-rater reliability of 99%. From that
point, the remaining articles were coded individually by a coder. Because of our interest in
evaluating the same practices as Chandler and Lyon (2001), our coding sheet captured
information about each practice.1 Once we coded the articles, we summarized the raw numbers and then used them to compute periodic use indices (PUIs—Stone-Romero, Weaver, &
Glenar, 1995) where possible. PUIs function like percentages; they reflect the frequency of
a practice divided by the number of relevant articles during the time period. As an example,
a PUI of .22 would suggest that 22% of the articles assessed during a particular time period
contained a research design practice. Using PUIs allows us to assess how each practice has
changed over time. We created ratios to assess practices when PUIs were not feasible. We
used PUIs and ratios because they provide more information than raw numbers; PUIs and
ratios contain two pieces of information (i.e., one raw number relative to another) and thus,
provide an advantage over raw numbers because they allow a multifaceted perspective
(Boyd, Gove, & Hitt, 2005a). For PUIs, we conducted z tests for the equality between two
proportions to examine changes over time. For ratios, we conducted mean difference tests.

Results
Data Source and Respondents
The results of our study are outlined in Table 1. As noted earlier, an important element of
research design is the nature of the sample. Across all the years we examined (2000-2002,
2005-2007), we found that 124 (51%) articles relied solely on primary data and most (i.e.,
106) of these were surveys; 98 articles (40%) relied solely on secondary data (e.g., archival
databases); and 17 articles (7%) used both primary and secondary data. As shown in Table
1, for the time period 2000 to 2002 (hereafter called early period), the PUIs for primary
only, secondary only, and mixed data sources were .63, .31, and .05, respectively. For the
time period 2005 to 2007 (hereafter called later period), the PUIs for those data sources
were .42, .47, and .09. The z test results indicate a significant increase in the use of

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 4----->Crook et al. / Research Design and Construct Measurement

195

Table 1
Content Analysis Results6
103 Samples; Early
Count/Ratio
Data source
Primary only
Survey only
Survey component
Interviews
Experiment
Secondary only
Secondary component
Mixed primary and secondary
Reliability
Studies that report all reliabilities
Ratio of studies with all reliabilities > .7
to studies lacking reliabilities > .7
Average scale reliabilities
Average inter-respondent reliabilities
Validity
Average number of measures with high
content validity per study
Average number of measures without
high content validity per study
Ratio of highly to not highly content
valid measures per study
Average number of measures justified
per study
Average number of measures not justified per study
Ratio of justified to not justified measures per study
Correlation matrices
Number of studies that reported a matrix
Number of studies that did not report a
matrix
Ratio of studies that reported versus did not
report a matrix
Ratio of studies that reported a full
matrix to studies that did not report a full
matrix
Studies that explicitly used EFA or CFA
IVs statistically related to DVs
IVs not statistically related to DVs
Ratio of related to unrelated
Studies that assessed nonresponse bias
Country from which sample was drawn
United States
Other (non-U.S.)
United States and/or Other

PUIs

139 Samples; Later
Count/Ratio

PUIs

Significance
p < .01
p < .01
p < .01
ns
ns
p < .05
p < .01
ns

65
57
63
4
2
32
37
5

.63
.55
.61
.04
.02
.31
.36
.05

59
49
59
3
5
66
77
12

.42
.35
.42
.02
.04
.47
.55
.09

39
1.69

.62

44
2.29

.75

.79
.80

.79
.81

8.46

9.92

1.73

.84

4.89

11.79

9.10

10.40

2.10

1.20

4.33

8.67

72
31

.70
.30

2.29

108
31

p < .01

p < .01

p < .01

.78
.22

3.48

55

.53

76

11
664
757
.87
45

.11
6.45
7.35
.79

4
701
663
1.05
38

57
38
8

.55
.37
.08

86
49
4

ns
ns
p < .01

.55

ns

.03
5.04
4.77

p < .05

.78

p < .01
ns

.62
.35
.03

ns
ns
ns
(continued)

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 5----->196 Organizational Research Methods

Table 1 (continued)
103 Samples; Early
Count/Ratio
Level of analysis
Individual
Group
Firm
Other
Model sophistication
Number of studies examining moderating
effects
Number of studies examining mediating
effects
Ratio of moderation or mediation studies
to just direct effects
Average number of control variables
Time dimension
Cross sectional
Longitudinal
Did not report

PUIs

139 Samples; Later
Count/Ratio

PUIs

Significance

31
3
60
9

.30
.03
.59
.09

32
6
77
24

.23
.04
.55
.17

ns
ns
ns
ns

15

.15

40

.29

p < .01

7

.07

10

.07

ns

.21

.36

p < .01

2.7

4.8

p < .01

66
19
18

.64
.18
.17

85
54
3

.61
.39
.02

ns
p < .01
p < .01

Note: CFA ¼ confirmatory factor analysis; DV ¼ dependent variables; EFA ¼ exploratory factor analysis;
IV ¼ independent variables; PUIs ¼ periodic use indices.

secondary data sources (p < .01) and a significant decrease in the use of primary data
sources (p < .01).
For studies reliant on survey data, the average response rate was 36%; 27% of the respondents were owners, 30% were managers, 7% were students, and 36% were mixed/other. The
response rates for the early and later periods were 36% and 35%, respectively, with no significant difference between periods in regards to the nature of respondents. This suggests
that response rates and the types of respondents have remained stable across time.

Reliability
Reliability refers to the degree that a measure is free from measurement error and, thus, is
inversely related to measurement error (Kerlinger & Lee, 2000). Reliability is indicated by
the consistency of a score derived from a measurement scheme (Schwab, 1999). Such consistency can be demonstrated within multiple measures of the same item (internal reliability), across multiple respondents (inter-rater or inter-observer reliability), across pools of
multiple measures of the same item (parallel forms reliability), and across time (test–retest
reliability). Reliability is critical to entrepreneurship research because unless entrepreneurship researchers can depend on the results of the measurement of their variables, they cannot with any confidence determine the relationships among their variables (Kerlinger &
Lee, 2000).
Like Chandler and Lyon (2001), we assessed whether authors established reliability.
Reliability should be established when authors rely on multiple item scales and when

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 6----->Crook et al. / Research Design and Construct Measurement

197

multiple persons respond to a survey or are involved in coding (i.e., inter-respondent or
inter-rater reliability). For studies relying on scales, 39 and 44 studies reported reliabilities
for all available scales for the early and later periods; the PUIs are .62 and .75, respectively,
and the ratio of studies that reported all acceptable reliabilities increased from 1.69 to 2.29
(p < .01). The average reliabilities were .79 in both time periods, which is considered acceptable (Nunnally, 1978). Encouragingly, only four early and four later studies had average
reliabilities outside the acceptable range. The average inter-respondent reliabilities were
extremely satisfactory at .80 and .81 for the early and later periods. By contrast, Chandler
and Lyon (2001, p. 105) found that ‘‘53% of studies examined reported acceptable reliabilities (i.e., >.7) for 75% or more of their measures. Twenty-nine percentage of the studies
reported coefficient alphas of less than .70 for more than 25% of their measures.’’ Overall,
we found that the average inter-respondent reliabilities were very satisfactory at .80 and .81
for the early and later periods.

Validity
Validity refers to the establishment of evidence that a measure depicts the intended construct (Kerlinger & Lee, 2000). Although there are many types and conceptualizations of
validity (Schriesheim, Cogliser, Scandura, Lankau, & Powers, 1999), Chandler and Lyon
(2001) suggest there are four important types of validity to entrepreneurship researchers:
content validity, substantive validity, structural validity, and external validity. Content
validity refers to the representativeness or sampling adequacy of the content of the measuring instrument. Substantive validity can be defined as the extent to which that measure is
judged to be reflective of, or theoretically linked to, the construct under investigation (Holden & Jackson, 1979). Structural validity refers to whether or not the structure (e.g., unidimensional or multidimensional) of the measured construct matches the structure of the
theoretical construct (Loevinger, 1957). Finally, external construct validity refers to
whether the test or measure is related to external constructs in the theorized way (Messick,
1988) and whether the findings from the sample are generalizable (Kerlinger & Lee, 2000).
Following Chandler and Lyon (2001), we use the terms substantive and external validity in
place of construct and concurrent and predictive validity, respectively. In addition, structural validity falls under the broader realm of construct validity.
We examined the studies for several aspects of validity. Like Chandler and Lyon (2001),
some of these aspects were assessed indirectly. Furthermore, some statistical analyses may
be used to provide evidence of multiple types of validity (e.g., full correlation matrices, factor analysis). As a means of assessing content and substantive validity, we assessed whether
each measure depicted the construct of interest.2 For example, if an article used experience
measures as proxies for strategically valuable resources, which reflect resources that are
valuable, rare, and difficult to imitate or substitute (Barney, 1991), we coded such measures
as measures as having high content validity. However, if an article used research and development intensity (i.e., research and development expenditures divided by sales) as a proxy
for a strategically valuable resource, we coded such measures as measures without high
content validity because such measures do not appear to fully meet the standards for being
‘‘strategically valuable’’ (Rouse & Daellenbach, 1999). Overall, the results suggest that
articles contain more highly valid than not highly valid measures per study (i.e., 8.46 to

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 7----->198 Organizational Research Methods

1.73 for early and 9.92 to .84 for later studies). Indeed, the ratio increased from 4.89 to
11.79, confirming that authors have substantially improved their use of valid measures over
time (p < .01). More broadly, we found that 89% of the measures contained content validity,
whereas Chandler and Lyon (2001, p. 106) found that ‘‘91% of the empirical studies they
reviewed had reasonable content validity for the major constructs.’’ We also coded whether
the authors justified the use of each measure. As an example, if an author stated that they
included a measure of firm size because it can affect firm performance, then we coded that
the author justified their measure. If an author said we controlled for size, but did not offer a
reason, we coded that the author did not justify their measure. The results suggest that
authors took more care in justifying their measures in the later period (10.4 vs. 9.1 for
early); the ratio of the number of measures that were justified to measures that were not
justified increased from 4.33 to 8.67 (p < .01).
Given that a full correlation matrix is essential to understanding the relationships among
variables, we coded whether authors reported a full correlation matrix. We found that 180
(74%) studies reported a matrix but that only 131 (54%) studies reported a full matrix.
Although there was an increase in the number of studies that reported matrices in the later
period (108 [PUI: .78] vs. 72 [PUI: .70] for early—p < .01), there was not a significant difference in the number of studies that reported full matrices.
As indirect indicators of substantive and external validity, we coded the number of independent-to-dependent variables in each study that were statistically related (Chandler &
Lyon, 2001). We found 664 and 701 statistically related independent-to-dependent variables for the early and later periods, respectively; the ratios of statistically related to unrelated variables for the periods were .87 and 1.05, a significant increase over time (p < .01).
In addition to the assessments that Chandler and Lyon (2001) conducted, we also examined
some other indicators of validity. We assessed whether articles assessed nonresponse bias.
Fortunately, most survey studies assessed such bias. Overall, 83 studies assessed nonresponse bias, and these assessments did not change over time as the PUIs for the early and
later periods were .79 and .78, respectively. As a means to examine generalizability and
external validity, we also assessed the country from which the sample was drawn. We found
that 143 (59%) used samples from the United States, 87 (36%) were samples drawn from
other countries, and 12 (5%) were drawn from both United States and non-U.S. countries.
There were no significant differences between the early and the later periods.

Level of Analysis
Like Chandler and Lyon (2001), we also assessed which level of analysis was investigated in each study. Encouragingly, however, we found that authors were clear about specifying the level of analysis. For both the early and the later periods, the majority of the
studies were conducted at the individual levels (31 [PUI: .30] for early and 32 [PUI: .23]
for later) and firm levels (60 [PUI: .59] for early and 77 [PUI: .55] for later). There were
a limited number of studies conducted at the group level, but no significant change over
time. These numbers were roughly the same as those in Chandler and Lyon (2001), who
found that 35% and 53% of their studies investigated individual levels and firm levels of
analysis. In addition, we found an increase in the number of ‘‘other’’ studies (9 vs. 24 with
PUIs of .09 and .17) that included cross-level research (i.e., 4 vs. 13) and levels other than

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 8----->Crook et al. / Research Design and Construct Measurement

199

individual, firm, and groups (e.g., patents). This suggests that most entrepreneurship
researchers continue to focus on the same levels of analysis, however, that some have also
begun to conduct more cross-level research.

Analytical Techniques and Model Sophistication
Dean et al. (2007) recently assessed analytical techniques and found that entrepreneurship research relies on the use of increasingly sophisticated data analytical techniques.
Because of the recent assessment by Dean et al., we did not replicate Chandler and Lyon’s
(2001) analysis with regard to specific data analytical techniques, but we did assess model
sophistication in two distinct ways. First, we coded whether the study tested moderation or
mediation effects. We found that a total of 55 (23%) studies tested moderation and 17 (7%)
tested mediation. There was a significant increase in the number of studies testing moderation between the early and the later periods (PUIs of .15 vs. .29, p < .01). The number of
studies in the later period is also substantially larger than what Chandler and Lyon
(2001) found. They reported that 18% of the ‘‘empirical studies [they] reviewed presented
and tested contingency frameworks’’ (p. 108). Second, we coded for the number of control
variables per study. The results indicate that researchers used significantly more control
variables per study in the later period (4.8 vs. 2.7, p < .01). Overall, these results are
encouraging as they indicate that the field is applying increasingly sophisticated models
while controlling for potentially important variables that might affect outcomes.

Time Dimension: Cross Sectional Versus Longitudinal Studies
Finally, we assessed the time dimension of the studies under investigation. We found that
the number of cross-sectional studies remained relatively constant across time (66 and 85
with PUIs of .64 vs. .61, ns), but that the number of longitudinal studies increased (19 and
54 with PUIs of .18 vs. .39, p < .01). Of course, the increase in longitudinal studies might be
an artifact because more studies in the early period did not clearly specify whether the data
were cross sectional or longitudinal. When these results are juxtaposed with Chandler and
Lyon’s (2001), who found that only 7% of studies used longitudinal designs, it is clear that
more researchers are using longitudinal designs.

Study Two: Survey of Entrepreneurship Researchers
To complement the content analysis findings, we surveyed expert entrepreneurship
researchers to get a sense of their opinions about various practices within the field. We considered researchers with editorial responsibilities (i.e., editors, associate editors, and board
members) at JBV or ETP as experts in the field (Dean et al., 2007). Twenty-two researchers
had editorial responsibilities at both journals. Overall, there were 147 researchers that we considered the ‘‘population’’ of experts, and we sent them personalized emails to solicit input.
We developed an online survey that assessed respondents’ opinions about the importance
of certain research design practices (e.g., reliability assessment) as well as their opinion
about whether the field has ‘‘closed the distance’’ between what researchers should be doing

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 9----->200 Organizational Research Methods

versus what they are doing in published high-quality research. In particular, we asked two
questions about each practice. The initial question asked how much authors needed to focus
on various research design and construct measurement practices in their research. All the
questions began with ‘‘to what extent do you agree that entrepreneurship researchers need
to focus on . . . .’’ This question established the perceived importance of the practices. The
follow-on question asked how much an expert agreed that entrepreneurship researchers
have focused on these practices over the last decade within published research. These questions were worded slightly differently to assess the extent to which an expert perceived gaps
between what authors should be doing about each practice and what they are currently
doing about each practice.
Fifty-one researchers—who have collectively co-authored over 740 A-caliber publications—provided input for an effective response rate of 35%. Because the respondents were
anonymous, we were unable to assess nonresponse bias directly. Instead, because research has
shown nonrespondents to be similar to late respondents (Armstrong & Overton, 1977), we
examined nonresponse bias by assessing differences in (a) rank and (b) years in the profession
between early and late respondents. We did not find any differences, and thus, have no reason
to believe nonresponse bias is present in our results, which are reported in Table 2.

Results
We conducted mean difference tests to investigate differences between the need for
improvement and the actual extent of improvement. Overall, the results of our survey indicate that a significant gap still exists between what entrepreneurship researchers are doing
and what they need to be doing. With respect to reliability, the evidence suggests that
researchers still need to increase focus on assessing reliability. In addition, the experts
revealed that researchers need to do more in terms of both discussing (p < .01) and examining validity (p < .01). Regarding external validity, there remains room for improvement,
particularly when determining whether nonresponse bias might have been present in the
sample.3 Finally, we asked respondents questions in two broad areas: fit between research
design and methods and measures used as well as whether the field has improved in terms of
more sophisticated modeling. We found that although there is still a sense that ‘‘fit’’ can be
improved between research design and the methods and measures used, researchers have
substantially improved in terms of more sophisticated modeling.

Exploratory Analyses
We were curious whether there were differences between journal sets, so we compared
the dedicated entrepreneurship journals (i.e., ETP and JBV) to the mainstream management journals. Our results indicate significant differences (i.e., p < .05) between journals
sets in just a few areas. We found significantly stronger practices for the mainstream management journals: (a) for the ratio of studies that contained complete correlation matrices
(.64 vs. .53), (b) for the ratio of highly valid to not highly valid measures (16.28 vs. 6.58),
(c) for the ratio of studies with scale measures with all reported reliabilities above .7
(2.17 vs. 1.92), and (d) for reporting explicit information on tests of nonresponse bias

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 10----->Crook et al. / Research Design and Construct Measurement

201

Table 2
Expert Opinions Regarding Different Design Elements in Published Research
Research Design Element

Level of Importance Assessment of Current State Difference Significance

Need for methodological rigor
Focus on construct measurement
Focus on reliability
Discuss validity
Examine validity
Assess nonresponse bias
Assess common method bias
Move beyond simple modeling
Fit between design and method

4.32
4.13
4.16
4.34
4.30
4.09
4.02
4.06
4.85

3.85
3.36
3.23
3.04
2.89
3.02
2.87
3.70
3.13

.47
.77
.92
1.30
1.41
1.07
1.15
.36
1.72

p
p
p
p
p
p
p
p
p

< .01
< .01
< .01
< .01
< .01
< .01
< .01
< .01
< .01

(5.25 vs. .86).4 However, the dedicated entrepreneurship journals used stronger practices
when justifying measures; the ratio of justified to not justified measures was 14.07 versus
3.86. In addition, the entrepreneurship journals used more sophisticated modeling; the
ratio of studies testing for moderation and mediation was .52 versus .22. We also examined, but did not find, differences between journals regarding whether they reported all
reliabilities.

Discussion and Recommendations
The results of our two studies should be viewed in light of their limitations. In the first
study, we examined 6 years of a specific set of journals. However, this sample appears
appropriate as the set of journals is widely recognized as the stronger set of journals for
entrepreneurship research (Chandler & Lyon, 2001; Dean et al., 2007). Had we examined
a different set of journals or studies published in different years, our findings might have
been different. Indeed, given that we assessed high quality journals, it is quite likely
research design and construct measurement practices in lesser quality journals would not
reflect the same progress. A second limitation is that our survey study assessed gaps, not
how close the field is to optimal construct measurement. Had we used the latter approach,
the results might have revealed that we are further than we think. Nonetheless, when viewing the results of the content analysis and of the survey, it appears that the findings of the
two studies paint a consistent picture.
Entrepreneurship is a relatively young field. During its youth, the field has experienced
impressive growth. Broadly speaking, the evidence we collected from 238 articles involving 242 samples and from 51 expert entrepreneurship researchers suggests that, alongside
this growth, the field has made significant strides in terms of methodological rigor and
maturity. Yet, the evidence also suggests that progression in research design is needed,
especially given that new studies typically build on earlier work (Kuhn, 1962). To the extent
that a study is not properly designed and when constructs are not properly measured, not
only is confidence in the findings of that study limited but also the field’s ability to build
on it in future studies.

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 11----->202 Organizational Research Methods

Construct Measurement
Reliability. In general, the results suggest that the field has improved but still needs to
progress in terms of some of its construct measurement practices. Reliability is critical to
advancing entrepreneurship because the use of unreliable measures across related studies
has been shown to lead to conflicting findings because of measurement error not substantive
issues (Boyd, Gove, & Hitt, 2005b). Our results related to reliability are somewhat mixed.
However, the results from our survey indicate that entrepreneurship researchers have
increased their focus on reliability in recent years, but that more progress is needed. Similarly, the results from our content analysis suggest that progress has been made, especially
related to internal reliability; the ratio of studies that use scales with all reported reliabilities
above .7 is increasing. However, regarding inter-respondent reliability, very few studies
reported such statistics in either time period, and no differences existed between early and
later periods. Although inter-respondent reliability does not appear to be a large concern, we
think more effort should be directed toward collecting data from multiple respondents.

Validity. The survey results related to discussing and examining validity indicate that
researchers have improved but can still advance in both of these areas. These findings are
broadly consistent with our content analysis findings. Regarding content validity, our
results suggest that authors used highly (content) valid measures the vast majority of the
time in both periods; however, the ratio of measures possessing highly content valid to measures lacking high content validity was significantly higher for the later period. Our proxy
for discussing validity was whether authors justified the use of a particular measure. Like
the survey results, the content analysis results suggest that significant strides have been
made over the past few years. This progress is encouraging, but still, all the measures used
neither had high content validity nor were justified in every study. This suggests that
although the field is getting better, there remains some room for improvement.
Our content analysis results also strived to capture whether substantive validity could be
assessed by anyone interested in an article’s findings. There were no differences in the ratio
of statistically related independent-to-dependent variables or the amount of complete correlation matrices furnished over time. Regarding the latter, we consider any number under
100% less than desirable.
Beyond Chandler and Lyon (2001), we strived to assess the external validity of studies’
findings via assessing data sources and the country or countries from which the sample data
were drawn. First, our assessment of data sources revealed that, overall, fewer studies currently
rely on primary data (e.g., survey), whereas more studies rely on secondary data (e.g., archival
database). There was no significant change in the number of studies that blended the two
approaches (i.e., mixed primary and secondary data). Given that combining different
approaches ‘‘triangulates’’ findings and increases confidence that results are valid and not simply methodological artifacts (Jick, 1979), entrepreneurship research that moves beyond relying
solely on one data source seems like a fruitful endeavor. Second, our results indicate that, for
both periods, the vast majority of entrepreneurship studies draw on single country samples
(>90%) and the bulk of these are from the United States (>55%). This suggests that more
cross-country samples are needed to improve the external validity of findings within the field.

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 12----->Crook et al. / Research Design and Construct Measurement

203

Table 3
Suggestions for Improving Research Design and Construct Measurement
Researchers (authors)
(1) When appropriate, assess, report, and discuss the reliability of all constructs
(2) Discuss validity and why all measures were used for a construct
(3) When available, use additional data sources to derive multiple indicators of constructs and reduce common
methods bias
(4) When possible and appropriate for the theory, use data analytic techniques that allow assessment of measurement error
(5) Thoroughly consider aspects of research design, methods, and measures prior to the consummation of the
study; perhaps have others examine these aspects beforehand
Reviewers and editors
(1) Require that a complete correlation matrix be reported
(2) When appropriate, require that a measure of reliability be reported
(3) Set a higher bar for publication regarding construct measurement
Senior scholars
(1) Lead by example by improving construct measurement practices in their own research
(2) Delve deeply into construct measurement issues in doctoral entrepreneurship curricula
(3) Offer domain-specific methods training to junior scholars

Model Sophistication
The results from both our survey and content analysis provide some evidence that entrepreneurship researchers are using more sophisticated modeling. Although such modeling is being
used, there is still a relatively low number of studies that investigate potential moderation or
mediation effects. Furthermore, the survey of experts noted that testing of more complex models is still needed. The results also show a noticeable increase in the number of longitudinal
designs (compared to cross-sectional) in the later period. This is a positive sign, given that
longitudinal designs are better equipped to assess causality. Finally, the results indicate that
the two most common levels of analysis are individual levels and firm levels but very few studies cross levels. With the advent of more advanced theory and modeling techniques (e.g., hierarchical linear modeling—Dean et al., 2007), more work appears needed to assess how
phenomenon at one level of analysis (e.g., industry) are related to outcomes at others (e.g., strategic groups or firms—Short, Ketchen Jr., Palmer, & Hult, 2007). Still, the promise associated
with advanced theory and modeling techniques can be realized only if the underlying design
and measures are solid. Overall, although model sophistication has improved, several important and interesting research questions can be answered if more progress is made in this area.

Moving Forward
Careful research design and construct measurement are central to how much confidence
and faith we can have in research results. Overall, the two studies taken together show that
the field still has progressed but still has some distance to go in terms of optimal research
design and construct measurement practices. Although the field and its stakeholders are best
served by optimal practices, there are still studies being published where there appears to be

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 13----->204 Organizational Research Methods

significant room for improvement. Thus, efforts to improve will be required from the field’s
key stakeholders. Accordingly, we summarize our suggestions for authors, reviewers, and
editors, as well as senior scholars in Table 3.
Perhaps the largest cause for the lack of focus on research design and construct measurement is that new entrepreneurship scholars lack confidence in their ability to use some
methods associated with reliability assessment (e.g., confirmatory factor analysis—Dean
et al., 2007). Whether it is a lack of confidence or competence with statistical analysis or inadequate consideration of reliability in study design, an appropriate remedy may be the creation
of domain-specific training for budding entrepreneurship scholars (Ireland, Webb, &
Coombs, 2005). Perhaps more than any other field, entrepreneurship researchers deal with
more diverse phenomenon and samples. To surmount these challenges, we suggest that doctoral entrepreneurship curricula delve more deeply into issues related to research design and
construct measurement in the domain of entrepreneurship. This ‘‘remedy’’ should help entrepreneurship researchers alleviate some long-held concerns. In our view, this progress rests on
authors, editors, reviewers, and especially, senior scholars within the field.
The largest gap identified by the experts was between the importance of having adequate fit
between the research design and methods and measures used and the extent to which authors
have had adequate fit among the research design and methods and measures. Future research
should examine the root cause of this inadequate fit among research design and methods and
measures so that it may be addressed. In the meantime, the field’s stakeholders should take steps
to continue to improve research design and construct measurement and benchmarking studies,
such as the ones we conducted, should be done periodically to assess the field’s progress.

Conclusion
Our overarching goal in this study was to answer to the question ‘‘are we there yet?’’ Our
assessment of current research design and construct measurement practices within entrepreneurship reveals that, in a word, the answer is ‘‘no’’—but we are a lot closer than we were
just a few years back. Indeed, we have seen significant improvement since Chandler and
Lyon’s (2001) assessment as well as between the early and the later periods we assessed.
Thus, our confidence in the findings and implications from extant research is good and
improving but is not yet what it could be.5 To this end, we suggest that authors pay more
attention to research design and construct measurement practices in their studies and that
more domain-specific training for budding entrepreneurship scholars be conducted. We
also suggest that editors and reviewers avoid and discourage others from using less-thanoptimal practices. Doing so will more quickly advance knowledge and the field.

Notes
1. We coded for each of Chandler and Lyon’s practices, except for those related to statistical analysis. Dean
et al. (2007) recently published a comprehensive review of these practices. However, our coding also allowed us
to look at some practices using measures beyond Chandler and Lyon (2001). For example, we included the number of full correlation matrices furnished in studies. Where possible, we also compared our results to Chandler
and Lyon’s. We thank an anonymous reviewer for this recommendation.
2. We use the term content validity instead of face validity, the term used by Chandler and Lyon (2001).

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 14----->Crook et al. / Research Design and Construct Measurement

205

3. As an anonymous reviewer insightfully pointed out, assessing nonresponse bias does not fully capture
whether findings possess external validity and thus generalize to other contexts. Sample selection is a key concern; if samples are not randomly drawn from a clearly defined sampling frame, assessments of nonresponse
bias will not yield whether findings possess external validity.
4. These numbers indicate that, for management journals, 5.25 studies reported nonresponse bias for every
one study that did not. The .86 figure means that .86 studies reported nonresponse bias for every study that did
not. This suggests that research published in mainstream management journals assess such bias much more frequently than dedicated entrepreneurship journals.
5. As one of our reviewers insightfully pointed out, although we should always strive to use best practices,
we doubt that any field, whether in the organizational sciences or in other domains of study, has achieved optimal research design and construct management practices.
6. For the PUIs, the results are based on z tests. For the reported ratios, the results are based on means differences tests. We report significant p values or ns for not significant.

References
Armstrong, J. S., & Overton, T. S. (1977). Estimating nonresponse bias in mail surveys. Journal of Marketing
Research, 14, 396-402.
Barney, J. B. (1991). Firm resources and sustained competitive advantage. Journal of Management, 17, 99-120.
Boyd, B. K., Gove, S., & Hitt, M. A. (2005a). Construct measurement in strategic management research: Illusion or reality? Strategic Management Journal, 36, 239-257.
Boyd, B. K., Gove, S., & Hitt, M. A. (2005b). Consequences of measurement problems in strategic management
research: The case of Ahimud and Lev. Strategic Management Journal, 36, 367-375.
Busenitz, L. W., West, G. P. III, Shepherd, D., Nelson, T., Chandler, G. N., & Zacharakis, A. (2003). Entrepreneurship research in emergence: Past trends and future directions. Journal of Management, 29, 285-308.
Chandler, G. N., & Lyon, D. W. (2001). Issues of research design and construct measurement in entrepreneurship research: The past decade. Entrepreneurship Theory and Practice, 25, 101-113.
Cohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for
the behavioral sciences (3rd ed.). Mahwah, NJ: Erlbaum.
Dean, M. A., Shook, C. L., & Payne, G. T. (2007). The past, present, and future of entrepreneurship research:
Data analytic trends and training. Entrepreneurship Theory and Practice, 31, 601-618.
Holden, R. R., & Jackson, D. N. (1979). Item subtlety and face validity in personality assessment. Journal of
Consulting and Clinical Psychology, 47, 459-468.
Ireland, R. D., Reutzel, C. R., & Webb, J. W. (2005). Entrepreneurship research in AMJ: What has been published, and what might the future hold? Academy of Management Journal, 48, 556-564.
Ireland, R. D., Webb, J. W., & Coombs, J. E. (2005). Theory and methodology in entrepreneurship research.
In D. J. Ketchen, & D. D. Bergh (Eds.). Research methodology in strategy and management (Vol. II,
pp. 111-141). San Diego, CA: Elsevier.
Jick, T. (1979). Mixing qualitative and quantitative methods: Triangulation in action. Administrative Science
Quarterly, 24, 602-611.
Kerlinger, F. N., & Lee, H. B. (2000). Foundations of behavioral research (4th ed.). Fort Worth, TX: Harcourt
College Publishers.
Kuhn, T. (1962). The structure of scientific revolutions. Chicago: University of Chicago Press.
Loevinger, J. (1957). Objective tests as instruments of psychological theory. Psychological Reports, 3, 635-694.
Low, M. B. (2001). The adolescence of entrepreneurship research: Specification of purpose. Entrepreneurship
Theory and Practice, 25, 17-25.
Messick, S. (1988). Validity. In R. L. Linn (Ed.). Educational measurement (3rd ed.). New York: Macmillan.
Nunnally, J. (1978). Psychometric theory. New York: McGraw Hill.
Rouse, M., & Daellenbach, U. (1999). Rethinking research methods for the resource-based perspective: Isolating the sources of sustainable competitive advantage. Strategic Management Journal, 20, 487-494.

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

<-----Page 15----->206 Organizational Research Methods
Schriesheim, C. A., Cogliser, C. C., Scandura, T. A., Lankau, M. J., & Powers, K. J. (1999). An empirical comparison of approaches for quantitatively assessing the content adequacy of paper-and-pencil measurement
instruments. Organizational Research Methods, 2, 140-156.
Schwab, D. (1999). Research methods for organizational studies. Mahwah, NJ: Lawrence Erlbaum Associates.
Shane, S., & Venkataraman, S. (2000). The promise of entrepreneurship as a field of research. Academy of Management Review, 26, 13-17.
Short, J., Ketchen, D. J. Jr., Palmer, T., & Hult, G. T. (2007). Firm, strategic group, and industry influences on
performance. Strategic Management Journal, 28, 147-167.
Stone-Romero, E., Weaver, A., & Glenar, J. (1995). Trends in research design and data analytic strategies in
organizational research. Journal of Management, 21, 141-157.
Summer, C. E., Bettis, R. A., Duhaime, I. H., Grant, J. H., Hambrick, D. C., & Snow, C. C., et al. (1990).
Doctoral education in the field of business policy and strategy. Journal of Management, 16, 361-398.

T. Russell Crook (PhD, Florida State University) is an assistant professor of management at the University
of Tennessee. His research—which focuses on topics related to strategy, entrepreneurship, and research
methods—has appeared in other outlets such as the Journal of Operations Management and Strategic Management Journal. He is currently on the editorial board at the Journal of Management.
Christopher L. Shook (PhD, Louisiana State University) is an associate professor and director of the Lowder
Center for Family Business and Entrepreneurship at Auburn University. He was a Fulbright Scholar at the Academy of Economic Studies in Bucharest, Romania. His research focuses on methodological issues in strategy and
entrepreneurship research, strategic decision making processes, and venture creation.
M. Lane Morris (PhD, University of Tennessee) is an associate professor and program director of the Human
Resource Management MS Degree Program at the University of Tennessee. He is the president of the Academy
of Human Resource Development and his research interests include: entrepreneurship, occupational stress/
health/wellness, work/life balance, human capital metrics, and individual and organization development.
Timothy M. Madden is a doctoral student in organizations and strategy at the University of Tennessee in Knoxville. His current research interest areas are business ethics, corporate social responsibility, entrepreneurship,
and management education.

Downloaded from http://orm.sagepub.com at UNIV OF WATERLOO on December 3, 2009

