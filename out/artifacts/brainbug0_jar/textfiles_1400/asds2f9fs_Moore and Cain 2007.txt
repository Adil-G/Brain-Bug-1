<-----Page 0----->Organizational Behavior and Human Decision Processes 103 (2007) 197–213
www.elsevier.com/locate/obhdp

Overconﬁdence and underconﬁdence: When and why people
underestimate (and overestimate) the competition q
Don A. Moore
a

a,*

, Daylian M. Cain

b,1

Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA
b
Harvard University, 1805 Cambridge Street, Cambridge, MA 02138, USA
Received 27 September 2005
Available online 3 November 2006

Abstract
It is commonly held that people believe themselves to be better than others, especially for outcomes under their control. However,
such overconﬁdence is not universal. This paper presents evidence showing that people believe that they are below average on skillbased tasks that are diﬃcult. A simple Bayesian explanation can account for these eﬀects and for their robustness: On skill-based
tasks, people generally have better information about themselves than about others, so their beliefs about others’ performances tend
to be more regressive (thus less extreme) than their beliefs about their own performances. This explanation is tested in two experiments that examine these eﬀects’ robustness to experience, feedback, and market forces. The discussion explores the implications for
strategic planning in general and entrepreneurial entry in particular.
 2006 Elsevier Inc. All rights reserved.
Keywords: Entrepreneurial entry; Overconﬁdence; Controllability; Skill; Competence; Entrepreneurship; Better-than-average; Reference group
neglect; Egocentrism; Diﬀerential regression; Comparative judgment

One of the most popular social psychology textbooks
states, ‘‘For nearly any subjective and socially desirable
dimension. . .most people see themselves as better than
average’’ (Myers, 1998, p. 440). For example, people
report themselves to be above average in driving ability,
q
The authors appreciate the insightful comments, on earlier versions
on this manuscript, by Linda Babcock, J. Nicolas Barbic, Max
Bazerman, Jason Dana, Paul Geroski, P.J. Healy, Chip Heath, Erik
Hoelzl, George Loewenstein, Daniel Lovallo, Rob Lowe, John Oesch,
John Patty, Vahe Poladian, Jesper Sorensen, Lise Vesterlund, and
Roberto Weber. Thanks to Sapna Shah and Sam Swift for help with
data collection. The authors also appreciate the support of National
Science Foundation Grant SES-0451736, a Berkman Faculty Development Grant at Carnegie Mellon, and the assistance of John Duﬀy in
the use of the Pittsburgh Experimental Economics Laboratory at the
University of Pittsburgh for collecting the experimental data.
*
Corresponding author. Fax: +1 412 268 7345.
E-mail addresses: dmoore@cmu.edu, don.moore@alumni.carleton.
edu (D.A. Moore), cain@fas.harvard.edu (D.M. Cain).
1
Fax: +1 617 495 7730.

0749-5978/$ - see front matter  2006 Elsevier Inc. All rights reserved.
doi:10.1016/j.obhdp.2006.09.002

their ability to get along with others, and their chances
of obtaining jobs that they like (College Board, 1976–
1977; Svenson, 1981; Weinstein, 1980). Some have
argued that the most important business decisions,
including the decision to found a new ﬁrm, enter an
existing market, or introduce a new product are routinely biased by such overconﬁdence (Cooper, Woo, & Dunkelberg, 1988; Dunning, Heath, & Suls, 2004; Hayward
& Hambrick, 1997; Malmendier & Tate, 2005; Odean,
1998; Zajac & Bazerman, 1991).
Recent evidence, however, has cast doubt on the generality of overconﬁdence. There are a number of diﬀerent domains in which people are systematically
underconﬁdent. For example, people believe that they
are below average in unicycle riding, computer programming, and their chances of living past 100 (Kruger, 1999;
Kruger & Burrus, 2004). It turns out that people tend to
predict that they will be better than others on easy tasks
where absolute performance is high, but worse than

<-----Page 1----->198

D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

others on diﬃcult tasks where absolute performance is
low (Hoelzl & Rustichini, 2005; Moore & Kim, 2003;
Windschitl, Kruger, & Simms, 2003). A number of
researchers have explained this eﬀect as egocentrism:
People focus on their own performances and neglect
consideration of others’ (Camerer & Lovallo, 1999;
Kruger, 1999). In this paper, we present a new explanation for these better-than-average (BTA) and worsethan-average (WTA) eﬀects.2 Our explanation holds
that BTA and WTA eﬀects are a natural consequence
of regressive estimates of others, which result from the
fact that people have better information about themselves than they do about others. We test this explanation using two experiments that examine the
robustness of BTA and WTA eﬀects to experience, feedback, and market forces. The results are consistent with
our hypotheses, and have some provocative
implications.
For the sake of exposition, let us introduce our
theory by considering beliefs about performance on a
one-question test where the answer is either right or
wrong. Before having seen the problem, and without
any information regarding its ease or diﬃculty, how
likely are you to solve it? One assumption might be that
performance will be uniformly distributed across possible outcomes (Fischhoﬀ & Bruine De Bruin, 1999; Fox
& Rottenstreich, 2003), leaving a 50% chance that you
will solve the problem. Such an ‘‘ignorance prior’’ might
make sense in the absence of better information. Whatever it is, this prior is simply your baseline expectation
for your performance.
After taking the test, let us say that you know
whether you solved the problem. What are you to
believe about others’ performances? If your own performance is useless for predicting others’ (e.g., if you think
that your good performance was based entirely on luck),
your estimation of others’ performances ought to
remain unchanged from your prior beliefs. Therefore,
doing well should leave you thinking that you did better
than others; and doing poorly should leave you thinking
that you did worse than others. Even if your beliefs
about your own performance are helpful for predicting
others’, so long as there remains uncertainty about others’ performances, your predictions of them should
depend on—and thus regress towards—the ignorance
prior. The upshot is that, when your absolute performance is better (or worse) than your prior expectations,
sensible Bayesian inference will lead you to make predictions of others’ performances that are between these priors and your current beliefs about your performance.
2

We use the terms better- and worse-than-average to be consistent
with prior work. We acknowledge that with skewed distributions, it is
indeed possible for the majority of people to be above (or below)
average. This concern, while valid, does not represent a problem for
the results of the experiments we present.

It is simple to extend this logic to a multi-item test: If
one begins with the assumption that one is just as likely
as others to get any given item correct, after having taken the test, one should estimate that others tend to score
somewhere between one’s own score and one’s prior
expectation. For example, suppose you initially expected
everyone to score about 70%, but you think you scored
about 90%. Depending on how indicative you feel your
score is of others’ scores, you might predict others to
score, say, 80%. If you scored 50%, you might predict
others to score, say, 60%. Notice that this perspective
does not imply a belief in diﬀerences of overall ability
between you and others—across both tests you would
predict the same average score for everyone, namely
70%. But, on each test, you would be right to expect differences between you and others, given better information about your own score on that test. For a more
formal development of this diﬀerential regression theory, see Appendix A.
Naturally, if the task includes no skill component
whatsoever and performance is yet to be determined
entirely by chance factors or factors outside one’s control, then there would be little reason for people, on
average, to predict that they would be above or below
average. Consistent with this reasoning, a number of
researchers studying BTA eﬀects have found that they
tend to be stronger on controllable tasks than on uncontrollable tasks (for a review, see Harris, 1996). For
instance, Camerer and Lovallo (1999) found that potential market entrants were excessively conﬁdent about
winning when competition was based on their skill but
not when winners were selected randomly. The authors
used this evidence to claim that high rates of entrepreneurial entry might be attributable to entrepreneurial
overconﬁdence. However, because prior studies have
employed easy tasks, the conclusion that people believe
they are better than average on all skill-based tasks
is unwarranted. Instead, our theory would predict
WTA eﬀects when the task is more diﬃcult than expected. We test this prediction in our ﬁrst experiment. The
ﬁrst experiment also tests our theory that BTA and
WTA eﬀects are attributable to the diﬀerential regressiveness in estimates of self vs. others. Experiment 2
addresses some shortcomings of Experiment 1 and
provides further support for our theory that better information about self than others produces diﬀerential
regressiveness.

Experiment 1: The market entry game
Our design builds on that of Camerer and Lovallo
(1999). They devised an N-player coordination game in
which, in each round, N players decide simultaneously
and without communication whether to enter a market
or not. Each market had a pre-announced capacity, c,

<-----Page 2----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

which determined how many entrants earned money:
Entrants ranked below c lost money, entrants ranked c
or above earned money, while non-entrants neither
earned nor lost any money. Each entrant’s payoﬀs
depended on his or her rank within the market, such
that more money was earned by better performance relative to other entrants.
Camerer and Lovallo’s key contribution over prior
market-entry experiments was manipulating whether
rankings were determined by either (a) a chance device,
or (b) the entrant’s skill (in answering trivia questions).
This manipulation was implemented within-subject, so
the same participants saw several rounds in which
entrants were ranked based on a skill-based task and
several rounds in which entrants were ranked randomly.
They found that skill-dependent payoﬀs encouraged
overconﬁdence and excess entry. Furthermore, excess
entry was highest in sessions for which it was common
knowledge that all participants were trivia enthusiasts,
suggesting that participants were neglecting consideration of the reference group (similar enthusiasts) with
which they would be competing.
However, our theory predicts underconﬁdence and
insuﬃcient entry as well. To test this, the new feature
of our design is that skill-dependant payoﬀs are based
on either an easy or a diﬃcult trivia game. Contrary to
the notion that overconﬁdence tends to be pervasive
on all skill-based competitions, we predict that participants will only believe they are better than others on
simple tasks, and thus, we expect excess entry only there.
We also test Camerer and Lovallo’s explanation: that
people focus on themselves and simply neglect consideration of others (rather than miscalculating others’ performance) when making comparative judgments.
Camerer and Lovallo called this ‘‘reference group
neglect’’ and others have simply called it egocentrism
(Chambers & Windschitl, 2004; Kruger, 1999). For
example, as examinations become more diﬃcult, students become more pessimistic about their ﬁnal grades,
even when it is common knowledge that the test will be
graded on a forced curve (Windschitl et al., 2003). While
our diﬀerential regression explanation would predict the
same eﬀect, the reference group neglect explanation
holds that such false pessimism arises because students
neglect to consider the fact that other students are also
likely to ﬁnd the test diﬃcult. In other words, students
trying to estimate their curved grades put too much
weight on their own absolute performances. The diﬀerential regression explanation, on the other hand,
hypothesizes that, regardless of the weighting attached
to it, estimates of others will be more regressive than
estimates of self. In summary, reference group neglect
is about errors in the weight one puts on estimates of
others’ performance, while diﬀerential regression is
about errors in the estimate that are weighed. We
will measure the diﬀerential weighting hypothesized by

199

reference group neglect, as well as other plausible alternative explanations, and show that the diﬀerential
regression hypothesized by our theory can better
account for our results.
The design of Experiment 1 includes several features
that should help people avoid the mistake of ignoring or
neglecting the competition: First, competitors are physically present, salient, and individuated. Second, participants engage in a series of competitions over several
rounds with full feedback each round, giving them the
opportunity to learn.
Method
In each round of our experiment, all seven participants in each experimental session were ranked relative
to each other, according to a pre-announced method.
Before the rankings were made public, we asked participants whether or not they wanted to enter into a competition in which only the three top-ranked entrants would
make money. After they decided whether to enter, participants answered a number of questions regarding their
own performances and the performances of others.
Finally, participants received full feedback regarding
absolute performances (of self and others), how many
participants chose to enter each round, and the relative
rankings of all (anonymously identiﬁed) entrants. The
entire process was repeated over 12 rounds, with the
three ranking methodologies (scores on a simple trivia
quiz, scores on a diﬃcult trivia quiz, or randomly generated scores) manipulated within session between rounds.
There were 13 experimental sessions, each with 7 people for a total of 91 individual participants. Participants
were students at Carnegie Mellon University. Each participant was endowed with $10. In each of the 12 rounds,
participants decided whether to enter the market or
whether to stay out and risk nothing. Entering the market meant either a loss or a gain, based on the entrant’s
rankings within that market. These payoﬀs are shown in
Table 1.
Table 1
Payoﬀs as a function of number of entrants and market rank
Rank

Payoﬀ

Cumulative
entrants

Cumulative
payoﬀ

Cumulative
expected payoﬀ
per entrant
(assuming ignorance
about rankings)

1st
2nd
3rd
4th
5th
6th
7th

$14
$10
$5
$10
$10
$10
$10

1
2
3
4
5
6
7

$14
$24
$29
$19
$9
$1
$11

$14
$12
$9.67
$3.50
$1.80
$0.17
$1.57

The table shows how much money was paid out in total (column 4)
and per entrant (column 5).

<-----Page 3----->200

D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

The system by which players were ranked was
announced publicly at the beginning of each round. In
four of the 12 rounds, rankings were determined randomly. After they decided whether to enter, participants
were assigned a randomly generated score from the set
of real numbers between 0 and 5, inclusive. In the
remaining eight (skill-based) rounds, rankings were
based on trivia quizzes taken at the beginning of the
round. Quizzes had ﬁve questions and a sixth tiebreaker
question. Four of these eight rounds’ quizzes were simple (with a mean score of 4.58 out of 5) and four were
diﬃcult (with a mean score of .41 out of 5). The tiebreaker questions were scored based on the answer’s distance from the correct numerical answer. The presence
of this tiebreaker question virtually eliminated the
chance of a tied score (there were none). The four simple
and four diﬃcult quizzes appear in Appendix B.
In order to rule out idiosyncratic eﬀects of order, we
varied the sequence in which participants encountered
the three diﬀerent ranking systems as follows. The diﬀerent ranking systems (R = random, S = simple, and
D = diﬃcult) were arranged in three diﬀerent orders
which varied across experimental sessions: RSD, DRS,
and SDR. Whatever sequence was arbitrarily chosen
for the session was repeated four times, making 12
rounds in four three-round blocks. So, for example, if
the ﬁrst three rounds used the sequence RSD, all participants in that session faced the same quizzes at the same
time, with ranking systems in the order: RSD–RSD–
RSD–RSD. The order in which participants encountered
the four diﬀerent simple and diﬃcult trivia quizzes was
also counterbalanced between experimental sessions.
In the eight skill-rank rounds, after taking the given
quiz but before seeing their scores, all participants
simultaneously made their entry decisions (to enter or
stay out). In the four random-rank rounds, there were
no quizzes, and all participants merely made their entry
decisions prior to learning their ranks. In all 12 rounds,
after participants made their entry decisions they then
answered the following questions:
1. How many people total do you think will enter the
market this round? Include yourself in this ﬁgure if
you chose to enter.
2. What percentage of the other six entrepreneurs in this
round do you think will score lower than you will
(regardless of whether anyone enters)?
3. How many questions (out of 5) do you think you got
correct this quiz? In random-rank rounds, this question was replaced with the question: What score
(out of 5) do you think you will get this round?
4. How many questions (out of 5) do you think the average participant will get correct this round? In
random-rank rounds, this question was replaced with
the question: What score (out of 5) do you think the
average participant will get this round?

5. If you chose to enter the market this round, what
rank do you think you will get?
At the end of each round, participants received full
feedback regarding each of the seven players’ individual
scores, entry decisions, and rankings. In the eight skillrank rounds, these scores were their trivia quiz performances; in the four random-rank rounds, these were
their randomly generated scores. This information was
posted using anonymous participant numbers on a
blackboard in view of all participants. Each participant
knew his or her own number, but did not know how the
other numbers corresponded to those individuals present. All of the 12 rounds’ results were left up for the
entire experimental session. Experiment 1 did not measure prior expectations regarding diﬃculty, but Experiment 2 did.
At the end of the 12 rounds, three rounds were
chosen at random to determine payoﬀs. The payoﬀs
for these three rounds were averaged, and this amount
was added to (or subtracted from) participants’ $10
endowment. Thus, the maximum possible payoﬀ was
$24 for a participant who entered and was ranked ﬁrst
in each of the three payoﬀ rounds ($10 endowment plus
an average of $14 in total over the three selected payoﬀ
rounds). It was also possible for a participant to leave
the experiment empty-handed if he entered and was
ranked 4th or below on each of the three payoﬀ rounds
($10 endowment minus an average loss of $10 in total).
Across all participants, the mean ﬁnal payoﬀ was $13.01
(with a range of $4 to $24).
Equilibrium predictions
As Table 1 (column 5) shows, entry has a positive
expected value so long as ﬁve or fewer players enter
the market, assuming players have no information
about their own relative ranks. If players are risk-neutral, then it is rational (i.e., there is a set of pure-strategy
Nash equilibria) for ﬁve players to enter each round.
Lacking some coordinating device for deciding which
of each session’s seven total players enter and which stay
out, there is a rational strategy (i.e., a mixed-strategy
equilibrium) that is somewhat more complicated to
compute, but the result is that all players enter with a
probability of 84%. Naturally, since only the top three
ranks actually win money, if all players know what their
ranks will be, then only the top three players (3/7 or 43%
of the potential entrants) will enter. Therefore, if all
players were unbiased and imperfectly informed, we
should expect between 43% and 84% of participants to
enter each round.
Predicting the equilibrium outcome without the
assumption of risk neutrality is more diﬃcult. Even
without information on their rankings, if everyone was
suﬃciently risk averse, no one would enter, and if everyone was suﬃciently risk seeking, everyone would enter.

<-----Page 4----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213
7
Difficult
Random

6

Simple
Entrants per round

So, following Camerer and Lovallo (1999), we use the
random-rank condition (when players cannot possibly
have useful information on their rankings) to provide
an empirical estimate of behavior given players’ risk
preferences. Deviations from entering 84% of the time
in random-rank conditions suggest particular deviations
from risk neutrality. And since all participants see all
conditions, their entry decisions in the diﬀerent conditions serve as within-subject controls for risk preferences. The diﬀerence in entry rates between the diﬀerent
conditions (random, simple, and diﬃcult) is the dependent measure of primary interest.

5
4
3
2
1
0
1

Hypotheses
Consistent with the diﬀerential regression explanation, we predict that participants will believe themselves
to be above average (and above median) on simple tests
but below average (and below median) on diﬃcult tests.
As a result, we predict that participants will enter too
frequently on simple-rank rounds and too rarely on
diﬃcult-rank rounds. We will take entry rates into
random-rank rounds as indicators of participants’
behavior given ignorance about their relative ranks
and given their risk preferences. We predict that entry
rates in random-rank rounds will lie between entry rates
in simple- and diﬃcult-rank rounds.
Results
The average random-rank round saw 4.27 entrants
(61% entry rate—suggesting slight risk aversion, on
average). In contrast to this baseline, the average simple-rank round saw 5.0 entrants and the average diﬃcult-rank round saw 2.94 entrants. In order to test for
the statistical signiﬁcance of these diﬀerences, we conducted a (4) · (3) within-subjects ANOVA in which each
of the 13 experimental sessions served as a single independent case. The four three-round blocks served as
the ﬁrst within-subjects factor and the three ranking systems served as the second within-subjects factor. The
results reveal a signiﬁcant eﬀect of the experimental condition, F (2, 24) = 39.17, p < .001, g2 = .77. Contrast
tests conﬁrm the signiﬁcance of both the diﬀerence
between entry rates in diﬃcult-and the random-rank
rounds (p < .001) and the diﬀerence between the simpleand random-rank rounds (p = .018).
The main eﬀect of block is not signiﬁcant, F (3,
36) = .82, p = .49, g2 = .06. Although the interaction
between block and ranking system (as shown in Fig. 1)
is marginally signiﬁcant in the overall ANOVA, F (6,
72) = 2.08, p = .07, g2 = .15, this does not appear to
result from a consistent reduction in the eﬀect of ranking
system as participants gained experience: Entry rates in
the diﬃcult, random, and simple markets were 2.9, 4.4,
and 5.1, respectively in the ﬁrst block and were similarly

201

2

3

4

Block

Fig. 1. Entry rates in the three diﬀerent ranking systems across the
four blocks. Error bars show standard errors.

2.9, 4.2, and 5.7, respectively, in the last block. Fig. 1
shows these means.
Explaining diﬀerences in rates of entry
There are four possible explanations for the systematic eﬀect of ranking systems on rates of entry: our diﬀerential regression explanation and three alternatives. The
ﬁrst alternative explanation is that participants believed
that others would stay out of simple-rank rounds and so
entry would have a higher expected value in simple
rounds. The data contradict this explanation: Participants predicted that there would be 5.2 entrants in the
average simple-rank round (there were 5), 4.5 entrants
the average random-rank round (there were 4.27), and
3.2 entrants in the average diﬃcult-rank round (there
were 2.94). These predictions are consistent over time
and do not systematically get either better or worse over
the 12 rounds of play. So participants expected more
competition in simple rounds, but more entered there
anyway.
The second alternative explanation is that potential
entrants systematically overestimated their own scores
more on simple tasks than on diﬃcult tasks. This explanation is also contradicted by the data—the opposite is
actually true. Participants underestimated their scores
on the simple quiz, reporting that they had gotten 4.41
out of 5 correct, when in fact they actually got 4.58. This
diﬀerence is revealed to be signiﬁcantly diﬀerent by a
comparison of actual vs. self-reported scores in a
(2) · (4) within-subjects ANOVA performed at the level
of the individual, where the four blocks served as the
second within-subjects factor, F (1, 90) = 19.03, p <
.001. On the diﬃcult test, by contrast, participants overestimated their scores, reporting that they had gotten .95
correct when in fact they had only gotten an average of
.41 correct, F (1, 90) = 78.20, p < .001.
While this pattern at ﬁrst seems incongruous, it
ought not to be surprising: The tendency for people to

<-----Page 5----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

overestimate their own performances more on diﬃcult
than on simple tasks is one of the more robust ﬁndings
in the literature on overconﬁdence and calibration (Burson, Larrick, & Klayman, 2006; Lichtenstein, Fischhoﬀ,
& Phillips, 1982). It can be readily explained using the
same regressive logic that we used to predict BTA eﬀects
on simple tasks and WTA eﬀects on diﬃcult tasks:
Because people have imperfect knowledge of their own
scores, their estimates of their own performances are
slightly regressive (Erev, Wallsten, & Budescu, 1994;
Juslin, Winman, & Olsson, 2000). If people’s estimates
of their own performances are slightly regressive, then
their estimates of the performances of others are likely
to be even more regressive. This follows from the fact
that people have better information about themselves
than they do about others, and so, people underestimate
others more on simple tasks than on diﬃcult tasks.
In simple rounds, our participants underestimated
their scores (which rules out the second alternative
explanation) and they expected more competition—yet
more entered there anyway. Before we test the reference
group neglect explanation, let us turn to our explanation
for the observed entry rates: diﬀerential regression. The
data are consistent with diﬀerential regression. People
underestimated others’ scores on simple quizzes more
than their own, reporting that others would score a
regressive 4.2 out of 5, but that they themselves would
score 4.41, F (1, 90) = 13.92, p < .001. On the other
hand, participants overestimated others’ scores on diﬃcult quizzes more than their own, reporting that others
would score a regressive 1.49 out of 5, but that they
themselves would score .95, F (1, 90) = 41.63, p < .001.
Because participants’ estimates of others are so
regressive, they believe themselves to be above median
on the simple quiz and below median on the diﬃcult
quiz. On the simple test, participants reported that they
expected to outscore 63% of the other participants
taking the same test. On the diﬃcult test, by contrast,
participants only expected to outscore only 46% of the
others. On the simple test, participants expected there
to be 5.2 entrants and expected that their rank among
entrants would be 2.6. On the diﬃcult test, participants
expected only 3.2 entrants yet expected to rank 2.7.
Fig. 2 shows patterns in participants’ beliefs about percentile rankings across the three treatments and four
blocks. Diﬀerences between simple and diﬃcult treatments persist throughout the experiment, despite the
provision of feedback. There is little evidence for learning in this ﬁgure.
These results provide a hint as to the reasons for the
durability of diﬀerences in entry rates across diﬀerent
ranking systems. Participants got consistent feedback
showing that they tended to underestimate their relative
performances on the diﬃcult quizzes and that they tended to overestimate their relative performances on the
simple quizzes; but they nevertheless had speciﬁc new

70
Difficult
65
Estimated percentile rank

202

Random
Simple

60
55
50
45
40
35
30
1

2

3

4

Block

Fig. 2. Participants’ estimated percentile rankings (percentage of
others worse than them) in the three diﬀerent ranking systems across
the four blocks. Error bars show standard errors.

information about each quiz that might have undermined their willingness to attend to this general historical fact. Even if an individual notices that she has
consistently overestimated her relative performance on
simple quizzes, if she takes a new quiz and scores highly
relative to her prior expectations, the inference that she
is likely to be above average may still be a sensible one.
So long as there is more uncertainty about others’ scores
than about her own, her predictions of others’ scores
will remain more regressive.
Fourth explanation: reference group neglect
We have presented evidence for the idea that estimates of others are regressive. The remaining question
is whether the diﬀerential regression explanation alone
can account for the observed diﬀerences in entry rates
between experimental treatments, or whether there is
any evidence of reference group neglect. Reference
group neglect posits that participants chose to enter on
simple rounds and stay out on diﬃcult rounds not
because they actually believed that they would score
any diﬀerently from others—but because they just were
not paying enough attention to others (Klar & Giladi,
1997; Kruger, 1999; Windschitl et al., 2003). If people
neglect to consider the group and instead focus on
themselves when estimating their relative standing, we
should observe that beliefs about own performance are
weighted more heavily than are beliefs about others.
In what follows, we test this prediction in a pair of
regression analyses (in Table 2) predicting comparative
judgments using performance by self and others. As
the remainder of the results section details, the substantial majority of the eﬀect of diﬃculty can be accounted
for by greater regressiveness in estimates of others than
of self.
Model 1 in Table 2 is the optimal model, predicting
participants’ actual percentile ranks within each round

<-----Page 6----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

203

Table 2
Actual and perceived value of participants’ own scores and the scores of others for estimating percentile rank
Model 1 predicting actual percentile rank

Model 2 predicting self-reported percentile rank

Independent variable

B

SE

Independent variable

B

SE

Own actual score
Actual average score

.201*
.201*

.005
.006

Own estimated score
Estimated average score

.111*
.077 *

.006
.008

R2
*

.56*

R2

.27*

p < .001.

using their own scores and actual average scores for the
round as independent variables. The results show, not
surprisingly, that participants’ own actual scores and
average scores have B coeﬃcients that are of similar size
but opposite signs. We compare this result with participants’ self-reported beliefs, using participants’ beliefs
about their own scores and beliefs about the average
score to predict their self-reported percentile rank. The
ﬁrst apparent diﬀerence between these two analyses is
more noise in self-reported beliefs than in actual performance, as shown by the smaller value of R2. It ought not
to be a shock that people’s estimates of their own scores
and their percentile rankings are imperfect.
The second and more important diﬀerence is that participants’ beliefs about their own scores were weighted
more heavily than were their beliefs about others’ scores.
To be precise, the weight attached to other (jBj = .077)
is 69% the size of the weight attached to self
(B = .111), and this diﬀerence is statistically signiﬁcant,
t (1088) = 3.4, p < .001. This shows evidence of reference group neglect but raises the following question:
What proportion of our primary result (the eﬀect of difﬁculty on entry rates) can be accounted for by diﬀerential regression and how much can be accounted for by
reference group neglect? In order to answer this question, we ﬁrst begin by assessing the experimental treatment’s eﬀect on entry decisions. We did this by
regressing entry rates on experimental treatment. The
independent variable in this regression was equal to 1
for simple-rank rounds, 0 for random-rank rounds,
and -1 for diﬃcult-rank rounds. When we conduct this
analysis at the level of the round, the R2 value of this
regression shows that the experimental treatment
accounts for 28% of the variation in entry rates across
all rounds, F (1, 154) = 59.67, p < .001. However, more
useful to our purposes is this analysis performed at the
level of the individual. There are two major reasons to
expect R2 to be lower in the regression conducted at
the individual level: First, participants’ entry decisions
are partially driven by their actual relative performance,
which is uncorrelated with the experimental treatment;
second, idiosyncratic individual-level factors such as risk
preferences aﬀect entry decisions. At the individual level,
since the dependent variable is dichotomous (entry
or not), a logistic regression is the more appropriate

statistical test.3 The Nagelkerke R2 value of this logistic
regression reveals that the experimental treatment
accounts for 7.9% of the variation in individual entry
decisions, and is statistically signiﬁcant, v2 (1) = 65.67,
p < .001. What this means is that 7.9% is the total size
of the eﬀect of diﬃculty on entry, and we must now
determine how much of it can be accounted for by differential regression and how much of it cannot be.
In order to assess the eﬀect of diﬀerential regressiveness, we next regressed entry decisions on participants’
beliefs about their relative performance, as measured
by the diﬀerence between their estimated absolute scores
for self and for others. Beliefs about relative performance
account for 24.4% of the variation in entry rates,
v2 (1) = 218.96, p < .0001. The mere fact that participants’ beliefs about relative performance are predictive
of their entry decisions is neither impressive nor interesting—it would be surprising if they were not. The interesting question is whether these beliefs about relative
performance can account for the eﬀect of the experimental manipulations on entry decisions. In order to test for
such a mediation eﬀect, we conducted a third regression
that included both experimental treatment and participants’ self-reports of relative performance. The resulting
Nagelkerke R2 value indicates that these two variables
combined account for 26.0% of the variation in entry
decisions. The inclusion of experimental treatment provides only a 6.7% increase in variation explained (over
the 24.4% using only the relative performance). However
the B coeﬃcient associated with experimental condition
remains signiﬁcant (B = .34, SE = .09, p < .001). The
signiﬁcance of experimental treatment suggests that
there is an eﬀect of diﬃculty that is distinct from participants’ beliefs about their relative standing. Of the total
7.9% of variation in entry decisions accounted for by
our experimental treatment, 1.6% (or 26%  24.4%) cannot be accounted for by participants’ self-reported beliefs

3

For the sake of simplicity, we present logistic regression analyses in
which each subject in each round serves as the unit of analysis (91
subjects · 12 rounds = 1092 observations). The results we present are
not appreciably diﬀerent when the same analyses are conducted using a
hierarchical linear model which treats subjects as random eﬀects and
accounts for the fact that experimental treatments are nested within
trial blocks which are in turn nested within experimental sessions.

<-----Page 7----->204

D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

about their own performance relative to others. This
1.6% represents 20% of the variation due to the experimental treatment that remains unexplained. Reference
group neglect is the most viable alternative explanation
for this unexplained 20%, but the substantial majority
of the eﬀect of diﬃculty (80%) can be accounted for by
greater regressiveness in estimates of others than of self.
Discussion
The results of the ﬁrst experiment show that conﬁdence regarding one’s competitive performance depends
on the type of competition. Contrary to prior evidence
(Camerer & Lovallo, 1999; Harris, 1996; Klein &
Kunda, 1994), we show that controllable tasks do not
necessarily elicit more overconﬁdence than chance tasks.
In diﬃcult-rank rounds, people avoided entering. People
overestimated others’ performances, leading them to
stay out of the competition despite the fact that they
accurately forecast few other entrants. Thus, skill-based
tasks do not always elicit overconﬁdence and entry rates
depend in part on how diﬃcult potential entrants see
the task.
Participants’ prior expectations regarding diﬃculty
play an important role in our theory, but the ﬁrst experiment did not measure them. Experiment 2 was designed
to address this shortcoming. Furthermore, our theory
posits a fundamental role for information about performance—one’s own and others’. Experiment 2 allows us
to observe the eﬀect of information on participants’
beliefs as they learn ﬁrst about their own performances
and then about the performances of others.

Experiment 2: The eﬀect of information on comparisons
Because our diﬀerential regression explanation
describes the mechanisms by which errors in entry
occur, it also oﬀers useful insights into which interventions might be useful for reducing errors and which
interventions are unlikely to be eﬀective. Experiment 2
tests these interventions. Participants were ﬁrst told that
they would be taking either a diﬃcult or simple quiz and
were then asked to predict the outcome (Time 1). After
taking the quiz (Time 2), participants were invited to
revise their answers to their prior estimates of absolute
and relative scores. Our theory would predict that information about one’s own performance provided at Time
2 would produce BTA on easy tasks and WTA on diﬃcult tasks. Finally, participants were given full information about how others scored on the same quiz they
took, and they were asked to report the same comparative judgments (Time 3). Our theory would not predict
BTA and WTA eﬀects at Time 3, in the presence of
excellent information about others. Previous research
has shown that information about others can reduce

BTA eﬀects (Alicke, Klotz, Breitenbecher, Yurak, &
Vredenburg, 1995). Here, we test whether it can also
reduce WTA eﬀects.
Methods
Participants
We recruited 128 undergraduate students at Carnegie
Mellon University by oﬀering them a base payment of
$2 plus from $0 to $8 on top of that. Experimental
sessions were advertised under the name ‘‘Games of
skill’’ with the following description: ‘‘Participants will
be playing a game in which they can earn money. How
much you get paid will depend on exactly how things come
out.’’
Design
The experiment had a 2 (quiz diﬃculty: simple vs. difﬁcult) · (3) (time of wager: before quiz vs. after quiz vs.
after results) mixed design. Quiz diﬃculty was manipulated between subjects and time served as a withinsubjects factor.
Procedure
Participants were each given $4 and invited to bet as
much as they wanted on winning a trivia competition
against a randomly chosen opponent. Participants
were truthfully told that their opponents’ scores would
be selected at random from a group of 144 students
who had previously taken these same quizzes as a part
of a diﬀerent study (reported in Moore & Kim, 2003,
Experiment 3). None of the 128 participants in the
present study had participated in that prior study.
The test would consist of 10 items plus an 11th tiebreaker question that virtually eliminated the chance
of a tied score. Winning participants would double
the amount they bet; those who lost would keep
only the un-wagered portion of their $4. Note that
the second and third time they bet, participants were
told that the most recent bet would be the one that
counted.
Participants in the simple quiz condition were told
they would take a simple trivia quiz and be shown the
following example question and answer:
What is the common name for the star inside our own
solar system?
Answer: The Sun
Participants in the diﬃcult quiz condition were told
they would be taking a diﬃcult trivia quiz and shown
the following example question and answer:
What is the name of the closest star outside our solar
system?
Answer: Proxima Centauri
Participants were then asked how much they wanted
to bet. After they bet, participants were given a questionnaire that asked:

<-----Page 8----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

(1)‘‘How many of the 10 questions do you think you
will get right?’’
(2)‘‘How many of the 10 questions do you think your
opponent will get right?’’
(3)‘‘What percentage of the group will have scores
below yours? (If you expect your score will be the
very best, then put 100. If you expect your score will
be exactly in the middle, put 50. If you expect your
score will be the lowest, put 0.)’’
Questions 1 and 2 were objective measures of absolute evaluation for self and for opponent. Question 3,
like the bet, was a direct measure of beliefs about relative standing. After participants had answered all
these questions, they were given an actual trivia test.
The questions from the diﬃcult and simple quizzes
are listed in Appendix C. Participants were then told,
‘‘Now that you have taken the quiz, you may choose to
revise your answers to these questions. Please answer
all the questions, whether or not you put the same
answers as before.’’ Then participants were asked
how much they wanted to bet and were asked
the same list of questions again. These were their
Time 2 responses.
After they had answered all the questions at Time 2,
participants were then given truthful feedback about
the scores of the previous test-takers from whose ranks
their randomly selected opponent would be drawn. For
example, those who took the simple quiz were
informed that: ‘‘The average score is 8.71 out of 10,
with a standard deviation of 1.1.’’ Those who took the
diﬃcult quiz were told: ‘‘The average score is 1.48 out
of 10, with a standard deviation of 1.01.’’ Participants
were also given a breakdown of the percentage of others who got each of the 11 possible scores (from 0 to
10) on the quiz. After they had a chance to review this
information, participants were told, ‘‘Now that you
have seen how others did, you may choose to revise your
answers to these questions. Please answer all the questions, whether or not you put the same answers as
before.’’ Then participants were asked how much they
wanted to bet and were asked the same list of three
questions again. These were their Time 3 responses.
The bet that was counted for computing payoﬀs was
this third and ﬁnal one.
Our diﬀerential regression explanation holds that
BTA eﬀects and WTA eﬀects result when people have
good information about themselves but lack information about others, such as at Time 2 after taking the
quiz. At Time 3, after getting good information about
others, these eﬀects should go away. Time 1 beliefs are
useful for assessing participants’ priors, but are based
on so little information that our theory would not make
strong predictions regarding their beliefs. We shall test
both our diﬀerential regression explanation and that of
reference group neglect.

205

Results and discussion
Manipulation check
As expected, the simple quiz resulted in higher scores
(M = 8.25 out of 10, SD = 2.01) than did the diﬃcult
quiz (M = 1.54 out of 10, SD = 1.34), F (1, 126) =
490.39, p < .001, g2 = .80.
Participants’ predictions at Time 1
At Time 1, participants who had seen only an easy
sample question (and were about to take—but had not
yet taken—the simple test), predicted that they would
score 7.22 (SD = 1.57) and that others would score
6.41 (SD = 1.79) out of 10. Those who saw only a diﬃcult sample question predicted that they would score
5.22 (SD = 1.90) and that others would score 4.92
(SD = 1.65). We analyzed these predictions using a 2
(diﬃculty) · (2) (target: self vs. other) mixed ANOVA.
The results reveal a main eﬀect of target, F (1, 124) =
11.97, p = .001, g2 = .09, since people predicted that
they would do better than would others. The diﬀerential
regression explanation cannot account for this eﬀect; the
results suggest some basic amount of self-enhancement.
The main eﬀect of diﬃculty is, of course, also signiﬁcant,
F (1, 124) = 46.13, p < .001, g2 = .27. The target · diﬃculty interaction eﬀect does not attain statistical signiﬁcance, F (1, 124) = 2.77, p = .099, g2 = .02.
Tests of diﬀerential regression at Time 2
At Time 2, the diﬀerential regression explanation
would hypothesize that people predict better relative
performance (BTA) on simple tasks and (WTA) worse
relative performance on diﬃcult tasks. This would manifest itself in a signiﬁcant interaction between diﬃculty
(simple vs. diﬃcult) and target (self vs. other). Indeed,
when we subjected estimates of absolute performance
to this 2 · (2) ANOVA, the diﬃculty · target interaction
emerges as signiﬁcant, F (1, 120) = 20.77, p < .001,
g2 = .15.4 At Time 2, participants reported believing
that they scored better (M = 8.30, SD = 1.49) than their
opponents (M = 7.83, SD = 1.28) on the simple quiz,
t (61) = 2.94, p = .005, g2 = .12. But they also reported
believing that they scored worse (M = 2.39, SD = 1.31)
than their opponents (M = 3.30, SD = 1.61) on the difﬁcult quiz, t (59) = 3.48, p = .001, g2 = .17.
Consistent with our theory, the increase in BTA and
WTA eﬀects from Time 1 to Time 2 is largely attributable to changes in beliefs about one’s own score. On
average, participants changed their estimates of their
own scores by 2.34 points (SD = 1.77). However, they
4

Naturally, the main eﬀect of diﬃculty is also signiﬁcant, since
participants predict higher scores on the simple than on the diﬃcult
test, F (1, 120) = 606.16, p < .001, g2 = .84. The main within-subjects
eﬀect of target (self vs. opponent) is not signiﬁcant, F (1, 120) = 1.40,
p = .24, g2 = .01.

<-----Page 9----->206

D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213
70
Difficult

65

Simple
Percentile rank

60
55
50
45
40
35
30
Time 1

Time 2

Time 3

Fig. 3. Participants’ estimated percentile rankings (percentage of
others worse than them) in the two diﬃculty conditions at three
points in time. Error bars show standard errors.

only changed their estimates of others’ scores by 1.92
points (SD = 1.76), and this diﬀerence is signiﬁcant by
paired-samples t-test, t (123) = 2.34, p = .02. Furthermore, these changes mediate the diﬀerence on bets
between diﬃculty conditions from Time 1 to Time 2.
We included these two change measures in a regression
predicting change in participants’ bets from Time 1 to
Time 2, along with a dummy variable for diﬃculty.
Their inclusion renders diﬃculty non-signiﬁcant,
b = .02, p = .87. As our theory would predict, changes
in self-estimates were a signiﬁcant predictor of changes
in bets, b = .47, p < .001. However, changes in other-estimates were not signiﬁcant, b = .13, p = .16.
Tests of diﬀerential regression at Time 3
The diﬀerential regression explanation would not predict BTA and WTA eﬀects at Time 3, when participants
have good information not only about themselves but
about others. Indeed, the same 2 · (2) ANOVA on absolute evaluations at Time 3 does not produce a signiﬁcant
target · diﬃculty interaction, F (1, 124) = .02, p = .89.5
On the simple quiz, participants predicted similar scores
for themselves (M = 8.31, SD = 1.62) and for their
opponents (M = 8.14, SD = 1.37). Likewise on the diﬃcult quiz, participants predicted similar scores for themselves (M = 2.08, SD = 1.19) and for their opponents
(M = 1.90, SD = .71). Fig. 3 shows participants’ self-reported percentile ranks. Furthermore, as our theory predicts estimates of others’ scores changed more from
Time 2 to Time 3 than did estimates of self. Estimates
of others changed by an average of 1 point
(SD = 1.18), whereas estimates of self only changed by
.4 points (SD = 1.05), and these two are signiﬁcantly

5

Naturally, the main eﬀect of diﬃculty remains signiﬁcant, F (1,
124) = 931.98, p < .001, g2 = .88. The main eﬀect of target remains
insigniﬁcant, F (1, 124) = 2.42, p = .07, g2 = .03.

diﬀerent from one another t (123) = 4.57, p < .001.
And consistent with our theory, the reduction in BTA
and WTA eﬀects on bets is mediated by changes in people’s beliefs about others, b = .31, p = .003, not the
self, b = .16, p = .07.
Tests of reference group neglect
The reference group neglect hypothesis predicts that
direct comparisons (like estimates of percentile rank)
will show stronger BTA and WTA eﬀects than will indirect comparisons (computed by subtracting absolute
estimates of others from self) which make others’ performances salient. The standard test is to regress comparative judgment on absolute evaluations of target and
referent. Using this standard test, we replicate the result
that comparative evaluation is strongly associated with
self-evaluation but more weakly predicted by absolute
evaluation of others. We regressed percentile estimates
on predictions of point scores by self and other for
responses at Time 1, before participants had taken the
actual test. As Table 3 shows, the b coeﬃcient for absolute self-evaluation is .86, p < .001, indicating that absolute and relative self-assessment are strongly correlated.
The b coeﬃcient for other-evaluation, however, is .53,
p < .001, is only 62% the magnitude of the coeﬃcient for
self. This ﬁnding is consistent with reference group
neglect.
Note that this diﬀerential weighting changes as people
gain information. At Time 2, when participants had more
information about themselves, the weight put on other-estimates (.49) is only 53% the size of the weight put on
self-estimates. But at Time 3, when people had better
information about others, other-estimates (b = 1.69) carry 92% the weight placed on self-estimates (b = 1.84). If
reference group neglect aﬀects how people bet, then we
ought to observe some eﬀect of test diﬃculty on bets, over
and above the eﬀect of diﬀerential regressiveness on estimates of self and others’ actual performances. We tested
this as we did in Experiment 1.6 The result was that 74%
of the eﬀect of diﬃculty on bets at Time 2 could be
explained by diﬀerential regression. However, this test
may claim too much credit for diﬀerential regression. As
the results in Table 3 highlight, better information about
self than others appears to produce both diﬀerential
regression and diﬀerential weighting. When they are confounded, this test will give all the credit to diﬀerential
regression over diﬀerential weighting.
At Time 3, the eﬀect of test diﬃculty on bets shrinks
dramatically: Diﬃculty accounts for only 3.4% of the
6
First, we began with the primary eﬀect of test diﬃculty on bets. At
Time 1, test diﬃculty only accounted for a statistically insigniﬁcant
1.9% of the variance in bets, as shown by the R2 value associated with a
regression predicting bets using a dummy variable for experimental
condition. At Time 2, however, diﬃculty accounts for 15% of the
variance in bets.

<-----Page 10----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

207

Table 3
Experiment 2’s results for the three diﬀerent measures of comparative judgment at three points in time.
Time

Self-reported comparative judgment

Simple vs. Diﬃcult
2

Eﬀect size (g )

Regression results
b (Self)

b (Other)

1
1
1

Bet
Percentile rank
Indirect comparison

.02
.04*
.03

.53***
.86***
1.07 

.27*
.53***
1.0 

2
2
2

Bet
Percentile rank
Indirect comparison

.15***
.16***
.15***

.92***
.93***
1.86 

.48**
.49***
1.53 

3
3
3

Bet
Percentile rank
Indirect comparison

1.67***
1.84***
3.10 

1.35***
1.69***
3.01 

.03*
<.001
.00

The third column shows the eﬀect size of the diﬀerence between simple and diﬃcult conditions, and asterisks show the signiﬁcance of the t-test
comparing diﬃculty conditions. Regression results predicting indirect comparative judgment for the three diﬀerent measures of comparative
judgment appear in the fourth and ﬁfth columns.
*
p < .05.
**
p < .01.
***
p < .001.
 
Independent variables perfectly account for dependent variable.

variance in bets. Our theory would not predict that test
diﬃculty would aﬀect comparative judgments when
people possess complete information regarding performance by self and others. Indeed, statistically speaking,
diﬀerential regressiveness accounts for only 3% of this
small eﬀect. The remaining 97% is most likely attributable to the egocentric overweighting of self-knowledge
over other-knowledge.

General discussion
The present results oﬀer two primary ﬁndings. First,
the results of the ﬁrst experiment show that conﬁdence
regarding one’s competitive performance depends on
the ease of the task. Contrary to prior evidence (Camerer & Lovallo, 1999; Harris, 1996; Klein & Kunda, 1994),
we show that controllable tasks do not necessarily elicit
more overconﬁdence than chance tasks. People underestimated others’ performances on simple tasks but overestimated them on diﬃcult tasks, leading them to enter
with conﬁdence on simple rounds despite the fact that
they accurately forecast numerous other entrants. Yet
in diﬃcult-rank rounds, people decided not to enter:
They overestimated others’ performances, leading them
to stay out of the competition despite the fact that they
accurately forecast few other entrants. Thus, skill based
tasks do not always elicit overconﬁdence; instead, conﬁdence and entry rates depend in part on how diﬃcult
potential entrants see the task.
The second contribution of this paper is that we
identify the cause for what appear to be myopic interpersonal comparisons, namely, better information about
one’s own performance than about the performance of
others. When a task is simpler than people expect it to
be, people’s estimates of others’ performances regress

downward and a majority will conclude that they are better than others. When a task is more diﬃcult than expected, estimates of others’ performances will regress upward
and a majority will conclude that they are worse than others. Experiment 2 shows that these WTA and BTA eﬀects
are strongest when people are conﬁdent regarding their
own performances but unsure of the performances of others. This may also explain why BTA and WTA eﬀects are
stronger when people compare themselves to some vague
group than when they compare themselves to a speciﬁc,
known individual (see Hoorens & Buunk, 1993; Klar,
Medding, & Sarel, 1996; Klein & Weinstein, 1997; Perloﬀ
& Fetzer, 1986; Price, 2001; Windschitl et al., 2003). It
may also help explain why BTA eﬀects have been shown
to be stronger for observable performances than for tasks
or traits where people only know about themselves and
cannot observe others’ performances directly (Dunning,
Meyerowitz, & Holzberg, 1989; Suls, Lemos, & Stewart,
2002). Given that diﬀerential regression explains most
of the eﬀect of diﬃculty on entry and on bets in our experiments, providing decision-makers with better information about the performance of others is likely to be the
most eﬀective way to eliminate this cause of myopic
comparisons.
Our studies experimentally manipulated the information people had, making it impossible for us to measure
diﬀerences in the degree to which people seek out information about themselves and others (for other studies
that have measured this, see Moore, Oesch, & Zietsma,
in press; Radzevick & Moore, 2006). However, our
results nevertheless show that people do not always use
information as they should. When making social comparisons, participants ought to have given the same weight to
information about themselves as to information about
others. The fact that one’s competitors are doing poorly
on some task is just as important as the fact that one is

<-----Page 11----->208

D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

doing poorly. Consistent with the reference group neglect
explanation, however, our participants’ self-reported
beliefs regarding their own performance were weighted
more heavily than their beliefs regarding opponents’ performance. But this egocentrism eﬀect is not the driving
factor behind WTA and BTA eﬀects and their consequences for behavior. Diﬀerential weighting (reference
group neglect) accounts for a small proportion of the
eﬀect of diﬃculty on entry rates and bets.
We should also note that, besides reference group
neglect, there are other viable explanations for why people’s judgments would appear to weight self more heavily
than others. For example, people easily conﬂate relative
and absolute evaluation on vague subjective measures
(Baron, 1997; Biernat, Manis, & Kobrynowicz, 1997).
The conﬂation error occurs when people answer the
question, ‘‘How good are you relative to others?’’ as if
they were answering the question ‘‘How good are
you?’’ (Klar & Giladi, 1999). Unlike diﬀerential weighting due to reference group neglect, the conﬂation
explanation is exceedingly mundane: Vague measures
facilitate confusion between relative and absolute
evaluation (Burson & Klayman, 2005; Moore, 2005).
Conﬂation can make comparative evaluations appear
to overweight the self because the person does not take
herself to be making a comparative evaluation.
Although we have attempted to distinguish the diﬀerential regression explanation from reference group
neglect, we should also note their fundamental compatibility. Both are caused by the greater accessibility and
quality of information about the self than about others.
Less information about others leads people to make
more regressive estimates of others but also probably
leads to further underweight of those regressive estimates
(Kruger, Windschitl, Burrus, Fessel, & Chambers, 2006).
Managerial and economic implications
Our results have implications for understanding entrepreneurial entry. While we do not take a stand on the
question of whether overall rates of actual entrepreneurial
entry are excessive, we instead note that rates of entry vary
considerably between industries. Some industries, such as
retail clothing stores, restaurants, and bars, are marked
by persistently high rates of entry and high rates of subsequent exit (US Small Business Administration, 2003).
Indeed, one of the stylized facts to emerge is that rates
of entry and exit are highly and positively correlated
(Dunne, Roberts, & Samuelson, 1988; Geroski, 1996;
Mata & Portugal, 1994). Diﬀerences in rates of entry are
not well accounted for by the size of an industry, the profitability of its ﬁrms, or barriers to entry (Geroski, 1996).
So, if new ﬁrms enter an industry because of above-normal proﬁts and exit an industry because of below-normal
proﬁts, then one would instead expect entry and exit to be
negatively correlated such that high failure rates meant

low entry rates, at least in the short term. Yet the correlation between entry and exit in any given year is around .7.
The results presented in this paper suggest a possible
explanation. Perhaps industries that see persistent high
rates of entry are those that potential entrants view as
‘‘easy’’ or in which they feel capable (Greico & Hogarth,
2004). Such industries are then likely to see more intense
competition, lower proﬁts, and higher rates of failure.
Experiment 1 shows that this might occur in spite of
entrants’ correct prediction that they will have lots of
competition in ‘‘simple’’ markets. Indeed, even when they
underestimate their own absolute abilities, entrants will
underestimate the abilities of their competition even
more.
However, given that our experimental participants
were not actual entrepreneurs with substantial quantities
of money at stake, we must be cautious about generalizing our results from errors made in the lab. Might actual
entrepreneurs learn to avoid the biases in comparative
judgment shown by participants in the present experiments? While it is possible that experience may allow
actual entrepreneurs to learn to overcome these errors,
it is unclear how much experience is needed for such
learning to take place. Participants in the market entry
game were students at a selective university and also
experienced 12 rounds with full feedback. Perhaps this
experiment included too few trials for them to learn to
solve this problem. If this is the case, however, entrepreneurs are likely to make the same mistake. After all,
even the most experienced entrepreneurs rarely get the
opportunity to start more than a handful of ﬁrms.
Furthermore, our experimental setup was more transparent, assessment of the competition was clearer, and
the causes of success were more obvious than they are
likely to be for most entrepreneurs.
We must note that both our theory and our results are
bounded. We do not claim that the diﬀerential regression
explanation (a cognitive explanation) accounts for all
BTA and WTA eﬀects. Motivation and bias do inﬂuence
comparative interpersonal judgments in important ways
(Kunda, 1990). It is also clear that other cognitive explanations such as reference group neglect can account for
some biases in comparative judgment and in strategic
decision making, as shown in the present ﬁndings as well
as in other work (Klar, 2002; Moore, 2004; Rose &
Windschitl, 2006; Windschitl et al., 2003). However, the
errors showcased in this paper seem to be more about
not having good information about one’s competition,
as opposed to merely ignoring the competition.

Appendix A. Formalization of the diﬀerential regression
explanation
In this appendix, we attempt to formalize our theory,
ﬁrst in general terms, then with a speciﬁc example. Our

<-----Page 12----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

point in this appendix is not to suggest that it is possible
to predict with certainty what individuals will believe—
given a multitude of constraints that may or may not be
realistic (we let our data speak to how real people
behave)—nor do we intend to provide a generalized
proof or justiﬁcation of the decision process we are suggesting is at work. Here, we wish only to provide insight
on why WTA/BTA eﬀects might occur in groups of reasonable people.
Let us restrict our analysis to some speciﬁc group of n
individuals who all expect to take a test together. These
people, on average, expect that their performances will
be average, relative to the others in the group. It is not
necessary that everyone believe that they are exactly
average, just that beliefs are balanced within the group.
In other words, for every person (or subset of people)
who believes that they are better than others (or better
than some individual) there is another person (or subset
of people) who believes that they are worse than others
(or worse than some individual) to the same degree, and
vice versa.
Within the group, let S be the average of all individuals’ estimates of the absolute performance of self; let
O = the average of all individuals’ estimates of the
absolute performance of others. To be precise:
S¼

ðs1 þ s2 þ . . . sn Þ
n

where sn is the nth person’s self-estimate.
O¼

ðo1 þ o2 þ . . . on Þ
n

where on is the nth person’s estimate of (the average of)
others’ scores (not including self).
SO¼

ðs1 þ s2 þ . . . sn Þ  ðo1 þ o2 þ . . . on Þ
n

and
SO¼

ðs1  o1 þ s2  o2 þ . . . sn  on Þ
n

S  O is equal to the average comparative judgment of
self to others. When S  O is positive, it implies a
BTA eﬀect: On average, people believe that they are
better than others. When S  O is negative, it implies
a WTA eﬀect: People believe they are worse than others.
At the outset, assume that S = O (on average, people
expect that their performance will be average, relative
to the others).
Let people acquire additional information about
their own (past or anticipated) performance. To the
extent that this information about self is diﬀerent
than prior information, it will justify updating beliefs
about one’s own performance (away from the prior).
To the extent that information about the self is more

209

useful for estimating performance by self than others,
it will lead to greater updating of S than of O. Therefore, estimates of others will tend to be closer to the
prior than will estimates of self. In other words, O
regresses to the prior more than does S. We call this
rule ‘‘Rule O’’:
Rule O: Since (whenever possible) O is more regressive than S, O must be closer to the prior than is S.
(When S is maximally regressive, O is equally
regressive.)
There are three possible relationships between S and
the prior:
Case 1: S > prior, meaning that, on average, people
think that they did better than they expected to do; thus
S > O (or else a contradiction follows; if we assume
S > O to be false, i.e., if O P S, and (as given) S > prior,
then O P S > prior, and O would be farther from the
prior than S, violating Rule O), and people will, on average, believe they performed better than others.
Case 2: S < prior, meaning that, on average, people
think that they did worse than they expected to do; thus
S < O (ELSE: O 6 S < prior, and O would be farther
from the prior than S, violating Rule O), and people
will, on average, believe they performed worse than
others.
Case 3: S = prior, meaning that, on average, people
think that they did as they expected to do; thus S = O
(ELSE: O > S = prior (or O < S = prior), and O would
be less regressive than S, violating Rule O), and people
will, on average, believe they performed the same as
others.
Example: Suppose there are three test takers, A, B, C,
each completing a test that is scored out of 100. Suppose
that, prior to taking the test, the average expected
score is 50. Suppose A’s actual score = 90; B’s actual
score = 65; C’s actual score = 40. The average actual
test-score = 65. On average, the test takers did better
than expected (65 > 50), even though some did worse
than expected (e.g., C scored 40). Suppose, for the sake
of simplicity, that all three know exactly how well they
themselves did, but they know their sense of others is
imperfect. Granted, speciﬁc numerical examples of estimates will depend on individual test-takers and speciﬁc
tasks. With imperfect information, however, as in
Bayesian updating, people’s best estimates (in this case
of others) will tend to fall between actual values and

Table 4
(Row A, Col B) = B’s prediction of A’s score
Predictor

Target

A
B
C

A

B

C

90
57.5
45

70
65
45

70
57.5
40

<-----Page 13----->210

D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

their prior expectations. Table 4 depicts reasonable estimates. In keeping with the idea that estimates of self are
less regressive than estimates of others, suppose that
estimates of others are computed using a somewhat
arbitrary equal weighting of the prior and actual
score: (prior + actual)/2. Since the average expected
score is 50, assume that priors of all people’s scores by
all others = 50. So, for example, if A scores 90, A will
predict that B scores (90 + 50)/2 = 70.
Next we calculate S and O for the group:
S ¼ ðA’s estimate of self þ B’s estimate of self
þ C’s estimate of selfÞ=3
¼ ð90 þ 65 þ 40Þ=3
¼ 65
O ¼ ½A’sðaverageÞestimate of others
þ B’sðaverageÞestimate of others
þ C’sðaverageÞestimate of others=3

3. What does MTV stand for?
4. On what continent is the country of Egypt?
5. What is the most widely spoken language in the US,
after English?
Tiebreaker: What is the height of the Eiﬀel Tower
(in feet)?
B.2. Simple Test 2
1. What was the ﬁrst name of the Carnegie who founded
the Carnegie Institute of Technology?
2. How many states are there in the United States?
3. In which month is Thanksgiving celebrated in the
United States?
4. Harrisburg is the capital of what US state?
5. On what continent is the country of France
located?

¼ ½1=2ð57:5 þ 45Þ þ 1=2ð70 þ 45Þ þ 1=2ð70 þ 57:5Þ=3

Tiebreaker: How many ﬁlms did Alfred Hitchcock
direct?

¼ ½51:25 þ 57:5 þ 63:75=3

B.3. Simple Test 3

¼ 57:5
Result: S (65) > O (57.5). On average, relying on
sensible rules of inference, but using systematically
imperfect information (and which is known to be
imperfect), people believe that they (S) are better than
others (O).
Note that C actually does (40) worse than expected
(50) and everyone knows it [but C knows it best; as
shown in the preceding table, where (C, C) = 40, while
(C, A) = 45, and (C, B) = 45]. Nevertheless, on average,
the group thinks it did better than expected (actual = 65 = estimated > expected = 50). Our theory holds
that, when this occurs, people will (on average) think
they did better than average. The S  O calculation
bears this out: S  O = 7.5 > 0.
The logic outlined above works just the same if
each person’s estimate of his or her own score is
also imperfect and known to be imperfect, and
therefore it also regresses toward the prior. The only
key requirement is that estimates of others be
more regressive than estimates of self, and Rule O
holds.

1. Which American civil rights leader gave a famous
speech in which he repeated the lines, ‘‘I have a
dream. . .’’
2. What American director was behind the movies, A.I.,
E.T., Minority Report, Saving Private Ryan, and
Jurassic Park?
3. What is the name of Pittsburgh’s professional hockey
team?
4. What Pennsylvania city is know for being at the
conﬂuence of the Allegheny and Monongahela
Rivers?
5. What country lies directly north of the United States?
Tiebreaker: How many member states are there in the
United Nations?
B.4. Simple Test 4

B.1. Simple Test 1

1. What American became the ﬁrst person to ever win
the Tour de France 6 times?
2. Paris is the capital of what country?
3. In what large US city is the famous Times Square
located?
4. Where in the human body is the cerebellum located?
5. What famous act of military aggression by Japan
happened on Dec 7, 1941 that brought the United
States into World War II?

1. Who was the ﬁrst president of the United States?
2. How many inches are there in a foot?

Tiebreaker: How many men signed the Declaration
of Independence?

Appendix B. Trivia quizzes (four simple, four diﬃcult),
experiment 1

<-----Page 14----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

211

B.5. Diﬃcult Test 1

B.7. Diﬃcult Test 3

1. In what European city would you ﬁnd the famous
Tivoli Gardens?
2. Truth or Consequences is a city in what US state?
3. What company’s research and development lab was
once known as the ‘‘House of Magic?’’
4. What is the largest moon of Saturn?
5. What African country lies directly south of Egypt?

1. Blues musician Huddie Ledbetter is better known by
what name?
2. What make and model of car holds the record
for being the most widely produced car in the
world?
3. Laudanum is a form of what drug?
4. Who was the president of Indonesia, as of August
2002?
5. What is the capital of Nepal?

Tiebreaker: In the 2000 US Census, what was the
population of Walla Walla, Washington?
B.6. Diﬃcult Test 2
1. Thomas Hooker is associated with the founding of
which of the thirteen American colonies?
2. Who is the only US president to have served two nonconsecutive terms in oﬃce?
3. In Quentin Tarrantino’s Reservoir Dogs, what is the
alias of the man who is revealed to be an undercover
police oﬃcer?
4. In The Odyssey, who was the son of Ulysses
(Odysseus)?
5. Who was voted Time magazine’s Man of the Year in
1938?
Tiebreaker: What is the land area of Morocco (in
square kilometers)?

Tiebreaker: Approximately how many pieces of art
did Pablo Picasso create during his lifetime?
B.8. Diﬃcult Test 4
1. Which team won the ﬁrst NBA Draft Lottery?
2. The Nobel Prizes are awarded in what two
cities?
3. Dr. Faustus is best known for selling what item?
4. What two South American countries are landlocked?
5. Pro football announcer John Madden coached which
team to a Super Bowl victory?
Tiebreaker: How many consecutive weeks did the
Pink Floyd album Dark Side of the Moon spend on
the billboard music charts?

Appendix C
Trivia questions used in the simple and diﬃcult trivia quizzes (Experiment 2).
Simple
1. How many inches are there in a foot?
2. What is the name of Pittsburgh’s professional
hockey team?
3. Which species of whale grows the largest?
4. Who is the President of the United States?
5. Harrisburg is the capital of what US state?
6. What was the ﬁrst name of the Carnegie who founded
the Carnegie Institute of Technology?
7. How many states are there in the United States?
8. What continent is Afghanistan in?
9. What country occupies an entire continent?
10. Paris is the capital of what country?

Diﬃcult
Which creature has the largest eyes in the world?
How many verses are there in the Greek national
anthem?
What company produced the ﬁrst color television
sold to the public?
How many bathrooms are there in the White
House (the residence of the US President)?
Which monarch ruled Great Britain the longest?
The word ‘‘planet’’ comes from the Greek word
meaning what?
What is the name of the traditional currency of
Italy (before the Euro)?
What is Avogadro’s number?
Who played Dorothy in ‘‘The Wizard of Oz’’?
Who wrote the musical ‘‘The Yeoman
of the Guard’’?

Tiebreaker question: How many people live in Pennsylvania?
Answers—Simple: (1) 12 (2) Penguins (3) Blue (4) George W. Bush (5) Pennsylvania (6) Andrew (7) 50 (8) Asia (9) Australia (10) France. Diﬃcult:
(1) Giant squid (2) 158 (3) RCA (4) 32 (5) Queen Victoria (6) wanderer (7) Lira (8) 6.02 · 1023(9) Judy Garland (10) Gilbert and Sullivan. Tiebreaker:
12,281,054.

<-----Page 15----->212

D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213

Appendix D. Supplementary data
Supplementary data associated with this article can
be found, in the online version, at doi:10.1016/j.obhdp.
2006.09.002.

References
Alicke, M. D., Klotz, M. L., Breitenbecher, D. L., Yurak, T. J., &
Vredenburg, D. S. (1995). Personal contact, individuation, and the
better-than-average eﬀect. Journal of Personality and Social Psychology, 68(5), 804–825.
Baron, J. (1997). Confusion of relative and absolute risk in valuation.
Journal of Risk and Uncertainty, 14, 301–309.
Biernat, M., Manis, M., & Kobrynowicz, D. (1997). Simultaneous
assimilation and contrast eﬀects in judgments of self and others.
Journal of Personality and Social Psychology, 73(2), 254–269.
Burson, K. A., & Klayman, J. (2005). Judgments of performance: The
relative, the absolute, and the in-between. Ann Arbor, unpublished
manuscript.
Burson, K. A., Larrick, R. P., & Klayman, J. (2006). Skilled or
unskilled, but still unaware of it: How perceptions of diﬃculty
drive miscalibration in relative comparisons. Journal of Personality
and Social Psychology, 90(1), 60–77.
Camerer, C. F., & Lovallo, D. (1999). Overconﬁdence and excess
entry: An experimental approach. American Economic Review,
89(1), 306–318.
Chambers, J. R., & Windschitl, P. D. (2004). Biases in social
comparative judgments: The role of nonmotivational factors in
above-average and comparative-optimism eﬀects. Psychological
Bulletin, 130(5).
College Board. (1976–1977). Student descriptive questionnaire.
Princeton, NJ: Educational Testing Service.
Cooper, A. C., Woo, C. Y., & Dunkelberg, W. C. (1988). Entrepreneurs’ perceived chances for success. Journal of Business Venturing,
3(2), 97–109.
Dunne, T., Roberts, M. J., & Samuelson, L. (1988). Patterns of ﬁrm
entry and exit in US manufacturing industries. Rand Journal of
Economics, 19(4), 495–515.
Dunning, D., Heath, C., & Suls, J. M. (2004). Flawed self-assessment:
Implications for health, education, and business. Psychological
Science in the Public Interest, 5(3), 69–106.
Dunning, D., Meyerowitz, J. A., & Holzberg, A. D. (1989). Ambiguity
and self-evaluation: The role of idiosyncratic trait deﬁnitions in
self-serving assessments of ability. Journal of Personality and Social
Psychology, 57(6), 1082–1090.
Erev, I., Wallsten, T. S., & Budescu, D. V. (1994). Simultaneous overand underconﬁdence: The role of error in judgment processes.
Psychological Review, 101(3), 519–527.
Fischhoﬀ, B., & De Bruin, Bruine (1999). Fifty-ﬁfty = 50%?. Journal of
Behavioral Decision Making 12(2), 149–163.
Fox, C. R., & Rottenstreich, Y. (2003). Partition priming in judgment
under uncertainty. Psychological Science, 14(3), 195–200.
Geroski, P. A. (1996). What do we know about entry? International
Journal of Industrial Organization, 13(4), 421–441.
Greico, D., & Hogarth, R. M. (2004). Excess entry, ambiguity seeking,
and competence: An experimental investigation, unpublished
manuscript.
Harris, P. (1996). Suﬃcient grounds for optimism? The relationship
between perceived controllability and optimistic bias. Journal of
Social and Clinical Psychology, 15(1), 9–52.
Hayward, M. L. A., & Hambrick, D. C. (1997). Explaining the
premiums paid for large acquisitions: Evidence of CEO hubris.
Administrative Science Quarterly, 42, 103–127.

Hoelzl, E., & Rustichini, A. (2005). Overconﬁdent: Do you put your
money on it? Economic Journal, 115(503), 305–318.
Hoorens, V., & Buunk, B. P. (1993). Social comparison of health
risks: Locus of control, the person-positivity bias, and unrealistic optimism. Journal of Applied Social Psychology, 23(4),
291–302.
Juslin, P., Winman, A., & Olsson, H. (2000). Naive empiricism and
dogmatism in conﬁdence research: A critical examination of the
hard-easy eﬀect. Psychological Review, 107(2), 384–396.
Klar, Y. (2002). Way beyond compare: Nonselective superiority and
inferiority biases in judging randomly assigned group members
relative to their peers. Journal of Experimental Social Psychology,
38(4), 331–351.
Klar, Y., & Giladi, E. E. (1997). No one in my group can be below the
group’s average: A robust positivity bias in favor of anonymous
peers. Journal of Personality and Social Psychology, 73(5), 885–901.
Klar, Y., & Giladi, E. E. (1999). Are most people happier than their
peers, or are they just happy? Personality and Social Psychology
Bulletin, 25(5), 585–594.
Klar, Y., Medding, A., & Sarel, D. (1996). Nonunique invulnerability:
Singular versus distributional probabilities and unrealistic optimism in comparative risk judgments. Organizational Behavior and
Human Decision Processes, 67(2), 229–245.
Klein, W. M. P., & Kunda, Z. (1994). Exaggerated self-assessments
and the preference for controllable risks. Organizational Behavior
and Human Decision Processes, 59(3), 410–427.
Klein, W. M. P., & Weinstein, N. D. (1997). Social comparison and
unrealistic optimism about personal risk. In B. P. Buunk & F. X.
Gibbons (Eds.), Health, coping, and well-being: Perspectives from
social comparison theory (pp. 25–61). Mahwah, NJ: Lawrence
Erlbaum Associates.
Kruger, J. (1999). Lake Wobegon be gone! The ‘‘below-average
eﬀect’’ and the egocentric nature of comparative ability judgments. Journal of Personality and Social Psychology, 77(2),
221–232.
Kruger, J., & Burrus, J. (2004). Egocentrism and focalism in unrealistic
optimism (and pessimism). Journal of Experimental Social Psychology, 40(3), 332–340.
Kruger, J., Windschitl, P. D., Burrus, J., Fessel, F., & Chambers, J. R.
(2006). The rational side of egocentrism in social comparisons,
unpublished manuscript.
Kunda, Z. (1990). The case for motivated reasoning. Psychological
Bulletin, 108(3), 480–498.
Lichtenstein, S., Fischhoﬀ, B., & Phillips, L. D. (1982). Calibration of
probabilities: The state of the art in 1980. In D. Kahneman, P.
Slovic, & A. Tversky (Eds.), Judgment under uncertainty: Heuristics
and biases (pp. 306–333). Cambridge, England: Cambridge University Press.
Malmendier, U., & Tate, G. (2005). CEO overconﬁdence and
corporate investment. Journal of Finance, (60), 6.
Mata, J., & Portugal, P. (1994). Life duration of new ﬁrms. Journal of
Industrial Economics, 42(3), 227–246.
Moore, D. A. (2004). Myopic prediction, self-destructive secrecy, and
the unexpected beneﬁts of revealing ﬁnal deadlines in negotiation.
Organizational Behavior and Human Decision Processes, 94(2),
125–139.
Moore, D. A. (2005). When good = better than average. Pittsburgh:
Tepper Working Paper 2004-E38.
Moore, D. A., & Kim, T. G. (2003). Myopic social prediction and the
solo comparison eﬀect. Journal of Personality and Social Psychology, 85(6), 1121–1135.
Moore, D. A., Oesch, J. M., & Zietsma, C. What competition? Myopic
self-focus in market entry decisions. Organization Science, in press.
Myers, D. G. (1998). Social psychology (5th ed.). New York: McGrawHill.
Odean, T. (1998). Volume, volatility, price, and proﬁt when all traders
are above average. Journal of Finance, 53(6), 1887–1934.

<-----Page 16----->D.A. Moore, D.M. Cain / Organizational Behavior and Human Decision Processes 103 (2007) 197–213
Perloﬀ, L. S., & Fetzer, B. K. (1986). Self-other judgments and
perceived vulnerability to victimization. Journal of Personality and
Social Psychology, 50(3), 502–510.
Price, P. C. (2001). A group size eﬀect on personal risk judgments.
Memory and Cognition, 29, 578–586.
Radzevick, J. R., & Moore, D. A. (2006). For the love of the game?
Betting, prediction, and myopic bias in athletic competition.
Pittsburgh: Tepper Working Paper 2005-E7.
Rose, J. P., & Windschitl, P. D. (2006). How egocentric optimism
change in response to feedback in repeated competitions, unpublished manuscript.
Suls, J. M., Lemos, K., & Stewart, H. L. (2002). Self-esteem, construal,
and comparisons with the self, friends, and peers. Journal of
Personality and Social Psychology, 82(2), 252–261.

213

Svenson, O. (1981). Are we less risky and more skillful than our fellow
drivers? Acta Psychologica, 47, 143–151.
US Small Business Administration. (2003). Longitudinal Establishment and Enterprise Microdata. Washington, DC: Oﬃce of
Advocacy (202-205-6530).
Weinstein, N. D. (1980). Unrealistic optimism about future life events.
Journal of Personality and Social Psychology, 39(5), 806–820.
Windschitl, P. D., Kruger, J., & Simms, E. (2003). The inﬂuence of
egocentrism and focalism on people’s optimism in competitions:
When what aﬀects us equally aﬀects me more. Journal of
Personality and Social Psychology, 85(3), 389–408.
Zajac, E. J., & Bazerman, M. H. (1991). Blind spots in industry and
competitor analysis: Implications of interﬁrm (mis)perceptions for
strategic decisions. Academy of Management Review, 16(1), 37–56.

