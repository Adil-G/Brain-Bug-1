<-----Page 0----->M PRA
Munich Personal RePEc Archive

As-if behavioral economics: Neoclassical
economics in disguise?
Berg, Nathan and Gigerenzer, Gerd

2010

Online at http://mpra.ub.uni-muenchen.de/26586/
MPRA Paper No. 26586, posted 01. December 2010 / 23:20

<-----Page 1----->As-If Behavioral Economics: Neoclassical Economics in Disguise?

Nathan Berg* and Gerd Gigerenzer

Abstract: Behavioral economics confronts a problem when it argues for its scientific relevance
based on claims of superior empirical realism while defending models that are almost surely
wrong as descriptions of true psychological processes (e.g., prospect theory, hyperbolic
discounting, and social preference utility functions). Behavioral economists frequently observe
that constrained optimization of neoclassical objective functions rests on unrealistic assumptions,
and proceed by adding new terms and parameters to that objective function and constraint set
that require even more heroic assumptions about decision processes as arising from solving an
even more complex constrained optimization problem. Empirical tests of these more highly
parameterized models typically rest on comparisons of fit (something equivalent to R-squared)
rather than genuine out-of-sample prediction. Very little empirical investigation seeking to
uncover actual decision processes can be found in this allegedly empirically-motivated
behavioral literature. For a research program that counts improved empirical realism among its
primary goals, it is startling that behavioral economics appears, in many cases, indistinguishable
from neoclassical economics in its reliance on as-if arguments to justify ―psychological‖ models
that make no pretense of even attempting to describe the psychological processes that underlie
human decision making. Another equally startling similarity is the single normative model that
both behavioral and neoclassical economists hold out as the unchallenged ideal for correctly
making decisions. There are differences: neoclassical economists typically assume that firms
and consumers conform to axioms such as transitivity, time consistency, Bayesian beliefs, and
the Savage axioms needed to guarantee that expected utility representations of risk preferences
exist, whereas behavioral economists commonly measure and model deviations from those
axioms. Nevertheless, both programs refer to the same norms, without subjecting to empirical
investigation the question of whether people who deviate from standard rationality are subject to
economically significant losses. In spite of its prolific documentation of deviations from
neoclassical norms, behavioral economics has produced almost no evidence that these deviations
are correlated with lower earnings, lower happiness, impaired health, inaccurate beliefs, or
shorter lives. We argue for an alternative methodological approach focused on veridical
descriptions of decision process and a non-axiomatic normative framework: ecological
rationality, which analyzes the match between decision processes and the environments in which
they are used. To make behavioral economics, or psychology and economics, a more rigorously
empirical science will require less effort spent extending as-if utility theory to account for biases
and deviations, and substantially more careful observation of successful decision makers in their
respective domains.

*Berg is Associate Professor of Economics at University of Texas-Dallas, nberg@utdallas.edu.
1

<-----Page 2----->As-If Behavioral Economics: Neoclassical Economics in Disguise?
Introduction
Behavioral economics frequently justifies its insights and modeling approaches with the
promise, or aspiration, of improved empirical realism (Rabin, 1998, 2002; Thaler, 1991;
Camerer, 1999, 2003). Doing economics with ―more realistic assumptions‖ is perhaps the
guiding theme of behavioral economists, as behavioral economists undertake economic analysis
without one or more of the unbounded rationality assumptions. These assumptions, which count
among the defining elements of the neoclassical, or rational choice, model, are: unbounded selfinterest, unbounded willpower, and unbounded computational capacity.
Insofar as the goal of replacing these idealized assumptions with more realistic ones
accurately summarizes the behavioral economics program, we can attempt to evaluate its success
by assessing the extent to which empirical realism has been achieved. Measures of empirical
realism naturally focus on the correspondence between models on the one hand, and the realworld phenomena they seek to illuminate on the other. This includes both theoretical models and
empirical descriptions. Of course, models by definition are abstractions that suppress detail in
order to focus on relevant features of the phenomenon being described. Nevertheless, given its
claims of improved realism, one is entitled to ask how much psychological realism has been
brought into economics by behavioral economists.
We report below our finding of much greater similarity between behavioral and
neoclassical economics‘ methodological foundations than has been reported by others. It
appears to us that many of those debating behavioral versus neoclassical approaches, or vice
versa, tend to dramatize differences. The focus in this paper is on barriers that are common to

2

<-----Page 3----->both neoclassical and behavioral research programs as a result of their very partial commitments
to empirical realism, indicated most clearly by a shared reliance on Friedman‘s as-if doctrine.
We want to clearly reveal our own optimism about what can be gained by increasing the
empirical content of economics and its turn toward psychology. We are enthusiastic proponents
of moving beyond the singularity of the rational choice model toward a toolkit approach to
modeling behavior, with multiple empirically grounded descriptions of the processes that give
rise to economic behavior and a detailed mapping from contextual variables into decision
processes used in those contexts (Gigerenzer and Selten, 2001).1
Together with many behavioral economists, we are also proponents of borrowing openly
from the methods, theories, and empirical results that neighboring sciences—including, and
perhaps, especially, psychology—have to offer, with the overarching aim of adding more
substantive empirical content. As the behavioral economics program has risen into a respectable
practice within the economics mainstream, this paper describes limitations, as we seem them, in
its methodology that prevent its predictions and insights from reaching as far as they might.
These limitations result primarily from restrictions on what counts as an interesting question (i.e.,
fitting data measuring outcomes, but not veridical descriptions of decision processes leading to
those outcomes); timidity with respect to challenging neoclassical definitions of normative
rationality; and confusion about fit versus prediction in evaluating a model‘s ability to explain
data. We turn now to three examples.
1

Singular definitions of what it means to behave rationally are ubiquitous in the behavioral
economics literature. One particularly straightforward articulation of this oddly neoclassical
tenet appearing as a maintained assumption in behavioral economics is Laibson (2002, p. 22),
who writes: ―There is basically only one way to be rational.‖ This statement comes from a
presentation to the Summer Institute of Behavioral Economics organized by the influential
―Behavioral Economics Roundtable‖ under the auspices of the Russell Sage Foundation (see
http://www.russellsage.org/programs/other/behavioral/, and Heukelom, 2007, on the extent of its
influence).
3

<-----Page 4----->As-If Behavioral Economics: Three Examples
Loss-Aversion and the Long-Lived Bernoulli Repair Program
Kahneman and Tversky‘s (1979) prospect theory provides a clear example of as-if
behavioral economics—a model widely cited as one of the field‘s greatest successes in
―explaining‖ many of the empirical failures of expected utility theory, but based on a problemsolving process that very few would argue is realistic. We detail why prospect theory achieves
little realism as a decision-making process below. Paradoxically, the question of prospect
theory‘s realism rarely surfaces in behavioral economics, in large part because the as-if doctrine,
based on Friedman (1953) and inherited from neoclassical economics, survives as a
methodological mainstay in behavioral economics even as it asserts the claim of improved
empirical realism. 2
According to prospect theory, an individual chooses among two or more lotteries
according to the following procedure. First, transform the probabilities of all outcomes
associated with a particular lottery using a nonlinear probability-transformation function. Then
transform the outcomes associated with that lottery (i.e., all elements of its support). Third,
multiply the transformed probabilities and corresponding transformed lottery outcomes, and sum
these products to arrive at the subjective value associated with this particular lottery. Repeat

2

Starmer (2005) provides an original and illuminating methodological analysis that ties as-if
theory, which appeared in Friedman and Savage a few years before Friedman‘s famous 1953
essay, to potential empirical tests that no one has yet conducted. Starmer shows that both
Friedman and Savage defended expected utility theory on the basis of the as-if defense.
Paradoxically, however, both of them wind up relying on a tacit model of mental process to
justify the proposition that mental processes should be ignored in economics. Starmer writes:
―This ‗as if‘ strategy entails that theories not be judged in terms of whether they are defensible
models of mental processes. So to invoke a model of mental process as a defence of the theory
would … not seem … consistent.‖
4

<-----Page 5----->these steps for all remaining lotteries in the choice set. Finally, choose the lottery with the
largest subjective value, computed according to the method above.
How should one assess the empirical realism achieved by this modeling strategy relative
to its predecessor, expected utility theory? Both prospect theory and expected utility theory
suffer from the shortcoming of assuming that risky choice always emerges from a process of
weighting and averaging (i.e., integration) of all relevant pieces of information. Both theories
posit, with little supporting evidence (Starmer, 2005) and considerable opposing evidence (e.g.,
Branstätter, Gigerenzer and Hertwig, 2006; Leland, 1994; Payne and Braunstein, 1978;
Rubinstein, 1988; Russo and Dosher, 1983), that the subjective desirability of lotteries depends
on all the information required to describe the lottery‘s distribution, in addition to auxiliary
functions and parameters that pin down how probabilities and outcomes are transformed. This is
not even to mention the deeper problem that in many, if not most, interesting choice problems
(e.g., buying a house, choosing a career, or deciding whom to marry), the decision maker knows
only a tiny subset of the objectively feasible action set (Hayek, 1945), the list of outcomes
associated with lotteries, or the probabilities of the known outcomes (Knight, 1921). These
assumptions in both expected utility theory and prospect theory—of transforming, multiplying
and adding, as well as exhaustive knowledge of actions and outcomes (i.e., event spaces
associated with each action)—are equally defensible, or indefensible, in both theories, since they
play nearly identical roles in both theories.
The similarities between prospect theory and expected utility theory should come as no
surprise. Gigerenzer (2008, p. 90) and Güth (1995, 2008) have described the historical
progression—from expected value maximization (as a standard of rationality) to expected utility
theory and then on to prospect theory—as a ―repair program‖ aimed at resuscitating the

5

<-----Page 6----->mathematical operation of weighted integration, based on the definition of mathematical
expectation, as a theory of mind. Expected-value maximization was once regarded as a proper
standard of rationality. It was then confronted by the St. Petersburg Paradox, however, and
Daniel Bernoulli began the repair program by transforming the outcomes associated with
lotteries using a logarithmic utility of money function (or utility of change in money—see
Jorland, 1987, on interpreting Bernoulli‘s units in the expected utility function). This
modification survived and grew as expected utility theory took root in 20 th century neoclassical
economics. Then came Allais‘ Paradox, which damaged expected utility theory‘s ability to
explain observed behavior, and a new repair appeared in the form of prospect theory, which
introduced more transformations with additional parameters, to square the basic operation of
probability-weighted averaging with observed choices over lotteries.
Instead of asking how real people—both successful and unsuccessful—choose among
gambles, the repair program focused on transformations of payoffs (which produced expected
utility theory) and, later, transformations of probabilities (which produced prospect theory) to fit,
rather than predict, data. The goal of the repair program appeared, in some ways, to be more
statistical than intellectual: adding parameters and transformations to ensure that a weightingand-adding objective function, used incorrectly as a model of mind, could fit observed choice
data. We return to the distinction between fit versus prediction below. The repair program is
based largely on tinkering with the mathematical form of the mathematical expectation operator
and cannot be described as a sustained empirical effort to uncover the process by which people
actually choose gambles.

Fehr’s Social Preference Program

6

<-----Page 7----->The insight that people care about others‘ payoffs, or that social norms influence
decisions, represents a welcome expansion of the economic analysis of behavior, which we
applaud and do not dispute.3 Fehr and Schmidt (1999), and numerous others, have attempted to
demonstrate empirically that people generally are other-regarding. Other-regarding preferences
imply that, among a set of allocations in which one‘s own payoff is exactly the same, people may
still have strict rankings over those allocations because they care about the payoffs of others.
Fehr and Schmidt‘s empirical demonstrations begin with a modification of the utility function
and addition of at least two new free parameters. Instead of maximizing a ―neoclassical‖ utility
function that depends only on own payoffs, Fehr and Schmidt assume that people maximize a
―behavioral‖ or other-regarding utility function. This other-regarding utility function, in addition
to a standard neoclassical term registering psychic satisfaction with own payoffs, includes two
arguments that are non-standard in the previous neoclassical literature: positive deviations of
own payoffs from other players‘ payoffs and negative deviations, each weighted with its own
parameter.
As a psychological model, Fehr and Schmidt are essentially arguing that, although it is
not realistic to assume individuals maximize a utility function depending on own payoffs alone,
we can add psychological realism by assuming that individuals maximize a more complicated
utility function. This social preferences utility function ranks allocations by weighting and
summing to produce a utility score for each allocation, and choice is by definition the allocation
with the highest score. The decision process that maximization of a social preferences utility
function implies begins, just like any neoclassical model, with exhaustive search through the
3

Binmore and Shaked (2007) argue that the tools of classical and neoclassical economics can easily take social
factors into account and need not be set off from neoclassical under the behavioral banner. But although Binmore
and Shaked are correct, in principle, that utility theory does not preclude other people‘s consumption from entering
the utility function, they fail to acknowledge the key role of the no-externalities assumption (i.e., no channels other
than price for individuals to affect each other) in the Welfare Theorems and for normative economics in general.

7

<-----Page 8----->decision maker‘s choice space. It assigns benefits and costs to each element in that space based
on a weighted sum of the intrinsic benefits of own payoffs, the psychic benefits of being ahead of
others, and the psychic costs of falling behind others. Finally, the decision maker chooses the
feasible action with the largest utility score based on weighted summation. If the weights on the
―social preferences‖ terms in the utility function registering psychic satisfaction from deviations
between own and other payoffs are estimated to be different than zero, then Fehr and Schmidt
ask us to conclude that they have produced evidence confirming their social preference model.
This approach almost surely fails at bringing improved psychological insight about the
manner in which social variables systematically influence choice in real-world settings. Think of
a setting in which social variables are likely to loom large, and ask yourself whether it sounds
reasonable that people deal with these settings by computing the benefits of being ahead of
others, the costs of falling behind the others, and the intrinsic benefits of own payoffs—and then,
after weighting and adding these three values for each element in the choice set, choosing the
best. This is not a process model but an as-if model. Could anyone defend this process on the
basis of psychological realism? In addition, the content of the mathematical model is barely more
than a circular explanation: When participants in the ultimatum game share equally or reject
positive offers, this implies non-zero weights on the ―social preferences‖ terms in the utility
function, and the behavior is then attributed to ―social preferences.‖
A related concern is the lack of attempts to replicate parameter estimates. Binmore and
Shaked (2007) raise this point in a critique of Fehr and Schmidt (1999)—and of experimental
economics more generally. Binmore and Shaked point out that, if Fehr and Schmidt‘s model is
to be taken seriously as an innovation in empirical description, then a single parameterized
version of it should make out-of-sample predictions and be tested on multiple data sets—without

8

<-----Page 9----->adjusting parameters to each new data set. According to Binmore and Shaked, Fehr and Schmidt
use very different (i.e., inconsistent) parameter estimates in different data sets. To appreciate the
point, one should recall the large number of free parameters in the Fehr and Schmidt model when
subjects are allowed to all have different parameters weighting the three terms in the utility
function. This huge number of degrees of freedom allows the model to trivially fit many sets of
data well without necessarily achieving any substantive improvements in out-of-sample
prediction over neoclassical models or competing behavioral theories. Binmore and Shaked
write:
[T]he scientific gold standard is prediction. It is perfectly acceptable to
propose a theory that fits existing experimental data and then to use the data to
calibrate the parameters of the model. But, before using the theory in applied
work, the vital next step is to state the proposed domain of application of the
theory, and to make specific predictions that can be tested with data that
wasn‘t used either in formulating the theory or in calibrating its parameters.
This may seem so basic as to not be worth repeating. Yet the distinction between fit and
prediction, which has been made repeatedly by others (Roberts and Pashler, 2000), seems to be
largely ignored in much of the behavioral economics literature. Behavioral models frequently
add new parameters to a neoclassical model, which necessarily increases R-squared. Then this
increased R-squared is used as empirical support for the behavioral models without subjecting
them to out-of-sample prediction tests.

Hyperbolic Discounting and Time-Inconsistency

9

<-----Page 10----->Laibson‘s (1997) model of impulsiveness consists, in essence, of adding a parameter to
the neoclassical model of maximizing an exponentially weighted sum of instantaneous utilities to
choose an optimal sequence of quantities of consumption. Laibson‘s new parameter reduces the
weight of all terms in the weighted sum of utilities except for the term representing utility of
current consumption. This, in effect, puts more weight on the present by reducing weight on all
future acts of consumption.
Thus, the psychological process involved has hardly changed at all relative to the
neoclassical model from which the behavioral modification was derived. The decision maker is
assumed to make an exhaustive search of all feasible consumption sequences, compute the
weighted sum of utility terms for each of these sequences, and choose the one with highest
weighted utility score. The parameters of this model are then estimated. To the extent that the
estimated value of the parameter that reduces weight on the future deviates from the value that
recovers the neoclassical version of the model with perfectly exponential weighting, Laibson
asks us to interpret this as empirical confirmation—both of his model, and of a psychological
bias to over-weight the present over the future.
Another example is O‘Donoghue and Rabin (2006), who suggest that willpower
problems can be dealt with by taxing potato chips and subsidizing carrots, to induce people to
overcome their biased minds and eat healthier diets. This formulation, again, assumes a virtually
neoclassical decision process based on constrained optimization in which behavior is finely
attuned to price and financial incentives, in contrast to more substantive empirical accounts of
actual decision processes at work in food choice (Wansink, 2006).

Neoclassical + New Parameters with Psychological Names = Behavioral Economics?

10

<-----Page 11----->A Widely Practiced Approach to Behavioral Economics: “More Stuff” in the Utility Function
In a frequently cited review article in the Journal of Economic Literature, Rabin (1998)
argues that ―greater psychological realism will improve mainstream economics.‖ He then goes
on to describe the improvement to economics that psychology has to offer, not as more accurate
empirical description of the decision processes used by firms and consumers, and not as a broad
search for new explanations of behavior. Rather, Rabin states that the motivation for behavioral
economists to borrow from psychology is to produce a more detailed specification of the utility
function: ―psychological research can teach us about the true form of the function U(x).‖ Thus,
rather than questioning the rationality axioms of completeness, transitivity, and other technical
requirements for utility function representations of preferences to exist—and ignoring the more
substantive and primitive behavioral question of how humans actually choose and decide—
Rabin lays out a behavioral economic research program narrowly circumscribed to fit within the
basic framework of Pareto, Hicks and Samuelson, historical connections that we return to below.
According to Rabin, the full scope of what can be accomplished by opening up economics to
psychology is the discovery of new inputs in the utility function.

Behavioral Utility Functions: Still Unrealistic As Descriptions of Decision Process
Leading models in the rise of behavioral economics rely on Friedman‘s as-if doctrine by
putting forward more unrealistic processes—that is, describing behavior as the process of
solving a constrained optimization problem that is more complex—than the simpler neoclassical
model they were meant to improve upon. Many theoretical models in behavioral economics
consist of slight generalizations of otherwise familiar neoclassical models, with new parameters

11

<-----Page 12----->in the objective function or constraint set that represent psychological phenomena or at least have
psychological labels.
To its credit, this approach has the potential advantage of facilitating clean statistical tests
of rational choice models by nesting them within a larger, more general model class so that the
rational choice model can be tested simply by checking parameter restrictions. But because the
addition of new parameters in behavioral models is almost always motivated in terms of
improving the realism of the model—making its descriptions more closely tied to observational
data—one can justifiably ask how much additional psychological realism is won from this kind
of modeling via modification of neoclassical models. The key point is that the resulting
behavioral model hangs onto the central assumption in neoclassical economics concerning
behavioral process—namely, that all observed actions are the result of a process of constrained
optimization. As others have pointed out, this methodology, which seeks to add behavioral
elements as extensions of neoclassical models, paradoxically leads to optimization problems that
are more complex to solve (Winter 1964, p.252, quoted in Cohen and Dickens, 2002; Sargent,
1993; Gigerenzer and Selten, 2001).4
Aside from this paradox of increasing complexity found in many bounded rationality
models, there is the separate question of whether any empirical evidence actually supports the
modified versions of the models in question. If we do not believe that people are solving
complex optimization problems—and there is no evidence documenting that the psychological

4

Lipman (1999) argues that it is okay if the model representing boundedly rational agents who
cannot solve problem P is the solution to a more complex problem P‘. Lipman‘s argument is that
the solution to this more complex problem is the modeler‘s ―representation‖ and should not be
interpreted as a claim that the decision maker actually solves the harder problem P‘. But this
strikes us as an indirect invocation of Friedman‘s as-if doctrine.
12

<-----Page 13----->processes of interest are well described by such models—then we are left only with as-if
arguments to support them.

Commensurability
A more specific methodological point on which contemporary behavioral and
neoclassical economists typically agree is the use of standard functional forms when specifying
utility functions, which impose the assumption—almost surely wrong—of universal
commensurability between all inputs in the utility function. In standard utility theory, where the
vector (x1,..,xj,…,xk,…xN) represents quantities of goods with the jth and kth element
represented by xj and xk, respectively, commensurability can be defined as follows. For any pair
of goods represented by the indexes j and k, j ≠ k, and for any reduction r in the kth good, 0 < r
<xj, there exists a quantity of compensation in units of the kth good, c > 0, such that the
consumer is at least as well off as she was with the original commodity bundle:
U(x1,..,xj - r,…,xk + c,…xN

1,..,xj,…,xk,…xN)

.

This is sometimes referred to as the Archimedean principle. Geometrically, commensurability
implies that all indifference curves asymptote to the x-axis and y-axis. Economically,
commensurability implies that when we shop for products represented as bundles of features
(e.g., houses represented as vectors of attributes, such as square footage, price, number of
bathrooms, quality of nearby schools, etc.), then no un-dominated items can be discarded from
the consideration set. Instead of shoppers imposing hard-and-fast requirements (e.g., do not
consider houses with less than 2000 square feet),commensurable utility functions imply that
smaller houses must remain in the consideration set. If the price is low enough, or the number of

13

<-----Page 14----->bathrooms is large enough, or the quality of schools is high enough, then a house of any size
could provide the ―optimal‖ bundle of features.
Edgeworth included commensurability among the fundamental axioms of choice.
Psychologists since Maslow have pointed out, however, that people‘s preferences typically
exhibit distinctly lexicographic structure. Moreover, the structures of environments that elicit
compensatory and noncompensatory strategies are relatively well known. An early review of
process tracing studies concluded that there is clear evidence for noncompensatory heuristics,
whereas evidence for weighting and adding strategies is restricted to tasks with small numbers of
alternatives and attributes (Ford et al., 1989).
Recently, researchers in psychology and marketing have produced new evidence of
lexicographic strategies that prove very useful in high-dimensional environments for quickly
shrinking choice sets down to a manageable set of alternatives. The reduction of size in the
consideration sets proceeds by allowing a few choice attributes to completely over-rule others
among the list of features associated with each element in the choice set. This obviates the need
for pairwise tradeoffs among the many pairs of choices and enables choice to proceed in a
reasonable amount of time (Yee, Dahan, Hauser and Orlin, 2007). In a choice set with N
undominated elements where each element is a vector of K features, complete ranking (needed to
find the optimum) requires consideration of KN(N-1)/2 pairwise tradeoffs, which is the number
of features of any alternative multiplied by a quadratic in the number of elements that represents
the number of unordered pairs in the choice set.
Although interesting game-theoretic treatments of lexicographic games have appeared
(Binmore and Samuelson, 1992; Blume, Brandenburger and Dekel, 1991), behavioral and
neoclassical economists routinely seem to forget the absurd implications of universal

14

<-----Page 15----->commensurability, with its unrealistic implication of ruling out lexicographic choice rules. If, for
example, x represents a positive quantity of ice cream and y represents time spent with one‘s
grandmother, then as soon as we write down the utility function U(x, y) and endow it with the
standard assumptions that imply commensurability, the unavoidable implication is that there
exists a quantity of ice cream that can compensate for the loss of nearly all time with one‘s
grandmother. The essential role of social interaction, and time to nurture high quality social
interactions as a primary and unsubstitutable source of happiness, is emphasized by Bruni and
Porta‘s (2007) recent volume on the economics of happiness. The disadvantage of ruling out
lexicographic choice and inference also rules out their advantage of time and effort savings, in
addition to improved out-of-sample prediction in some settings (Czerlinski, Gigerenzer, and
Goldstein, 1999; Gigerenzer and Brighton, 2009).

Fit Versus Prediction
Given that many behavioral economics models feature more free parameters than the
neoclassical models they seek to improve upon, an adequate empirical test requires more than a
high degree of within-sample fit (i.e., increased R-squared). Arguing in favor of new, highly
parameterized models by pointing to what amounts to a higher R-squared (sometimes even only
slightly higher) is, however, a widely practiced rhetorical form in behavioral economics
(Binmore and Shaked, 2007).
Brandstätter et al. (2006) showed that cumulative prospect theory (which has five
adjustable parameters) over-fits in each of four data sets. For instance, among 100 pairs of twooutcome gambles (Erev et al., 2002), cumulative prospect theory with a fit-maximizing choice of

15

<-----Page 16----->parameters chooses 99 percent of the gambles chosen by the majority of experimental subjects.
That sounds impressive. But, of course, more free parameters always improves fit.
The more challenging test of a theory is in prediction using a single set of fixed
parameters. Using the parameter values estimated in the original Tversky and Kahneman (1992)
study, cumulative prospect theory could predict only 75 percent of the majority choices. The
priority heuristic (a simple lexicographic heuristic with no adjustable parameters), in contrast,
predicts 85 percent of majority choices. Moreover, when the ratio of expected values is larger
than two (so-called ―easy problems‖ where there is wide consensus among most subjects that one
gamble dominates the other), cumulative prospect theory does not predict better than expected
value or expected utility maximization (Brandstätter, Gigerenzer and Hertwig, 2008, Figure 1).
When the ratio of expected values is smaller, implying less consensus among subjects about the
ranking of two gambles, the priority heuristic predicts far better than cumulative prospect theory.
Thus, in prediction, cumulative prospect theory does not perform better than models with no free
parameters.
Examples of psychological parameters introduced to generalize otherwise standard
neoclassical models include Kahneman and Tversky‘s (1979) prospect theory in which new
parameters are needed to pin down the shape of functions that under- or over-weight
probabilities; Laibson‘s (1997) model of impulsiveness expressed in terms of new parameters
controlling the shape of non-exponential weights in the inter-temporal optimization problem
referred to as hyperbolic discounting; and Fehr and Schmidt‘s (1999) psychic weights on
differences between own and others‘ payoffs. There are many other examples, which include
overconfidence (with at least three different versions concerning biases in first and/or second
moments and own beliefs versus the beliefs of others); biased belief models; ―mistake‖ or

16

<-----Page 17----->tremble probabilities; and social preference utility functions with parameters that measure
subjective concern for other people‘s payoffs.
By virtue of this modeling strategy based on constrained optimization, with virtually all
empirical work addressing the fit of outcomes rather than justifying the constrained optimization
problem-solving process itself, behavioral economics follows the Friedman as-if doctrine in
neoclassical economics focusing solely on outcomes. By adding parameters to increase the Rsquared of behavioral models‘ fit, many behavioral economists tacitly (and sometimes explicitly)
deny the importance of correct empirical description of the processes that lead to those decision
outcomes.

Behavioral and Neoclassical Economics Share a Single Normative Model
Is there such a thing as normative behavioral economics? At first, behavioral economists
such as Tversky, Kahneman, Frank and Thaler almost unanimously said no (Berg, 2003).

The Early Normative View: Deviations Are Strictly Descriptive, No Normative Behavioral
Economics Needed
Tversky and Kahneman (1986) write:
The main theme of this article has been that the normative and the descriptive
analysis of choice should be viewed as separate enterprises. This conclusion
suggests a research agenda. To retain the rational model in its customary
descriptive role, the relevant bolstering assumptions must be validated. Where
these assumptions fail, it is instructive to trace the implications of the
descriptive analysis.

17

<-----Page 18----->Perhaps it was a reassuring sales pitch when introducing behavioral ideas to neoclassical
audiences. But for some reason, early behavioral economists argued that behavioral economics
is purely descriptive and does not in any way threaten the normative or prescriptive authority of
the neoclassical model. These authors argued that, when one thinks about how he or she ought
to behave, we should all agree that the neoclassical axioms ought to be satisfied. This is
Savage‘s explanation for his own ―mistaken‖ choice after succumbing to the Allais Paradox and
subsequently revising it ―after reflection‖ to square consistently with expected utility theory
(Starmer, 2004). By this unquestioning view toward the normative authority of the neoclassical
model, the only work for behavioral economics is descriptive—to document empirical deviations
from neoclassical axioms: transitivity violations, expected utility violations, time-inconsistency,
non-Nash play, non-Bayesian beliefs, etc.
Fourteen years before writing ―Libertarian Paternalism,‖ Thaler also explicitly warms not
to draw normative inferences from his work (Thaler, 1991, p. 138):
A demonstration that human choices often violate the axioms of rationality
does not necessarily imply any criticism of the axioms of rational choice as a
normative idea. Rather, the research is simply intended to show that for
descriptive purposes, alternative models are sometimes necessary.
Continuing this discussion of what behavioral economics implies about the use of
rationality axioms in normative analysis, Thaler (1991, p. 138) argues that the major contribution
of behavioral economics has been the discovery of a collection of ―illusions,‖ completely
analogous to optical illusions. Thaler interprets these ―illusions‖ as unambiguously incorrect
departures from the ―rational‖ or correct way of making decisions. Thaler is explicit in accepting
neoclassical axioms of individual preferences (e.g. transitivity, completeness, non-satiation,

18

<-----Page 19----->monotonicity, and the Savage axioms which guarantee that preferences over risky payoffs can be
represented by an expected utility function) as the proper normative ideal when he writes: ―It
goes without saying that the existence of an optical illusion that causes us to see one of two equal
lines as longer than the other should not reduce the value we place on accurate measurement. On
the contrary, illusions demonstrate the need for rulers!‖
In his interpretation of optical illusions, Thaler does not seem to realize that, if the human
faculty of visual perception mapped two-dimensional images directly onto our retinas and into
the brain without filtering, then we would have an objectively inferior grasp on reality. Consider
a photograph of railroad tracks extending into the distance, which appear narrower and narrower
when projected into two-dimensional space but are filtered in our minds as maintaining constant
width in three-dimensional space. Thaler seems to suggest that when we see the train tracks
narrowing in their two-dimensional representation, we would be more rational to see them as
narrowing rather than synthesizing the third dimension that is not really there in the photo.
Without deviating from this direct translation of the information in two-dimensional space, our
minds would perceive the tracks as uneven and unsuitable for any train to run on.
To correctly perceive reality, perceptive faculties must add information, make intelligent
bets, and consequently get it wrong some of the time. A line that extends into the third
dimension has a shorter projection on the retina than a horizontal line of the same length. Our
brains correct for this by enlarging the subjective length of the line that extends into the third
dimension, which works in the real three-dimensional world, but results in optical illusions when
interpreting information on two-dimensional paper. Our brains are intelligent exactly because
they make informed guesses, and go beyond the information given. More generally, intelligent
systems depend on processes that make useful errors (Gigerenzer, 2008).

19

<-----Page 20----->Yet, in showing that human decisions contradict the predictions of expected utility
theory, there is no analog to the straight lines of objectively equal length. Unlike the simple
geometric verification of equal lengths against which incorrect perceptions may be verified, the
fact that human decisions do not satisfy the axioms underlying expected utility theory in no way
implies an illusion or a mistake. Expected utility theory is, after all, but one model of how to
rank risky alternatives. Those who insist that standard neoclassical theory provides a singularly
correct basis for normative analysis in spite of systematic departures in the empirical record
assert, in effect, that behavioral economics is a purely descriptive field of inquiry (Berg 2003).

A Second Normative View: Designing Policy to Achieve Conformity With Neoclassical Norms
Fast forward 10 years, and behavioral economists now can be found regularly offering
prescriptive policy advice based on behavioral economics models. The stakes have risen in
recent years and months, as financial market crises generate new skepticism about the
―rationality of markets.‖ Behavioral economists who decades ago pitched the behavioral
approach to the neoclassical mainstream as a purely descriptive enterprise (e.g., Tversky and
Kahneman, 1986; Thaler, 1991; Frank, 1991—and nearly everyone else published in top-ranked
economics journals), now advocate using behavioral concepts for prescriptive policy purposes
(Thaler and Sunstein, 2008; Frank, 2008; Amir, Ariely, Cooke, Dunning, Epley, Koszegi,
Lichtenstein, Mazar, Mullainathan, Prelec, Shafir, and Silva, 2005). This evolution in boldness
about looking for prescriptive implications of behavioral economics does not, unfortunately,
imply increased boldness about modifying the neoclassical axiomatic formulations of rationality
as the unquestioned gold standard for how humans ought to behave.

20

<-----Page 21----->One specific example of this view that humans are biased and pathological—based on the
biases and heuristics literature‘s abundant empirical accounts of deviations from neoclassical
rationality axioms (but not tied empirically to substantive economic pathology)—is Bernheim
and Rangel (2007). They suggest new approaches to regulation and policy making based on the
dominant behavioral economics view of ubiquitous behavioral pathology. Jolls, Sunstein and
Thaler (1998) write of the need to write laws that ―de-bias‖ individual decision making. Rather
than resting on direct observation of bad-performing decision-making processes embedded in
real-world decision-making domains, these prescriptive claims follow from psychological
parameter estimates fitted, in many cases, to a single sample of data. The estimated parameter
that maximizes fit leads to a rejection of the neoclassical model nested within the encompassing
behavioral model, and readers are asked to interpret this as direct, prima facie evidence of
pathological decision making in need of correction through policy intervention.

Predictably Stupid, Smart, or None of the Above
Rabin (2002) says psychology teaches about departures from rationality. Diamond
(2008) writes that a major contribution of ―behavioral analysis is the identification of
circumstances where people are making ‗mistakes.‘‖ Beshears, Choi, Laibson and Madrian
(2008) introduce a technique for identifying mistakes, formulated as mismatches in revealed
preference versus what they call normative preferences, which refer to preferences that conform
to neoclassical axioms. To these writers (and many if not most others in behavioral economics),
the neoclassical normative model is unquestioned, and empirical investigation consists primarily
of documenting deviations from that normative model, which are automatically interpreted as
pathological. In other words, the normative interpretation of deviations as mistakes does not

21

<-----Page 22----->follow from an empirical investigation linking deviations to negative outcomes. The empirical
investigation is limited to testing whether behavior conforms to a neoclassical normative ideal.
Bruni and Sugden (2007) point out the similar methodological defense needed to
rationalize the common normative interpretations in both neoclassical and behavioral economics:
The essential idea behind the discovered preference hypothesis is that
rational-choice theory is descriptive of the behaviour of economic agents who,
through experience and deliberation, have learned to act in accordance with
their underlying preferences; deviations from that theory are interpreted as
short-lived errors.
The discussion of methodological realism with respect to the rational choice framework
almost necessarily touches on different visions of what should count as normative. It is a great
irony that most voices in behavioral economics, purveyors of a self-described opening up of
economic analysis to psychology, hang on to the idea of the singular and universal supremacy of
rational choice axioms as the proper normative benchmarks against which virtually all forms of
behavior are to be measured. Thus, it is normal rather than exceptional to read behavioral
economists championing the descriptive virtues of expanding the economic model to allow for
systematic mistakes and biased beliefs and, at the same time, arguing that there is no question as
to what a rational actor ought to do.
This odd tension between descriptive openness and normative dogmaticism is interesting,
and future historians of behavioral economics will surely investigate further the extent to which
this hardening of the standard normative model in the writings of behavioral economists served
as compensation for out-and-out skeptics of allowing psychology into economics—perhaps, in
order to persuade gatekeepers of mainstream economics to become more accepting of behavioral

22

<-----Page 23----->models when pitched as an exclusively descriptive tool. One reason why the tension is so
interesting is that almost no empirical evidence exists documenting that individuals who deviate
from economic axioms of internal consistency (e.g., transitive preferences, expected utility
axioms, and Bayesian beliefs) actually suffer any economic losses. No studies we are aware of
show that deviators from rational choice earn less money, live shorter lives, or are less happy.
The evidence, to date, which we describe in a later section, suggests rather the opposite.
Like neoclassical economists, behavioral economists assert that logical deduction rather
than inductively derived descriptions of behavioral process are the proper starting point for
economic analysis. Behavioral economists allow that real people‘s beliefs (and nearly
everything else the neoclassical model specifies) may deviate from this deductive starting point
in practice. But they insist that these individuals who deviate from axiomatic rationality should
aspire to minimize deviance and conform to the neoclassical ideal as much as possible.

Ecological Rationality
A Definition Based On The Extent Of Match Between Behavior and Environments
It is no trivial question as to whether substantive rather than axiomatic rationality requires
preferences to exist at all. The essentializing concept of a stable preference ordering ignores the
role of context and environment as explanatory variables that might condition what it means to
make a good decision. In this regard, preferences in economics are analogous to personality
traits in psychology. They seek to explain behavior as a function of exclusively inherent and
essential contents of the individual rather than investigating systematic interaction of the
individual and the choice or decision environment.

23

<-----Page 24----->In contrast, the normative framework of ecological rationality eschews universal norms
that generalize across all contexts, and instead requires decision processes to match well with the
environments in which they are used (Gigerenzer, Todd, and the ABC Group, 1999). Ecological
rationality focuses on the question of which heuristics are adapted to which environments.
Vernon Smith‘s definition of ecological rationality is virtually the same, except that he replaces
―heuristics‖ with ―institutions‖ or ―markets.‖
When heuristics, or decision processes—or action rules—function well in particular
classes of environments, then ecological rationality is achieved. When systematic problems
arise, the diagnosis does not lay blame exclusively on badly behaved individuals (as in
behavioral economics) or external causes in the environment (as in many normative analyses
from sociology). Rather, problems are diagnosed in terms of mis-matched decision process and
environment, which suggests more degrees of freedom (than the universally pathological view
based on a normative ideal of omniscience) when prescribing corrective policy and new
institutional design.

Better Norms
Given the explicitly stated commitment in behavioral economics to empiricism and
broader methodological openness (borrowing from psychology and sociology), it is surprising
that behavioral economics would adhere so closely to the normative neoclassical model, because
there are real alternatives in terms of positive normative frameworks from fields such as
psychology, Austrian economics, social economics, biology, and engineering. In spite of
hundreds of papers that purport to document various forms of ―irrationality‖ (e.g., preference
reversals, deviations from Nash play in strategic interaction, violations of expected utility theory,

24

<-----Page 25----->time inconsistency, non-Bayesian beliefs), there is almost no evidence that such deviations lead
to any economic costs.5 Thus—separate from the lack of evidence that humans make highstakes decisions by solving constrained optimization problems—much of the behavioral
economics research program is predicated on an important normative hypothesis for which there
is, as yet, very little evidence.
Are people with intransitive preferences money-pumped in real life? Do expected utility
violators earn less money, live shorter lives, or feel less happy? Do non-Bayesians
systematically misperceive important frequencies and incur real economic losses as a result?
These questions would seem to be the key stylized facts in need of firm empirical
justification in order to motivate the prolific research output in behavioral economics
documenting biases and deviations. But instead of empirical motivation, behavioral
economics—while justifying itself in terms of more rigorous empiricism—puzzlingly follows the
neoclassical tradition laid out by Pareto in justifying its normative positions by vague,
introspective appeals to reasonableness, without empirical inquiry (Starmer, 2005).
Our own empirical research tries to answer some of these questions about the economic
costs of deviating from neoclassical axioms, with surprising results. Expected utility violators
and time-inconsistent decision makers earn more money in experiments (Berg, Johnson, Eckel,
2009). And the beliefs about PSA testing of non-Bayesians are more accurate than those of
perfect Bayesians—that is, better calibrated to objective risk frequencies in the real-world
decision-making environment (Berg, Biele and Gigerenzer, 2008). So far, it appears that people
who violate neoclassical coherence, or consistency, axioms are better off as measured by
correspondence metrics such as earnings and accuracy of beliefs. Recall that according to
5

One recent example is DeMiguel et al (forthcoming), who finds that portfolios which deviate from the normative
CAPM model by using a simple 1/N heuristic produce higher expected returns and lower risk, relative to portfolios
chosen according to CAPM.

25

<-----Page 26----->rationality norms requiring only internal coherence, one can be perfectly consistent, and yet
wrong about everything (Hammond, 1996).
There are a growing number of theoretical models, too, where individuals (Dekel, 1999;
Compte and Postlewaite, 2004) and markets (Berg and Lien, 2005) do better with incorrect
beliefs. These results pose fundamental questions about the normative status of assumptions
regarding probabilistic beliefs and other core assumptions of the rational choice framework. If
individuals and aggregates both do better (Berg and Gigerenzer, 2007) when, say, individuals
satisfice instead of maximize, then there would seem to be no market discipline or evolutionary
pressure (arguments often invoked by defenders of the normative status of rationality axioms) to
enforce conformity with rationality axioms, which focus primarily on internal consistency rather
than evaluation of outcomes themselves.
In a variety of binary prediction tasks, Gigerenzer, Todd and the ABC Group (1999)
have shown that simple heuristics that ignore information and make inferences based on
lexicographic rather than compensatory (weighting and adding) decision procedures are more
accurate in prediction than full-information Bayesian and regression models that simultaneously
weight and consider all available information. Berg and Hoffrage (2008) provide theoretical
explanations for why ignoring free information can be adaptive and successful. Starmer (2005)
makes a number of relevant points on this issue, and Gilboa, Postlewaite and Schmeidler (2004)
expand on the arguments of Hammond‘s (1996) regarding the normative insufficiency of internal
coherence alone. These authors are highly unusual in expressing doubt about whether Bayesian
beliefs, and other normative axioms of internal consistency, should be relied upon as normative
principles.

26

<-----Page 27----->Gaze Heuristic
How do baseball players catch fly balls? Extending Friedman‘s as-if model of how
billiards players select their shots, one might follow the neoclassical as-if modeling approach and
assume that baseball players use Newtonian physics. According to this as-if theory of catching a
fly ball, players would rely upon variables such as initial position, initial velocity, rotation and
wind speed, to calculate the terminal position of the ball and optimal direction in which to run.
There are several observable facts that are inconsistent with this as-if model, however.
First, baseball players catching fly balls do not typically run to the landing position of the ball
and wait for it there. They frequently run away from the ball first, backing up, before reversing
course inward toward the ball, which is not predicted by the as-if theory. Finally, experiments
that ask baseball players to point to the landing location of the ball reveal that experts with
specialized training in catching balls have a very difficult time pointing to the landing position of
the ball. Nevertheless, because they consistently catch fly balls, these players are employing a
decision process that gets them to the proper location at the proper time. This process is the gaze
heuristic.
The gaze heuristic is a genuine process model that explains how the player puts his or her
body in the proper position to catch fly balls. When a fly ball is hit, the player waits until the
ball reaches a sufficiently high altitude. The player then fixes this angle between his or her body
and the ball and begins running to maintain this angle at a nearly constant measure. To keep the
angle fixed as the ball begins to plummet toward earth, one must run to a position that eventually
converges to directly under the ball.
Maintaining a fixed angle between the player and the ball gets the body to the right place
at the right time. This process of maintaining the angle implies that sometimes players will have

27

<-----Page 28----->to back up before running inward toward home plate. This process also does not depend on any
optimally chosen parameters. For example, there is a wide and dense range of angles that the
player can choose to maintain and still catch the ball. No ―optimal angle‖ is required.
The benefits of this genuine process model are many. For one, we have a realistic
description of how balls are caught, answering to the descriptive goal of science. For the
normative and prescriptive dimensions, the benefits are perhaps even more noticeable. Suppose
we were to use the as-if model to design a policy intervention aimed at inducing better
performance catching fly balls. The as-if theory suggests that we should provide more or clearer
information about initial position, initial velocity, wind speed and ball rotation. That could
mean, for example, that a computer monitor in the outfield instantly providing this information to
outfielders would improve their performance. Should we take this seriously?
In contrast, the gaze heuristic suggests that patience to allow the ball to reach high
overhead, good vision to maintain the angle, and fast running speed are among the most
important inputs into success at catching fly balls. Thus, process and as-if models make distinct
predictions (e.g., running in a pattern that keeps the angle between the player and ball fixed
versus running directly toward the ball and waiting for it under the spot where it will land; and
being able to point to the landing spot) and lead to distinct policy implications about
interventions, or designing new institutions, to aid and improve human performance.

Empirical Realism Sold, Bought and Re-Sold
This section summarizes the historical trajectory of debates about empirical realism in
economics in the 20th century that is more stylized than detailed, but nevertheless describes a
hypothesis about the status of claims to realism in economics. This summary underscores links

28

<-----Page 29----->between debates about, and within, behavioral economics, and the long-standing influence of
Pareto in the shift away from psychology toward the as-if interpretation of models and deemphasis of decision-making process in economics. Dismissing empirical realism as an
unneeded element in the methodology of economics, the post-Pareto neoclassical expansion
under the guidance of Paul Samuelson might be described as: ―empirical realism sold.‖ In other
words, after Pareto‘s arguments took root in mainstream English language economics, the field
proceeded as if it no longer cared much about empirical realism regarding the processes that give
rise to economic decisions.
When behavioral economics arrived upon the scene, its rhetoric very explicitly tied its
own program and reason for being to the goal of improved empirical realism. This initial phase
of behavioral economics could be referred to as ―empirical realism bought,‖ because
practitioners of behavioral economics, as it was first trying to reach a broader audience,
emphasized emphatically a need for psychology and more empirical verification of the
assumptions of economics.
Then, perhaps after discovering that the easiest path toward broader acceptance into the
mainstream was to put forward slightly modified neoclassical models based on constrained
optimization, the behavioral economics program shed its ambition to empirically describe
psychological process, adopting Friedman‘s as-if doctrine. Thus, the second phase in the
historical trajectory of behavioral economics is described here as: ―empirical realism re-sold.‖

Realism Sold
Bruni and Sugden (2007) point out interesting parallels between proponents of behavioral
economics (who argued for testing the assumptions of the rational choice model with

29

<-----Page 30----->observational data against defenders of neoclassical economics arguing in favor of unbounded
rationality assumptions) and participants in an earlier methodological debate. The earlier debate
took place within neoclassical economics about the role of psychology in economics, in which
Vilfredo Pareto played a prominent role. According to Bruni and Sugden, the neoclassical
program, already underway as Pareto wrote, took a distinct turn as Hicks and Allen, Samuelson,
and Savage, made use of Pareto‘s arguments against using anything from psychology (e.g., the
Fechner-Weber Law used earlier as a foundation for assuming diminishing marginal utility, or
the beginnings of experimental psychology as put forth in Wilhelm Wundt‘s Grundzuge der
physiologischen Psychologie published in 1874) in economics. Pareto argued in favor of
erecting a clear boundary insulating economic assumptions from certain forms of empirical
inquiry and, rather than inductive empiricism, he advocated much greater emphasis on logical
deduction.
The psychology of Pareto‘s day was hardly vacuous as some defenders of the Pareto-led
shift away from psychology in economics have claimed. And Pareto was enthusiastic about
using psychology and sociology to solve applied problems, even as he argued that economics
should be wholly distinct and reliant solely on its own empirical regularities. Pareto argued for a
deductive methodology very much like the contemporary rational choice model in which all
decisions were to be modeled as solutions to constrained optimization problems. To understand
how Pareto could use ideas and data from psychology and sociology in some settings but argue
unequivocally for eliminating these influences from economics, Bruni and Sugden explain that
the neoclassical economics of Pareto‘s time, which changed dramatically as a result of his
positions, was seen as encompassing complementary psychological and economic branches
within a common research paradigm:

30

<-----Page 31----->This programme was not, as behavioural economics is today, a self-consciously
distinct branch of the discipline: it was a central component of neoclassical
economics. Neoclassical economics and experimental psychology were both
relatively young enterprises, and the boundary between them was not sharply
defined. According to what was then the dominant interpretation, neoclassical
theory was based on assumptions about the nature of pleasure and pain. Those
assumptions were broadly compatible with what were then recent findings in
psychophysics. Neoclassical economists could and did claim that their theory was
scientific by virtue of its being grounded in empirically-verified psychological
laws. … Viewed in historical perspective, behavioural economists are trying to
reverse a fundamental shift in economics which took place from the beginning of
the twentieth century: the ‗Paretian turn‘. This shift, initiated by Vilfredo Pareto
and completed in the 1930s and 1940s by John Hicks, Roy Allen and Paul
Samuelson, eliminated psychological concepts from economics by basing
economic theory on principles of rational choice.
Pareto‘s deliberate shift away from psychology also entailed a shift away from large
categories of empirical source material. In this sense, the so-called Paretian turn in the
history of economics can be summarized, perhaps too simply, but not inaccurately, as a
divestiture of earnest empirical inquiry into the processes by which firms and consumers
make decisions. The question of decision process, in the eyes of Pareto, Hicks and
Samuelson, was a solved problem with a singular answer: choice in economics was defined
as the solution to an appropriately specified constrained optimization problem. This relieved
economics from investigating further the question of how firms and consumers actually make

31

<-----Page 32----->decisions, and shifted the terms of economic analysis toward the business of discovering
parameters in objective functions and constraint sets, whose maximizing action rule
(mapping exogenous parameters into actions) seemed to capture the regularities that
economists regarded, based on introspection, as natural and self-evident, such as downwardsloping demand curves or diminishing marginal utility.
Pareto argued that, for simplification, economics should assume that subjective
beliefs about the economic environment coincide with objective facts. Thus, for Pareto and
many who re-launched Pareto‘s program in the 1930s, the question of how well people‘s
subjective experience of economic phenomena match the objective structure of the
environment is assumed away. There is no question of calibration, or correspondence to the
real-world. Pareto defended this by limiting the domain of phenomena to which economic
theory was to be applied, in sharp contrast to promulgators of the Pareto program who later
claimed that deductive logic of rational choice models vastly expanded the range of realworld phenomena to which the theory applies.

Realism Bought
Advocates for behavioral economics who have come to prominence in the last two
decades frequently make the case that economics will benefit by more openly embracing the
empirical lessons of psychological experiments, economic experiments, and standard
econometric data sources filtered through models that allow for behavioral phenomena, such as
loss aversion in choice under uncertainty and quasi-hyperbolic discounting in inter-temporal
choice. This phase in the history of behavioral economics can be described as ―empirical realism
bought‖—bought in the sense of the economics discipline siding with arguments made by

32

<-----Page 33----->contemporaries of Pareto who disagreed with him, arguing in favor of using psychological data
and behavioral regularities put forward by psychologists in economics (e.g., Pantaleoni,
1889/1960).

Realism Re-Sold
In an earlier section ―As-If Behavioral Economics,‖ we considered three prominent
theories, often cited as leading examples of the success of behavioral economics. We argued,
however, that these three models are not serious attempts at psychological realism and rather rely
on Friedman‘s as-if defense to justify modeling psychological choice as the solution to an even
more elaborate constrained optimization problem. These models exemplify the ―realism re-sold‖
phase in the historical trajectory of behavioral economics. ―Realism re-sold‖ describes
behavioral economics‘ retreat from investigating actual decision processes, conforming instead
to Friedman‘s as-if defense of unrealistic models. The unrealistic models now being defended are
endowed with additional parameters given psychological labels, resting on the claim that people
behave as if they are solving a complicated constrained optimization problem with bounds on
self-interest, willpower, or computational capacity explicitly modeled in the objective function or
constraint set. This strange new methodological configuration, motivated in terms of improved
empirical realism, and defended—not on the grounds of corresponding to actual decision
processes—but according to the as-if line of defense, can be described as As-If Behavioral
Economics.

Pareto as Precursor to As-If

33

<-----Page 34----->To the neoclassicals following Pareto‘s position, an economics defined by axioms of
perfect internal consistency as the standard of rationality was to provide essential insights into
how consumers and firms‘ behavior would change when shifting from one equilibrium to
another as a result of a change in a single exogenous parameter. Thus, the methodology was to
maintain in all cases—rather than test or investigate—the assumptions of transitive preference
orderings, expected utility axioms (after Savage), and beliefs that are internally coherent by
satisfying Bayes Rule. A number of neoclassical economists acknowledged that predicted
changes in behavior generated by shifting from one equilibrium to another in response to an
exogenous change, of course, abstracts from many other influences that are potentially important
(i.e., those that psychologists and sociologists focus on).
The neoclassicals argued, however, that their predictions, free from psychological or
sociological factors, were good enough (ironically, a satisficing argument about the aspirations
of their theory), and should be interpreted as predictions about behavior after many repetitions
when, it was assumed, behavior would converge to the ideal choice predicted by rational choice
theory. Bruni and Sugden (2007) point out problems with this position, some of which Pareto
was aware of, and some of which seem to persist in the defenses of rational choice theory offered
today.
An interesting point of contrast is the very recent justifications for behavioral economics
put forward by leading behavioral economists such as Rabin and Thaler, and these same authors‘
earlier writings in which much deeper skepticism was occasionally expressed about the utility
function framework. An example is Thaler‘s writing in the first round of Journal of Economic
Perspectives ―Anomalies‖ series, where Thaler‘s conclusions would sometimes mention deep
doubts that good descriptions of behavior could ever be achieved without deeper methodological

34

<-----Page 35----->revisions in economics. Not surprisingly, the part of the behavioral economics movement that
won easiest acceptance was the part which was methodologically closest to neoclassical norms,
following the path of constrained optimization models with an additional psychological
parameter or two.
It is striking that the behavioral economists who successfully sold psychology to
neoclassical economists are among the most hardened and staunch defenders of the normative
status of the neoclassical model. Whereas neoclassical economists frequently interpret their
models as essentialized approximations, from which deviations are expected to average out in the
aggregate, many behavioral economists use the rationality standard of neoclassical economics
more literally and rigidly than their neoclassical colleagues.
In contrast to the un-psychological spirit of much writing on psychology in behavioral
economics, there are some, such as Conlisk (1996), who appreciate that contemporary
psychology‘s use of the term heuristics (i.e., shortcut decision processes not generally derived by
solving a constrained optimization problem) often implies a useful shortcut to solving a difficult
problem—and not a pathological deviation from axiomatic rationality. Particularly when the cost
of information is high, or the optimization problem has many dimensions that make its solution
very costly or impossible, a heuristic can provide a valuable procedure for making the decision
well. The study of ecological rationality has shown that the function of heuristics is not
restricted to this short-cut interpretation, also known as the accuracy-effort trade-off. By
ignoring information, a heuristic can be more accurate in making predictions in a changing and
uncertain world than a strategy that does not conditions on all available information—so-called
less-is-more effects (Gigerenzer and Brighton, 2009).

35

<-----Page 36----->The debates between behavioral economics and neoclassical economics echo earlier
debates in economics from the first half of the 20th century. An interesting dimension of
historical similarity are the debates about decision-making processes, prominent in the
psychology literature, but virtually absent in both postwar neoclassical economics and
contemporary behavioral economics. These missing debates about decision-making process in
economics concern whether constrained optimization is realistic or empirically justified, and
whether a more directly empirical account of decision-making process can lead to better
descriptive and normative economics. The seemingly opposing subfields of neoclassical and
behavioral economics, it seems, rely on a common rhetorical strategy that traces back to the
methodological shifts in economics away from psychology around the time of Pareto.

If Economics Becomes an Empirical Science…
Critiques Of Rationality Assumptions Are Nothing New
Long before the contemporary behavioral economics program came to prominence, the
economics discipline saw a good deal of complaining about the strictures of rationality
assumptions—especially the ones required to rationalize a utility function representation of a
preference ordering, and the self-interested rational actor model—long before Herbert Simon or
the current leaders of the behavioral economics program began writing. One recalls Veblen‘s
conspicuous consumption in The Theory of the Working Class (1899), Keynes‘s ―animal spirits‖
in the general theory (1936), Galbraith‘s ―Rational and Irrational Consumer Preference‖ (1938),
and Hayek‘s (1945) critique of the disconnect between maximization of given preferences over
known choice sets versus ―the economic problem which society faces,‖ which rests on the
radical limitations on economic actors‘ knowledge.

36

<-----Page 37----->In fact, earlier writers before the rise of general equilibrium theory and subsequent
ascendancy of highly rationalist game theory in the 1980s frequently expressed interest in
decision processes other than those posited in the rational choice model. One finds deep
sympathy in Smith‘s (1759/1997) writings on sentiments, and in writers going back to antiquity
(Bruni and Porta, 2007), for the proposition that economic behavior takes multiple forms
depending on social context.6 In this light, it would seem that the singularity of the rational
choice model within neoclassical economists‘ methodological toolkit in post-war North
American economics (together with its strict normative interpretation) is anomalous when
compared to longer-standing norms allowing for a much wider range of behavioral models in
economics.
Proponents of genuine process models would argue that, especially when predicting how
a new policy or institution will perform, the range of variation in the data used to fit various
models may not give illuminating predictions over the relevant ranges of variables after policy
and institutions shift. If the actual process generating economic decisions is better understood,
however, then social science has a firmer basis to make important predictions about behavior
under new and imagined institutional arrangements. Process models would therefore play a
crucial role in furthering both the creativity and predictive accuracy of economists attempting to
imagine and design new institutions—where success hangs upon how such institutions might
interact with the repertoire of heuristics and behavioral rules widely used in a population.

Naming Problem7

6

Ashraf, Camerer and Loewenstein‘s (2007) article, ―Adam Smith, Behavioral Economist,‖
pushes this claim to an extreme.
7

The term behavioral economics seems to have been coined by the psychologist George Katona, who established
the Survey Research Center (SRC), part of the Institute for Social Research (IRS) at University of Michigan. Amos

37

<-----Page 38----->In thinking about future histories of behavioral economics, the term ―behavioral‖ itself is
already problematic on two counts at least. First, as many have pointed out, it seems ironic that a
social science would need to call itself ―behavioral‖—distinguishing itself from apparently nonbehavioral social sciences? Given the anti-empirical flavor of as-if defenses of economic
analysis that is explicitly uncurious about the ―black box‖ of mind that generates economic
decisions, the behavioral label could have implied a useful critique. However, when one digs
into the methodological arguments put forward in behavioral economics, the apparent
distinctions appear slight.
At a recent meeting of the Society for the Advancement of Behavioral Economics, one
board member suggested that the group dissolve, arguing that behavioral economics had become
mainstream, and therefore no distinction or group to advocate on its behalf was needed. Whether
this merging of behavioral economics and mainstream economics represents a change in the
mainstream or a capitulation of the motive behind the behavioral program aimed at improved
realism is open to debate.
A second aspect of the naming problem inherent in ―behavioral economics,‖ which may
seem trivial, but underscores links to another research program that has run into serious barriers,
is potential confusion with the behaviorist movement. Behaviorism is very much distinct from
both the behavioralism of pre-Pareto neoclassicals and contemporary behavioral economists.
(John Broadus Watson published his treatise on the behaviorist approach to psychology in 1913).
Bruni and Sugden (2007) describe the behaviorist movement in psychology as having ―denied
the scientific status of introspection.‖ This is almost equivalent to the denial by some
economists, both behavioral and neoclassical, that actual decision processes of firms and

Tversky obtained his PhD at the University of Michigan under the supervision of Clyde Coombs and Ward
Edwards.

38

<-----Page 39----->consumers are important—that only outcomes of decision processes are appropriate objects for
scientific inquiry. Thus, one important theme of the behaviorist program agrees with the as-if
Friedman doctrine, carried forward in contemporary behavioral economics by those who argue
that the goal of their models is not to provide a veridical description of the actual decision
processes being used by economic agents, but to predict the outcome (a particular action or
decision).

The Route Not (Yet?) Taken: Process Models Addressing EU Violations, Time Inconsistency, and
Other-Regarding Behavior
Economists like Herbert Simon, Reinhardt Selten, and Vernon Smith illustrate that there
is a positive route not taken in behavioral economics, which is more empirical, more open to
alternative normative interpretations of deviations from neoclassical theory, and more descriptive
of actual decision processes rather than reliant on extensions of Friedman‘s as-if methodology.
Perhaps counterintuitively, the issue of normative interpretation is critical for these thinkers in
gauging how far their descriptive work can move away from neoclassical theory and achieve
more data-driven descriptions of how decisions are made. Simon, for example, thought that
expected utility theory was both normative and descriptively inadequate. Selten proposes
elaborate satisficing explanations of choice under uncertainty. And Vernon Smith holds that if
someone consciously violates EU, then this does not imply that he or she made an error.
Regarding the three examples of as-if behavioral economics given in the second section
in this paper, one can point to genuine process models that handle the very same behavioral
phenomena without as-if justification. Tversky‘s elimination by aspects described a process to
choose between two alternatives that could be gambles. Unfortunately, Tversky abandoned his

39

<-----Page 40----->attempts to use lexicographic structure to model choice under uncertainty when he joined
Kahneman and turned to the repair program. The priority heuristic, mentioned earlier, is another
process model, and it predicts the experimental data better than as-if cumulative prospect theory.
Regarding time inconsistency, Rubinstein (2003) put forward a process model for
temporal discounting that provides an attractive alternative to the as-if hyperbolic discounting
story. The ecological rationality of various forms of time-inconsistency was documented by
Leland (2002), Rosati et al. (2007) and Heilbroner et al. (2008), who showed that characteristics
of the decision maker‘s environment can explain some differences in discount rates. For
example, if one lives among lots of greedy companions rather than alone, this tends to make one
less patient.
Regarding other-regarding behavior, Henrich et al. (2001) tried but could not find Homo
Economicus in 15 small-scale societies in remote locations. They found that offers and
rejections in the ultimatum game are related to extent to which these societies‘ production
technologies required workers to cooperate (e.g., hunting in groups) or fend for themselves (e.g.,
gathering food alone). Carpenter and Seki (2006) report a similar finding about two groups of
Japanese fishermen and women. They find that groups who pool the payoffs from all boats‘
daily catches play the ultimatum game much more cooperatively than groups that reward the
members of each boat more individualistically based on the value of each boat‘s own daily catch.

Empirical Realism: Past to Present
Bruni and Sugden (2007) in their discussion of Hicks and other founders of contemporary
neoclassical economics (vis a vis neoclassical economics before Pareto‘s influence came to
dominate) write:

40

<-----Page 41----->If economics is to be a separate science, based on laws whose truth is to be
treated as axiomatic, we have to be very confident in those laws. Otherwise,
we are in danger of creating an complex structure of internally consistent
theory which has no correspondence with reality.
This correspondence with reality is the essence of the empirical approach to economics. How
else do we get to be ―very confident‖ in the laws of economics?
The origins of behavioral economics are many, without clear boundaries or singularly
defining moments. And yet, even a cursory look at articles published in economics today versus,
say, 1980, reveals a far-reaching, distinctly behavioral shift. 8 A striking element in the
arguments of those who have successfully brought behavioral economics to mainstream
economics audiences is the close similarity to Friedman‘s as-if defense.
In prospect theory, behavioral economics has added parameters rather than psychological
realism to model choice under uncertainty. In modeling other-regarding behavior, utility
functions have been supplemented with parameters weighting decision makers‘ concern for
receiving more, or less, than the group average. Time inconsistency observed in experiments has
prompted a large empirical effort to pin down parameters in objective functions that hang onto
the assumption of maximization of a time-separable utility function, but with non-exponential

8

One can cite many concrete events as markers of the emergence of behavioral economics, or
psychology and economics, onto a broader stage with wide, mainstream appeal. One might
imagine that such a list would surely include Herbert Simon‘s Nobel Prize in 1978. But that was
a time at which very little behavioral work appeared in the flagship general interest journals of
the economics profession. A concise and of course incomplete timeline would include: Richard
Thaler‘s ―Anomalies‖ series, which ran in the Journal of Economic Perspectives starting in
1987; hiring patterns at elite business schools and economics departments in the 1990s; frequent
popular press accounts of behavioral economics in The Economist, New York Times and Wall
Street Journal in the last 10 years; and the 2002 Nobel Prize being awarded to an experimental
economist and a psychologist. The 1994 Nobel Prize was shared by another economist who is an
active experimenter, Reinhardt Selten.
41

<-----Page 42----->weighting schemes that have taken on psychological labels that purport to measure problems
with willpower. Described as a new empirical enterprise to learn the true preferences of real
people, the dominant method in behavioral economics can be better described as filtering
observed action through otherwise neoclassical constrained optimization problems with new
arguments and parameters in the utility function.
We have tried to investigate to what extent behavioral economists‘ attempts to filter data
through more complexly parameterized constrained optimization problems succeeds in achieving
improved empirical realism and, in so doing, distinguishing behavioral from neoclassical
economics. The primary finding is that of widespread similarity in the neoclassical and
behavioral research programs. This suggests common limitations in their ultimate historical
trajectories and scientific achievements. To become more genuinely helpful in improving the
predictive accuracy and descriptive realism of economic models, more attention to decision
process will be required, together will bolder normative investigation using a broader set of
prescriptive criteria.

42

<-----Page 43----->References
Amir, O., Ariely, D., Cooke, A., Dunning, D., Epley, N., Gneezy, U., et al. (2005). Psychology,
behavioral economics, and public policy. Marketing Letters, 16(4), 443–454.
Ashraf, N., Camerer, C. F., and Loewenstein, G. (2005). Adam Smith, behavioral economist.
Journal of Economic Perspectives, 19(3), 131–145.
Berg, N. (2003). Normative behavioral economics, Journal of Socio-Economics, 32, 411-427.
Berg, N., Biele, G., and Gigerenzer, G. (2008). Consistency versus accuracy of beliefs:
Economists surveyed about PSA. University of Texas-Dallas Working Paper.
Berg, N. and Gigerenzer, G. (2007). Psychology implies paternalism?: Bounded rationality may
reduce the rationale to regulate risk-taking. Social Choice and Welfare, 28(2), 337-359.
Berg, N., and Hoffrage, U. (2008). Rational ignoring with unbounded cognitive capacity.
Journal of Economic Psychology, 29, 792-809.
Berg, N., Eckel, C., and Johnson, K. (2009). Inconsistency Pays?: Time-inconsistent subjects and
EU violators earn more. University of Texas-Dallas Working Paper.
Berg, N., and Lien, D. (2005). Does society benefit from investor overconfidence in the ability of
financial market experts?. Journal of Economic Behavior and Organization, 58, 95-116.
Bernheim, B., and Rangel, A. (2005). Behavioral public economics: Welfare and policy analysis
with non-standard decision-makers. NBER Working Paper no. 11518.
Beshears, J, Choi, J. J., Laibson, D., and Madrian, B. C. (2008). How are preferences revealed?
Journal of Public Economics, 92(8-9), 1787-1794.
Binmore, K. G., and Samuelson, L. (1992). Evolutionary stability in repeated games played by
finite automata. Journal of Economic Theory, 57(2), 278-305.

43

<-----Page 44----->Binmore, K. and Shaked, A. (2007). Experimental Economics: Science or What?, London, ELSE
Working Paper 263.
Blume, L., Brandenburger, A., and Dekel, E. (1991). Lexicographic probability and choice under
uncertainty, Econometrica, 59, 61–79.
Brandstätter, E., Gigerenzer, G., and Hertwig, R. (2006). The priority heuristic: Making choices
without trade-offs. Psychological Review, 113(2), 409-432.
Brandstätter, E., Gigerenzer, G., and Hertwig, R. (2008). Risky choice with heuristics: Reply to
Birnbaum (2008), Johnson, Schulte-Mecklenbeck, and Willemsen (2008) and Rieger and
Wang (2008). Psychological Review, 115, 281-290.
Bruni, L., and Porta, P. L. (2007). Handbook on the Economics of Happiness. Cheltanham, UK;
Northhampton, MA: Elgar.
Bruni, L., and Sugden, R. (2007). The road not taken: How psychology was removed from
economics, and how it might be brought back. Economic Journal, 117(516), 146-173.
Camerer, C. (1999). Behavioral economics: Reunifying psychology and economics. Proceedings
of the National Academy of Sciences of the United States of America, 96(19), 10575-10577.
Camerer, C. (2003). Behavioral Game Theory: Experiments In Strategic Interaction. New York,
N.Y. Princeton, N.J.: Russell Sage Foundation; Princeton University Press.
Carpenter, J. and Seki, E. (2006). Competitive Work Environments and Social Preferences: Field
experimental evidence from a Japanese fishing community, Contributions to Economic
Analysis & Policy Berkeley Electronic Press, 5(2), Article 2.
Cohen, J., and Dickens, W. (2002). A foundation for behavioral economics. American Economic
Review, 92(2), 335-338.
Compte, O., and Postlewaite, A. (2004). Confidence-enhanced performance. The American
Economic Review, 94(5), 1536-1557.

44

<-----Page 45----->Conlisk, J. (1996). Why bounded rationality? Journal of Economic Literature, 34(2), 669-700.
Czerlinski, J., Gigerenzer, G., and Goldstein, D. G. (1999). How good are simple heuristics? In
Gigerenzer, G., Todd, P. M., and the ABC Research Group, Simple heuristics that make
us smart (pp. 97–118). New York: Oxford University Press.
Dekel, E. (1999). On the evolution of attitudes toward risk in winner-take-all games. Journal of
Economic Theory, 87, 125–143.
Diamond, P. (2008). Behavioral economics. Journal of Public Economics, 92(8-9), 1858-1862.
Fehr, E. and Schmidt, K. (1999). A theory of fairness, competition and cooperation. Quarterly
Journal of Economics, 114, 817–868.
Frank, R. H. (1991). Microeconomics and Behavior. New York; London; Montreal and Sydney:
McGraw-Hill.
Frank, R. (2008). Lessons from behavioral economics: Interview with Robert Frank. Challenge,
51(3), 80-92.
Friedman, M. (1953). Essays in Positive Economics. Chicago, Ill.: University of Chicago Press.
Galbraith, J. K. (1938). Rational and irrational consumer preference. The Economic Journal,
48(190), 336-342.
Gigerenzer, G. (2008). Rationality for mortals. New York: Oxford University Press.
Gigerenzer, G. and Brighton, H. (2009). Homo heuristicus: Why biased minds make better
inferences. Topics in Cognitive Science, 1, 107-143.
Gigerenzer, G. and Goldstein, D. G. (1996). Reasoning the fast and frugal way: Models of
bounded rationality. Psychological Review, 103, 650-669.
Gigerenzer, G. and Selten, R. (2001). Bounded Rationality: The Adaptive Toolbox. Cambridge:
MIT Press.

45

<-----Page 46----->Gigerenzer, G., Todd, P. M., and ABC Research Group. (1999). Simple Heuristics that Make us
Smart. New York: Oxford University Press.
Gilboa, I., Postlewaite, A., and Schmeidler, D. (2004). Rationality of belief, or: Why
Bayesianism is Neither Necessary nor Sufficient for Rationality, Cowles Foundation
Discussion Papers 1484, Cowles Foundation, Yale University.
Güth, W. (1995). On ultimatum bargaining - A personal review. Journal of Economic
Behavior and Organization, 27, 329-344.
Güth, W. (2008). (Non-) behavioral economics - A programmatic assessment. Journal of
Psychology, 216, 244-253.
Hastie, R. and Rasinski, K. A. (1988). The concept of accuracy in social judgment. In D. BarTal and A. W. Kruglanski: The Social Psychology of Knowledge. New York, NY; Paris,
France: Cambridge University Press; Editions de la Maison des Sciences de l'Homme, pp.
193-208.
Hammond, K.R. (1996). Human Judgment and Social Policy: Irreducible Uncertainty,
Inevitable Error, Unavoidable Injustice. New York: Oxford University Press.
Hayek, F. A. (1945). The use of knowledge in society. American Economic Review, 35(4), 519530.
Heukelom, F. (2007). Who are the behavioral economists and what do they say?. Tinbergen
Institute Discussion Papers 07-020/1, Tinbergen Institute.
Heilbronner, S.R., Rosati, A.G., Stevens, J.R., Hare, B., and Hauser, M.D. 2008. A fruit in the
hand or two in the bush? Ecological pressures select for divergent risk preferences in
chimpanzees and bonobos. Biology Letters 4:246-249.

46

<-----Page 47----->Heinrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., Gintis, H., and McElreath, R. (2001).
In search of homo economicus: Behavioral experiments in 15 small-scale societies, American
Economic Review, 91, 73-78.
Jolls, C., Sunstein, C. R., and Thaler, R. H. (1998). A behavioral approach to law and economics.
Stanford Law Review, 50, 1471-1541.
Jorland, G. (1987). The Saint Petersburg Paradox 1713 – 1937. In Krüger, L., Daston, L. and
Heidelberger, M. (Eds.). (1987). The probabilistic revolution: Vol. I. Ideas in history (pp.
157-190) Cambridge, MA: MIT Press.
Kahneman, D., and Tversky, A. (1979). Prospect theory: An analysis of decision under risk.
Econometrica, 47(2), 263-291.
Keynes, J. M. (1936/1974). The General Theory of Employment, Interest And Money. London;
New York: Macmillan; Cambridge University Press: for the Royal Economic Society.
Knight, F.H. (1921). Risk, Uncertainty and Profit. 1933 reprint, London: L.S.E.
Laibson, D. (1997). Golden eggs and hyperbolic discounting. The Quarterly Journal of
Economics, 112(2), 443-477.
Laibson, D. (2002). Bounded rationality in economics. [PowerPoint Slides]. Retrieved from UC
Berkley 2002 Summer Institute on Behavioral Economics website, organized by the
Behavioral Economics Roundtable, Russell Sage Foundation. Accessed February 22, 2009.
Web site: http://elsa.berkeley.edu/symposia/sage02/slides/laibson3.pdf.
Leland, J. W. (1994). Generalized similarity judgments: An alternative explanation for choice
anomalies. Journal of Risk and Uncertainty, 9, 151–172.
Lipman, B. (1999). Decision theory without logical omniscience: Toward an axiomatic
framework for bounded rationality. Review of Economic Studies, 66, 339-361.

47

<-----Page 48----->O'Donoghue, T. and Rabin, M. (2006) Optimal sin taxes. Journal of Public Economics, 90(1011), 1825-1849.
Pantaleoni, M., and Bruce, T. B. (1898). Pure Economics. London; New York: Macmillan.
Payne, J. W., and Braunstein, M. L. (1978). Risky choice: An examination of information
acquisition behavior. Memory and Cognition, 6, 554–561.
Rabin, M. (1998). Psychology and economics. Journal of Economic Literature, 36(1), 11-46.
Rabin, M. (2002). A perspective on psychology and economics. European Economic Review,
46(4-5), 657-685.
Roberts, S., and Pashler, H. (2000). How pervasive is a good fit? A comment on theory testing.
Psychological Review, 107, 358–367.
Rosati, A.G., Stevens, J.R., Hare, B., and Hauser, M.D. 2007. The evolutionary origins of human
patience: temporal preferences in chimpanzees, bonobos, and human adults. Current
Biology 17:1663-1668.
Rubinstein, A. (1988). Similarity and decision-making under risk (Is there a utility theory
resolution to the Allais-paradox?). Journal of Economic Theory, 46, 145–153.
Russo, J. E., and Dosher, B. A. (1983). Strategies for multiattribute binary choice. Journal of
Experimental Psychology, Learning, Memory, and Cognition, 9, 676–696.
Sargent, T. J. (1993). Bounded Rationality in Macroeconomics. Oxford: Oxford University
Press.
Smith, A. (1759/1997). The Theory of Moral Sentiments.Washington, D.C.: Regnery Pub.
Starmer, C. (2004). Friedman's risky methodology. University of Nottingham Working Paper.
Starmer, C. (2005). Normative notions in descriptive dialogues. Journal of Economic
Methodology, 12(2), 277-289.

48

<-----Page 49----->Thaler, R. H. (1991). Quasi Rational Economics. New York: Russell Sage Foundation.
Thaler, R. H., and Sunstein, C. R. (2008). Nudge: Improving Decisions about Health, Wealth,
and Happiness. New Haven: Yale University Press.
Tversky, A., and Kahneman, D. (1986). Rational choice and the framing of decisions.
The Journal of Business, 59(4), S251-S278.
Veblen, T. (1994). The Theory of The Leisure Class. New York: Dover Publications.
Wansink, B. (2006) Mindless Eating: Why We Eat More than We Think. New York: Bantam
Books.
Winter, S. G., Jr. (1964). Economic ‗Natural Selection‘ and the theory of the firm. Yale Econ.
Essays, 4(1), 225-72.
Yee, M., Dahan, E., Hauser, J. R., and Orlin, J. (2007). Greedoid-based noncompensatory
inference. Marketing Science, 26(4), 532-549.

49

