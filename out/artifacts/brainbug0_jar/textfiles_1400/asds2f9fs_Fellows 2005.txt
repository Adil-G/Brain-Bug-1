<-----Page 0----->BEHAVIORAL
10.1177/1534582304273251
Fellows
/ HUMAN
ANDDECISION
COGNITIVE
MAKING
NEUROSCIENCE REVIEWS

The Cognitive Neuroscience of
Human Decision Making:
A Review and Conceptual Framework
Lesley K. Fellows
Montreal Neurological Institute
Decision making, the process of choosing between options, is a
fundamental human behavior that has been studied intensively
by disciplines ranging from cognitive psychology to economics.
Despite the importance of this behavior, the neural substrates of
decision making are only beginning to be understood. Impaired
decision making is recognized in neuropsychiatric conditions
such as dementia and drug addiction, and the inconsistencies
and biases of healthy decision makers have been intensively studied. However, the tools of cognitive neuroscience have only
recently been applied to understanding the brain basis of this
complex behavior. This article reviews the literature on the cognitive neuroscience of human decision making, focusing on the
roles of the frontal lobes, and provides a conceptual framework
for organizing this disparate body of work.

tions of decision-making research in disciplines as varied
as cognitive psychology, economics, and computer science (Baron, 1994; Kahneman, Slovic, & Tversky, 1982;
Lipshitz, Klein, Orasanu, & Salas, 2001; Stirling, 2003),
which may be very useful in guiding this fledgling field.
This article will attempt to clarify what is known about
the neural bases of human decision making and what is
not. First, the literature on decision making in patients
with frontal lobe damage will be reviewed. In the past several years, this work has had an important impact on the
study of both normal and pathological decision making.
However, the inconsistencies and difficulties in interpreting this growing body of work highlight the need for
a more systematic approach. Although there is no true
consensus model of decision making, consistent features
can be found in theories emerging from various disciplines. These recurring themes can be distilled into a
useful framework for beginning to understand how the
general processes of decision making may be represented in the brain. In the second part of this article, I
will propose such a framework and review the cognitive
neuroscience literature that addresses these more
fundamental aspects of decision making.

Key Words: executive function, prefrontal cortex, frontal lobes, amygdala, reward, impulsivity

Decision making is a vital component of human

behavior. Like other executive processes, it involves the
synthesis of a variety of kinds of information: multimodal
sensory inputs, autonomic and emotional responses,
past associations, and future goals. These inputs must be
integrated with information about uncertainty, timing,
cost-benefit, and risk and then applied to select appropriate actions. This processing has further practical constraints: It must be completed rapidly and must retain
some degree of flexibility to be useful in a changing environment. Despite this daunting complexity, recent work
using a variety of methods has begun to elucidate the
component processes underlying decision making and
to localize these processes in the human brain.
This is a new focus for cognitive neuroscience.
Inroads have been made in understanding elements of
decision making, but the connections between these elements remain unclear. However, there are long tradi-

NEUROANATOMY
The cognitive neuroscience literature on decision
making has focused on a limited set of brain regions.
Author’s Note: This work was supported by National Institutes of
Health Grant R21 NS045074, a clinician-scientist award from the Canadian Institutes of Health Research, and an award from the Fonds de la
recherche en sante de Quebec. I would like to thank Martha Farah and
Andrea Heberlein for comments on earlier drafts of the article.
Behavioral and Cognitive Neuroscience Reviews
Volume 3 Number 3, September 2004 159-172
DOI: 10.1177/1534582304273251
© 2004 Sage Publications

159

<-----Page 1----->160

BEHAVIORAL AND COGNITIVE NEUROSCIENCE REVIEWS

Figure 1: Ventral and Lateral Views of the Frontal Lobes, Showing the Approximate Borders of the Various Sectors Described in the Text.

There has been particular interest in the role of the
prefrontal cortex in general and the ventromedial
prefrontal cortex in particular. The terminology used to
describe these regions can be confusing. Although strict
boundaries are rarely defined, the ventromedial frontal
lobe (VMF) includes the medial portion of orbitofrontal
cortex (OFC) and the ventral portion of the medial wall
of the frontal lobes. The lateral OFC is often grouped
with the ventral portion of the lateral convexity of the
frontal lobe and labeled the ventrolateral frontal lobe
(VLF), although this distinction is more frequent in
functional imaging than in lesion studies. Most commonly, these ventral areas are contrasted with
dorsolateral prefrontal cortex (DLF; for a detailed discussion of the relationship between these areas and
Brodmann’s nomenclature, see Chiavaras, LeGoualher,
Evans, & Petrides, 2001; Petrides & Pandya, 1999, 2002).
This broad division of prefrontal cortex is a reasonable starting point because, generally speaking, these
three cortical areas have different cytoarchitecture and
different patterns of connectivity, both in humans
(Petrides & Pandya, 1999, 2002) and in nonhuman primates (Barbas, 2000; Dombrowski, Hilgetag, & Barbas,
2001). Ventral medial prefrontal and caudal
orbitofrontal regions are classified as paralimbic cortex.
These cortical areas are closely connected to limbic
structures such as the amygdala and hypothalamus, provide descending inputs to midbrain structures including
periaqueductal gray and substantia nigra, and receive
polymodal sensory inputs (An, Bandler, Ongur, & Price,
1998; Barbas, 2000; Ghashghaei & Barbas, 2002; Ongur,
An, & Price, 1998; Price, Carmichael, & Drevets, 1996;
Rempel-Clower & Barbas, 1998). In contrast, DLF is
heteromodal association cortex, with more restricted
sensory inputs, and has been implicated in working
memory and selective attentional processes. However,
all areas of the prefrontal cortex are heavily intercon-

nected, emphasizing the synthetic and integrative role of
the region in general (Barbas, 2000). The cortical anatomy is summarized in Figure 1; for additional accessible
reviews of this anatomy, see Mesulam (2003) and Stuss
and Levine (2002).
Several subcortical structures have been implicated in
decision making. The amygdala appears to have a role,
perhaps in part through its important interconnections
with the OFC. Recent work has also begun to examine
the role of mesolimbic and mesocortical dopaminergic
projections (arising in the substantia nigra and ventral
tegmental area of the midbrain and terminating in the
ventral striatum-nucleus accumbens, dorsal striatum
[caudate-putamen], and prefrontal cortex) that have
been implicated in reward and addiction (Schultz, 2002;
Wise, 2002). There is also some work supporting a role
for prefrontal serotonin in aspects of reinforcementdriven learning and deciding (Clarke, Dalley, Crofts,
Robbins, & Roberts, 2004; Rogers et al., 2003; Rogers,
Owen, et al., 1999).
This should not be viewed as a definitive list of the
neuroanatomical substrates of decision making but
rather as a useful starting point for understanding the
existing literature. It reflects the biases of the initial
investigators in this relatively new field, primarily driven
by clinical observations of patients with focal brain damage and more recently by the results of functional imaging studies of reward processing in humans and the
insight that the poor choices of substance abusers suggests a role for the reward circuitry first identified in animal models of addiction in normal and pathologic
human decision making (Bechara & Damasio, 2002;
Bechara, Dolan, & Hindes, 2002; Cavedini, Riboldi,
Keller, D’Annucci, & Bellodi, 2002; Grant, Contoreggi,
& London, 2000). Indeed, recent single-unit work in
monkeys has demonstrated reward-sensitive neurons in
a variety of other regions, including the sensory

<-----Page 2----->Fellows / HUMAN DECISION MAKING
thalamus (Komura et al., 2001) and parietal cortex (Platt
& Glimcher, 1999; Sugrue, Corrado, & Newsome, 2004).
LESIONS
Most of the lesion studies of decision making in
humans have focused on the ventromedial prefrontal
cortex. Focal lesions in humans that involve VMF structures generally spare lateral prefrontal areas (and vice
versa), so this division is theoretically sound, clinically
relevant, and experimentally pragmatic. It is important
to keep in mind that the common causes of VMF damage
are generally different from the common causes of VLF
or DLF damage (aneurysm rupture, tumor resection
and traumatic brain injury in the former, ischemic or
hemorrhagic stroke in the latter two). This may introduce confounds, in that the patient populations may differ systematically in other respects, such as older age and
history of vascular risk factors in the case of ischemic
stroke and the possibility of nonfocal damage due to
acute hydrocephalus, edema, and surgical trauma in the
case of aneurysm rupture, for example. In addition,
lesions that affect these different cortical areas also tend
to involve different subcortical structures. Individuals
with VMF damage may also have involvement of the basal
forebrain, the genu of the corpus callosum, and the septum; DLF damage may be associated with damage to the
dorsal caudate-putamen and intervening white matter
tracts; and VLF damage often extends posteriorly to
involve the insula. Finally, VMF damage is frequently
bilateral, whereas lateral frontal damage is rarely so.
FUNCTIONAL IMAGING
Susceptibility artifact limits the detectable blood oxygenation level dependent (BOLD) signal from the OFC,
making this area challenging to image with standard
functional magnetic resonance imaging (fMRI) techniques. Methods exist to address this limitation
(Deichmann, Gottfried, Hutton, & Turner, 2003; Wilson
& Jezzard, 2003). Positron emission tomography (PET)
studies do not suffer from this problem. In addition, the
surface anatomy of the human OFC is quite variable. A
probability map has recently been developed to allow
functional imaging results to be related more rigorously
to the cytoarchitecture of this area (Chiavaras et al.,
2001).
STUDIES OF COMPLEX DECISION MAKING
Clinicians have reported strikingly impaired decision
making in patients with VMF damage many times over
the past century (reviewed in Cottle & Klineberg, 1974;
Eslinger & Damasio, 1985; Damasio, 1994; Loewenstein,

161

Weber, Hsee, & Welch, 2001). Such reports have emphasized impairments in the emotional aspect of decision
making. Empirical studies of elements of decision making following frontal lobe damage have been undertaken more recently (e.g., Godefroy & Rousseaux, 1996,
1997; Miller, 1992; Miller & Milner, 1985). The work of
Bechara et al. using a gambling task has been particularly
influential in shaping the direction of such work in the
past several years. This laboratory task was developed to
capture the elements of risk, reward, and punishment
that this group hypothesized were at the root of the
decision-making impairment of VMF patients (Bechara,
Damasio, Damasio, & Anderson, 1994). This now wellknown task, here referred to as the Iowa gambling task
(IGT), requires the participant to repeatedly choose
from four decks of cards with the goal of winning as
much play money as possible. Each card is associated
with a win, and some cards also carry losses. Overall,
choosing from two of the decks results in larger wins but
even larger losses, whereas choosing from the other two
results in small wins but even smaller losses. As participants progress through the 100 trials, they gradually
learn to avoid the riskier (so-called disadvantageous)
decks and choose more often from the lower stakes,
overall advantageous decks. Bechara et al. (1994)
reported that participants with VMF damage performed
quite differently, persistently choosing more often from
the riskier (ultimately disadvantageous) decks. This
observation was extended by examining the skin conductance responses (SCR) in 6 VMF participants while
performing the IGT. Control participants showed
enhanced SCR prior to choosing cards, with choices
from the risky decks preceded by larger SCR than
choices from the safe decks even before controls were
able to explicitly report deck contingencies. In contrast,
the VMF participants showed minimal SCR prior to card
choices, and this response did not distinguish between
safe and risky decks (Bechara, Damasio, Tranel, &
Damasio, 1997).
The authors of this study argued that VMF damage
impaired the acquisition of emotional or “dispositional”
knowledge about the decks, knowledge that biased controls away from the risky decks. A similar pattern of
behavioral and autonomic results has been reported following bilateral amygdala damage (Bechara, Damasio,
Damasio, & Lee, 1999). These findings have led to the
more general “somatic marker” hypothesis that emotional information, indexed by the autonomic state of
the body, can influence decision making under uncertainty (Bechara, Damasio, & Damasio, 2000, 2003;
Damasio, 1994). Dissenting evidence regarding the
importance of the autonomic body state in risky decision
making has come from a study of patients with high cervical spinal cord lesions who performed the IGT nor-

<-----Page 3----->162

BEHAVIORAL AND COGNITIVE NEUROSCIENCE REVIEWS

mally (despite the inability to generate normal sympathetic autonomic responses; North & O’Carroll, 2001)
and from a study in young normal participants that
found that SCR magnitude related to the magnitude of
anticipated rewards or punishments in the IGT, rather
than the riskiness of the choice (Tomb, Hauser, Deldin,
& Caramazza, 2002). Recent work has also shown that
normal participants acquire sufficient explicit knowledge of the contingencies to support good performance, even at very early stages of the task, making
recourse to a somatic marker explanation unnecessary
(Maia & McClelland, 2004).
The Iowa group has reported a number of follow-up
studies in patients with frontal lobe damage. One contrasted the effects of VMF damage (9 participants) with
the effects of dorsolateral or dorsomedial frontal damage (six left hemisphere, four right hemisphere) on the
IGT. Although direct statistical comparison of the performance of the two groups is not provided in the article,
VMF participants chose fewer than 50 cards from the
safe decks, whereas participants with dorsal frontal damage, like normal participants, chose more than 50 cards.
The role of working memory in IGT performance was
also examined in this study; deficits in working memory
were correlated with deficits on the IGT but were not the
sole explanation for impaired IGT performance
(Bechara, Damasio, Tranel, & Anderson, 1998). More
recently, this group has reported that unilateral right
VMF damage (n = 4) but not left VMF damage (n = 3)
leads to impaired IGT performance compared to
control participants (Tranel, Bechara, & Denburg,
2002).
Efforts by other centers to replicate these behavioral
findings have led to mixed results. Manes et al. (2002)
administered the IGT to 19 patients with focal frontal
lobe damage. To pinpoint the key area(s) responsible
for poor performance on this task, they studied participants with unilateral damage and classified the damage
as involving one of three frontal regions (OFC, DLF,
dorsomedial frontal) or as “large” if it extended into two
or more of these regions. Damage to the OFC alone did
not impair IGT performance, whereas isolated damage
to DLF or dorsomedial regions and large lesions were all
accompanied by impaired IGT performance. A followup study added another 27 patients to permit analysis of
laterality effects (albeit collapsed across the frontal subregions). In keeping with the small series of Tranel et al.
(2002), right frontal damage was associated with the
worst performance on the IGT. However, left frontal
damage was also associated with impairment (Clark,
Manes, Antoun, Sahakian, & Robbins, 2003). The poor
performance of participants with right hemisphere damage was evident even when ventromedial regions were
spared.

Fellows and Farah (2005) replicated the original finding that VMF damage is associated with impaired IGT
performance in a group of 9 participants, most of whom
had bilateral damage. However, in the same study, unilateral DLF damage (in 12 participants) was also found to
be associated with impaired performance on the task,
regardless of the involved hemisphere.
The complexity of the IGT makes it difficult to resolve
these conflicting findings. At the least it seems clear that
abnormal performance on the task is not a specific sign
of VMF dysfunction. This has implications for interpreting the results of studies using IGT performance as a
probe of VMF function in conditions including addiction, psychopathy, personality disorders, and studies of
normal states such as gender differences and variations
in serum testosterone (Overman, 2004; Bechara &
Damasio, 2002; Best, Williams, & Coccaro, 2002;
Cavedini, Riboldi, D’Annucci, et al., 2002; Cavedini,
Riboldi, Keller, et al., 2002; Grant et al., 2000; Reavis &
Overman, 2001; van Honk, Hermans, Putman,
Montagne, & Schutter, 2002).
A number of investigators have examined specific
aspects of IGT performance in an effort to explain the
impairment of VMF participants in terms of simpler
underlying processes. Approaches have included cognitive modeling applied to the results of the original IGT
(Busemeyer & Stout, 2002); close variants of the IGT
designed to emphasize putative underlying processes
(Bechara, Tranel, & Damasio, 2000; Fellows & Farah,
2005), which I will discuss further in the second half of
this article; and new gambling tasks intended to examine
particular component processes (Rogers, Everitt, et al.,
1999; Sanfey, Hastie, Colvin, & Grafman, 2003).
Busemeyer and Stout (2002) modeled the trial-bytrial IGT behavior of normal participants with a cognitive decision model that incorporated reinforcementexpectancy learning, the relative weighting of losses over
gains, and sensitivity of choices to the reinforcement
expectancy (Busemeyer & Stout, 2002). Such a model
approximated the performance of normal controls
fairly well. According to this model, the impaired IGT
performance of patients with Huntington’s disease was
primarily due both to impaired learning and to a gradual
loss of sensitivity to reinforcement expectancy as the task
progressed, perhaps reflecting nonspecific fatigue.
Whether the component processes suggested by this
model relate to specific functions of different frontal
areas remains to be seen.
The task developed by Rogers, Everitt, et al. (1999) is
intended to evaluate risk seeking using a gambling paradigm that (unlike the IGT) requires no learning. On
each trial, participants choose between a high or low
probability gamble and then decide how much play
money to bet on the outcome. Three studies have been

<-----Page 4----->Fellows / HUMAN DECISION MAKING
published that examined the performance of participants with frontal lobe damage on this task. The first
found that VMF damage (n = 10) was associated with a
tendency to make riskier choices than either controls or
participants with DLF damage (n = 10) but to bet smaller
sums of money (Rogers, Everitt, et al., 1999). However, a
larger group of participants (n = 31) who had suffered
rupture of anterior communicating artery aneurysm
(which typically results in varying degrees of VMF damage, although this was not assessed radiologically in the
study) chose the riskier gamble no more often than controls but bet larger sums (Mavaddat, Kirkpatrick, Rogers,
& Sahakian, 2000). Finally, Manes et al. (2002) reported
that isolated damage to the OFC, DLF, or dorsomedial
frontal cortex did not lead to significant impairment on
this task compared to controls; damage to two or more of
these regions led to poor performance, with such participants both choosing riskier gambles and placing larger
bets (Manes et al., 2002). Sanfey et al. (2003) have also
examined risk-taking behavior in participants with frontal damage with yet another card-based gambling task
that manipulated the variance of wins and losses while
keeping the overall expected value (probability × outcome value) for each deck constant. Participants with
VMF involvement (n = 9) did not show the normal pattern of preferring lower variance decks. A subset of this
group (n = 5) appeared to be risk-seeking, in that they
preferred the high variance decks, whereas the remaining 4 VMF participants followed a pattern similar to the
control group. The risk-seeking subgroup did not differ
from those who performed normally on demographic
variables, IQ, or neuroanatomical grounds, except that
they had more associated DLF damage. However, DLF
damage in the absence of VMF injury (n = 4) did not lead
to risk-seeking behavior in this task.
Bechara, Tranel, et al. (2000) manipulated the
reward and punishment values of the original IGT deck
to test whether VMF participants were hypersensitive to
reward or hyposensitive to punishment and found no
evidence of either as measured by task performance or
the magnitude of the SCR generated in response to
reinforcement.
In sum, extensive damage to the frontal lobes is associated with riskier-than-normal choices in both these tasks.
However, these results taken together do not permit
strong claims about the role of any particular frontal
region and in fact indicate that restricted frontal damage
is often not associated with risk seeking, defined as either
a preference for high variance in outcomes or an
increased willingness to “play the long shot.”
It is not easy to reconcile the inconsistent findings in
this series of studies, in large part because of the complexity of the tasks used to measure decision making.

163

Figure 2: Schematic Summarizing a Simple Three-Stage Model of
Decision Making and Listing Some of the Processes That
May Be Involved at Each Stage.

One way to address this difficulty is to look at much simpler component processes of decision making.
DISSECTING DECISION MAKING
The study of decision making in normal individuals
has generated a variety of frameworks for understanding
the building blocks of this complex behavior. One useful
(albeit certainly oversimplified) model of decision making, derived from classical decision-making theory,
breaks decision making down into three interrelated
processes (Baron, 1994; Herrnstein & Prelec, 1991;
Lipshitz et al., 2001); these are outlined in Figure 2.
Options are first identified or generated, evaluated, and
finally a choice is made, as reflected in behavior. The
existing decision-making literature in general, and the
cognitive neuroscience literature in particular, has
focused on evaluation (and, to a lesser degree, choice).
However, there is no a priori reason to believe that VMF
patients, or indeed others who show clinical signs of disordered decision making, are necessarily or solely
impaired in the evaluative aspect of decision making.
Although the distinctions between these three phases of
decision making are to some extent arbitrary, such a
model provides a starting point for a more systematic
examination of the component processes of decision
making. In the pages that follow, the discussion will be
organized according to this more comprehensive
framework.
IDENTIFICATION OF OPTIONS
Option identification has been little studied, even in
normal participants, despite its obvious importance in
real-life decision making (reviewed in Johnson & Raab,
2003). This phase of decision making is most crucial in
relatively unstructured situations, often present in real

<-----Page 5----->164

BEHAVIORAL AND COGNITIVE NEUROSCIENCE REVIEWS

life, but less often in the laboratory (Fischoff, 1996).
Insights into this “front end” of decision making have
been gleaned by having participants think aloud as they
consider a difficult decision. A related approach is to
provide large amounts of potentially relevant information and observe how participants go about sifting
through this information in order to make a decision.
The literature based on these “think aloud” and information search strategy paradigms in normal participants
suggests that effective option identification requires at
least two processes: first, generating or recognizing
options and, second, applying a “stopping rule” when
enough options have been considered (Baron, 1994;
Butler & Scherer, 1997; Gigerenzer & Todd, 1999;
Lipshitz et al., 2001; Saad & Russo, 1996).
Although option generation has not been evaluated
using these tasks in clinical populations, one innovative
think-aloud study of participants with frontal damage
performing an ill-structured financial planning task
found that they had more difficulty structuring the problem, pursued fewer of the (explicitly provided) goals,
and were less systematic in generating options for achieving each goal than controls were (Goel, Grafman, Tajik,
Gana, & Danto, 1997). Case reports of patients with frontal damage tested on other complex, real-world decision
and planning tasks also support the idea that such damage may have an impact at this early stage of decision
making (Grafman & Goel, 2000; Satish, Streufert, &
Eslinger, 1999), and such patients often have difficulty in
open-ended, unstructured task environments in general
(Shallice & Burgess, 1991). One might also postulate
parallels with other forms of self-initiated generation
tasks, such as fluency tasks, often impaired in prefrontal
patients in other domains (e.g., verbal, figural; Stuss &
Benson, 1986) and associated with activations in DLF in
functional imaging studies (Schlosser et al., 1998).
Although patients with VMF damage are anecdotally
characterized as impulsive, both clinical observations
and experimental work in such patients suggest that they
may spend too long contemplating decision options
(Eslinger & Damasio, 1985; Rogers, Everitt, et al., 1999),
raising the possibility that they are impaired in applying
stopping rules (Gigerenzer & Todd, 1999). Whether or
not VMF damage leads to difficulties in evaluation of
options, such damage may independently affect option
generation. This aspect of decision making deserves
more detailed study, both to better understand whether
impairments at this stage contribute to the real-life difficulties of such patients and for the light such an understanding might shed on other conditions notable for
prolonged decision-making times, such as obsessivecompulsive disorder (Cavedini, Riboldi, D’Annucci,
et al., 2002).

ASSESSING VALUE
This aspect of decision making has been the most
intensively studied to date. The field is very new and has
been approached from a variety of interesting directions, ranging from animal learning to economics. This
heterogeneity makes it difficult to summarize the existing findings; I will first provide a brief overview of the various conceptions of value and then review work that has
looked at more specific aspects of value representation
in the brain.
Value can be considered as a (subjective) property of
a stimulus. Economists apply the term utility to this concept (reviewed in Glimcher & Rustichini, 2004). In anim al l e ar ni ng te r m s, thi s p r o p e r ty has b e e n
operationalized in terms of how hard an organism is willing to work to obtain that stimulus (reinforcement).
Reinforcement value can vary along various objective
dimensions, including probability, delay, and kind
(Shizgal, 1997). Value can also be measured as a relative
property, in preference judgment paradigms, for
example.
How do humans assign value to options? This is a
question that occupies researchers across many disciplines, resulting in a plethora of models. Simple in concept, it is a process that may be very complex in its
instantiation. The value of a given stimulus is not fixed: It
depends on external factors, such as the other available
options, and it depends on internal factors, such as satiety. A banana may seem very attractive to a hungry individual but is likely to have a much lower value if that individual has just eaten a pound of bananas in one sitting. It
depends on the delay before the stimulus can be
obtained and, perhaps relatedly (Holt, Green, &
Myerson, 2003; Rachlin, Raineri, & Cross, 1991), on the
probability that it will be obtained. A dollar right now is
worth more to most people than 10 dollars they will not
receive for 6 months, a phenomenon known as temporal
discounting. Temporal discounting has been extensively
studied in normal participants and addiction research as
an explanation for impulsive decision making (Ainslie,
2001; Bickel, Odum, & Madden, 1999; Coffey, Gudleski,
Saladin, & Brady, 2003; Critchfield & Kollins, 2001; Kirby
& Herrnstein, 1995; Kirby & Marakovic, 1996; Kirby,
Petry, & Bickel, 1999; Madden, Begotka, Raiff, &
Kastern, 2003), but little is known about how
reinforcement and time are integrated in the brain.
Furthermore, comparing different options, or valuing a single option with both pros and cons, would seem
to require a mechanism for encoding quite different factors on a common scale. This has led to the speculation
that there may be a common neural “currency,” a neural
mechanism for encoding value that would integrate
these diverse considerations, allowing apples and

<-----Page 6----->Fellows / HUMAN DECISION MAKING
oranges (or indeed, bananas) to be compared to allow a
choice to be made (Montague & Berns, 2002).
Before reviewing the evidence for this and related
concepts, it is worth clarifying the relevant terminology.
Value is a very general term. Even the more specific concepts of reward and punishment have multiple meanings, many of which are confounded in common usage
but may nevertheless have different neural substrates.
Reward encompasses both the incentive value of a stimulus and its hedonic properties: how hard you are willing
to work for something and how much you like it. There is
some evidence that liking and wanting may be mediated,
at least in part, by distinct neural systems. Following nearcomplete dopamine depletion, rats no longer work to
obtain rewards but continue to demonstrate apparently
normal affective responses to pleasant and unpleasant
tastes, as measured by stereotyped behavioral reactions
(Berridge & Robinson, 1998). A mutant rat model with
increased synaptic dopamine showed increased willingness to work for reward but again unchanged affective
responses to tastes (Pecina, Cagniard, Berridge,
Aldridge, & Zhuang, 2003). Studies in humans have
been less conclusive but in some cases have shown that
wanting or craving of drugs of abuse, but not the euphoria they induce, is reduced by pretreatment with dopamine antagonists (Brauer & de Wit, 1996; Modell,
Mountz, Glaser, & Lee, 1993).
Reward may also be operationalized more generally
as a guide for learning. That is, concordance between an
anticipated reward and its delivery reinforces behavior,
whereas a mismatch between reward expectancy and
delivery leads to changes in behavior. Phasic dopamine
signaling has been shown to encode such information,
termed reward expectancy error, in monkey models
(Schultz, 2002; Tremblay & Schultz, 1999), and some
data from imaging studies are consistent with a similar
role in humans (Aron et al., 2004; Martin-Soelch et al.,
2001; O’Doherty, Dayan, Friston, Critchley, & Dolan,
2003; Pagnoni, Zink, Montague, & Berns, 2002).
There are yet other ways to operationalize reward:
Reward value has been conceived of by several authors as
an intrinsic stimulus property (reviewed in Baxter &
Murray, 2002; Rolls, 2000). Human studies of the processing of primary rewards (such as food, pleasant odors,
pleasant sounds) have shown activation, albeit variably,
in the same limbic and cortical reward circuits identified
in animals: midbrain, ventral striatum, medial
prefrontal and orbitofrontal cortex as well as the insula
and, in some cases, amygdala (Blood & Zatorre, 2001;
Kringelbach, O’Doherty, Rolls, & Andrews, 2003;
O’Doherty et al., 2000; O’Doherty, Deichmann,
Critchley, & Dolan, 2002; Small, Zatorre, Dagher, Evans,
& Jones-Gotman, 2001). Studies of other forms of
reward such as beautiful paintings or beautiful faces

165

have shown activations in various components of the
same circuitry (Aharon et al., 2001; Kawabata & Zeki,
2004; O’Doherty, Winston, et al., 2003), as has cocaine
infusion in cocaine-dependent participants (Breiter &
Rosen, 1999). Experiments using money as reinforcement have had more mixed results. Several studies have
used simple reaction time tasks and compared activations on trials with monetary reward with unrewarded
trials. Knutson and colleagues have published a series of
studies using such a paradigm and have shown that activity in the nucleus accumbens increases as a function of
magnitude of anticipated reward and is correlated with
self-rated happiness about the possible outcome
(Knutson, Adams, Fong, & Hommer, 2001). When
reward anticipation is contrasted with experience of the
reward outcome, activity in the ventral striatum is
detected in the anticipatory phase, whereas deactivation
in both ventral striatum and ventromedial prefrontal
cortex occurred during the outcome phase when anticipated rewards were not obtained (Knutson, Fong,
Adams, Varner, & Hommer, 2001). However, another
group using a similar paradigm found little difference in
the areas activated by anticipation of reward and reward
delivery (Breiter, Aharon, Kahneman, Dale, & Shizgal,
2001). Finally, a study that examined the effect of monetary reward on a more difficult task (n-back) found deactivation in several areas, including the ventral striatum,
when rewarded trials were compared with unrewarded
trials (Pochon et al., 2002). It seems likely that a clear
understanding of the interaction between reward and
other cognitive processes will require both a clear definition of the aspect of reward being studied and careful
attention to the time course of reward anticipation and
delivery.
One of the striking properties of stimulus-reward
associations is the need for flexibility. A growing body of
work has examined the neural correlates of the changing reward value of a fixed stimulus. There are two basic
paradigms for examining this issue: The first alters
“rewardingness” by changing the internal state of the
participant, such as through selective satiety. The second
changes the reward value of the stimulus itself, as in the
operant conditioning paradigms of reversal learning or
extinction. Imaging studies of selective satiety in humans
have found regions of caudal OFC in which activity
relates to the current reward value of either a taste or
odor, rather than unvarying features of the stimulus
(Kringelbach et al., 2003; O’Doherty et al., 2000). These
findings agree with single-unit recordings in monkey
OFC, which have identified neurons that respond to
stimuli only when these are motivationally salient (Rolls,
2000), that have firing patterns that reflect current
reward value (Wallis & Miller, 2003), or that distinguish
between the subjective value of different kinds of reward

<-----Page 7----->166

BEHAVIORAL AND COGNITIVE NEUROSCIENCE REVIEWS

as indicated by subsequent choice behavior (Tremblay &
Schultz, 1999). Other work has also implicated the
basolateral amygdala in similar paradigms in both rat
and monkey models (Baxter & Murray, 2002). An fMRI
study of classical appetitive conditioning in humans
found increased activation in the amygdala for the CS+,
which was modulated by selective satiety (Gottfried,
O’Doherty, & Dolan, 2003). There is some evidence
from disconnection experiments in the monkey (Baxter,
Parker, Lindner, Izquierdo, & Murray, 2000) and rat
(Schoenbaum, Setlow, Saddoris, & Gallagher, 2003) that
flexible stimulus-reinforcement associations depend on
the interaction between amygdala and OFC.
Further support for the hypothesis that the OFC
mediates the representation of the current reinforcement value of a stimulus comes from studies of humans
with VMF damage. Reversal learning and extinction, two
forms of flexible stimulus-reinforcement associative
learning, are impaired following VMF damage (Fellows
& Farah, 2003; Hornak et al., 2004; Rolls, Hornak, Wade,
& McGrath, 1994). This is selectively related to VMF
damage; DLF damage does not impair simple reversal
learning in monkeys (Dias, Robbins, & Roberts, 1996) or
in humans (Fellows & Farah, 2003). Hornak et al. (2004)
found that some participants with DLF damage had difficulty on a more complex, probabilistic reversal learning
task, seemingly on the basis of inattention rather than
impaired stimulus-reinforcement processing. The
human studies have not detected consistent laterality
effects, and at least some participants with unilateral
OFC damage demonstrate normal reversal learning
(Hornak et al., 2004). A lesion study in monkeys directly
examined this question and found that either right or
left OFC damage (in conjunction with ipsilateral
amygdala damage) is sufficient to impair reversal learning (Izquierdo & Murray, 2004). An fMRI study of reversal learning with play money reward and punishment
found that activity in the bilateral medial OFC was
greater with the experience of reward than punishment
(O’Doherty, Kringelbach, Rolls, Hornak, & Andrews,
2001).
Can this robust converging evidence for a role for the
medial OFC in the flexible representation of stimulusreinforcement associations shed light on the impaired
IGT performance of human participants with damage to
this area? Normal performance of the IGT appears to
require reversal learning; cards are presented in a fixed
order that induces an initial preference for the ultimately riskier decks that must then be overcome as losses
begin to accrue. Fellows and Farah (2005) tested the
hypothesis that impaired performance on the IGT of
VMF participants reflected an underlying deficit in
reversal learning. Nine VMF participants were abnormal
on the original IGT but performed as well as normal par-

ticipants on a variant task that shuffled the card order of
the original task to eliminate the reversal learning
requirement. Their improvement also correlated well
with how impaired they were on a simpler measure of
reversal learning.
It may be the case that the IGT literature on the role of
VMF in decision making is best framed as an impaired
ability to flexibly update stimulus-reinforcement associations, which has the advantage of linking these findings
to the larger body of work on the forms of associative
learning just reviewed. An interesting, but at this point
open, question is whether the real-life behavioral disturbances of these participants can also be traced to deficits
in fundamental stimulus-reinforcement processing.
Some preliminary correlational evidence supports this
possibility (Fellows & Farah, 2003; Rolls et al., 1994).
Although the weight of evidence to date from functional imaging studies seems to support the general concept that the same regions important in reward processing in animal models are involved in reward processing
in humans, and over a range of rewards, this conclusion
must be regarded as very tentative. As reviewed above,
the ventral striatum/nucleus accumbens and medial
OFC seem to show detectable activity in response to
reward compared to unrewarded baseline in most studies, and many researchers have also reported a change in
amygdala activity. However, the laterality of these effects
has varied, as has the more precise location of OFC activity, and even these more robust effects have not been
seen in all studies. Consistent patterns of activation
across reward types in at least some elements of a putative reward circuit would appear to be a minimum
requirement for the hypothesis that there is a common
neural currency for reward, or at least the simplest form
of this hypothesis. Such a claim would be supported most
compellingly by studies in which reward type (and perhaps other factors, such as delay to reward, or probability
of reward) were varied within subjects and showed both
common areas of activation and activity that scaled with
subjective preference as indicated by the participants’
choices. Finally, unconfounding salience and reward
remains a challenge for such studies (Zink, Pagnoni,
Martin-Skurski, Chappelow, & Berns, 2004).
The principal difficulties in interpreting the existing
literature are that the way in which reward has been
operationalized has varied, anticipation of reward and
reward outcome have not been consistently disambiguated, and the sensitivity of individual fMRI studies for
detecting signal in crucial areas of VMF susceptible to
artifact has not always been specified. Certainly, questions remain about the neural bases of the incentive and
hedonic aspects of reward and whether anticipation and
experience of reward are mediated by distinct neural
circuits.

<-----Page 8----->Fellows / HUMAN DECISION MAKING
As discussed above, the relationship between reinforcement value and time is a central concept in a variety
of models of impulsiveness (see Evenden, 1999, for
review) and decision making (Ariely & Zakay, 2001). Particular attention has been paid to this concept in
research on addiction, using a variety of methods.
These studies have found an association between shortsightedness, defined by several different measures, and
pathological real-life decision making, reflected in substance abuse (Kirby et al., 1999; Monterosso, Ehrman,
Napier, O’Brien, & Childress, 2001; Petry, Bickel, &
Arnett, 1998). Myopia for future reinforcement has
been suggested as an explanation for the impaired decision making of VMF patients, both in life and in the lab
(Bechara et al., 1994; Bechara, Tranel, et al., 2000), and
in rats, both the nucleus accumbens and the OFC seem
to play a role in responding to delayed reward, in that
lesions to either of these areas lead to a tendency to
choose small, immediate over larger, delayed rewards,
that is, to show steeper temporal discounting (Cardinal,
Pennicott, Sugathapala, Robbins, & Everitt, 2001;
Mobini et al., 2002). One recent fMRI study used an analogous design, with human participants either choosing
small, immediate rewards or accepting small, immediate
losses in return for an eventual large payoff. The immediate reward condition was related to activation in the
OFC, whereas the trade-off choice also recruited DLF,
among other areas (Tanaka et al., 2004). A broadly similar pattern was found by a second group using a different
temporal discounting task (McClure, Laibson,
Loewenstein, & Cohen, 2004). However, when temporal
discounting was examined in participants with frontal
injury using a standard discounting task (in which participants make hypothetical choices between sums of
money across different delays), neither DLF nor VMF
damage systematically affected the rate at which reward
lost its subjective value as a function of delay (Fellows &
Farah, in press).
Although this last study did not find evidence of temporal myopia for reward following frontal damage, the
VMF participants did seem to think differently about the
future. VMF damage (but not DLF damage or
nonfrontal damage) was associated with selective foreshortening of the window of time that participants considered when thinking about the future. Another study
has reported that patients with OFC damage overestimate the passage of time over short (second to minute)
intervals (Berlin, Rolls, & Kischka, 2004). Interestingly,
the foreshortened future time measure in the first study
correlated with self-reported apathy but not impulsivity.
Although the group with OFC damage reported by
Berlin et al. (2004) was more impulsive than a comparison group with DLF damage, this did not correlate with
performance on the time estimation task.

167

CHOICE: THE DECISION IN ACTION
Studies of normal decision making have documented
the frequent dissociation between hypothetical preferences and actual choices (Barlas, 2003; Slovic &
Lichtenstein, 1971). More extreme forms of this dissociation are often mentioned, at least as anecdotes, in studies of patients with frontal lobe damage. For example,
some such patients are said to persist in making punished choices while saying “No!” (Rolls et al., 1994). This
suggests the possibility that preferences and choices
are also dissociable in the brain. There is some evidence from functional imaging studies and monkey
neurophysiology work that associating a reward with a
stimulus and choosing an action on the basis of reward
are mediated, at least in part, by different neural structures, although in many tasks, these two processes are
confounded. As reviewed above, the OFC appears to play
a crucial role in forming flexible stimulus-reinforcement
associations or what might be considered the “perceptual” side of reinforcement processing. In contrast, studies examining reward-guided response selection (the
action side of reinforcement processing) have generally
focused on medial prefrontal regions, although the DLF
has also been implicated in representing both reward
and responses, at least in monkeys (Wallis & Miller,
2003). The caudate nucleus may also play a particular
role in contingently rewarded action as opposed to the
passive experience of reward (Tricomi, Delgado, & Fiez,
2004), perhaps by virtue of its connections with the
medial and dorsolateral prefrontal cortex (Alexander,
Delong, & Strick, 1986).
There is evidence that medial prefrontal areas are
involved in representing value and perhaps more so
when a response is required. An fMRI study of a speeded
motor response task that was either unrewarded or
rewarded with small amounts of money showed greater
activation of the medial prefrontal cortex in the
rewarded condition. The BOLD signal in this region
increased as the amount of money won increased, and
deactivation was seen when no win was obtained
(Knutson, Fong, Bennett, Adams, & Hommer, 2003).
Interestingly, no further signal changes were detected in
the “punished” condition, when money was lost. In an
fMRI study of reversal learning, O’Doherty, Critchley,
Deichmann, and Dolan (2003) found that activity in the
medial prefrontal cortex predicted subsequent choice.
That is, reduced activity in this area was more likely to be
followed by a response shift on the subsequent trial. Similar reductions in activity prior to punishment-induced
response switching were observed in the dorsal anterior
cingulate cortex (ACC) in that study and in at least one
other study (Bush et al., 2002).

<-----Page 9----->168

BEHAVIORAL AND COGNITIVE NEUROSCIENCE REVIEWS

There is also evidence from event-related potential
studies that the medial prefrontal cortex plays a role in
rapid monitoring of outcome value and that this monitoring is related to subsequent choices. A midline negative potential beginning at about 200 ms was detected in
response to the outcome of a simple gambling game with
monetary wins and losses. The amplitude of this potential was larger for losses than for wins and scaled with the
amount of money at stake. The magnitude of the potential was systematically related to the riskiness of the decision taken on the next trial, leading the authors to argue
that this potential reflected a rapid assessment of the
motivational value of an outcome, which influenced subsequent decision making. Dipole modeling indicates
that the source is likely in or near the anterior cingulate
cortex (Gehring & Willoughby, 2002).
There are also clues from the animal literature that
the medial prefrontal cortex is important in motivated
behavior. Rats with lesions to the anterior cingulate
choose small rewards that require little physical effort, in
contrast to their prelesion willingness to work harder for
larger rewards (Walton, Bannerman, Alterescu, & Rushworth, 2003). Single-unit recordings from monkey ACC
have found that a relatively large proportion of neurons
are sensitive to the proximity of reward delivery in a
multistep task (Shidara & Richmond, 2002). Lesions to
this area in monkeys impaired learning of reward–motor
response associations, although stimulus-reward learning remained intact (Hadland, Rushworth, Gaffan, &
Passingham, 2003).
Preference judgments are yet another way of measuring the relative value of stimuli. These have the advantage of relating clearly to everyday behavior but the disadvantage of conflating value and choice, liking and
wanting. The difficulties of interpretation that arise
from this are illustrated by the lack of consistency
between the animal and human literatures on preference judgments. The monkey literature suggests a role
for the amygdala and OFC in this process, although evidence for a role for the OFC is not consistent. Bilateral
lesions of either of these structures lead to abnormal
food preferences in monkeys. This was expressed both as
a tendency to choose foods that are not preferred by normal monkeys in a two-choice task and by inconsistent
preference ordering for unfamiliar foods (Baylis &
Gaffan, 1991). In contrast, a recent study reported stable
preferences for familiar, palatable foods in monkeys with
bilateral OFC lesions (Izquierdo, Suda, & Murray, 2004).
The handful of fMRI studies of preference judgment
in humans have generally not detected changes in activity in either the OFC or amygdala. This may be related to
technical issues brought about by signal loss in the
region of the OFC (see earlier), but it may also be due to

a focus on choice rather than evaluation in the tasks.
When participants made preference judgments of food
items (presented as pictures) compared to determinations of simple visual features of the same stimuli, activations were seen in the anterior medial frontal cortex as
well as the ACC, superior parietal lobe, and insula
(Paulus & Frank, 2003). Another study asked participants to make either value judgments about (famous)
people, places, or activities or to recall semantic or episodic information about the same stimuli. Again, the
anterior medial frontal cortex was more active for the
evaluative versus either the episodic or semantic recall
task (Zysset, Huber, Ferstl, & von Cramon, 2002). A study
focusing on the evaluation of famous people found activations in the medial prefrontal cortex, VLF, and ACC
compared to a nonevaluative baseline task involving the
same stimuli (Cunningham, Johnson, Gatenby, Gore, &
Banaji, 2003). A recent H2O15-PET study explicitly
attempted to disambiguate value from motivated choice
with a region-of-interest design that focused on the
amygdala and OFC (Arana et al., 2003). Participants passively viewed descriptions of food taken from restaurant
menus and were asked to either imagine the dishes or
choose the dish they preferred. Both the amygdala and
medial OFC were more active when participants chose
between highly preferred foods than between less preferred foods, and activity in the amygdala correlated with
the participants’ predetermined food preferences,
regardless of whether a choice was required. A similar
area of the OFC was more active in the choice than nochoice condition. Interestingly, no activations were
detected in the medial prefrontal regions in the exploratory whole-brain analysis in this study.
As yet, studies of the brain basis of evaluation and
choice do not yield entirely consistent results. It does
seem clear that the ventral and medial prefrontal cortex
mediate some or many aspects of reinforcement processing in humans as in animal models. The correlational
nature of functional imaging results leaves open
whether the activity detected in these areas is necessary
for evaluation and choice. However, it is tempting to
speculate that different clinical manifestations of frontal
lobe damage may relate in part to disruption of different
aspects of reinforcement processing. That is, the impulsive and/or erratic choices often associated with ventral
prefrontal injury might relate to impairment in associating stimuli with context-specific reinforcement. In contrast, the abulia (or “lack of will”) classically related to
medial prefrontal damage could reflect disruption of
reinforcement-guided responding. Studies relating
these symptoms to precisely defined aspects of evaluation and choice will be important in linking these basic
cognitive neuroscience findings to the clinic.

<-----Page 10----->Fellows / HUMAN DECISION MAKING
CONCLUSION
Cognitive neuroscience is just beginning to provide
data relevant to developing a brain-based understanding
of human decision making. There have been two main
approaches to this topic to date. The first of these
approaches attempts to capture the key aspects of hard
decisions, such as choices that pit reward magnitude
against risk using laboratory tasks. Studies using these
relatively complex tasks have sparked renewed interest
in long-standing ideas about the relationship between
emotion and cognition and have at the least shown that
the frontal lobes play an important role in making tough
choices. However, efforts to understand the neural processes involved in performing these tasks at a finer level
of resolution have led to decidedly mixed results.
A second approach to this problem that may help to
elucidate the neural bases of decision making is to examine it at the component process level. This has several
advantages: First, the extensive literature on normal
human decision making can be used to identify theoretically meaningful candidate processes. Such a framework
also forces more clarity in defining and operationalizing
these processes. Second, existing data from several areas
of neuroscience, ranging from associative learning to
addiction research to studies of impulsivity, can provide
starting points for developing hypotheses about the neural bases of these component processes. This article has
attempted to frame the existing literature in such terms.
REFERENCES
Aharon, I., Etcoff, N., Ariely, D., Chabris, C. F., O’Connor, E., &
Breiter, H. C. (2001). Beautiful faces have variable reward value:
fMRI and behavioral evidence. Neuron, 32, 537-551.
Ainslie, G. (2001). Breakdown of will. Cambridge, UK: Cambridge University Press.
Alexander, G. E., DeLong, M. R., & Strick, P. L. (1986). Parallel organization of functionally segregated circuits linking basal ganglia
and cortex. Annual Review of Neuroscience, 9, 357-381.
An, X., Bandler, R., Ongur, D., & Price, J. L. (1998). Prefrontal cortical projections to longitudinal columns in the midbrain
periaqueductal gray in macaque monkeys. Journal of Comparative
Neurology, 401, 455-479.
Arana, F. S., Parkinson, J. A., Hinton, E., Holland, A. J., Owen, A. M., &
Roberts, A. C. (2003). Dissociable contributions of the human
amygdala and orbitofrontal cortex to incentive motivation and
goal selection. Journal of Neuroscience, 23, 9632-9638.
Ariely, D., & Zakay, D. (2001). A timely account of the role of duration
in decision making. Acta Psychologica, 108, 187-207.
Aron, A. R., Shohamy, D., Clark, J., Myers, C., Gluck, M. A., &
Poldrack, R. A. (2004). Human midbrain sensitivity to cognitive
feedback and uncertainty during classification learning. Journal of
Neurophysiology, 92, 1144-1152.
Barbas, H. (2000). Complementary roles of prefrontal cortical
regions in cognition, memory, and emotion in primates. Advances
in Neurology, 84, 87-110.
Barlas, S. (2003). When choices give in to temptations: Explaining
the disagreement among importance measures. Organizational
Behavior and Human Decision Processes, 91, 310-321.
Baron, J. (1994). Thinking and deciding (2nd ed.). Cambridge, UK:
Cambridge University Press.

169

Baxter, M. G., & Murray, E. A. (2002). The amygdala and reward.
Nature Reviews Neuroscience, 3, 563-573.
Baxter, M. G., Parker, A., Lindner, C. C., Izquierdo, A. D., & Murray,
E. A. (2000). Control of response selection by reinforcer value
requires interaction of amygdala and orbital prefrontal cortex.
Journal of Neuroscience, 20, 4311-4319.
Baylis, L. L., & Gaffan, D. (1991). Amygdalectomy and ventromedial
prefrontal ablation produce similar deficits in food choice and in
simple object discrimination learning for an unseen reward.
Experimental Brain Research, 86, 617-622.
Bechara, A., & Damasio, H. (2002). Decision-making and addiction
(part I): Impaired activation of somatic states in substance
dependent individuals when pondering decisions with negative
future consequences. Neuropsychologia, 40, 1675-1689.
Bechara, A., Damasio, H., & Damasio, A. R. (2000). Emotion, decision making and the orbitofrontal cortex. Cerebral Cortex, 10, 295307.
Bechara, A., Damasio, H., & Damasio, A. R. (2003). Role of the
amygdala in decision-making. Annals of the New York Academy of Sciences, 985, 356-369.
Bechara, A., Damasio, A. R., Damasio, H., & Anderson, S. W. (1994).
Insensitivity to future consequences following damage to human
prefrontal cortex. Cognition, 50(1-3), 7-15.
Bechara, A., Damasio, H., Damasio, A. R., & Lee, G. P. (1999). Different contributions of the human amygdala and ventromedial
prefrontal cortex to decision-making. Journal of Neuroscience, 19,
5473-5481.
Bechara, A., Damasio, H., Tranel, D., & Anderson, S. W. (1998). Dissociation of working memory from decision making within the
human prefrontal cortex. Journal of Neuroscience, 18, 428-437.
Bechara, A., Damasio, H., Tranel, D., & Damasio, A. R. (1997). Deciding advantageously before knowing the advantageous strategy. Science, 275, 1293-1295.
Bechara, A., Dolan, S., & Hindes, A. (2002). Decision-making and
addiction (part II): Myopia for the future or hypersensitivity to
reward? Neuropsychologia, 40, 1690-1705.
Bechara, A., Tranel, D., & Damasio, H. (2000). Characterization of
the decision-making deficit of patients with ventromedial
prefrontal cortex lesions. Brain, 123, 2189-2202.
Berlin, H. A., Rolls, E. T., & Kischka, U. (2004). Impulsivity, time perception, emotion and reinforcement sensitivity in patients with
orbitofrontal cortex lesions. Brain, 127(pt. 5), 1108-1126.
Berridge, K. C., & Robinson, T. E. (1998). What is the role of dopamine in reward: Hedonic impact, reward learning, or incentive
salience? Brain Research Brain Research Reviews, 28, 309-369.
Best, M., Williams, J. M., & Coccaro, E. F. (2002). Evidence for a dysfunctional prefrontal circuit in patients with an impulsive aggressive disorder. Proceedings of the National Academy of Sciences of the
United States of America, 99, 8448-8453.
Bickel, W. K., Odum, A. L., & Madden, G. J. (1999). Impulsivity and
cigarette smoking: Delay discounting in current, never, and exsmokers. Psychopharmacology, 146, 447-454.
Blood, A. J., & Zatorre, R. J. (2001). Intensely pleasurable responses
to music correlate with activity in brain regions implicated in
reward and emotion. Proceedings of the National Academy of Sciences of
the United States of America, 98, 11818-11823.
Brauer, L. H., & de Wit, H. (1996). Subjective responses to d-amphetamine alone and after pimozide pretreatment in normal, healthy
volunteers. Biological Psychiatry, 39, 26-32.
Breiter, H. C., Aharon, I., Kahneman, D., Dale, A., & Shizgal, P.
(2001). Functional imaging of neural responses to expectancy
and experience of monetary gains and losses. Neuron, 30, 619-639.
Breiter, H. C., & Rosen, B. R. (1999). Functional magnetic resonance
imaging of brain reward circuitry in the human. Annals of the New
York Academy of Sciences, 877, 523-547.
Busemeyer, J. R., & Stout, J. C. (2002). A contribution of cognitive
decision models to clinical assessment: Decomposing performance on the Bechara gambling task. Psychological Assessment, 14,
253-262.
Bush, G., Vogt, B. A., Holmes, J., Dale, A. M., Greve, D., Jenike, M. A.,
et al. (2002). Dorsal anterior cingulate cortex: A role in reward-

<-----Page 11----->170

BEHAVIORAL AND COGNITIVE NEUROSCIENCE REVIEWS

based decision making. Proceedings of the National Academy of Sciences of the United States of America, 99, 523-528.
Butler, A., & Scherer, L. (1997). The effects of elicitation aids, knowledge, and problem content on option quantity and quality. Organizational Behavior and Human Decision Processes, 72, 184-202.
Cardinal, R. N., Pennicott, D. R., Sugathapala, C. L., Robbins, T. W., &
Everitt, B. J. (2001). Impulsive choice induced in rats by lesions of
the nucleus accumbens core. Science, 292, 2499-2501.
Cavedini, P., Riboldi, G., D’Annucci, A., Belotti, P., Cisima, M., &
Bellodi, L. (2002). Decision-making heterogeneity in obsessivecompulsive disorder: Ventromedial prefrontal cortex function
predicts different treatment outcomes. Neuropsychologia, 40, 205211.
Cavedini, P., Riboldi, G., Keller, R., D’Annucci, A., & Bellodi, L.
(2002). Frontal lobe dysfunction in pathological gambling
patients. Biological Psychiatry, 51, 334-341.
Chiavaras, M. M., LeGoualher, G., Evans, A., & Petrides, M. (2001).
Three-dimensional probabilistic atlas of the human orbitofrontal
sulci in standardized stereotaxic space. Neuroimage, 13, 479-496.
Clark, L., Manes, F., Antoun, N., Sahakian, B. J., & Robbins, T. W.
(2003). The contributions of lesion laterality and lesion volume to
decision-making impairment following frontal lobe damage.
Neuropsychologia, 41, 1474-1483.
Clarke, H. F., Dalley, J. W., Crofts, H. S., Robbins, T. W., & Roberts, A. C.
(2004). Cognitive inflexibility after prefrontal serotonin depletion. Science, 304, 878-880.
Coffey, S. F., Gudleski, G. D., Saladin, M. E., & Brady, K. T. (2003).
Impulsivity and rapid discounting of delayed hypothetical
rewards in cocaine-dependent individuals. Experimental Clinical
Psychopharmacology, 11, 18-25.
Cottle, T. J., & Klineberg, S. L. (1974). The present of things future. New
York: Free Press.
Critchfield, T. S., & Kollins, S. H. (2001). Temporal discounting: Basic
research and the analysis of socially important behavior. Journal of
Applied Behavior Analysis, 34, 101-122.
Cunningham, W. A., Johnson, M. K., Gatenby, J. C., Gore, J. C., &
Banaji, M. R. (2003). Neural components of social evaluation.
Journal of Personality and Social Psychology, 85, 639-649.
Damasio, A. R. (1994). Descartes error: Emotion, reason, and the human
brain. New York: Avon Books.
Deichmann, R., Gottfried, J. A., Hutton, C., & Turner, R. (2003).
Optimized EPI for fMRI studies of the orbitofrontal cortex.
Neuroimage, 19(2 pt. 1), 430-441.
Dias, R., Robbins, T. W., & Roberts, A. C. (1996). Dissociation in
prefrontal cortex of affective and attentional shifts. Nature, 380,
69-72.
Dombrowski, S. M., Hilgetag, C. C., & Barbas, H. (2001). Quantitative
architecture distinguishes prefrontal cortical systems in the rhesus monkey. Cerebral Cortex, 11, 975-988.
Eslinger, P. J., & Damasio, A. R. (1985) Severe disturbances of higher
cognition after bilateral frontal lobe ablation: Patient EVR. Neurology, 35, 1731-1741.
Evenden, J. L. (1999). Varieties of impulsivity. Psychopharmacology,
146, 348-361.
Fellows, L. K., & Farah, M. J. (2003). Ventromedial frontal cortex
mediates affective shifting in humans: Evidence from a reversal
learning paradigm. Brain, 126, 1830-1837.
Fellows, L. K., & Farah, M. J. (2005). Different underlying impairments in decision-making following ventromedial and
dorsolateral frontal lobe damage in humans. Cerebral Cortex, 15,
58-63.
Fellows, L. K., & Farah, M. J. (in press). Dissociable elements of
human foresight: A role for the ventromedial frontal lobes in
framing the future, but not in thinking about future rewards.
Neuropsychologia.
Fischoff, B. (1996). The real world: What good is it? Organizational
Behavior and Human Decision Processes, 65, 232-248.
Gehring, W. J., & Willoughby, A. R. (2002). The medial frontal cortex
and the rapid processing of monetary gains and losses. Science,
295, 2279-2282.

Ghashghaei, H. T., & Barbas, H. (2002). Pathways for emotion: Interactions of prefrontal and anterior temporal pathways in the
amygdala of the rhesus monkey. Neuroscience, 115, 1261-1279.
Gigerenzer, G., & Todd, P. M. (1999). Simple heuristics that make us
smart. New York: Oxford University Press.
Glimcher, P. W., & Rustichini, A. (2004). Neuroeconomics: The
consilience of brain and decision. Science, 306, 447-452.
Godefroy, O., & Rousseaux, M. (1996). Binary choice in patients with
prefrontal or posterior brain damage: A relative judgement theory analysis. Neuropsychologia, 34, 1029-1038.
Godefroy, O., & Rousseaux, M. (1997). Novel decision making in
patients with prefrontal or posterior brain damage. Neurology, 49,
695-701.
Goel, V., Grafman, J., Tajik, J., Gana, S., & Danto, D. (1997). A study of
the performance of patients with frontal lobe lesions in a financial
planning task. Brain, 120(pt. 10), 1805-1822.
Gottfried, J. A., O’Doherty, J., & Dolan, R. J. (2003). Encoding predictive reward value in human amygdala and orbitofrontal cortex.
Science, 301, 1104-1107.
Grafman, J., & Goel, V. (2000). Role of the right prefrontal cortex in
ill-structured planning. Cognitive Neuropsychology, 17, 415-536.
Grant, S., Contoreggi, C., & London, E. D. (2000). Drug abusers show
impaired performance in a laboratory test of decision making.
Neuropsychologia, 38, 1180-1187.
Hadland, K. A., Rushworth, M. F., Gaffan, D., & Passingham, R. E.
(2003). The anterior cingulate and reward-guided selection of
actions. Journal of Neurophysiology, 89, 1161-1164.
Herrnstein, R. J., & Prelec, D. (1991). Melioration: A theory of distributed choice. Journal of Economic Perspectives, 5, 137-156.
Holt, D. D., Green, L., & Myerson, J. (2003). Is discounting impulsive?
Evidence from temporal and probability discounting in gambling
and non-gambling college students. Behavioural Processes, 64, 355367.
Hornak, J., O’Doherty, J., Bramham, J., Rolls, E. T., Morris, R. G., Bullock, P. R., & Polkey, C. E. (2004). Reward-related reversal learning
after surgical excisions in orbito-frontal or dorsolateral prefrontal
cortex in humans. Journal of Cognitive Neuroscience, 16, 463-478.
Izquierdo, A., & Murray, E. A. (2004). Combined unilateral lesions of
the amygdala and orbital prefrontal cortex impair affective processing in rhesus monkeys. Journal of Neurophysiology, 91, 20232039.
Izquierdo, A., Suda, R. K., & Murray, E. A. (2004). Bilateral orbital
prefrontal cortex lesions in Rhesus monkeys disrupt choices
guided by both reward value and reward contingency. Journal of
Neuroscience, 24, 7540-7548.
Johnson, J. G., & Raab, M. (2003). Take the first: Option-generation
and resulting choices. Organizational Behavior and Human Decision
Processes, 91, 215-229.
Kahneman, D., Slovic, P., & Tversky, A. (Eds.). (1982). Judgment under
uncertainty: Heuristics and biases. Cambridge, UK: Cambridge University Press.
Kawabata, H., & Zeki, S. (2004). Neural correlates of beauty. Journal of
Neurophysiology, 91, 1699-1705.
Kirby, K. N., & Herrnstein, R. J. (1995). Preference reversals due to
myopic discounting of delayed rewards. Psychological Sciences, 6, 8389.
Kirby, K. N., & Marakovic, N. (1996). Modeling myopic decisions: Evidence for hyperbolic delay-discounting within subjects and
amounts. Organizational Behavior and Human Decision Processes, 64,
22-30.
Kirby, K. N., Petry, N. M., & Bickel, W. K. (1999). Heroin addicts have
higher discount rates for delayed rewards than non-drug-using
controls. Journal of Experimental Psychology: General, 128, 78-87.
Knutson, B., Adams, C. M., Fong, G. W., & Hommer, D. (2001). Anticipation of increasing monetary reward selectively recruits nucleus
accumbens. Journal of Neuroscience, 21(16), RC159.
Knutson, B., Fong, G. W., Adams, C. M., Varner, J. L., & Hommer, D.
(2001). Dissociation of reward anticipation and outcome with
event-related fMRI. Neuroreport, 12, 3683-3687.
Knutson, B., Fong, G. W., Bennett, S. M., Adams, C. M., & Hommer, D.
(2003). A region of mesial prefrontal cortex tracks monetarily

<-----Page 12----->Fellows / HUMAN DECISION MAKING
rewarding outcomes: Characterization with rapid event-related
fMRI. Neuroimage, 18, 263-272.
Komura, Y., Tamura, R., Uwano, T., Nishijo, H., Kaga, K., Ono, T.
(2001). Retrospective and prospective coding for predicted
reward in the sensory thalamus. Nature, 412, 546-549.
Kringelbach, M. L., O’Doherty, J., Rolls, E. T., & Andrews, C. (2003).
Activation of the human orbitofrontal cortex to a liquid food stimulus is correlated with its subjective pleasantness. Cerebral Cortex,
13, 1064-1071.
Lipshitz, R., Klein, G., Orasanu, J., & Salas, E. (2001). Taking stock of
naturalistic decision making. Journal of Behavioral Decision Making,
14, 331-352.
Loewenstein, G. F., Weber, E. U., Hsee, C. K., & Welch, N. (2001). Risk
as feelings. Psychological Bulletin, 127, 267-286.
Maia, T. V., & McClelland, J. L. (2004). A reexamination of the evidence for the somatic marker hypothesis: What participants really
know in the Iowa gambling task. Proceedings of the National Academy
of Sciences of the United States of America, 101, 16075-16080.
Madden, G. J., Begotka, A. M., Raiff, B. R., & Kastern, L. L. (2003).
Delay discounting of real and hypothetical rewards. Experimental
and Clinical Psychopharmacology, 11, 139-145.
Manes, F., Sahakian, B., Clark, L., Rogers, R., Antoun, N., Aitken, M.,
& Robbins, T. (2002). Decision-making processes following damage to the prefrontal cortex. Brain, 125(pt. 3), 624-639.
Martin-Soelch, C., Leenders, K. L., Chevalley, A. F., Missimer, J.,
Kunig, G., Magyar, S., et al. (2001). Reward mechanisms in the
brain and their role in dependence: Evidence from
neurophysiological and neuroimaging studies. Brain Research
Reviews, 36(2-3), 139-149.
Mavaddat, N., Kirkpatrick, P. J., Rogers, R. D., & Sahakian, B. J.
(2000). Deficits in decision-making in patients with aneurysms of
the anterior communicating artery. Brain, 123(pt, 10), 2109-2117.
McClure, S. M., Laibson, D. I., Loewenstein, G., & Cohen, J. D.
(2004). Separate neural systems value immediate and delayed
monetary rewards. Science, 306, 503-507.
Mesulam, M. M. (2003). Some anatomic principles related to behavioral neurology and neuropsychology. In T. E. Feinberg & M. J.
Farah (Eds.), Behavioral neurology and neuropsychology (2nd ed., pp.
45-56). New York: McGraw-Hill.
Miller, L., & Milner, B. (1985). Cognitive risk-taking after frontal or
temporal lobectomyII. The synthesis of phonemic and semantic
information. Neuropsychologia, 23, 371-379.
Miller, L. A. (1992). Impulsivity, risk-taking, and the ability to synthesize fragmented information after frontal lobectomy.
Neuropsychologia, 30, 69-79.
Mobini, S., Body, S., Ho, M. Y., Bradshaw, C. M., Szabadi, E., Deakin,
J. F., et al. (2002). Effects of lesions of the orbitofrontal cortex on
sensitivity to delayed and probabilistic reinforcement.
Psychopharmacology, 160, 290-298.
Modell, J. G., Mountz, J. M., Glaser, F. B., & Lee, J. Y. (1993). Effect of
haloperidol on measures of craving and impaired control in alcoholic subjects. Alcoholism, Clinical and Experimental Research, 17,
234-240.
Montague, P. R., & Berns, G. S. (2002). Neural economics and the biological substrates of valuation. Neuron, 36, 265-284.
Monterosso, J., Ehrman, R., Napier, K. L., O’Brien, C. P., & Childress,
A. R. (2001). Three decision-making tasks in cocaine-dependent
patients: Do they measure the same construct? Addiction, 96, 18251837.
North, N. T., & O’Carroll, R. E. (2001). Decision making in patients
with spinal cord damage: Afferent feedback and the somatic
marker hypothesis. Neuropsychologia, 39, 521-524.
O’Doherty, J., Critchley, H., Deichmann, R., & Dolan, R. J. (2003).
Dissociating valence of outcome from behavioral control in
human orbital and ventral prefrontal cortices. Journal of Neuroscience, 23, 7931-7939.
O’Doherty, J., Kringelbach, M. L., Rolls, E. T., Hornak, J., & Andrews,
C. (2001). Abstract reward and punishment representations in
the human orbitofrontal cortex. Nature Neuroscience, 4, 95-102.
O’Doherty, J., Rolls, E. T., Francis, S., Bowtell, R., McGlone, F., Kobal,
G., et al. (2000). Sensory-specific satiety-related olfactory activation of the human orbitofrontal cortex. Neuroreport, 11, 399-403.

171

O’Doherty, J., Winston, J., Critchley, H., Perrett, D., Burt, D. M., &
Dolan, R. J. (2003). Beauty in a smile: The role of medial
orbitofrontal cortex in facial attractiveness. Neuropsychologia, 41,
147-155.
O’Doherty, J. P., Dayan, P., Friston, K., Critchley, H., & Dolan, R. J.
(2003). Temporal difference models and reward-related learning
in the human brain. Neuron, 38, 329-337.
O’Doherty, J. P., Deichmann, R., Critchley, H. D., & Dolan, R. J.
(2002). Neural responses during anticipation of a primary taste
reward. Neuron, 33, 815-826.
Ongur, D., An, X., & Price, J. L. (1998). Prefrontal cortical projections
to the hypothalamus in macaque monkeys. Journal of Comparative
Neurology, 401, 480-505.
Overman, W. H. (2004). Sex differences in early childhood, adolescence, and adulthood on cognitive tasks that rely on orbital
prefrontal cortex. Brain Cognition, 55, 134-147.
Pagnoni, G., Zink, C. F., Montague, P. R., & Berns, G. S. (2002). Activity in human ventral striatum locked to errors of reward prediction. Nature Neuroscience, 5, 97-98.
Paulus, M. P., & Frank, L. R. (2003). Ventromedial prefrontal cortex
activation is critical for preference judgments. Neuroreport, 14,
1311-1315.
Pecina, S., Cagniard, B., Berridge, K. C., Aldridge, J. W., & Zhuang, X.
(2003). Hyperdopaminergic mutant mice have higher wanting
but not liking for sweet rewards. Journal of Neuroscience, 23, 93959402.
Petrides, M., & Pandya, D. N. (1999). Dorsolateral prefrontal cortex:
Comparative cytoarchitectonic analysis in the human and the
macaque brain and corticocortical connection patterns. European
Journal of Neuroscience, 11, 1011-1036.
Petrides, M., & Pandya, D. N. (2002). Comparative cytoarchitectonic
analysis of the human and the macaque ventrolateral prefrontal
cortex and corticocortical connection patterns in the monkey.
European Journal of Neuroscience, 16, 291-310.
Petry, N. M., Bickel, W. K., & Arnett, M. (1998). Shortened time horizons and insensitivity to future consequences in heroin addicts.
Addiction, 93, 729-738.
Platt, M. L., & Glimcher, P. W. (1999) Neural correlates of decision
variables in parietal cortex. Nature, 400, 233-238.
Pochon, J. B., Levy, R., Fossati, P., Lehericy, S., Poline, J. B., Pillon, B.,
et al. (2002). The neural system that bridges reward and cognition
in humans: An fMRI study. Proceedings of the National Academy of Sciences of the United States of America, 99, 5669-5674.
Price, J. L., Carmichael, S. T., & Drevets, W. C. (1996). Networks
related to the orbital and medial prefrontal cortex: A substrate for
emotional behavior? Progress in Brain Research, 107, 523-536.
Rachlin, H., Raineri, A., & Cross, D. (1991). Subjective probability
and delay. Journal of the Experimental Analysis of Behavior, 55, 233244.
Reavis, R., & Overman, W. H. (2001). Adult sex differences on a decision-making task previously shown to depend on the orbital
prefrontal cortex. Behavioral Neuroscience, 115, 196-206.
Rempel-Clower, N. L., & Barbas, H. (1998). Topographic organization of connections between the hypothalamus and prefrontal
cortex in the rhesus monkey. Journal of Comparative Neurology, 398,
393-419.
Rogers, R. D., Everitt, B. J., Baldacchino, A., Blackshaw, A. J.,
Swainson, R., Wynne, K., et al. (1999). Dissociable deficits in the
decision-making cognition of chronic amphetamine abusers,
opiate abusers, patients with focal damage to prefrontal cortex,
and tryptophan-depleted normal volunteers: Evidence for
monoaminergic mechanisms. Neuropsychopharmacology, 20, 322339.
Rogers, R. D., Owen, A. M., Middleton, H. C., Williams, E. J., Pickard,
J. D., Sahakian, B. J., et al. (1999). Choosing between small, likely
rewards and large, unlikely rewards activates inferior and orbital
prefrontal cortex. Journal of Neuroscience, 19, 9029-9038.
Rogers, R. D., Tunbridge, E. M., Bhagwagar, Z., Drevets, W. C.,
Sahakian, B. J., & Carter, C. S. (2003). Tryptophan depletion alters
the decision-making of healthy volunteers through altered processing of reward cues. Neuropsychopharmacology, 28, 153-162.

<-----Page 13----->172

BEHAVIORAL AND COGNITIVE NEUROSCIENCE REVIEWS

Rolls, E. T. (2000). The orbitofrontal cortex and reward. Cerebral Cortex, 10, 284-294.
Rolls, E. T., Hornak, J., Wade, D., & McGrath, J. (1994). Emotionrelated learning in patients with social and emotional changes
associated with frontal lobe damage. Journal of Neurology, Neurosurgery, and Psychiatry, 57, 1518-1524.
Saad, G., & Russo, J. (1996). Stopping criteria in sequential choice.
Organizational Behavior and Human Decision Processes, 67, 258-270.
Sanfey, A. G., Hastie, R., Colvin, M. K., & Grafman, J. (2003). Phineas
gauged: Decision-making and the human prefrontal cortex.
Neuropsychologia, 41, 1218-1229.
Satish, U., Streufert, S., & Eslinger, P. J. (1999). Complex decision
making after orbitofrontal damage: Neuropsychological and strategic management simulation assessment. Neurocase, 5, 355-364.
Schlosser, R., Hutchinson, M., Joseffer, S., Rusinek, H., Saarimaki, A.,
Stevenson, J., et al. (1998). Functional magnetic resonance imaging of human brain activity in a verbal fluency task. Journal of Neurology, Neurosurgery, and Psychiatry, 64, 492-498.
Schoenbaum, G., Setlow, B., Saddoris, M. P., & Gallagher, M. (2003).
Encoding predicted outcome and acquired value in orbitofrontal
cortex during cue sampling depends upon input from basolateral
amygdala. Neuron, 39, 855-867.
Schultz, W. (2002). Getting formal with dopamine and reward. Neuron, 36, 241-263.
Shallice, T., & Burgess, P. W. (1991). Deficits in strategy application
following frontal lobe damage in man. Brain, 114(pt. 2), 727-741.
Shidara, M., & Richmond, B. J. (2002). Anterior cingulate: Single
neuronal signals related to degree of reward expectancy. Science,
296, 1709-1711.
Shizgal, P. (1997) Neural basis of utility estimation. Current Opinion in
Neurobiology, 7, 198-208.
Slovic, P., & Lichtenstein, S. (1971). Comparison of Bayesan and
regression approaches to the study of information processing in
judgment. Organizational Behavior and Human Decision Processes, 6,
649-744.
Small, D. M., Zatorre, R. J., Dagher, A., Evans, A. C., & Jones-Gotman,
M. (2001). Changes in brain activity related to eating chocolate:
From pleasure to aversion. Brain, 124(pt. 9), 1720-1733.
Stirling, W. C. (2003). Satisficing games and decision making: With applications to engineering and computer science. Cambridge, UK: Cambridge University Press.
Stuss, D. T., & Benson, D. F. (1986). The frontal lobes. New York: Raven
Press.

Stuss, D. T., & Levine, B. (2002). Adult clinical neuropsychology: Lessons from studies of the frontal lobes. Annual Review of Psychology,
53, 401-433.
Sugrue, L. P., Corrado, G. S., & Newsome, W. T. (2004). Matching
behavior and the representation of value in the parietal cortex.
Science, 304, 1782-1787.
Tanaka, S. C., Doya, K., Okada, G., Ueda, K., Okamoto, Y., &
Yamawaki, S. (2004). Prediction of immediate and future rewards
differentially recruits cortico-basal ganglia loops. Nature Neuroscience, 7, 887-893.
Tomb, I., Hauser, M., Deldin, P., & Caramazza, A. (2002). Do somatic
markers mediate decisions on the gambling task? Nature Neuroscience, 5, 1103-1104.
Tranel, D., Bechara, A., & Denburg, N. L. (2002). Asymmetric functional roles of right and left ventromedial prefrontal cortices in
social conduct, decision-making, and emotional processing. Cortex, 38, 589-612.
Tremblay, L., & Schultz, W. (1999). Relative reward preference in primate orbitofrontal cortex. Nature, 398, 704-708.
Tricomi, E. M., Delgado, M. R., & Fiez, J. A. (2004). Modulation of
caudate activity by action contingency. Neuron, 41, 281-292.
van Honk, J., Hermans, E. J., Putman, P., Montagne, B., & Schutter,
D. J. (2002). Defective somatic markers in sub-clinical psychopathy. Neuroreport, 13, 1025-1027.
Wallis, J. D., & Miller, E. K. (2003). Neuronal activity in primate
dorsolateral and orbital prefrontal cortex during performance of
a reward preference task. European Journal of Neuroscience, 18, 20692081.
Walton, M. E., Bannerman, D. M., Alterescu, K., & Rushworth, M. F.
(2003). Functional specialization within medial frontal cortex of
the anterior cingulate for evaluating effort-related decisions. Journal of Neuroscience, 23, 6475-6479.
Wilson, J. L., & Jezzard, P. (2003). Utilization of an intra-oral diamagnetic passive shim in functional MRI of the inferior frontal cortex.
Magnetic Resonance in Medicine, 50, 1089-1094.
Wise, R. A. (2002). Brain reward circuitry: Insights from unsensed
incentives. Neuron, 36, 229-240.
Zink, C. F., Pagnoni, G., Martin-Skurski, M. E., Chappelow, J. C., &
Berns, G. S. (2004). Human striatal responses to monetary reward
depend on saliency. Neuron, 42, 509-517.
Zysset, S., Huber, O., Ferstl, E., & von Cramon, D. Y. (2002). The anterior frontomedian cortex and evaluative judgment: An fMRI
study. Neuroimage, 15, 983-991.

