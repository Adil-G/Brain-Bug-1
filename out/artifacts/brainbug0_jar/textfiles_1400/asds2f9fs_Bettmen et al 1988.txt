<-----Page 0----->-RO

_**J4

-H

<-----Page 1----->ONR - Technical Report 88-1

A Componential Analysis of
Cognitive Effort in Choice

2
lames R. Bettmnin, Eric J. ?ohnson
and John W. Payne

September, 1988
1

Center for Decision Studies
Fuqua School of Business
Duke University-

2

2S
Wharton School
University of Pennsylvania

Sponsored by:
Perceptual S&'ences ProgramsOffice of Naval Research
Contract Number N00014-80-C-00114

-

SE0SP19 1988.-

3

Approved for Public Release: Distribution Unlimited

'

In Press:
Organizational Behavior and Human Decision Processes

88~

<-----Page 2----->iVt, 9Y 8 7

SECURITY CLASSIFICATION OF THIS PAGE

REPORT DOCUMENTATION PAGE
Ia

REPORT SECURITY CLASSIFICATION

lb RESTRICTIVE

MARKINGS

UNCLASSIFIED
2a

SECURITY CLASSIFICATION AUTHORITY

3

DISTRIBUTION/AVAILABILITY

OF REPORT

DISTRIBUTION

APPROVED FOR PUBLIC RELEASE:
UNLIMITED

2b DECLASSIFICATION/ DOWNGRADING SCHEDULE
4. PERFORMING ORGANIZATION REPORT NUMBER(S)

5 MONITORING ORGANIZATION REPORT NUMBER(S)

ONR Tech Report 88-1

N/A

6a NAME OF PERFORMING ORGANIZATION

6b OFFICE SYMBOL
(if applicable)

7a NAME OF MONITORING ORGANIZATION

Duke University

Office of Naval Research

6c. ADDRESS (City, State, and ZIPCode)

7b

Fuqua School of Business
Duke University

ADDRESS (City, State, and ZIP Code)

Arlington, VA

22217-5000

Durham. NC

27706
Ba. NAME OF FUNDINGISPONSORING
ORGANIZATION

8b OFFICE SYMBOL
(If applicable)

Office of Naval Olesearch

9 PROCUREMENT INSTRUMENT IDENTIFICATION NUMBER

Code 1142PS

Contract N00014-80-C-0114

8c. ADDRESS (City, State, and ZIP Code)

Arlington,

VA

22117-5000

10 SOURCE OF FUNDING NUMBERS
PROGRAM
PROJECT
TASK

WORK UNIT

ELEMENT NO

ACCESSION NO

61153N
11

NO

NO

RR-042-09

4425

R&T 4425063

TITLE (Include Security Classification)

A Componential Analysis of Cognitive Effort in Choice
12

PERSONAL AUTHOR(S)

Bettman, J.R., Johnson, E.J., & Payne, J.W.

I

13a TYPE OF REPORTe
h

Research

13b TIME COVERED
FROM

14. DATE OF REPORT (Year Month,Day)

TO

_

I

September, 1989

15 PAGE COUNT

i

16 SUPPLEMENTARY NOTATION

17

COSATI CODES
FIELD

GROUP

18 SUBJECT TERMS (Continue on reverse if necessary and identify by block number)

SUB-GROUP

19 ABSTRACT (Continue on reverse if necessary and identify by block number)

A model for measuring the cognitive effort required to execute a decision strategy is
proposed. The model is based on a set of elementary information processes (EIP's),
(e.g., reads, comparisons, additions) that are assumed to be common components of
decision strategies. A weighted sum of EIP's model is shown to provide good
predictions for response time and subjective reports of effort. Estimates of the
time and effort associated with each EIP seem plausible and consistent with those
found in other cognitive tasks. Overall, the EIP approach to conceptualizing and
measuring the effort of executing a choice strategy receives strong support. .

20 DISTRIBUTION/AVAILABILITY OF ABSTRACT
rUNCLASSIFIEDUNLIMITED
3
0 SAME AS RPT
22a NAME OF RESPONSIBLE INDIVIDUAL
|
GERALD S. MALECKI

DD FORM 1473,84 MAR

21 ABSTRACT SECURITY CLASSIFICATION
E] DTIC USERS

UNCLASSIFIED
22b TELEPHONE (Include Area Code) 22c OFFICE SYMBOL
(202) 696-4741
1Code
1142PS,

83 APR edition may be used until exhausted
All other editions are obsolete
|.
p

SECURITY CLASSIFICATION OF THIS PAGE

q--.

<-----Page 3----->Choice Effort

2
A major finding of the last decade of decision research is that an
individual may use many different kinds of strategies in making a decision,
contingent upon task demands (Payne, 1982; Abelson & Levi, 1985).

The use of

multiple strategies raises the fundamental issue of how people decide what to do.
An approach advocated by many researchers is to look at various decision
strategies as having differing advantages and disadvantages, and to hypothesize
that an individual might select the strategy that is, in some sense, best for the
task.

Several factors, such as the chance of making an error, justifiability

(Tversky, 1972), and the avoidance of conflict (Hogarth, 1987), can play an
important role in strategy selection.

However, in the current paper we focus on

another factor that is generally assumed to exert a major influence on strategy
use, the effort (cognitive resources) required to perform a strategy (Beach &
Mitchell, 1978; Johnson & Payne, 1985; Paquette & Kida, 1988; Russo & Dosher,
1983; Wright, 1975).
The notion that different decision strategies require different amounts of
computational effort to execute seems obvious.

The strategy of expected utility

maximization, for instance, requires a person to process all reinvest problem
information and to trade off values and beliefs.

The lexicographic choice rule

(Tversky, 1969), on the other hand, chooses the alternative which is best on the
most important attribute, ignoring much of the potentially relevant problem
information.

Thus, there appear to be clear differences among decision

strategies in the amount of information that is processed in making a choice.
At a more precise level of analysis, however, a comparison among decision
strategies in terms of cognitive effort is much more difficult.
%

In part this is

because the decision strategies that have been prnposed in the literature have
varied widely in terms of their formal expression.

Some have been proposed as

formal mathematical models (e.g., elimination-by-aspects, Tversky, 1972), and
ab

<-----Page 4----->"ILI

V_

M*.

Choice Effort

3
others as verbal process descriptions (e.g.,
dimensions rule, Russo & Dosher, 1983).

the majority of confirming

What is needed is a language that could

be used to express a diverse set of decision strategies in terms of a common set
of cognitive operations.

Such a language would provide a unifying framework for

describing strategies and allow strategy selection to be investigated at an
information processing level rather than at a more general level of analysis,
such as analytic vs. nonanalytic (Beach and Mitchell, 1978) or analytic vs.
intuitive (Hammond, 1986).

Such a language would also allow a more detailed

analysis of the components of processing (effort) involved when a particular
decision strategy is used to solve a particular decision problem (Maule, 1985).
In other words, one could examine whether the amount of information to be
processed is the major determinant of effort, or whether the specific mix of
cognitive operations which is utilized affects effort.
In addition to the problem of conceptualizing effort, another difficulty is
actually measuring the effort associated with a given strategy.

There have been

a number of measurement techniques proposed for the related concept of mental
workload (Gopher & Donchin, in press; Wickens, 1984), ranging from self-reports
to response times to physiological measures.

However, the different measures of

workload, such as response latencies, secondary tasks, or self-reports, do not
always agree.

Hence, Gopher and Donchin (in press) recommend the use of multiple

measures, along with a detailed theoretical analysis of the expected workload of
a task.
The purposes of this paper are 1) to conceptualize and develop a metric for
modeling decision effort;

2) to characterize the effort put forth by subjects

using different decision strategies in different choice environments;

and 3) to

assess the degree to which the proposed model is able to fit the subjects'
effort.

To accomplish these goals, we first develop a metric of decision effort

<-----Page 5----->Choice Effort
4

based on the concept of elementary information processes (Chase, 1978; Newell &
Simon, 1972).

We then examine the effort required by subjects to use different

decision strategies in choice environments varying in complexity, using two
indicators of strategy execution effort:
task difficulty.

decision latencies and self-reports of

We then use the proposed elementary information processes (EIP)

approach to modeling decision effort to predict these two indicators of strategy
execution effort.

Our overall goal is not to propose a complete theory of mernal

workload, but rather to illustrate an approach to measuring the execution effort
of choice strategies.
In the following section, previous attempts to conceptualize and measure
decision effort are briefly addressed, and the proposed approach is outlined.
Then the methodology and results of a study designed to test this approach are
described in detail.
Measuring Decision Effort.
The theoretical construct of mental effort has a long and venerable history
in psychology (Kahneman, 1973; Navon & Gopher, 1979; Thomas, 1983).

However,

there have been only a few attempts to model and compare decision rules in terms
of an effort metric.
Two studies that attempted to directly measure the execution effort of
various decision rules are Wright (1975) and Bettman & Zins (1979).

In each

study, subjects were instructed to use particular decision rules to solve certain
problems.

The percent of correct judgments using the rules and self-reports of

task difficulty or ease of use were obtained in both studies.

In addition,

Bettman and Zins obtained a measure of the time taken to apply a rule to a
problem.

The results clearly show that the rules were perceived to differ in the

effort required.

.

For example, a lexicographic rule was generally perceived as

less effortful than other decision rules.

That rule also tended to be the most

.

<-----Page 6----->Choice Effort
5
accurate and quickest in its execution.
significant limitations.

However, these two studies had

First, neither study employed a method beyond initial

instruction to ensure that subjects actually used the prescribed decision rules.
Second, neither study provided a conceptual basis (model) for why a certain
decision rule would be expected to be more or less effortful in a particular
task.

That is, neither study attempted to model the components of decision-

making effort.
Shugan (1980) suggested an effort metric based upon one operation, the
binary comparison of two alternatives on an attribute.
involved more comparisons.

More effortful decisions

Shugan also showed that the effort of strategies

would vary with certain task characteristics like the correlational structure
among attributes.

Unfortunately, using the binary comparison as the fundamental

unit of effort restricts Shugan's analysis to certain decision rules.
Nonetheless, Shugan's work implies that any approach to modeling strategy effort
must be sensitive to the joint effects of strategy and task.
Based upon the work of Newell and Simon (1972), Huber (1980) and Johnson
(1979) offered decompositions of choice strategies using more extensive sets of
components.

Each independently suggested that decision strategies be described

by a set of elementary information processes (EIPs).

A decision rule or strategy

was represented as a sequence of mental events, such as reading a piece of
information into STM (short-term memory), multiplying a probability and a payoff,
or comparing the values of two alternatives on an attribute.

Johnson and Payne

(1985) and Payne, Bettman, and Johnson (In press) employed a similar set of EIPs
for decision making, shown in Table 1, and constructed production system
implementations of several different choice strategies.
A particular set of EIPs, like thp nne given

-

represents a

theoretical judgment regarding the appropriate level of decomposition for

'.

<-----Page 7----->Choice Effort
6
decision processes.

For instance, the product operator might itself be

decomposed into more elementary processes.

We hypothesize, however, that the

above level of decomposition provides the basis for meaningful comparisons among
decision strategies in terms of effort.

Furthermore, we propose that a general

measure of decision effort is the number of component EIPs required to execute a
particular strategy in a particular task environment.

This notion of measuring

decision effort in terms of the number of EIPs builds on an idea for measuring
processing effort proposed by Newell and Simon (1972).

Empirical support for

this approach in areas other than decision making has been provided by showing a
relationship between the predicted number of EIPs used and response times for a
variety of cognitive tasks (Card, Moran, & Newell, 1983; Carpenter & Just, 1975).

Table I
Elementarr EIP's Used in Decision Strategies

READ

Read an alternative's value on an attribute into STM

COMPARE

Compare two alternatives on an attribute

DIFFERENCE

Calculate the size of the difference of two alternatives for
an attribute

ADD

Add the values of an attribute in STM

PRODUCT

Weight one value by another (Multiply)

ELIMINATE

Remove an alternative or attribute from consideration

MOVE

Go to next element of external environment

CHOOSE

Announce preferred alternative and stop process

<-----Page 8----->Choice Effort
7
To validate this componential (EIP) approach to measuring choice effort, we
examine four models of decision effort based upon EIPs.

The simplest model of

decision effort using EIPs would be to treat each component process as equally
effortful and simply sum the numbers of each component process to get an overall
measure of effort (the equal-weighted EIP model).

A second and slightly more

complex model would allow the effort required by each individual component to
vary (the weighted EIP model).
individual operations.

Total effort would then be a weighted sum of the

A third model would allow the effortfulness of the

individual EIPs to vary across rules (the weighted EIP by rule model).

While

such variation is of course a possibility, one would hope that the effortfulness
of an EIP would not vary as a function of the particular strategy of which it is
a component.

The goal of developing a unifying framework for describing

different decision strategies would be more difficult to attain if the sequence
of operations or the rule used affected the effort required for an EIP.

Finally,

the fourth model would allow the required effort for each EIP to vary across
individuals (the weighted EIP by individual model).

One might expect that some

individuals might find certain EIPs (e.g., the PRODUCT operator) relatively more
difficult than other individuals, for example.
An alternative model of effort which is not based on the componential
approach is also considered.

This model is based on the number of items of

information processed by a particular strategy in a choice environment.

Since it

is easy to monitor information acquisition behavior, this might be called an
explicit behavioral model of strategy effort.

A model based solely on

information acquisition implies that the specific type of processing done on the
information acquired makes little or no difference in determining decision
effort.

Such a model represortq a base-line model of -- fort in that the details

of processing are ignored.

%

%%

<-----Page 9----->Choice Effort

8
These models of strategy effort are

investigated with data from subjects

using different rules across choice tasks varying in complexity (number of
alternatives and attributes).

The performance of the subjects on the various

rules in the different tasks is characterized by two indicators of execution
effort:

the time to make a response and self-reports of effort.

We then use the
"e

The details of the

models proposed above to predict these two indicators.
methodology used are presented next.

Method
Overview
Subjects were trained to use six different strategies for making decisions.
Each strategy was used in a separate session to make twenty decisions

for

decision problems ranging in size from two to six alternatives and from two to
four attributes.

Subjects used a computer-based information acquisition system

to acquire information and make decisions among sets of alternatives
Payne, Schkade, & Bettman, 1986).

The computer-based acquisition system

monitored the subjects' information sequences;
acquisition;

recorded the uverall time

made by the subject (i.e.,
choice).

(Johnson,

recorded latencies for each

for each problem;

and recorded any errors

departures from the prescribed search pattern or

In addition, subjects rated the difficulty of each choice and the

effort each required on two response scales presented at the end of each decision
problem.

Subjects also provided data in a seventh session for twelve choi e

problems of various sizes where the subject was free to use any strategy desired.
Some suggestive findings from these data are considered in the discussion
section.
We describe

the details of the methodology for the prescribed stiategy

sessions as follows:

first, the six decision strategies used are Cscribed,

followed by a description and examples of how the EIP counts were generated.

-

.

.

fl,

1 -

_~ ...

..

.

-

<-----Page 10----->S'

Choice Effort
9
Then the generation of the sets of twenty decision problems is discussed,
followed by details on the computer-based acquisition system.

The experimental

procedure is then discussed in detail and some preliminary analyses are reported.
Finally, the details of the proposed models and an overview of the major analyses

-

performed are presented.
Decision Strategies
Rules Used.

Six different decision strategies were used in the prescribed

strategy portion of the experiment:

weighted additive; equal weighted additive;

lexicographic; elimination by aspects;
confirming dimensions.

satisficing (conjunctive); and majority of

Each of these rules was implemented as a production

system model (for examples see Johnson and Payne, 1985;
Johnson, In press).

0

Payne, Bettman, and

These particular rules were selected for two reasons:

1)

each rule has been a focus of previous research on choice processes; and 2) this
set of rules provides a broad coverage of the set of basic elementary operations

0

(EIPs) used as the components in our conceptualization of strategy execution
effort.

We first describe the strategies, and then the elementary operations are

considered.

•

A typical choice problem in our study consists of a set of alternative job
candidates, each of whom is described by scores, or ratings, on various selection
attributes or criteria (e.g.,

leadership potential and motivation).

For each

S

attribu e, an importance weight and a .utoff value specifying a minimally
acceptable level for that attribute were also displayed.

Different decision

strategies might use both weights and cutoffs, one of the two, or neither, as

J,
S

described below.
The weighted additive (WADD) rule requires the subject to develop an
evaluation for each alternative by multiplying each weight times the attribute
rating and adding those products for all attributes.

0

I

The alternative with the

'I.-N

<-----Page 11----->Choice Effort

-

10
highest evaluation is selected.

In the equal weighted additive (EQW) model,

the

evaluation for each alternative is obtained by adding the ratings for all the
attributes, with the alternative with the highest evaluation selected.

No

weights or cutoffs are used.
(LEX) rule requires the subject to first find the most

The lexicographic
important attribute

S

(the attribute with the largest weight) and then search the

values on that attribute for the alternative with the highest value.
alternative is selected, unless there are ties.

That

In this case, those tied

alternatives are examined on the second most important attribute.

.

•

That process

continues until a winner is found.
The elimination by aspects
most

(EBA) strategy also begins by determining the

important attribute and examining that attribute's cutoff value.

Next, all

alternatives with ratings below the cutoff for that attribute are eliminated.
This process continues with the second most important attribute, and so on, until

S

one alternative remains.

.

The satisficing (conjunctive) (SAT) rule requires the

subject to consider one alternative at a time, comparing each attribute

to the

cutoff value.

If any attribute is below the cutoff value, that alternative is

rejected.

first alternative which has values which pass the cutoffs for all

The

attributes is chosen.
Finally, the majority of confirming dimensions rule (MCD) processes pairs
of alternatives.

The values of the two alternatives are compared for each

attribute, and a running score is kept:

if the first alternative has a greater

value on an attribute than the second, one is added to the score;
alternative

is greater, one is subtracted;

score is not changed.

if the two alternatives are tied, the

After all attributes have been examined, if the score is

positive, the first alternative
alternative

if the second

is retained;

is retained;

if the score is negative, the second

and if the score is zero, the alternative winning the

0

<-----Page 12----->Choice Effort
11
comparison on the last attribute is retained.

Thus, the general idea is to

retain the alternative which is better on the most criteria.

The alternative

which is retained is then compared to the next alternative remaining among the
set of alternatives.

If no other alternative remains, the retained alternative

is selected.
Calculating EIP Counts
To describe the steps a subject followed in more detail and to show how EIP
counts were determined, we first consider the particular EIPs used and then
present two mor3 detailed examples of rules applied to a particular decision
problem.

The major EIPs utilized were MOVES, READS, ADDIIIONS, PRODUCTS,

COMPARES, ELIMINATIONS, and DIFFERENCES.

A MOVE involves moving to another piece

of information, while a READ consists of acquiring that information (moving it to
short term memory).

Since MOVES and READS are perfectly correlated in our

experiment, we will only consider READS (acquisitions) in this study.
PRODUCTS (of weights and ratings), and DIFFERENCES are self-evident.

ADDITIONS,
COMPARES

involved comparing two pieces of information and determining the larger (two
ratings, two overall alternative scores, two weights, a rating and cutoff, etc.).
Finally, ELIMINATIONS could be either discarding an att-.bute (because it had
already been used) or an alternative (because its score was surpassed, it failed
a cutoff, etc.).
Examples.

Two examples will be considered in more detail, a weighted

adding case and an EBA example.
comments are in order.

Before doing this, however, some general

First, the number of EIPs required for a particular

decision is a function of the specific rule used, the size of the problem (the
number of alternatives and attributes), and the specific values of the data.
Rules which examine all of the ratings

for each alternative, such as the weighted

adding rule, need more EIPs than rules which may process only part of the data,

<-----Page 13----->Choice Effort

12
such as the EBA rule.

Larger problems also tend to require more EIPs.

Problems

with more values which surpass cutoffs will also generally require more EIPs.
Second, in the specification of the rules, an attempt was made to take advantage
of the left to right, top to bottom natural reading order.

Table 2
(a)
Example of a Four Alternative, Three Attribute Decision Problem

Attributes
Alternatives

Leadership

Creativity

Experience

6(1)

4(2)

2(3)

7(5)

4(6)

Weights
A

4(4)

B

2(7)

7(8)

2(9)

C

6(10)

6(11)

3(12)

D

5(13)

7(14)

2(15)

(b)

Example of a Three Alternative, Four Attribute Decision Problem

Attributes
Alternatives

Leadership

Motivation

Creativity

Experience

Weights

4(1)

5(2)

3(3)

6(4)

Cutoffs

7(5)

4(6)

6(7)

6(8)

A

6(9)

5(10)

7(11)

7(12)

B

7(13)

4(14)

3(15)

6(16)

C

4(17)

3(18)

4(19)

4(20)

<-----Page 14----->Choice Effort

13
For the weighted adding rule, consider the 4 alternative, 3 attribute
decision problem shown in Table 2a.

The numbers in parentheses are labels that

will be used for convenience for identifying the sequence of acquisitions in the
Subjects were instructed to acquire the first weight (1) and then the

following.

rating on the first attribute (4).
retained the score.

They then multiplied these two numbers and

This process was repeated (sequence (2),

until alternative A was finished.

there would be six READS,

(1),

(7),

(2),

(3),

(8),

(9).

(6))

After processing the first

0

three PRODUCTS, two ADDS, and no

COMPARISONS, DIFFERENCES, or ELIMINATIONS.
be

(3),

For the first alternative, the total score of

60 was simply retained as the current best.
alternative,

(5),

For alternative B,

Then the total score

the sequence would
•

for B, 44, would be

compared to the current best, and the current best of 60 would be retained.

The

assumption was made that in the comparison of total scores, the losing
alternative was not explicitly eliminated.
store the one retained.

Thus,

Rather, the subject would merely

_

after two alternatives we would have twelve READS,

six PRODUCTS, four ADDS, one COMPARISON, no DIFFERENCES, and no ELIMINATIONS.
This process would be repeated for the remaining two alternatives
(10),

(2),

(11),

(3),

(12),

(1),

(13),

(2),

(14),

(3),

(15)).

(sequence (1),

S

Hence, the

production system model predicts that in total this problem would require 24
iA

READS,

8 ADDITIONS,

12 PRODUCTS, 3 COMPARISONS, no DIFFERENCES, and no

S

ELIMINATIONS.
The example of a three alternative, four attribute problem shown in Table
2b is used to clarify the EBA rule specification.
the most important attribute.

The subject had to first find

This was done by starting with the first weight

and comparing it to the second, retaining the larger (the second).
was then compared to the third, and the second was retained.

The second

Then the second was

compared to the fourth, and the fourth (experience) was retained as the most

7.

<-----Page 15----->Choice Effort
14
important attribute.
(4).

The sequence of acquisitions would thus be

There would be four READS and three COMPARISONS.

(1),

(2),

(3),

Then the subject acquired

the cutoff for experience and examined the value for all alternatives on
experience, comparing each value to the cutoff and eliminating any alternative
not passing the cutoff.

In this case, the sequence would be

(20), with alternative C eliminated.

(8),

(12),

(16),

and

The total EIPs thus far would be eight

READS, six COMPARISONS, and one ELIMINATION.

Then the experience attribute would

be eliminated, and the weights for the remaining three criteria would be acquired
and compared, resulting in motivation's being selected as the second most
important attribute

(sequence (1),

(2),

(3)).

Then the cutoff for motivation was

acquired and the values for the retained alternatives, A and B, were compared to
the cutoff (sequence (6),

(10),

(14)).

At this point, there would be a total of

14 READS, 10 COMPARISONS, and two ELIMINATIONS.

Both A and B passed the cutoff,

so the subject would then eliminate the motivation attribute and return to the
weights to determine the third most important remaining attribute, leadership
(sequence (1),

(3)).

Then the cutoff for leadership was examined, A and B were

compared to the cutoff, and A was eliminated.
(5),

(9),

(13).

B would then be chosen (sequence

In total, there would be 19 READS, 13 COMPARISONS, and four

ELIMINATIONS (two attributes and two alternatives).
These examples illustrate

two principles:

the number of EIPs varies with

problem size and with the particular values used, and different rules use
different subsets of the EIPs.

With regard to the second point, the weighted

adding rule uses READS, ADDITIONS, PRODUCTS, and COMPARISONS;
adding rule uses READS, ADDITIONS, and COMPARISONS;

the equal weighted

the lexicographic rule uses

READS, COMPARISONS, and ELIMINATIONS; the EBA rule uses READS, COMPARISONS, and
ELIMINATIONS; the satisficing rule uses READS, COMPARISONS, and ELIMINATIONS; and
the MCD rule uses READS, ADDITIONS, COMPARISONS, ELIMINATIONS, and DIFFERENCES.

P

<-----Page 16----->Choice Effort
15
It should also be noted that certain rules

(weighted adding, equal weighted

adding) have the same EIP counts for any problems of the same size (i.e.,
the same number of alternatives and attributes).

with

On the other hand, the other

rules (lexicographic, EBA, satisficing, and MCD) can have different EIP counts
values of the
even for problems of the same size, depending upon the particular
data.

This property of the rules affected the selection of decision problems for

the experiment, as discussed next.
Selection of the Decision Problems
As noted above, subjects completed twenty decision problems for each of the

,

by taking several
These decision problems were generated

six decision rules.
factors into account.

First, pilot studies revealed that problems with more than

four attributes were extremely difficult for subjects, particularly for the
weighted adding rule.

Second, problems with more than six alternatives caused

crowding on the computer display used in the information acquisition system.

.

Hence, the decision problems varied from two to six alternatives and two to four
attributes.

This generated 15 possible sizes, ranging from two alternatives and

two attributes to six alternatives and four attributes.

0

adding rules, since problem size
For the weighted adding and equal-weighted

5.'

determines the EIP count, one problem of each size was included, making fifteen%
decision problems.

Then five problem sizes were randomly selected to complete

the twenty decision problems.

0

Values for the weights and ratings were assigned

randomly, with the restriction that no overall scores for alternatives

in the

same problem set were tied.

5*,44

0

For the remaining rules, several problems were generated for each problem
size that represented low, intermediate and high EIP counts for that size (e.g.,
for a three alternative, four attribute EBA problem, elimination of two

0

alternatives on the first attribute would lead to a low count, retention of all
.-.

44

44 -

-

-.

,

0
-

1

<-----Page 17----->Choice Effort

16
three alternatives until the last attribute would be a high count, and the
operations used for the example described above might be an intermediate count).
Then sets of twenty problems were randomly selected for each rule from the total
set of forty-five size/count combinations.

Note that this selection procedure

implies that for each rule other than weighted adding and equal-weighted adding
there may be some problem sizes which were not selected.
The random selection procedure just described was repeated many times in an
attempt to deal with correlation problems among the EIP counts.

Since the EIP

counts were to be used as independent variables in regression models to predict
decision times and self reports of effort, it was desirable that their
intercorrelations across all 120 decision problems should be as low as possible
to avoid multi-collinearity problems (Kmenta 1986).

As noted above, however,

certain rules use only some EIPs and not others, so there are some correlations
that will be high because of the definition of the rules.

For example, the

correlation between COMPARISONS and ELIMINATIONS will tend to be high because
rules with no ELIMINATIONS (e.g., the adding rules) tend to do very few
COMPARISONS, whereas rules with many COMPARISONS also have more ELIMINATIONS.

To

minimize these intercorrelation problems, we repeated the random selection
procedure 1,000 times and selected the set of 120 decision problems with the
smallest intercorrelations.
3.

The resulting intercorrelations are shown in Table

Despite these efforts, we were unable to further reduce the highest, COMPARES

and ELIMINATIONS, for the reasons outlined above.
The Computer-Based Information Acquisition System
A computer-based information acquisition system called MOUSELAB was
utilized in carrying out the experiment (Johnson, Payne, Schkade, & Bettman,
1986).

The subject saw a matrix display on the computer monitor for each

<-----Page 18----->Choice Effort
17

Table 3
Intercorrelations Among EIP Counts for the 120 Decision Problems Selected

Operators
ADDITIONS

READS

PRODUCTS

.487

ADDITIONS

COMPARES

ELIMINATIONS

DIFFERENCES

.543

.541

.280

.272

.591

-.259

-.495

.140

-.302

-.374

-.146

.852

.492

PRODUCTS
COMPARES
ELIMINATIONS

.158

decision problem.

The rows of the matrix were labeled weights, cutoffs, and then

the names of the alternatives to be considered.

the names of the attributes.

The columns were labeled with

At the bottom of the monitor screen were boxes used

to indicate choice of an alternative (hence termed choice boxes).

For an example

of this display, see Figure 1.
Initially, the matrix display provides only the labels for the rows and
columns and the choice boxes.
the screen.

The information is hidden in the blank cells on

To acquire information, the subject must move a cursor controlled by

the mouse to the desired cell of the matrix.
information.

The cell then opens, displaying the

For each decision, the subject would use the mouse to acquire the

appropriate information in the sequence specified by the current strategy.
MOUSELAB recorded the sequence in which cells were opened and the time spent in
each cell.

The time measurements use the system clock of the personal computer,

providing a resolution of approximately 17 milliseconds.

After the requisite

<-----Page 19----->Choice Effort

18
information had been examined, the subject moved to the appropriate choice box
and clicked a button on the mouse to designate the chosen alternative.

%

A crucial feature of MOUSELAB for the present study is the ability to
monitor the seauence of acquisitions made by a subject.

Since the EIP models of

effort we propose require EIP counts for each problem, it is crucial that
subjects use the strategy exactly as it is specified, so that the EIP counts can
be predicted accurately.

For example, to ensure that the EIP counts for the

weighted adding and EBA examples given above are correct, we must monitor that
subjects follow the exact acquisition sequence for each rule.

MOUSELAB includes

a move monitoring feature, which allows the correct sequence of cells to be
specified for each decision problem.

If the subject enters a "wrong" cell, the

cell will not open, and after two seconds the computer will emit an audible buzz.
The attempt to enter an incorrect cell is also recorded in the output information
about the subject's move sequence.

Hence, trials where a specified number of

I

incorrect moves has occurred can later be discarded or analyzed as error trials
if desired.
An analysis of a typical decision task for this study using Fitts Law
(Card, Moran, & Newell, 1983) indicates that subjects could move between
information cells in less than 100 milliseconds. This suggests that the time to
move the mouse is limited mainly by the time it takes to think where to point,
not the movement of the mouse itself.
Procedure
Overview.
of several days.

Subjects participated in eight separate sessions over a period
Each session lasted from one to one and a half hours.

No more

than two sessions were run in one day, and separate sessions were at least four
hours apart.

The first session taught subjects the decision rules and

•I

<-----Page 20----->4I"

Mm

& *

w

Choice Effort

19
familiarized them with MOUSELAB.

In each of the subsequent six estimation

sessions, a subject made twenty choices using a different specified rule.
order of the rules was randomized across subjects.

The

The final session had twelve

choice problems where the subject was free to use any strategy desired.
problems all had four attributes and a third of the problems had two,

These

four, and

six alternatives, respectively.
Subjects.

Subjects were seven adults, ranging in age from 21 to 34, and

included four males and three females.

They varied in their prior awareness of

the decision making literature, ranging from graduate students who had studied
decision making to non-students who had never been exposed to those concepts.
Training.

It was crucial thaL subjects thoroughly learn the six decision

0

strategies to be used (weighted adding, equal-weighted adding, lexicographic,
elimination by aspects, satisficing, and majority of confirming dimensions) and
learn to use MOUSEIAB.

Hence, a familiarization session was developed.

Subjects

were first introduced to the mouse and were shown how to use it to open the
cells, respond to various response scales, and indicate a choice.
practicing these

After

tasks, subjects were next given a training session for the

S

decision rules which was developed using the MOUSELAB system.
The subject was informed that the decisions to be made were personnel
decisions involving selection of job candidates.

These selections were to be

made according to the rules specified by different divisions of their company,
and the sets of candidates might have both differing numbers of candidates

(from

two to six) and different amounts of information on each candidate (from two to
four attributes).

The

four possible attributes were leadership potential,

creativity, job experience, and motivation.

The left to right ordering of the

subset of these attributes used on any given trial was randomized.

S

<-----Page 21----->Choice Effort
Figure 1

20

Example of*A Stimulus Display Usingt Mouselab System

A4

r"

p..,

%

%

ZleI
i -,

<-----Page 22----->Choice Effort

21
Next, subjects were introduced to the ratings used to describe each
candidate on each attribute.

Ratings ranging from 2 (poor) to 7 (excellent) were

used as the information in each cell.

Subjects were then introduced to the ideas

of importance weights for the attributes and cutoffs for the attributes.

They

were then asked to select the most important attribute and to pick candidates
surpassing a cutoff to provide training using these ideas.

These concepts were

then reviewed before the decision rules were introduced.
For each rule, the subject was first given a thorough written description
of the rule on the computer monitor.

Then the subject was given several decision

problems and told to apply the rule using the mouse.

The move monitoring system

was used on the last trial to inform subjects of mistakes.

The subject was also

told what the correct choice using the rule should have been.

Thus, subjects had

accuracy feedback on both the sequence of acquisitions and their choices during
training.

Following these practice trials, the next rule was presented.

The

rules were presented in the familiarization session in an order ranging from
simple to more complex:

equal-weighted adding, lexicographic, satisficing,

elimination by aspects, weighted adding, and majority of confirming dimensions.
Finally, after all six rules had been presented, subjects were given six
practice trials, one for each rule.

These trials introduced the use of two

response scales to measure the difficulty of the decision task and how effortful
the decision was.

Thp first scale asked the subject to rate how difficult the

choice was to make on a scale ranging from 0 (not difficult at all) to 10
(extremely difficult).

The second scale asked the subject to rate how much

effort he or she put into making the choice on a scale ranging from 0 (hardly any
effort) to 10 (a great deal of effort).
was threefold:

The purpose of these six practice trials

1) to introduce the response scale; 2) to consolidate the

learning of the rules; and 3) to introduce subjects to the range of difficulty in

<-----Page 23----->Choice Effort
22
the problems so that they could calibrate their use of the response scales more
This latter purpose was

accurately during the actual estimation sessions.

accomplished by selecting a variety of problem sizes and difficulty levels for
the six practice trials.
Estimation Sessions.

At the beginning of each session, the subject was

given a review of that session's decision rule.

The rule was described again,

and several practice trials were given, with feedback on the accuracy of the
acquisition sequence and choice.

Then subjects were given a sequence of decision

problems where they had to make two consecutive choices using the rule with no
errors in acquisition sequence or alternative chosen.

Following successful

completion of these trials to criterion, the actual experimental trials for that
session began.
As noted above, the twenty choice problems for each decision rule were
presented to the subject on an IBM Personal Computer via the MOUSELAB software.
Subjects used a Mouse Systems mouse as a pointing device.

These problems were

randomly ordered (the random order was the same for all subjects).

For each

problem, the subject followed the sequence of acquisitions implied by the rule.

a

The move monitoring system described above was used to monitor subjects'
adherence to the correct sequence for the rule.

Subjects then indicated the

alternative chosen, and responded to the difficulty and effort scales described
above.

For each choice, MOUSELAB recorded the sequence of acquisitions, the time

of entry and exit for each cell, the alternative chosen, and values on the two
response scales.

The overall latencies for the choice and the two scale

responses were also recorded.
After completing all eight sessions, subjects received $40 for their
participation.

In addition, they were told that three $5 bonuses would be paid

%

,

%

<-----Page 24----->Choice Effort

23
for (1) above average performance in terms of overall accuracy, (2) minimization
of incorrect search, and (3) speed of decision, respectively.

In other words,

subjects were informed that they could earn an additional payment of up to $15
dollars depending upon their performance.
Preliminary Analyses
Before the major analyses could be performed, the data were analyzed to
determine the prevalence of errors, the existence of speed-accuracy tradeoffs,
and the relationship between the two self-report measures of effort.
Subjects selected incorrect alternatives on 11.4% of the trials.

In

addition, .8% of the trials contained severe deviations from the correct sequence
of acquisitions specified for that trial (i.e., more than two "buzzes"), even
though the correct alternative was still selected.
total of 12.2% error trials.

Taken together, this yields a

Over half of these errors come from the weighted

adding (27.1%) and elimination by aspects (32.2%) rules.
error trials of both types were removed from the data.

For all analyses, all
However, analyses

performed when all trials were included show virtually identical results.
To examine the possible existence of speed-accuracy tradeoffs, response
latency was correlated with error, both across and within strategies.

Overall,

the correlation between time for each decision and the probability of an error
was .15 (p<.0001).

Similar positive correlations were obtained for each rule,

subject, and rule by subject combination.

In no case was there a significant

negative correlation

these data are relatively free from

which indicat-ed rt

any concerns with speed-accuracy tradeoffs.
Finally, the two self-report measures of effort and difficulty were
examined.

Their intercorrelation was .85, suggesting that they measure the same

%

<-----Page 25----->Choice Effort
24
underlying construct.

A principal components analysis showed that the first

factor accounted for 93% of the variance in the scores, so the two ratings were
added to form an overall index of subjective effort.
For the analyses we report, the various models described below are
estimated using different independent variables.

In every model, however, dummy

variables representing the subject and session (i.e.,

0

the order of that session

among the six estimation sessions) are included, as are variables representing
the linear and quadratic effects of trial (i.e.,
decision problems within any session).

the order among the twenty

These variables, although statistically

significant, account for small portions of the explained variance and simply
allow for changes in the intercept term across sessions and subjects and for any
effects of practice across trials to be taken into account.

0

Since the effects

are not theoretically important for our purposes, they are not reported in the
discussion of the results.
Overview of the Analyses
As discussed above, we consider two major indicators of strategy execution
effort for the experimental data collected: response times and an index of selfreported effort.

In the results section below, we first present data showing the

S
.

average performance across subjects on these two measures for the various rules
for the different problem sizes.

0

Following this attempt to characterize strategy performance, we examine the
two classes of models for effort we outlined above:
and EIP.

behavioral (informational)

Recall that the behavioral model attempts to explain effort using the

only overtly observable behavior, the number of information acquisitions (READS).
Four different EIP models are also examined.
all EIPs are given the same weight.

In the equal-weighted EIP model,

In contrast, the weighted EIP model allows

each EIP to have its own characteristic effect upon each dependent measure.

The

V

<-----Page 26----->Choice Effort
25
third model, the weighted EIP by rule model, allows the effect of each EIP to
vary by rule.

Finally, the weighted EIP by individual model allows the

coefficient of each EIP to vary by individual.

Note that allowing the

coefficients to vary across individuals is what characterizes this model;
individual subject dummy variables which allow the intercet term to vary over
individuals are included in all of the models, as are the session and trial
variables described above.

Regression analyses are performed using the

independent variables specified by each model and response latency and selfreported effort as dependent variables.

We can assess the relative fit for each

model and test the significance of certain model comparisons.

1

Results
Before considering the fit of the various models, we first present the
average response times and self-reported effort levels for the various rules for
each problem size.

Then we consider the models of response times and self-

reports of effort, respectively.
Average Response Times and Self-Reports of Effort by Strategy and Problem
Size
Table 4 summarizes the average response times, and Table 5 presents the
self-reports of effort for each decision strategy for the different problem
sizes.

These data are averaged across all seven subjects.

Recall that some

problem sizes were not used for some strategies because of the selection
procedure described above.
As would be expected, decision problems of increasing complexity, i.e.,
more alternatives and/or more attributes, take longer and are viewed as more
effortful.

An analysis of variance of response times showed both a main effect

of number of alternatives (F(4,647) - 46.21, p < .0001; means of 17.2, 22.5,
26.3, 36.4, and 57.8 seconds for 2,3,4,5, and 6 alternatives respectively) and

<-----Page 27----->I..%

Choice Effort

26
number of attributes (F(2,647)

-

52.36, p < .0001; means of 22.0, 29.3, and 41.0

seconds for 2,3, and 4 attributes, respectively).

Similarly, for self-reports of

effort there were main effects for both number of alternatives (F(4,647) - 26.52,
p < .0001; means of 2.7, 3.5, 4.2, 5.3, and 6.1 for 2,3,4,5, and 6 alternatives,
respectively) and number of attributes (F(2,647) - 33.19, p < .0001; means of
a-'

3.5, 4.1, and 5.5 for 2,3, and 4 attributes, respectively).
Of perhaps greater interest, the effects of task complexity vary by
strategy.

For response time, there were significant rule by number of

alternatives (F(20,647) - 8.78, p < .0001) and rule by number of attributes

e

(F(10,647) - 3.38, p < .0005) interactions, and a marginally significant rule by
number of alternatives by number of attributes interaction (F(30,647) - 1.46, p <
.06).

For the self-reports of effort, there was a significant rule by number of

P

alternatives interaction (F(20,647) - 1.98, p < .007), a marginally significant
rule by number of attributes interaction (F(10,647) - 1.77, p < .07), and a nonsignificant three-way interaction (F(30,647)

-

.43, ns).

0

The form of these

interactions is that the weighted additive rule shows much more rapid increases
in response time and generally shows more rapid increases in self-reports of

S

effort as a function of increases in task complexity than the other strategies.

"

This is of course consistent with a great deal of other research showing that
individuals shift toward simplifying decision heusistics as a function of
increases in task complexity, particularly with increases in number of
alternatives (Payne, 1982).

.. J

<-----Page 28----->.0Z

Choice Effort

27%

Table 4
Average Response Times by Strategy and Problem Size
Qe

Strategy
Problem Size
Number
of
Alternatives

2

Number
of
Attributes

WADD

EOW

LEX

EBA

SAT

MCD

2

1 8 4a

12.7

8.7

.b

8.5

--

3

24.6

11.5

12.5

15.7

12.8

18.2

4

33.0

15.8

17.7

--

23.9

24.9

2

35.0

26.8

17.0

11.9

--

17.8

3

41.6

18.3

16.4

17.2

16.5

22.1

4

64.6

21.5

21.5

27.8

38.4

2

39.4

21.3

12.6

14.3

13.8

16.7

3

47.8

29.7

18.7

17.6

22.3

32.2

4

77.5

40.5

26.9

26.1

24.2

2

46.7

29.0

15.0

19.5

3

76.6

46.3

26.2

14.4

30.4

33.1

4

86.7

42.2

36.0

36.4

36.8

47.1

2

62.7

39.0

20.0

....

3

162.4

48.3

31.7

18.5

41.6

32.5

4

154.5

71.7

46.7

31.0

62.5

46.9

0

3

4

5

6

--

--

....

28.7

aS in seconds
a Average response time,
b This problem size not selected for this rule.

%%

<-----Page 29----->Choice Effort

28
Table 5
Average Self-Reports of Effort by Strategy and Problem Size

Strategy
Problem Size
Number
of
Alternatives

2

3

4

5

6

Number
of
Attributes

WADD

EOW

LEX

EBA

SAT

MCD

_.b

1.73

--

2.43

1.79

2.57

3.33

--

1.74

4.48

3.40

1.83

2.49

--

3.92

5.73

2.78

2.97

3.48

4

5.67

3.45

2.65

2

6.85

3.23

3

7.63

4

.7 7 a

.93

2.56

2

3

3

3.81

1.67

1.4

4

6.90

2.34

2

4.91

3

3.34

4.47

--

3.67

5.46

2.45

2.62

2.37

4.58

3.01

3.20

3.37

3.72

4.70

8.61

6.35

4.49

5.15

4.08

2

7.08

3.73

2.94

2.90

....

3

6.37

4.07

4.73

2.87

5.50

6.74

4

10.29

5.71

5.20

6.22

5.35

7.22

2

6.88

4.76

2.13

3

8.92

5.40

5.03

2.52

6.59

6.33

4

11.0

9.72

6.22

5.40

7.24

6.00

b This problem size not selected for this rule.

5.02

....

a Average self-reported effort, on a O(low) to l0(high) scale

--

<-----Page 30----->Choice Effort
29
The results presented above and in Tables 4 and 5 demonstrate that the
various rules perform differently in different task environments in terms of two
indicators of effort, response time and self-reports of effort.

The central

question of interest, however, is whether the componential framework proposed
above can provide a unifying treatment of the effort required by these rules and
model these differences in effort.

Hence, we examine the extent to which the

various models outlined above fit the data summarized in Tables 4 and 5.

We

explore that issue for response time and self-reports of effort in turn.
Analysis of Response Times
Table 6 provides a summary of the degrees of fit for the four EIP models
and the behavioral model (reads only). 2

All the models provide good fits for the

overall response times (p < .0001). Note that the fit of the weighted EIP model
is significantly better than that of the behavioral model (F(5,713) - 81.4, p <
.0001) or that of the equal-weight EIP model (F(5, 713) - 78.9, p <

.0001).3

0

Thus, it appears that a model of cognitive effort in choice, as measured by
response time, requires concern not only for the amount of information processed

e

(READS), but also the various processes applied to that information (e.g.,

I

Products and Comparisons), with differential weighting of those operators.
Table 6
Degree of Fit for Models of Response Time and Self-Reports of Effort
R 2 Values
Model

Response Time

Self-Reports of Effort

Behavioral

.75

.56

Equal-weighted EIP

.75

.55

Weighted EIP

.84

.59

Weighted EIP by Rule

.84

.61

Weighted EIP by Individual

.90

.80

%."
S

.V3

<-----Page 31----->Choice Effort

30
While the degree of fit for the weighted EIP model is impressive, we must
examine whether more complex models improve the fit.

First, we consider the

weighted EIP by rule model, which allows the time for each EIP to vary by rule,
to determine whether the EIPs require the same time for each rule.

The weighted

EIP model is a special case of the weighted EIP by rule model, so the
significance of the incremental fit can be tested.

The incremental fit is not

significant (F(13, 700) - 1.37, ns); hence, the assumption that each operation
requirc- a constant amount of time independent of the strategy in which it is
used seems reasonable.
The weighted EIP by individual model allows the times for the EIPs to vary
across subjects.

Even if individuals use the same strategy, they may differ in

the amount of time required for each component process (Hunt, 1978; R. Sternberg,
1977).

This model achieves an R-

.90, with significantly better fit than the

weighted EIP model (Incremental R 2 - .06, F(36, 677) - 10.9, p < .0001).

These

individual differences are considered further in the discussion section.
Thus, based upon the analyses of response times, the weighted EIP model,
and hence the EIP conceptualization of decision effort, receives strong support.
4
The EIP times appear to vary across individuals, although not across rules.

These results also hold up well in cross-validation.

Estimating the model

on one-half of the data and using these estimates to predict the other half
yields average R 2 values of .74,

.73, .81, .82, and .88 for the behavioral,

equal-weighted EIP, weighted EIP, weighted EIP by rule, and weighted EIP by
individual models, respectively.
Estimates of EIP Times.

Since the weighted EIP model received strong

support, estimates of the times for each operator are shown in Table 7.

Although

the estimates vary to some extent across individuals, as a first approximation we
consider the pooled results.

*V%~%

'I, '%
*b~)~h% ' SSS.?.S
~

~

<-----Page 32----->Choice Effort

31

Table 7
Coefficient Estimates of Response Time and Self-Reports of Effort for EIP's
EIP

DIFFERENCES
Response
Time

READS

ADDITIONS

PRODUCTS

COMPARISONS

ELIMINATIONS

1.19

.84

2.23*

.09

1.80*

.10

.08

.19*

.04

.32

Self-Reports
of Effort

.32

-.12

Significantly different from zero at p < .05.

The coefficients are all positive, with most significantly different from
zero.

The estimates also tend to agree with estimates for similar EIPs provided

by other studies.

The READ EIP combines encoding information with the motor

activity of moving the mouse.
6.55, p < .0001).

Its estimated latency is 1.19 seconds (t(713) -

This estimate is plausible, since it might consist of the

movement of the mouse, estimated to be in the range of .2

-

.8 seconds by

Johnson, Payne, Schkade, and Bettman (1986), and an eye fixation, estimated to
require a minimum of .2 seconds (Russo, 1978).

ADDITIONS and SUBTRACTIONS both

take less than one second, with estimates of .84 (t(713) - 4.54, p < .0001) and
.32 (t(713) - .98, n.s.) respectively.

These values are not significantly

different (t(713) - 1.03, n.s.) and are consistent with those provided by
Dansereau (1969), Groen and Parkman (1972), and others (see Chase, 1978, Table 3,
p. 76).

Our estimate for the PRODUCT EIP, 2.23 seconds (t(713) - 10.36, p <

.0001), is larger than that commonly reported in the literature.
The time for COMPARISONS is very short, .08 seconds (t(713) - .22, n.s.),
and that for ELIMINATIONS, 1.80 seconds (t(713) - 3.00, p < .01), is relatively
long.

This may reflect the collinearity of COMPARES and ELIMINATIONS.

<-----Page 33----->Choice Effort

32
In sum, based both upon its degree of fit and the generally plausible time
estimates for the EIPs, the proposed weighted EIP model receives impressive
support when response times are used as an indicator of effort.

The next set of

results examines the performance of the various models when self-reports of
effort form the indicator of effort.
Analysis of Self-Reports of Effort
There are several reasons why self-reports of effort are interesting as a
second indicator of decision effort.

First, self-reported effort might tap

Y.7

different aspects of strategy execution effort and might not be closely related
to decision latency.

As Kahneman (1973) observed, two different mental tasks may

*

take similar amounts of time, but one might be seen as much more effortful than
the other.

This speculation receives some support in our data:

the overall

correlation between time and the self-reported effort index is .29.

Secondly,

while the analysis of latency helps validate the proposed EIP conceptualization

0

of effort, self-perceptions of effort may also be important in understanding why
decision-makers avoid certain strategies.

However, several cogent arguments for

caution in the use of self-reported measures of effort should also be noted.

S

Foremost among these is the possibility that subjects cannot accurately report
demands on cognitive resources (Gopher and Donchin, In press), or that such
reports do not allow comparisons across tasks which make widely differing

0

demands.
Model Fit.

From the results shown in Table 6, it can be seen that the

absolute levels of fit are lower than for the response latencies, but are still
highly significant (p < .0001).6

The weighted EIP model again provides

significantly greater fit than the behavioral (F(5, 713)
equal-weighted EIP (F(5, 713)

-

13.0, p < .0001) models.

-

10.0, p < .0001) or

'.

U
%

A,

<-----Page 34----->Choice Effort

33
The weighted EIP model of subjective effort can also be compared to more
complex models.

The weighted EIP by rule model shows a small, but statistically

significant increase in fit

(incremental R-

.02, F(13,700) - 2.6,

p <

.002).

C

The weighted EIP by individual model shows a substantial increase in fit
(incremental R 2

-

.21, F(36,677) - 20.1, p <

.0001).

Hence, the results essentially replicate those for response times.

The

weighted EIP model provides the best explanation of decision-makers' self reports
of the effort associated with each decision problem, and the effort estimates
appear to vary across individuals, but only slightly across rules.
Cross-validation of these results is also encouraging.
of

.53,

.54,

.58,

Average R 2 values

.60, and .78 are obtained for the behavioral, equal-weighted

EIP, weighted EIP, weighted EIP by rule, and weighted EIP by individual models,
7

respectively.
Estimates of EIP Effort.

Estimates of the subjective effort associated

with each EIP from the weighted EIP model pooled across subjects are given in
Table 7.

These estimates represent the increase in reported effort per EIP on

the sum of two 0-10 scales.
operator,

.32.

However, the high intercorrelation between ELIMINATIONS and

COMPARISONS (.85)
small (.04)

The largest estimate is for the ELIMINATION

must temper any interpretation of this coefficient and the

coefficient for COMPARISONS.

The PRODUCT operator, as might be

expected, is seen as fairly effortful, with a coefficient of

.19, while the

coefficients for READS and ADDITIONS are also significantly positive. 8
Discussion
The concept of effort plays a major role in attempts to understand the
contingent use of processing strategies.

An approach to measuring the effort

associated with different decision strategies is propused in this study, using a
set of elementary operators (i.e.,

READS, ADDITIONS, COMPARISONS, PRODUCTS,

.4'

<-----Page 35----->Choice Effort
DIFFERENCES, and ELIMINATIONS) as a common "language" for describing decision
strategies.

This set of operators is used to generate a metric of the effort

required to execute a decision strategy in terms of the number of EIPs involved.
The empirical results yielded strong support for this proposed componential
approach to strategy effort.

A model of effort based upon weighted EIP counts

(the weighted EIP model) provided good fits for response times and self-reports
of effort, two different measures of decision effort.

In addition to this

0

absolute level of fit, the weighted EIP model also was statistically superior to
a behavioral model using only reads and to an equal-weight EIP model

for each of

the two indicators of effort.

%

The estimates of time taken for each EIP were mostly plausible and in line
with prior research, hence providing additional confidence in the approach.

We

also examined the potential generalizability of our results to a broader range of
cognitive tasks.

Specifically, estimates of the times taken for various EIPs

S
A.-.

drawn from studies of other information processing tasks (see Johnson and Payne,

-

1985, p. 406 for the specific values of these estimates) were used as weights to
produce an analogue to the weighted EIP model for response times. 9

That is, the

values drawn from the literature for the time for each EIP were used as
coefficients of the EIP counts to produce a predicted response time.

This model

produced an R 2 value of .81, only slightly below that of the weighted EIP model.
The performance of this model provides encouraging evidence that the componential
approach may generalize to a variety of cognitive tasks.

In addition, the

'.A

closeness of the time estimates for individual EIPs obtained from our study of
decision making to those derived from other cognitive tasks, noted above,
provides support for the generalizability of our experimental procedure.
In general, the weights for various EIPs nbtained in our study were
essentially the same regardless of the decision strategy used.

Hence, the

-A

<-----Page 36----->-V.51'

Choice Effort

35
original assumptions of serial processing and independence of EIP duration across
rules and problem sizes made by Johnson and Payne (1985) receive encouraging
support in this research.

Taken together, these results imply that a small

number of simple operators can be viewed as the futdamental components from which
decision rules are constructed (Bettman, 1979; Bettman and Park, 1980).
However, the results do suggest significant individual differences in the
effort associated with individual EIPs.

For example, for some individuals

computational operators were relatively more difficult than comparisons.
others, this difference was not present.

For

This suggests the possibility that

individuals Llay choose different rules in part because different component EIPs
may be relatively more or less difficult or effortful across individuals.

In

addition, although the evidence for a model of effort based upon EIP counts is
impressive, the findings presented thus far are all limited to a situation in
which individuals were required to use various strategies which had been
prescribed for them.

Suppose, however, that parameters characterizing individual

subjects' performance on this constrained choice task (e.g., average EIP times)
could be related to those subjects' behaviors in a choice task where subjects
*were

free to use whatever strategy they wished.

This would provide suggestive

evidence for the proposed componential approach.
As an exploratory analysis, we related the average times spent by subjects
on arithmetic operations (ADDITIONS and PRODUCTS) in the constrained choice tasks
to the processing patterns used by those subjects in the unconstrained choices
they made in the final experimental session.

In particular, processing patterns

of three subjects for whom arithmetic operators were relatively more expensive
(took more time) were compared to those of the four subjects for whom such
operators took less time.

The subjects for whom the arithmetic operators were

relatively more expensive showed significantly greater variability in their

|

%

<-----Page 37----->fl

*--

at.6

-

..

a '.,

*

Choice Effort
36
information acquisition across attributes and alternatives on the twelve
unconstrained choices.

A

This result is conpistent with use of more heuristic,

non-compensatory processes rather than use of computationally expensive
compensatory strategies such as weighted adding (Payne, 1976).

Showing that

performance on the constrained task can be related to processing in unconstrained
choice situations offers suggestive support for the EIP approach, although the
small sample size precludes strong conclusions.
Another contribution of the study is more methodological.

The MOUSELAB

decision-monitoring software and hardware worked exceptionally well in providing
detailed data about the decision task.

The ability to monitor the sequence of

acquisitions, measure latencies, and in general maintain experimental control.•
over the choice task makes this system potentially very valuable for a variety of
research issues in decision making and other areas of cognition.
The attainment of experimental control, necessary to predict the operators
used and implement the proposed EIP models of effort, is not without costs.

0

In

the constrained decision task, subjects do not select strategies; rather, they
apply given rules.

Hence, the task eliminates many difficult problems normally

faced by individuals making decisions.

Subjects did not have to select or

construct a strategy, and the sequence of operations was specified.

%

Thus, they

did not have to engage in possibly effortful control processes determining what
to do next.

0

0

In addition, by providing all of the weights, cutoffs, and ratings,

the need for potentially difficult valuation processes was eliminated.

Finally,

some of the timing estimates are undoubtedly affected by the specific apparatus
used (i.e.,

the matrix display and the mouse).

These restrictions may be less

worrisome, however, given the analyses of the "free choice" task.

Since the

timing estimates derived from the constrained choices predict aspects of the
processing patterns in the free choices, our confidence in the procedures used is
IO

a,*

%a'

<-----Page 38----->%
Choice Effort
37
increased.

However, further research relaxing these restrictions on processing

%

flexibility would be desirable.
A second set of caveats is that although an approach which breaks down
decision strategies into more detailed components seems to be strongly supported

r/d

as an approach to measuring decision effort, we have focused on a particular
level of detail in taking such an approach.

For example, one could model

multiplications in terms of underlying arithmetic operations (e.g.,
1969) or anchoring and adjustment (Lopes, 1982).

Dansereau,

In addition, one could extend

S

our models to include EIPs that model the transfer of information to long-term
memory and various mental "bookkeeping" operations.

W

Modeling cognitive effort at the level of EIPs allows us to examine how the

0

effort associated with various strategies might vary as a function of differences
in task environments.

In particular, such variation in effort across task

environments could be predicted by computer simulation of the performance of
different heuristics in such enviror'ren!s.

As an example of this approach,

Payne, Bettman, and Johnson (In press) used both computer simulation and process
tracing experiments to examine the joint effects of effort and accuracy in the
adaptive use of decision processes.

V
0

N
S

A Monte-Carlo simulation study utilized the

proposed measure of effort based on EIP counts, along with various measures of
accuracy, to identify heuristic choice strategies that approximated the accuracy
of normative procedures

tnd required substantially less effort.

heuristic, however, did well across all task environments.

No single

Thus, a decision

I

maker striving to maintain a high level of accuracy with a minimum of effort
would have to use a variety of heuristics.
Payne, Bettman, and Johnson (In press) then tested the degree of

e

correspondence between the efficient processing strategies for a given decision
problem identified by the simulations and the actual information processing

0
0*

-%

2.

6:7-

1

<-----Page 39----->Choice Effort
38
behavior exhibited by people.

People were shown to be highly adaptive in their

responses to changes in the nature of the alternatives available to them, and to
the presence or absence of time pressure.

The results for actual decision

behavior tended to validate the patterns expected on the basis of the simulation
estimates.

Of particular interest was the finding that people were sensitive to

changes in decision context that impact the relative accuracy of heuristics as
well as affecting relative effort.
Taken together, the present results, plus those reported in Payne, Bettman,
and Johnson (In press), support the hypothesis that decision-makers choose
strategies as a function of a strategy's demand for mental resources, i.e.,

the

effort required to use a strategy, and the strategy's ability to produce an
accurate response.

However, it is important to recognize that a cost/benefit

viewpoint of strategy selection does not rule out the possibility of other
factors impacting strategy usage.

For example, there is growing evidence that

justifiability may influence the choice of processing strategy (e.g.,
Kim, 1987).

Tetlock &

In addition, more perceptual factors such as the decision frame

(Tversky & Kahneman, 1981) may also influence strategy use.

Nonetheless, the

present study, along with others, shows how measures of decision effort can be
predictive of strategy use.
Finally, the approach to measuring cognitive effort developed in this paper
may also have applied value.

For example, recently it has been suggested that

the use of nutritional information in the supermarket by consumers might be
improved by decreasing the effort costs associated with processing that
information (Russo, Staelin, Nolan, Russell, and Metcalf, 1986).

The methodology

developed in this paper could be used to test the impact of different information
displays on the use of a preferred decision strategy.

A related area of

<-----Page 40----->Choice Effort

39
application would be the design of computer-based decision aids (Keen & ScottMorton, 1978; Kleinmuntz & Schkade, 1988).

<-----Page 41----->Choice Effort
40

References
Abelson, R.P.,
G.

& Levi, A.

(1985). Decision Making and Decision Theory. In
I0

Lindzey and E. Aronson (Eds.), The Handbook of Social Psychology. Vol.l.

New York:
Beach, L. R.,

Random House.

& Mitchell, T. R.

decision strategies.
Bettman, J. R.

(1978). A contingency model for the selection of

Academy of Management Review, 3, 439-449.

(1979). An information processing theory of consumer choice.

Reading, MA:
Bettman, J. R.,

Addison-Wesley.

& Park, C. W.

(1980).

Effects of prior knowledge, experience,

and phase of the choice process on consumer decision processes:
analysis.
Bettman, J.

R.,

Journal of Consumer Research, 7, 234-248.
& Zins, M.

decision making.
Card, S. K.,

A protocol

Carpenter, P. A.,

Information format and choice

task effects in

Journal of Consumer Research, 6, 141-153.

Moran, T. P.,

interaction.

(1979).

& Newell, A.

Hillsdale, NJ:
& Just, M. A.

(1983). The psychology of human-computer

Lawrence Erlbaum.

(1975).

Sentence comprehension:

psycholinguistic processing model of verification.

A

Psychological Review,

82, 45-73.
Chase, W. G.

(1978). Elementary information processes.

In W.

Handbook of Learning and Cognitive Processes, Volume 5.

K. Estes

(Ed.),

Hillsdale, NJ:

Lawrence Erlbaum.
Dansereau, D. F.

(1969). An information processing model of mental

multiplication.
Ghiselli, E.,

Campbell, J.P.,

behavioral
Gopher, D.,

Unpublished dissertation, Carnegie-Mellon University.

sciences.

& Donchin, E.

& Zedeck, S. (1981).

San Francisco:
(In press).

Measurement theory for the

W.H. Freeman.

Workload:

An examination of the concept.

In K. Boff & L. Kaufman (Eds.), Handbook of Perception and Human
Performance.

•

New York:

John Wiley.

•

,

-

-

. o

. . , ..

,

,.

', '

'

"

-

-

-

.

<-----Page 42----->FO ME

-

.-

-

-d

-4

-

Choice Effort

41
Performance.
Groen, G. J.,

New York:

& Parkman, J. M.

addition.
Hammond, K. R.

John Wiley.
(1972). A chronometric analysis of simple

Psychological Review, L9, 329-343.
(1986). A theoretically based review of theory and research in

judgment and decision making.

Report 260, Center for Research on Judgment

-

and Policy, Institute of Cognitive Science, University of Colorado.
Hogarth, R.M. (1987). Judgement and Choice, Second Edition.

New York:
•

John Wiley.
Huber, 0.

(1980). The influence of some task variables on cognitive operations

in an information-processing decision model.

Acta Psychologica, 45, 187-

196.
Hunt, E. B.

(1978). Mechanics of verbal ability.

Psychological Review, 85, 109-

130.
Johnson, E. J.

(1979). Deciding how to decide:

The effort of making a decision.

,

Unpublished manuscript, University of Chicago.
Johnson, E. J.,

& Payne, J.W.

(1985). Effort and accuracy in choice.

Management

Science, 31, 394-414.
Johnson, E. J.,

Payne, J. W.,

Schkade, D. A.,

& Bettman, J. R.

Monitoring information processing and decisions:

(1986).

The mouselab system.

Unpublished manuscript, Fuqua School of Business, Duke University.
Kahneman, D.

(1973). Attention and effort.

Englewood Cliffs, NJ:

Prentice-

Hall.
Keen, P. G. W.,

& Scott-Morton, M. S.

organizational perspective.

(1978). Decision support systems:

Reading:

Mass.:

*

Addison-Wesley.

*.%*...%
"

An

P

<-----Page 43----->*

q

S

Choice Effort
42

*

(1985). Children's decision strategies and their adaptation to task

Klayman, J.

Organizational Behavior and Human Decision Processes 35,

characteristics.

179-201.
Kleinmuntz, D.,

& Schkade, D.A. (1988).

The Cognitive implications of

information displays in computer-supported decision making.
2010-88, Alfred P.

Sloan School of Management, Massachussetts

Working Paper
Institute of

Technology.
Kmenta, J.

(1986). Elements of econometrics, Second Edition.

New York:

Macmillan.
Lopes, L. L.

(1982). Toward a procedural theory of judgment.

Unpublished

manuscript, University of Wisconsin.
Maule, A. J.

(1985).

Cognitive approaches to decision making.

(Ed.), Behavioral Decision Making.
Navon, D.,

.

& Gopher, D.

In G. Wright

New York: Plenum.

(1979). On the economy of the human processing system.

Psychological Review, 86, 214-255.
Neter, J.,

& Wasserman, W.

Ill.:

(1974). applied linear statistical models.

Homewood,

Richard D. Irwin.

Newell, A., & Simon, H. A.

•

(1972). Human problem solving.

Englewood Cliffs, NJ:

Prentice-Hall.
Paquette, L.,

& Kida, T.

(1988).

The effect of decision strategy and task

complexity on decision performance.
Decision Processes, 41
Payne, J. W.
making:

S

Organizational Behavior and Human

128-142.

_"

(1976). Task complexity and contingent processing in human decision
An information search and protocol analysis.

0

Organizational

Behavior and Human Performance, 16, 366-387.
Payne, J. W.
382-402.

(1982). Contingent decision behavior.

Psychological Bulletin, 92,

1

<-----Page 44----->Choice Effort

43
Payne, J. W.,

Bettman, J. R.,

& Johnson, E. J.

selection in decision making.

(In press). Adaptive strategy

Journal of Experimental Psychology:

Learning, Memory. and Cognition.
Russo, J. E.

(1978). Eye fixations can save the world:

Critical evaluation and

4

comparison between eye fixations and other information processing
methodologies.

In H. K. Hunt (Ed.), Advances in Consumer Research, Volume

Ann Arbor, Michigan:
Russo, J. E.,

& Dosher, B. A.

choice.

%

Association for Consumer Research.
(1983). Strategies for multiattribute binary

Journal of Experimental Psychology:

Learning, Memory, and
-- I

Cognition, 9, 676-696.
Russo, J. E.,

Staelin, R.,

Nolan, C. A.,

Russell, G. J.,

(1986). Nutrition information in the supermarket.

& Metcalf, B. L.

6

Journal of Consumer

Research, 13, 48-70.
Shugan, S. M.

(1980). The cost of thinking.

Journal of Consumer Research, 7,

99-111.
Siegler, R.

(1986).

Strategy choice procedures and the development of

multiplication skill.
Sternberg, R.

Unpublished manuscript, Carnegie-Mellon University.

(1977). Component processes in analogical reasoning.

Psychological Review, 84, 353-378.
Tversky, A.,

& Kahneman, D. (1981). The Framing of Decisions and the Psychology

of Choice. Science, 211, 453-458.
Wickens, C. D.
Ohio:
Wright, P. L.

(1984). Engineering psychology and human performance.

Columbus,

Charles E. Merrill.
(1975). Consumer choice strategies:

Simplifying vs. optimizing.

Journal of Marketing Research, 11, 60-67.

pr i. '

<-----Page 45----->Choice Effort

44
Authors' Notes
The research reported in this paper was supported by a contract from the
Engineering Psychology Program of the Office of Naval Research.

authorship is arbitrary.
project.

The order of

Each author contributed equally to all phases of this

Requests for reprints should be sent to James R. Bettman, Center for

Decision Studies, Fuqua School of Business, Duke University, Durham, North
Carolina

27706.

.4'.

.4.

4-v.

"%
'4. '

<-----Page 46----->Choice Effort
45
Footnotes
'The behavioral model and the equal-weight EIP model are special cases of
(or nested within) the weighted EIP model.

Hence, the additional fit provided by

the weighted EIP model over each of these two simpler models can be tested
statistically (Neter and Wasserman, 1974, p. 89).

Similarly, the weighted EIP

-

model is nested within the weighted EIP by rule and weighted EIP by individual
models.
-

2Although not the subject of interest in this paper, for completeness a
model using as independent variables the number of alternatives, the number of
attributes, their product, and a dummy variable for each rule was estimated.

The

R2 values for response time and self-reported effort were .65 and .57,
respectively.
3

The degrees of freedom for the numerator in these comparisons represent

the difference between the use of six EIP variables for the weighted EIP model
and one variable for the behavioral and equal-weight EIP models.

0

The degrees of

freedom for the denominator reflect the total trials and the total number of
variables used for the weighted EIP model (Neter and Wasserman, 1974, p. 89).
4

The models were also estimated using the response time data disaggregated

to the level of individual acquisitions, the total time spent on each
alternative, and the total time spent on each attribute.

The relative fits of

•

the various models essentially replicate those of the aggregate results, although
the absolute levels of fit are of course lower for the more disaggregate
analyses.
5

S

There may be a distinction between anticipated effort and experienced

effort, with the former being the effort a strategy is predicted to require for
solving a problem and the latter reflecting the effort actually used.
current study, we focus on experienced effort.

In the

While strategy selection may be a

function of anticipated effort, we argue that a major basis for estimating

WML Mr

%

0

<-----Page 47----->.7 h

i 'A T)L XF

,

Choice Effort

46
function of anticipated effort, we argue

that a major basis

for estimating

anticipated effort is experienced effort on previous decision tasks.

Hence,

analysis of experienced effort can potentially lead to insights into the bases
for anticipated effort.

The relationship between anticipated and experienced

effort is an important topic for study, but it is beyond the scope of the current
investigation.
6

1f we assume that the two self-report measures of effort and difficulty

which we combined to form the self-reported effort index are both fallible.measures of effort, the intercorrelation between them (r-.85) serves as a
baseline to assess the reliability of this index.
unreliability would provide an estimated R

of

.83 for the weighted EIP model for

effort (Ghiselli, Campbell, and Zedeck, 1981, p.
7

Correcting for this

290.)

To further explore the robustness of the results, models for both response

times and self-reports of effort were run which added variables for the number of
alternatives, the number of attributes, and dummy variables for rules
weighted EIP by individual model.

While these additional variables produced

significant increases in fit (F(11,666) -3.30, p <
<

.0002 and F(11,666) - 4.04, p

.0001 for response time and effort respectively),

small (.005
8

to the

S

the increases in R 2 are very

and .012 for response time and effort, respectively).

Although errors are only indirectly related to decision effort, for the

0

sake of completeness logistic regressions were run using the behavioral, equal-

'p.

weighted EIP, and weighted EIP models with a 0-1 dependent variable representing
whether or not an error was made on a particular choice problem.
values from this analysis were

.62,

performed better than the behavioral model

9

2

.63, and .64 for the behavioral, equal-

weighted EIP, and weighted EIP models, respectively.

equal-weighted EIP model (p -

The pseudo-R

(p <

.

The weighted EIP model.

.05) but not better than the

.105).

The weighted EIP model for times was the only model. considered, since the

%

<-----Page 48----->Choice Eftokt

9

,

The weighted EIP model for times was the only model considered, since the

estimates from the literature only refer to times and do not account for
individual differences or differences across rules.

i
..

.-

a,
I--

.!

-o

.o

S

,

