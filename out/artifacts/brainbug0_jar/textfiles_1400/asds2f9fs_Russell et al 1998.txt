<-----Page 0----->Copyright 1998 by the American Psychological Association, Inc.
0022-0167/98/$3.00

Journal of Counseling Psychology
1998, Vol. 45, No.1, 18-29

Analyzing Data From Experimental Studies: A Latent Variable Structural
Equation Modeling Approach
Daniel W. Russell, Jeffrey H. Kahn,
and Richard Spoth

Elizabeth M. Altmaier
University of Iowa

Iowa State University
This article illustrates the use of structural equation modeling (SEM) procedures with latent
variables to analyze data from experimentalstudies. These proceduresallow the researcher to
remove the biasingeffects of randomand correlatedmeasurementerror on the outcomesof the
experiment and to examine processes that may account for changes in the outcome variables
that are observed.Analyses of data from a Project Family study,an experimentalintervention
project with rural families that strives to improve parenting skills, are presented to illustrate
the use of these modeling procedures. Issues that arise in applying SEM procedures, such as
sample size and distributionalcharacteristicsof the measures,arc discussed.

two groups of research participants, with 30 participants in
each group. The magnitude of the treatment effect corresponded to half a standard deviation unit (d = .50), an effect
that Cohen (1988) terms moderate in size. As can be seen,
statistically significant differences between the two groups
were detected when the dependent variable was measured
without error (i.e., the reliability equals 1.0). Decreases in
the reliability of the dependent variable are associated with
decreases in the magnitude of the t value. As a consequence,
the difference between the groups based on a t test became
nonsignificant as the reliability of the dependent variable
dropped below .80. Thus, methods for enhancing the reliability of the outcome measures in experimental studies would
clearly serve to enhance the statistical power of the investigations.
Second, as shown below, correlated measurement error
can also affect tests of experimental intervention effects. For
example, assume that in evaluating the effect ofthe intervention described previously, we conducted an ANCOVA in
which we controlled for individual differences in levels of
paternal hostility in testing the effect of the intervention on
subsequent levels of paternal hostility. Our two assessments
may overestimate the stability of the underlying construct
(paternal hostility) by using the same measurement procedures. That is, the correlation between the assessments
partly reflects individual differences in paternal hostility and
partly reflects the impact of using the same measurement
procedures at two points (see discussion by Cole, 1987). So,
for example, if a self-report measure of paternal hostility
toward the child is used, the two assessments will correlate
in part because of the common method of assessment. As a
result of correlated measurement error, the dependent variable may not be very sensitive to the effect of the intervention program on levels of paternal hostility.
Finally, researchers often focus on outcome variables
(e.g., child behavior following a parenting skills intervention) and fail to closely examine mediating processes that
may be responsible for the outcomes that are observed (see
Spoth, in press; Spoth, Redmond, Haggerty, & Ward, 1995;
Spoth, Redmond, & Shin, in press). For example, in the

Statistical techniques such as t test, analysis of variance
(ANOVA), and analysis of covariance (ANCOVA) have
been used to determine the effects of experimental interventions. These statistical procedures test for mean differences
between groups following the intervention. For example,
imagine that a parenting intervention program is delivered to
persons who have been randomly assigned to the experimental group, whereas those who were assigned to the control
group are placed on a waiting list for the intervention. The
impact of the program is then tested on outcomes of parental
behavior (e.g., maternal or paternal disciplinary techniques)
and child behavior (e.g., aggressive behavior at school) by
comparing the means of these two groups on the dependent
variables, using statistical techniques such as Student's t test
orANOVA.
This traditional approach to the analysis of data has, in our
view, three limitations that undermine what can be learned.
First, researchers often ignore the impact that lack of
reliability in the dependent variable can have on the
statistical power of their tests of treatment effects. As an
illustration of this point, Table I presents the results of a
simulation study in which comparisons were made between
Daniel W. Russell,Departmentsof Psychology and Statisticsand
Center for Family Research in Rural Mental Health, Iowa State
University;Jeffrey H. Kahn, Departmentof Psychology, Iowa State
University; Richard Spoth, Center for Family Research in Rural
Mental Health, Iowa State University; Elizabeth M. Altmaier,
College of Education and Center for Health Services Research,
Universityof Iowa.
This article is based on a presentation made at the Rural Mental
Health Preventive Services Workshop, National Institute of Mental
Health, Rockville, Maryland, September 1995. Preparation of this
article was supported by National Institute on Drug Abuse Grant
DA 070 29-01A1 and by National1nstituteon Mental Health Grant
MH 49217-01 AI.
Correspondence concerning this article should be addressed to
Daniel W. Russell, Center for Family Research in Rural Mental
Health, Iowa State University, 2625 North Loop Drive, Suite 500,
Ames, Iowa 50010-8296. Electronic mail may be sent via Internet
to drusseU@iastate.edu.
18

<-----Page 1-----><-----Page 2-----><-----Page 3-----><-----Page 4-----><-----Page 5-----><-----Page 6-----><-----Page 7-----><-----Page 8-----><-----Page 9-----><-----Page 10-----><-----Page 11----->