<-----Page 0----->ORGANIZATIONAL

BEHAVIOR

A Componential

AND

HUMAN

DECISION

PROCESSES

45, 111-139(I%@)

Analysis of Cognitive Effort in Choice
JAMES R. BETTMAN

Center for Decision

Studies, Fuqua School of Business, Duke University

ERIC J. JOHNSON
Wharton School, University

of Pennsylvania

AND
JOHN W. PAYNE
Center for Decision

Studies, Fuqua School of Business, Duke Universiv

We examine the effort required to execute decision strategies and propose a
set of elementary information processes (EIPs) (e.g., reads, additions, comparisons) as a common language for describing these strategies. Based upon
these component processes, a model for measuring the effort required to execute a decision strategy is proposed which suggests that effort is a weighted
sum of EIPs. We test several variants of this model by attempting to predict
decision latencies and subjective reports of effort. The proposed weighted EIP
model provides good predictions for response time and subjective effort, and
estimates of the time and effort associated with each EIP seem plausible and
consistent with those found in other cognitive tasks. The time and effort required by each EIP do not vary substantially across rules; however, there are
significant individual differences. On balance, the EIP approach to conceptualizing and measuring the effort of executing a choice strategy receives strong
support. 8 1990 Academic Press, Inc.

A major finding of the last decade of decision research is that an individual may use many different kinds of strategies in making a decision,
contingent upon task demands (Abelson & Levi, 1985; Payne, 1982). The
use of multiple strategies raises the fundamental issue of how people
decide what to do. An approach advocated by many researchers is to look
at various decision strategies as having differing advantages and disadvantages, and to hypothesize that an individual might select the strategy
The research reported in this paper was supported by a contract from the Perceptual
Science Program of the OflIce of Naval Research. The order of authorship is arbitrary.
Each author contributed equally to all phases of this project. Requests for reprints should be
sent to James R. Bettman, Center for Decision Studies, Fuqua School of Business, Duke
University, Durham, NC 27706.
111

0749-5978/90
$3.00
Copyright
0 1990 by Academic Press, Inc.
All rights of reproduction
in any form reserved.

<-----Page 1----->112

BETTMAN,

JOHNSON,

AND

PAYNE

that is, in some sense, best for the task. Several factors, such as the
chance of making an error, justifiability (Tversky, 1972), and the avoidance of conflict (Hogarth, 1987), can play an important role in strategy
selection. However, in the current paper we focus on another factor that
is generally assumed to exert a major influence on strategy use, the effort
(cognitive resources) required to perform a strategy (Beach & Mitchell,
1978; Johnson & Payne, 1985; Paquette & Kida, 1988; Russo & Dosher,
1983; Wright, 1975).
The notion that different decision strategies require different amounts
of computational effort to execute seems obvious. The strategy of expected utility maximization, for instance, requires a person to process all
relevant problem information and to trade off values and beliefs. The
lexicographic choice rule (Tversky, 1969), on the other hand, chooses the
alternative which is best on the most important attribute, ignoring much
of the potentially relevant problem information. Thus, there appear to be
clear differences among decision strategies in the amount of information
that is processed in making a choice.
At a more precise level of analysis, however, a comparison among
decision strategies in terms of cognitive effort is much more difficult. In
part this is because the decision strategies that have been proposed in the
literature have varied widely in terms of their formal expression. Some
have been proposed as formal mathematical models (e.g., eliminationby-aspects, Tversky, 1972), and others as verbal process descriptions
(e.g., the majority of confirming dimensions rule, Russo & Dosher, 1983).
What is needed is a language that could be used to express a diverse set
of decision strategies in terms of a common set of cognitive operations.
Such a language would provide a unifying framework for describing strategies and allow strategy selection to be investigated at an information
processing level rather than at a more general level of analysis, such as
analytic vs nonanalytic (Beach & Mitchell, 1978) or analytic vs intuitive
(Hammond, 1986). Such a language would also allow a more detailed
analysis of the components of processing (effort) involved when a particular decision strategy is used to solve a particular decision problem
(Maule, 1985). In other words, one could examine whether the amount of
information to be processed is the major determinant of effort, or whether
the specific mix of cognitive operations which is utilized affects effort.
In addition to the problem of conceptualizing effort, another difficulty
is actually measuring the effort associated with a given strategy. There
have been a number of measurement techniques proposed for the related
concept of mental workload (Gopher & Donchin, in press; Wickens,
1984), ranging from self-reports to response times to physiological measures. However, the different measures of workload, such as response
latencies, secondary tasks, or self-reports, do not always agree. Hence,

<-----Page 2----->CHOICE

EFFORT

113

Gopher and Donchin (in press) recommend the use of multiple measures,
along with a detailed theoretical analysis of the expected workload of a
task.
The purposes of this paper are (1) to conceptualize and develop a metric
for modeling decision effort; (2) to characterize the effort put forth by
subjects using different decision strategies in different choice environments; and (3) to assess the degree to which the proposed model is able
to fit the subjects’ effort. To accomplish these goals, we first develop a
metric of decision effort based on the concept of elementary information
processes (Chase, 1978; Newell & Simon, 1972). We then examine the
effort required by subjects to use different decision strategies in choice
environments varying in complexity, using two indicators of strategy execution effort: decision latencies and self-reports of task difficulty. We
then use the proposed elementary information processes (EIP) approach
to modeling decision effort to predict these two indicators of strategy
execution effort. Our overall goal is not to propose a complete theory of
mental workload, but rather to illustrate an approach to measuring the
execution effort of choice strategies.
In the following section, previous attempts to conceptualize and measure decision effort are briefly addressed, and the proposed approach is
outlined. Then the methodology and results of a study designed to test
this approach are described in detail.
MEASURING DECISION EFFORT
The theoretical construct of mental effort has a long and venerable
history in psychology (Kahneman, 1973; Navon & Gopher, 1979; Thomas, 1983). However, there have been only a few attempts to model and
compare decision rules in terms of an effort metric.
Two studies that attempted to directly measure the execution effort of
various decision rules are Wright (1975) and Bettman and Zins (1979). In
each study, subjects were instructed to use particular decision rules to
solve certain problems. The percent of correct judgments using the rules
and self-reports of task difficulty or ease of use were obtained in both
studies. In addition, Bettman and Zins (1979) obtained a measure of the
time taken to apply a rule to a problem. The results clearly show that the
rules were perceived to differ in the effort required. For example, a lexicographic rule was generally perceived as less effortful than other decision rules. That rule also tended to be the most accurate and quickest in
its execution. However, these two studies had significant limitations.
First, neither study employed a method beyond initial instruction to ensure that subjects actually used the prescribed decision rules. Second,
neither study provided a conceptual basis (model) for why a certain de-

<-----Page 3----->114

BETTMAN,

JOHNSON, AND PAYNE

cision rule would be expected to be more or less effortful in a particular
task. That is, neither study attempted to model the components of decision-making effort.
Shugan (1980) suggested an effort metric based upon one operation, the
binary comparison of two alternatives on an attribute. More effortful
decisions involved more comparisons. Shugan also showed that the effort
of strategies would vary with certain task characteristics like the correlational structure among attributes. Unfortunately, using the binary comparison as the fundamental unit of effort restricts Shugan’s analysis to
certain decision rules. Nonetheless, Shugan’s work implies that any approach to modeling strategy effort must be sensitive to the joint effects of
strategy and task.
Based upon the work of Newell and Simon (1972), Huber (1980) and
Johnson (1979) offered decompositions of choice strategies using more
extensive sets of components. Each study independently suggested that
decision strategies be described by a set of EIPs. A decision rule or
strategy was represented as a sequence of mental events, such as reading
a piece of information into STM (short-term memory), multiplying a probability and a payoff, or comparing the values of two alternatives on an
attribute. Johnson and Payne (1985) and Payne, Bettman, and Johnson
(1988) employed a similar set of EIPs for decision making, shown in Table
1, and constructed production system implementations of several different choice strategies.
A particular set of EIPs, such as the one given above, represents a
theoretical judgment regarding the appropriate level of decomposition for
decision processes. For instance, the product operator might itself be
decomposed into more elementary processes. We hypothesize, however,
that the present level of decomposition provides the basis for meaningful
comparisons among decision strategies in terms of effort. Furthermore,
we propose that a general measure of decision effort is the number of
component EIPs required to execute a particular strategy in a particular
TABLE 1
ELEMENTARYEIPs USED IN DECISIONSTRATEGIES
READ
COMPARE
DIFFERENCE
ADD
PRODUCT
ELIMINATE
MOVE
CHOOSE

Read an alternative’s value on an attribute into STM
Compare two alternatives on an attribute
Calculate the size of the difference of two alternatives for
an attribute
Add the values of an attribute in STM
Weight one value by another (Multiply)
Remove an alternative or attribute from consideration
Go to next element of external environment
Announce preferred alternative and stop process

<-----Page 4----->CHOICE

EFFORT

115

task environment. This notion of measuring decision effort in terms of the
number of EIPs builds on an idea for measuring processing effort proposed by Newell and Simon (1972). Empirical support for this approach in
areas other than decision making has been provided by showing a relationship between the predicted number of EIPs used and response times
for a variety of cognitive tasks (Card, Moran, & Newell, 1983; Carpenter
& Just, 1975).
To validate this componential (EIP) approach to measuring choice effort, we examine four models of decision effort based upon EIPs. The
simplest model of decision effort using EIPs would be to treat each component process as equally effortful and simply sum the numbers of each
component process to get an overall measure of effort (the equalweighted EZP model). A second and slightly more complex model would
allow the effort required by each individual component to vary (the
weighted EZP model). Total effort would then be a weighted sum of the
individual operations. A third model would allow the effortfulness of the
individual EIPs to vary across rules (the weighted EZP by rule model).
While such variation is of course a possibility, one would hope that the
effortfulness of an EIP would not vary as a function of the particular
strategy of which it is a component. The goal of developing a unifying
framework for describing different decision strategies would be more difficult to attain if the sequence of operations or the rule used affected the
effort required for an EIP. Finally, the fourth model would allow the
required effort for each EIP to vary across individuals (the weighted EZP
by individual model). One might expect that some individuals might find
certain EIPs (e.g., the PRODUCT operator) relatively more difficult than
other individuals, for example.
An alternative model of effort which is not based on the componential
approach is also considered. This model is based on the number of items
of information processed by a particular strategy in a choice environment.
Since it is easy to monitor information acquisition behavior, this might be
called an explicit behavioral model of strategy effort. A model based
solely on information acquisition implies that the specific type of processing done on the information acquired makes little or no difference in
determing decision effort. Such a model represents a base-line model of
effort in that the details of processing are ignored.
These models of strategy effort are investigated with data from subjects
using different rules across choice tasks varying in complexity (number of
alternatives and attributes). The performance of the subjects on the various rules in the different tasks is characterized by two indicators of
execution effort: the time to make a response and self-reports of effort.
We then use the models proposed above to predict these two indicators.
The details of the methodology used are presented next.

<-----Page 5----->116

BETTMAN,

JOHNSON,

AND

PAYNE

METHOD

Overview
Subjects were trained to use six different strategies for making decisions. Each strategy was used in a separate session to make 20 decisions
for decision problems ranging in size from two to six alternatives and from
two to four attributes. Subjects used a computer-based information acquisition system to acquire information and make decisions among sets of
alternatives (Johnson, Payne, Schkade, & Bettman, 1989). The computerbased acquisition system monitored the subjects’ information sequences;
recorded latencies for each acquisition; recorded the overall time for each
problem; and recorded any errors made by the subject (i.e., departures
from the prescribed search pattern or choice). In addition, subjects rated
the difficulty of each choice and the effort each required on two response
scales presented at the end of each decision problem. Subjects also provided data in a seventh session for 12 choice problems of various sizes
where the subject was free to use any strategy desired. Some suggestive
findings from these data are considered in the discussion section,
We describe the details of the methodology for the prescribed strategy
sessions as follows: First, the six decision strategies used are described,
followed by a description and examples of how the EIP counts were
generated. Then the generation of the sets of 20 decision problems is
discussed, followed by details on the computer-based acquisition system.
The experimental procedure is then discussed in detail and some preliminary analyses are reported. Finally, the details of the proposed models
and an overview of the major analyses performed are presented.
Decision Strategies
Rules used. Six different decision strategies were used in the prescribed
strategy portion of the experiment: weighted additive; equal weighted
additive; lexicographic; elimination by aspects; satisficing (conjunctive);
and majority of confirming dimensions. Each of these rules was implemented as a production system model (for examples see Johnson &
Payne, 1985; Payne et al., 1988). These particular rules were selected
for two reasons: (1) each rule has been a focus of previous research on
choice processes; and (2) this set of rules provides a broad coverage of the
set of basic elementary operations (EIPs) used as the components in our
conceptualization of strategy execution effort. We first describe the strategies, and then the elementary operations are considered.
A typical choice problem in our study consists of a set of alternative job
candidates, each of whom is described by scores, or ratings, on various
selection attributes or criteria (e.g., leadership potential and motivation).
For each attribute, an importance weight and a cutoff value specifying a

<-----Page 6----->CHOICE

EFFORT

117

minimally acceptable level for that attribute were also displayed. Different decision strategies might use both weights and cutoffs, one of the two,
or neither, as described below.
The weighted additive (WADD) rule requires the subject to develop an
evaluation for each alternative by multiplying each weight times the attribute rating and adding those products for all attributes. The alternative
with the highest evaluation is selected. In the equal weighted additive
(EQW) model, the evaluation for each alternative is obtained by adding
the ratings for all the attributes, with the alternative with the highest
evaluation selected. No weights or cutoffs are used.
The lexicographic (LEX) rule requires the subject to first find the most
important attribute (the attribute with the largest weight) and then search
the values on that attribute for the alternative with the highest value. That
alternative is selected, unless there are ties. In this case, those tied alternatives are examined on the second most important attribute. That process continues until a winner is found.
The elimination by aspects (EBA) strategy also begins by determining
the most important attribute and examining that attribute’s cutoff value.
Next, all alternatives with ratings below the cutoff for that attribute are
eliminated. This process continues with the second most important attribute, and so on, until one alternative remains. The sutisficing (conjunctive) (SAT) rule requires the subject to consider one alternative at a time,
comparing each attribute to the cutoff value. If any attribute is below the
cutoff value, that alternative is rejected. The first alternative with values
that pass the cutoffs for all attributes is chosen.
Finally, the majority of confirming dimensions rule (MCD) processes
pairs of alternatives. The values of the two alternatives are compared for
each attribute, and a running score is kept: if the first alternative has a
greater value on an attribute than the second, one is added to the score;
if the second alternative is greater, one is subtracted; if the two alternatives are tied, the score is not changed. After all attributes have been
examined, if the score is positive, the first alternative is retained; if the
score is negative, the second alternative is retained; and if the score is
zero, the alternative winning the comparison on the last attribute is retained. Thus, the general idea is to retain the alternative which is better on
the most criteria. The alternative which is retained is then compared to
the next alternative remaining among the set of alternatives. If no other
alternative remains, the retained alternative is selected.
Calculating EIP Counts
To describe the steps a subject followed in more detail and to show how
EIP counts were determined, we first consider the particular EIPs used
and then present two more detailed examples of rules applied to a par-

<-----Page 7----->118

BETTMAN,

JOHNSON,

AND

PAYNE

titular decision problem. The major EIPs utilized were MOVES,
READS, ADDITIONS, PRODUCTS, COMPARES, ELIMINATIONS,
and DIFFERENCES. A MOVE involves moving to another piece of
information, while a READ consists of acquiring that information (moving it to short-term memory). Since MOVES and READS are perfectly
correlated in our experiment, we will only consider READS (acquisitions)
in this study. ADDITIONS, PRODUCTS (of weights and ratings), and
DIFFERENCES are self-evident. COMPARES involve comparing two
pieces of information and determining the larger of the two (two ratings,
two overall alternative scores, two weights, a rating and cutoff, etc.).
Finally, ELIMINATIONS
could be either discarding an attribute (because it had already been used) or an alternative (because its score was
surpassed, it failed a cutoff, etc.).
Examples. Two examples will be considered in more detail, a weighted
adding case and an EBA example. Before doing this, however, some
general comments are in order. First, the number of EIPs required for a
particular decision is a function of the specific rule used, the size of the
problem (the number of alternatives and attributes), and the specific values of the data. Rules which examine all of the ratings for each alternative, such as the weighted adding rule, need more EIPs than rules which
may process only part of the data, such as the EBA rule. Larger problems
also tend to require more EIPs. Problems with more values which surpass
cutoffs will also generally require more EIPs. Second, in the specification
of the rules, an attempt was made to take advantage of the left to right, top
to bottom natural reading order.
For the weighted adding rule, consider the four alternative, threeattribute decision problem shown in Table 2a. The numbers in parentheses are labels that will be used for convenience for identifying the sequence of acquisitions in the following. Subjects were instructed to acquire the first weight (1) and then the rating on the first attribute (4). They
then multiplied these two numbers and retained the score. This process
was repeated (sequence (2), (5), (3), and (6)) until alternative A was finTABLE 2a
EXAMPLE OF A FOUR-ALTERNATIVE, THREE-ATTRIBUTE DECISION PROBLEM
Attributes
Leadership

Alternatives
Weights
A
B
C
D

6 (1)
4 (4)
2 (7)
6 (10)
5 (13)

Creativity
4 (2)
7 (5)
7 (8)
6 (11)
7 (14)

Experience
2
4
2
3

(3)
(6)
(9)
(12)
2 (15)

<-----Page 8----->CHOICE

119

EFFORT

ished. For the first alternative, the total score of 60 was simply retained
as the current best. After processing the first alternative, there would be
six READS, three PRODUCTS, two ADDS, and no COMPARISONS,
DIFFERENCES, or ELIMINATIONS. For alternative B, the sequence
would be (l), (7), (2), (8), (3), and (9). Then the total score for B, 44, would
be compared to the current best, and the current best of 60 would be
retained. The assumption was made that in the comparison of total
scores, the losing alternative was not explicitly eliminated. Rather, the
subject would merely store the one that was retained. Thus, after two
alternatives we would have twelve READS, six PRODUCTS, four
ADDS, one COMPARISON, no DIFFERENCES, and no ELIMINATIONS. This process would be repeated for the remaining two altematives Wwnce Cl), WI), (2), (ll), (3), (12), (11, (13), (2), (14), (3), and
(15)). Hence, the production system model predicts that in total this problem would require 24 READS, 8 ADDITIONS, 12 PRODUCTS, 3 COMPARISONS, no DIFFERENCES, and no ELIMINATIONS.
The example of a three-alternative, four-attribute problem shown in
Table 2b is used to clarify the EBA rule specification. The subject had to
first find the most important attribute. This was done by starting with the
first weight and comparing it to the second, and retaining the larger (the
second). The second was then compared to the third, and the second was
retained. Then the second was compared to the fourth, and the fourth
(experience) was retained as the most important attribute. The sequence
of acquisitions would thus be (I), (2), (3), and (4). There would be four
READS and three COMPARISONS. Then the subject acquired the cutoff
for experience and examined the value for all alternatives on experience,
comparing each value to the cutoff and eliminating any alternative not
passing the cutoff. In this case, the sequence would be (8), (12), (16), and
(20), with alternative C eliminated. The total EIPs thus far would be a
eight READS, six COMPARISONS, and one ELIMINATION.
Then the
experience attribute would be eliminated, and the weights for the remaining three criteria would be acquired and compared, resulting in motivaTABLE 2b
EXAMPLE OF A THREE-ALTERNATIVE, FOUR-ATTRIBUTE DECISION PROBLEM
Attributes
Alternatives

Leadership
Weights
cutoffs

A
B
C

4
7
6
7
4

(1)
(5)
(9)
(13)
07)

Motivation
5
4
5
4
3

(2)
(6)
(10)
(14)
(18)

Creativity
3
6
7
3
4

(3)
(7)
(11)
(1%
(19)

Experience
6 (4)

6 (t-4)
7 (12)

6 (16)
4 (20)

<-----Page 9----->120

BETTMAN,

JOHNSON,

AND

PAYNE

tion’s being selected as the second most important attribute (sequence (l),
(2), (3)). Then the cutoff for motivation was acquired and the values for
the retained alternatives, A and B, were compared to the cutoff (sequence
(6), (lo), (14)). At this point, there would be a total of 14 READS, 10
COMPARISONS, and two ELIMINATIONS.
Both A and B passed the
cutoff, so the subject would then eliminate the motivation attribute and
return to the weights to determine the third most important remaining
attribute, leadership (sequence (l), (3)). Then the cutoff for leadership
was examined, A and B were compared to the cutoff, and A was eliminated. B would then be chosen (sequence (5), (9), (13)). In total, there
would be 19 READS, 13 COMPARISONS, and four ELIMINATIONS
(two attributes and two alternatives).
These examples illustrate two principles: the number of EIPs varies
with problem size and with the particular values used, and different rules
use different subsets of the EIPs. With regard to the second point, the
weighted adding rule uses READS, ADDITIONS, PRODUCTS, and
COMPARISONS; the equal weighted adding rule uses READS, ADDITIONS, and COMPARISONS; the lexicographic rule uses READS,
COMPARISONS, and ELIMINATIONS;
the EBA rule uses READS,
COMPARISONS, and ELIMINATIONS:
the satisficing rule uses
READS, COMPARISONS, and ELIMINATIONS;
and the MCD rule
uses READS, ADDITIONS, COMPARISONS, ELIMINATIONS,
and
DIFFERENCES.
It should also be noted that certain rules (weighted adding, equal
weighted adding) have the same EIP counts for any problems of the same
size (i.e., with the same number of alternatives and attributes). On the
other hand, the other rules (lexicographic, EBA, satisticing, and MCD)
can have different EIP counts even for problems of the same size, depending upon the particular values of the data. This property of the rules
affected the selection of decision problems for the experiment, as discussed next.
Selection of the Decision Problems

As noted above, subjects completed 20 decision problems for each of
the six decision rules. These decision problems were generated by taking
several factors into account. First, pilot studies revealed that problems
with more than four attributes were extremely difficult for subjects, particularly for the weighted adding rule. Second, problems with more than
six alternatives caused crowding on the computer display used in the
information acquisition system. Hence, the decision problems varied
from two to six alternatives and two to four attributes. This generated 15
possible sizes, ranging from two alternatives and two attributes to six
alternatives and four attributes.

<-----Page 10----->CHOICE

EFFORT

121

For the weighted adding and equal-weighted adding rules, since problem size determines the EIP count, one problem of each size was included, for a total of 15 decision problems. Then five problem sizes were
randomly selected to complete the 20 decision problems. Values for the
weights and ratings were assigned randomly, with the restriction that no
overall scores for alternatives in the same problem set were tied.
For the remaining rules, several problems were generated for each
problem size that represented low, intermediate, and high EIP counts for
that size (e.g., for a three-alternative, four-attribute EBA problem, elimination of two alternatives on the first attribute would lead to a low count,
retention of all three alternatives until the last attribute would be a high
count, and the operations used for the example described above might be
an intermediate count). Then sets of 20 problems were randomly selected
for each rule from the total set of 45 size/count combinations. Note that
this selection procedure implies that for each rule other than weighted
adding and equal-weighted adding there may be some problem sizes
which were not selected.
The random selection procedure just described was repeated many
times in an attempt to deal with correlation problems among the EIP
counts. Since the EIP counts were to be used as independent variables in
regression models to predict decision times and self reports of effort, it
was desirable that their intercorrelations across all 120 decision problems
should be as low as possible to avoid multicollinearity problems (Kmenta,
1986). As noted above, however, certain rules use only some EIPs and
not others, so there are some correlations that will be high because of the
definition of the rules. For example, the correlation between COMPARISONS and ELIMINATIONS will tend to be high because rules with no
ELIMINATIONS (e.g., the adding rules) tend to do very few COMPARISONS, whereas rules with many COMPARISONS also have more
ELIMINATIONS.
To minimize these intercorrelation problems, we repeated the random selection procedure 1000 times and selected the set of
120 decision problems with the smallest intercorrelations. The resulting
intercorrelations are shown in Table 3. Despite these efforts, we were
unable to further reduce the highest, COMPARES and ELIMINATIONS, for the reasons outlined above.
The Computer-Based Information Acquisition System
A computer-based information acquisition system called MOUSELAB
was utilized in carrying out the experiment (Johnson et al., 1989). The
subject saw a matrix display on the computer monitor for each decision
problem. The rows of the matrix were labeled weights, cutoffs, and then
the names of the alternatives to be considered. The columns were labeled
with the names of the attributes. At the bottom of the monitor screen were

<-----Page 11----->122

BETTMAN,

JOHNSON,

AND

PAYNE

TABLE 3
INTERCORRELATIONS AMONG EIP COUNTS FOR THE 120 DECISION PROBLEMS SELECTED
Operators
ADDITIONS
READS
ADDITIONS
PRODUCTS
COMPARES
ELIMINATIONS

,487

PRODUCTS

COMPARES

.543
.591

,541
- .259
- .302

ELIMINATIONS

DIFFERENCES

.280
- .495
- .314
.852

.272
.140
-.146
.492
.158

boxes used to indicate choice of an alternative (hence termed choice
boxes). For an example of this display, see Fig. 1.
Initially, the matrix display provides only the labels for the rows and
columns and the choice boxes. The information is hidden in the blank
cells on the screen. To acquire information, the subject must move a
cursor controlled by the mouse to the desired cell of the matrix. The cell
then opens, displaying the information. For each decision, the subject
uses the mouse to acquire the appropriate information in the sequence
specified by the current strategy. MOUSELAB recorded the sequence in
which cells were opened and the time spent in each cell. The time measurements use the system clock of the personal computer, providing a
LEADER

CREATE

WEIGHTS

CUTOFFS

CAND A

CAND B

CAND C

CAND D

FIGURE 1.

EXPER

<-----Page 12----->CHOICE

EFFORT

123

resolution of approximately 17 ms. After the requisite information had
been examined, the subject moved to the appropriate choice box and
clicked a button on the mouse to designate the chosen alternative.
A crucial feature of MOUSELAB for the present study is the ability to
monitor the sequence of acquisitions made by a subject. Since the EIP
models of effort we propose require EIP counts for each problem, it is
crucial that subjects use the strategy exactly as it is specified, so that the
EIP counts can be predicted accurately. For example, to ensure that the
EIP counts for the weighted adding and EBA examples given above are
correct, we must monitor that subjects follow the exact acquisition sequence for each rule. MOUSELAB includes a move monitoring feature,
which allows the correct sequence of cells to be specified for each decision problem. If the subject enters a “wrong” cell, the cell will not open,
and after 2 s the computer will emit an audible buzz. The attempt to enter
an incorrect cell is also recorded in the output information about the
subject’s move sequence. Hence, trials where a specified number of incorrect moves has occurred can later be discarded or analyzed as error
trials if desired.
An analysis of a typical decision task for this study using Fitts Law
(Card et al., 1983) indicates that subjects could move between information cells in less than 100 ms. This suggests that the time to move the
mouse is limited mainly by the time it takes to think where to point, not
the movement of the mouse itself.
Procedure
Overview. Subjects participated in eight separate sessions over a period
of several days. Each session lasted from one to one and a half hours. No
more than two sessions were run in one day, and separate sessions were
at least four hours apart. The first session taught subjects the decision
rules and familiarized them with MOUSELAB. In each of the subsequent
six estimation sessions, a subject made 20 choices using a different specified rule. The order of the rules was randomized across subjects. The
final session had 12 choice problems where the subject was free to use any
strategy desired. These problems all had four attributes and a third of the
problems had two, four, and six alternatives, respectively.
Subjects. Subjects were seven adults, ranging in age from 21 to 34, and
included four males and three females. They varied in their prior awareness of the decision making literature, ranging from graduate students
who had studied decision making to nonstudents who had never been
exposed to those concepts.
Training. It was crucial that subjects thoroughly learn the six decision
strategies to be used (weighted adding, equal-weighted adding, lexicographic, elimination by aspects, satisficing, and majority of confirming

<-----Page 13----->124

BETTMAN,

JOHNSON,

AND

PAYNE

dimensions) and learn to use MOUSELAB. Hence, a familiarization session was developed. Subjects were first introduced to the mouse and were
shown how to use it to open the cells, to respond to various response
scales, and to indicate a choice. After practicing these tasks, subjects
were next given a training session for the decision rules which was developed using the MOUSELAB system.
The subject was informed that the decisions made were personnel decisions involving selection of job candidates. These selections were made
according to the rules specified by different divisions of their company,
and the sets of candidates might have both differing numbers of candidates (from two to six) and different amounts of information on each
candidate (from two to four attributes). The four possible attributes were
leadership potential, creativity, job experience, and motivation. The left
to right ordering of the subset of these attributes used on any given trial
was randomized.
Next, subjects were introduced to the ratings used to describe each
candidate on each attribute. Ratings ranging from 2 (poor) to 7 (excellent)
were used as the information in each cell. Subjects were then introduced
to the ideas of importance weights for the attributes and cutoffs for the
attributes. They were then asked to select the most important attribute
and to pick candidates surpassing a cutoff to provide training using these
ideas. These concepts were then reviewed before the decision rules were
introduced.
For each rule, the subject was first given a thorough written description
of the rule on the computer monitor. Then the subject was given several
decision problems and was told to apply the rule using the mouse. The
move monitoring system was used on the last trial to inform subjects of
mistakes. The subject was also told what the correct choice using the rule
should have been. Thus, subjects had accuracy feedback on both the
sequence of acquisitions and their choices during training. Following
these practice trials, the next rule was presented. The rules were presented in the familiarization session in an order ranging from simple to
more complex: equal-weighted adding, lexicographic, satisticing, elimination by aspects, weighted adding, and majority of confirming dimensions .
Finally, after all six rules had been presented, subjects were given six
practice trials, one for each rule. These trials introduced the use of two
response scales to measure the difftculty of the decision task and how
effortful the decision was. The first scale asked the subject to rate how
difficult the choice was to make on a scale ranging from 0 (not difficult at
all) to 10 (extremely difticult). The second scale asked the subject to rate
how much effort he or she put into making the choice on a scale ranging
from 0 (hardly any effort) to 10 (a great deal of effort). The purpose of

<-----Page 14----->CHOICE

EFFORT

125

these six practice trials was threefold: (1) to introduce the response scale;
(2) to consolidate the learning of the rules; and (3) to introduce subjects to
the range of difficulty in the problems so that they could calibrate their
use of the response scales more accurately during the actual estimation
sessions. This latter purpose was accomplished by selecting a variety of
problem sizes and difficulty levels for the six practice trials.
Estimation sessions. At the beginning of each session, the subject was
given a review of that session’s decision rule. The rule was described
again, and several practice trials were given, with feedback on the accuracy of the acquisition sequence and choice. Then subjects were given a
sequence of decision problems where they had to make two consecutive
choices using the rule with no errors in acquisition sequence or alternative
chosen. Following successful completion of these trials to criterion, the
actual experimental trials for that session began.
As noted above, the 20 choice problems for each decision rule were
presented to the subject on an IBM Personal Computer via the MOUSELAB software. Subjects used a Mouse Systems mouse as a pointing device. These problems were randomly ordered (the random order was the
same for all subjects). For each problem, the subject followed the sequence of acquisitions implied by the rule. The move monitoring system
described above was used to monitor subjects’ adherence to the correct
sequence for the rule. Subjects then indicated the alternative chosen, and
responded to the difficulty and effort scales described above. For each
choice, MOUSELAB recorded the sequence of acquisitions, the time of
entry and exit for each cell, the alternative chosen, and values on the two
response scales. The overall latencies for the choice and the two scale
responses were also recorded.
After completing all eight sessions, subjects received $40 for their participation. In addition, they were told that three $5 bonuses would be paid
for (1) above-average performance in terms of overall accuracy, (2) minimization of incorrect search, and (3) speed of decision, respectively. In
other words, subjects were informed that they could earn an additional
payment of up to $15 dollars depending upon their performance.
Preliminary Analyses
Before the major analyses could be performed, the data were analyzed
to determine the prevalence of errors, the existence of speed-accuracy
tradeoffs, and the relationship between the two self-report measures of
effort.
Subjects selected incorrect alternatives on 11.4% of the trials. In addition, .8% of the trials contained severe deviations from the correct sequence of acquisitions specified for that trial (i.e., more than two
“buzzes”), even though the correct alternative was still selected. Taken

<-----Page 15----->126

BETTMAN,

JOHNSON,

AND

PAYNE

together, this yields a total of 12.2% error trials. Over half of these errors
come from the weighted adding (27.1%) and elimination by aspects
(32.2%) rules. For all analyses, all error trials of both types were removed
from the data. However, analyses performed when all trials were included
show virtually identical results.
To examine the possible existence of speed-accuracy tradeoffs, response latency was correlated with error, both across and within strategies. Overall, the correlation between time for each decision and the
probability of an error was .15 (p < .OOOl).Similar positive correlations
were obtained for each rule, subject, and rule-by-subject combination. In
no case was there a significant negative correlation, which indicated that
these data are relatively free from any concerns with speed-accuracy
tradeoffs.
Finally, the two self-report measures of effort and difficulty were examined. Their intercorrelation was .85, suggesting that they measure the
same underlying construct. A principal components analysis showed that
the first factor accounted for 93% of the variance in the scores, so the two
ratings were added to form an overall index of subjective effort.
For the analyses we report, the various models described below are
estimated using different independent variables. In every model, however, dummy variables representing the subject and session (i.e., the
order of that session among the six estimation sessions) are included, as
are variables representing the linear and quadratic effects of trial (i.e., the
order among the 20 decision problems within any session). These variables, although statistically significant, account for small portions of the
explained variance and simply allow for changes in the intercept term
across sessions and subjects and for any effects of practice across trials to
be taken into account. Since the effects are not theoretically important for
our purposes, they are not reported in the discussion of the results.
Overview of the Analyses
As discussed above, we consider two major indicators of strategy execution effort for the experimental data collected: response times and an
index of self-reported effort. In the results section below, we first present
data showing the average performance across subjects on these two measures for the various rules for the different problem sizes.
Following this attempt to characterize strategy performance, we examine the two classes of models for effort we outlined above: behavioral
(informational) and EIP. Recall that the behavioral model attempts to
explain effort using the only overtly observable behavior, the number of
information acquisitions (READS). Four different EIP models are also
examined. In the equal-weighted EZP model, all EIPs are given the same
weight. In contrast, the weighted EZP model allows each EIP to have its

<-----Page 16----->I27

CHOICE EFFORT

own characteristic effect upon each dependent measure.The third model,
the weighted EZP by rule model, allows the effect of each EIP to vary by
rule. Finally, the weighted EZP by individual model allows the coefficient
of each EIP to vary by individual. Note that allowing the coeficients to
vary across individuals is what characterizes this model; individual subject dummy variables which allow the intercept term to vary over individuals are included in all of the models, as are the session and trial
variables described above. Regression analyses are performed using the
independent variables specified by each model and response latency and
self-reported effort as dependent variables. We can assessthe relative fit
for each model and test the significance of certain model comparisons.’
RESULTS

Before considering the fit of the various models, we first present the
averageresponsetimes and self-reported effort levels for the various rules
for each problem size. Then we consider the models of response times
and self-reports of effort, respectively.
Average Response Times and Self-Reports
Problem Size

of Effort by Strategy and

Table 4 summarizesthe average response times, and Table 5 presents
the self-reports of effort for each decision strategy for the different problem sizes. These data are averaged across all seven subjects. Recall that
some problem sizes were not used for some strategies because of the
selection procedure described above.
As would be expected, decision problems of increasing complexity
(i.e., more alternatives and/or more attributes) take longer and are viewed
as more effortful. An analysis of variance of response times showed both
a main effect of number of alternatives (F(4,647) = 46.21, p < .OOOl;
meansof 17.2,22.5,26.3, 36.4, and 57.8 s for 2, 3,4, 5, and 6 alternatives,
respectively) and number of attributes (F(2,647) = 52.36, p < .OOOl;
means of 22.0, 29.3, and 41.0 s for 2,3, and 4 attributes, respectively).
Similarly, for self-reports of effort there were main effects for both number of alternatives (F(4,647) = 26.52, p < .OOOl;means of 2.7, 3.5, 4.2,
5.3, and 6.1 for 2, 3,4, 5, and 6 alternatives, respectively) and number of

r The behavioral model and the equal-weight EIP model are special cases of (or nested
within) the weighted EIP model. Hence, the additional fit provided by the weighted EIP
model over each of these two simpler models can be tested statistically (Neter & Wasserman, 1974, p. 89). Similarly, the weighted EIP model is nested within the weighted EIP by
rule and weighted EIP by individual models.

<-----Page 17----->128

BETTMAN,

JOHNSON, AND PAYNE
TABLE 4

AVERAGE RESPONSE TIMES BY STRATEGY AND PROBLEM SIZE

--

Problem size
Number
of
alternatives
2

3

4

5

6

Number
of
attributes

WADD

2
3
4
2
3
4
2
3
4
2
3
4
2
3
4

18.4"
24.6
33.0
35.0
41.6
64.6
39.4
47.8
77.5
46.7
76.6
86.7
62.7
162.4
154.5

Strategy
EOW

LEX

EBA

SAT

-b
15.7
-

8.5
12.8
23.9

12.7

8.7

11.5

12.5

15.8
26.8
18.3
21.5
21.3
29.7
40.5
29.0
46.3
42.2
39.0
48.3
71.7

17.7
17.0
16.4
21.5
12.6
18.7
26.9
15.0
26.2
36.0
20.0
31.7
46.7

11.9
17.2

14.3
17.6
26.1
19.5
14.4
36.4

18.5
31.0

16.5
27.8
13.8
22.3
24.2

30.4
36.8

41.6
62.5

MCD
18.2
24.9
17.8
22.1
38.4
16.7
32.2

33.1
47.1
28.7
32.5
46.9

* Average response time, in s.
b This problem size not selected for this rule.

attributes (F(2,647) = 33.19,~ < BOOI; means of 3.5,4.1, and 5.5 for 2,3,
and 4 attributes, respectively).
Of perhaps greater interest, the effects of task complexity vary by
strategy. For response time, there were significant rule x number of
alternatives (F(20,647) = 8.78, p < .OOOl)and rule x number of attributes
(F( 10,647) = 3.38, p < .OOOS)interactions, and a marginally significant
rule x number of alternatives x number of attributes interaction
(F(30,647) = 1.46, p < AX). For the self-reports of effort, there was a
significant rule x alternatives interaction (F(20,647) = 1.98, p < .007), a
marginally significant rule x number of attributes interaction (F(10,647)
= 1.77, p < .07), and a nonsignificant three-way interaction (F(30,647) =
.43, ns). The form of these interactions is that the weighted additive rule
shows much more rapid increases in response time and generally shows
more rapid increases in self-reports of effort as a function of increases in
task complexity than the other strategies. This is of course consistent with
a great deal of other research showing that individuals shift toward simplifying decision heusistics as a function of increases in task complexity,
particularly with increases in number of alternatives (Payne, 1982).
The results presented above and in Tables 4 and 5 demonstrate that the

<-----Page 18----->CHOICE

129

EFFORT

TABLE 5
AVERAGESELF-REPORTSOFEFFORTBYSTRATEGYANDPROBLEMSIZE
Problem size
Number
of
alternatives
2
3

4

5
6

Number
of
attributes

WADD

EQW

LEX

EBA

SAT

MCD

2
3
4
2
3
4
2
3
4
2
3
4
2
3
4

3.77”
3.81
6.90
4.91
5.73
5.67
6.85
7.63
8.61
7.08
6.37
10.29
6.88
8.92
11.0

2.56
1.67
2.34
3.40
2.78
3.45
3.23
3.01
6.35
3.73
4.07
5.71
4.76
5.40
9.72

.93
1.4
3.33
1.83
2.97
2.65
2.45
3.20
4.49
2.94
4.73
5.20
2.13
5.03
6.22

-b
2.43
2.49
3.48
2.62
3.37
5.15
2.90
2.87
6.22
2.52
5.40

1.73
1.79
1.74
3.34
3.67
2.37
3.72
4.08
5.50
5.35
6.59
7.24

2.57
4.48
3.92
4.47
5.46
4.58
4.70
6.74
7.22
5.02
6.33
6.00

Strategy

’ Average self-reported effort, on a 0 (low) to 10 (high) scale.
b This problem size not selected for this rule.

various rules perform differently in different task environments in terms
of two indicators of effort, response time and self-reports of effort. The
central question of interest, however, is whether the componential framework proposed above can provide a unifying treatment of the effort required by these rules and model these differences in effort. Hence, we
examine the extent to which the various models outlined above fit the
data summarized in Tables 4 and 5. We explore that issue for response
time and self-reports of effort in turn.
Analysis

of Response Times

Table 6 provides a summary of the degrees of fit for the four EIP
models and the behavioral model (reads only).’ All the models provide
good fits for the overall response times (p < .OOOl).Note that the fit of the
* Although not the subject of interest in this paper, for completeness a model using as
independent variables the number of alternatives, the number of attributes, their product,
and a dummy variable for each rule was estimated. The R* values for response time and
self-reported effort were .65 and .57, respectively.

<-----Page 19----->130

BETTMAN,

JOHNSON,

AND

PAYNE

TABLE 6
DEGREE OF FIT FOR MODELS OF RESPONSE TIME AND SELF-REPORTS OF EFFORT

R2 values
Model

Response time

Self-reports of effort

Behavioral
Equal-weighted EIP
Weighted EIP
Weighted EIP by rule
Weighted EIP by individual

.75
.I5
.84
.84
.90

.56
.55
.59
.61
.80

weighted EIP model is significantly better than that of the behavioral
model (F(5,713) = 81.4, p < .OOOl)or that of the equal-weight EIP model
(F(5,713) = 78.9, p < .0001).3 Thus, it appears that a model of cognitive
effort in choice, as measured by response time, requires concern not only
for the amount of information processed (READS), but also for the various processes applied to that information (e.g., Products and Comparisons), with differential weighting of those operators.
While the degree of fit for the weighted EIP model is impressive, we
must examine whether more complex models improve the fit. First, we
consider the weighted EIP by rule model, which allows the time for each
EIP to vary by rule, to determine whether the EIPs require the same time
for each rule. The weighted EIP model is a special case of the weighted
EIP by rule model, so the significance of the incremental fit can be tested.
The incremental fit is not significant (F(13,700) = 1.37, ns); hence, the
assumption that each operation requires a constant amount of time independent of the strategy in which it is used seems reasonable.
The weighted EIP by individual model allows the times for the EIPs to
vary across subjects. Even if individuals use the same strategy, they may
differ in the amount of time required for each component process (Hunt,
1978; Sternberg, 1977). This model achieves an R* = 90, with significantly better fit than the weighted EIP model (Incremental R* = .06;
F(36,677) = 10.9, p < .OOOl).These individual differences are considered
further in the discussion section.
Thus, based upon the analyses of response times, the weighted EIP
model, and hence the EIP conceptualization of decision effort, receives
3 The degrees of freedom for the numerator in these comparisons represent the difference
between the use of six EIP variables for the weighted EIP model and one variable for the
behavioral and equal-weight EIP models. The degrees of freedom for the denominator
reflect the total trials and the total number of variables used for the weighted EIP model
(Neter & Wasserman, 1974, p. 89).

<-----Page 20----->CHOICE

131

EFFORT

TABLE 7
COEFFICIENT ESTIMATES OF RESPONSE TIME AND SELF-REPORTS OF EFFORT FOR EIPs
EIP
READS
Response
time
Self-reports
of effort
* Significantly

ADDITIONS

PRODUCTS

COMPARISONS

ELIMINATIONS

DIFFERENCES

1.19*

.84*

2.23*

.09

1.80*

.32

.10*

.0f3*

.19*

.04

.32*

.12

different

from zero at p < .05.

strong support. The EIP times appear to vary across individuals, although
not across rules.4
These results also hold up well in cross-validation. Estimating the
model on one-half of the data and using these estimates to predict the
other half yields average R2 values of .74, .73, .81, .82, and .88 for the
behavioral, equal-weighted EIP, weighted EIP, weighted EIP by rule, and
weighted EIP by individual models, respectively.
Estimates of EZP times. Since the weighted EIP model received strong
support, estimates of the times for each operator are shown in Table 7.
Although the estimates vary to some extent across individuals, as a first
approximation we consider the pooled results.
The coefficients are all positive, with most significantly different from
zero. The estimates also tend to agree with estimates for similar EIPs
provided by other studies. The READ EIP combines encoding information with the motor activity of moving the mouse. Its estimated latency is
1.19s (t(713) = 6.55, p < .OOOl).This estimate is plausible, since it might
consist of the movement of the mouse, estimated to be in the range of .2
to .8 s by Johnson et al. (1986), and an eye fixation, estimated to require
a minimum of .2 s (Russo, 1978). ADDITIONS and DIFFERENCES
both take less than 1 s, with estimates of .84 (t(713) = 4.54, p < .OOOl)and
.32 (t(713) = .98, INS),respectively. These values are not significantly
different (t(713) = 1.03, ns) and are consistent with those provided by
4 The models were also estimated using the response time data disaggregated to the level
of individual acquisitions, the total time spent on each alternative, and the total time spent
on each attribute. The relative fits of the various models essentially replicate those of the
aggregate results, although the absolute levels of fit are of course lower for the more disaggregate analyses.

<-----Page 21----->132

BETTMAN,

JOHNSON,

AND

PAYNE

Dansereau (1%9), Groen and Parkman (1972), and others (see Chase,
1978,Table 3, p. 76). Our estimate for the PRODUCT EIP, 2.23 s (t(713)
= 10.36,p < .OOOl),is larger than that commonly reported in the literature.
The time for COMPARISONS is very short, .08 s (t(713) = .22, ns),
and that for ELIMINATIONS, 1.8 s (t(713) = 3.00, p < .Ol), is relatively
long. This may reflect the collinearity of COMPARES and ELIMINATIONS.
In sum, based both upon its degree of fit and the generally plausible
time estimates for the EIPs, the proposed weighted EIP model receives
impressive support when responsetimes are used as an indicator of effort.
The next set of results examines the performance of the various models
when self-reports of effort form the indicator of effort.
Analysis of Self-Reports of EfSort

There are several reasons why self-reports of effort are interesting as a
second indicator of decision effort. First, self-reported effort might tap
different aspects of strategy execution effort and might not be closely
related to decision latency, As Kahneman (1973) observed, two different
mental tasks may take similar amounts of time, but one might be seen as
much more effortful than the other. This speculation receives some support in our data: the overall correlation between time and the selfreported effort index is .29. Secondly, while the analysis of latency helps
validate the proposed EIP conceptualization of effort, self-perceptions of
effort may also be important in understanding why decision-makers avoid
certain strategies. However, several cogent arguments for caution in the
use of self-reported measures of effort should also be noted. Foremost
among these is the possibility that subjects cannot accurately report demands on cognitive resources (Gopher & Donchin, in press), or that such
reports do not allow comparisons across tasks which make widely differing demands.5
Model fit. From the results shown in Table 6, it can be seen that the
absolute levels of fit are lower than for the response latencies, but are still

5 There may be a distinction between anticipated effort and experienced effort, with the
former being the effort a strategy is predicted to require for solving a problem and the latter
reflecting the effort actually used. In the current study, we focus on experienced effort.
While strategy selection may be a function of anticipated effort, we argue that a major basis
for estimating anticipated effort is experienced effort on previous decision tasks. Hence,
analysis of experienced effort can potentially lead to insights into the bases for anticipated
effort. The relationship between anticipated and experienced effort is an important topic for
study, but it is beyond the scope of the current investigation.

<-----Page 22----->CHOICE EFFORT

133

highly significant (p < .0001).6 The weighted EIP model again provides
significantly greater fit than the behavioral (F(5,713) = 10.0, p C .OOOl)or
equal-weighted EIP (F(5,713) = 13.0, p < .OOOl)models.
The weighted EIP model of subjective effort can also be compared to
more complex models. The weighted EIP by rule model shows a small,
but statistically significant increase in tit (incremental R2 = .02; F(13,700
= 2.6, p < .002). The weighted EIP by individual model shows a substantial increase in fit (incremental R2 = .21; F(36,677) = 20.1, p <
.oool).
Hence, the results essentially replicate those for response times. The
weighted EIP model provides the best explanation of decision-makers’
self-reports of the effort associated with each decision problem, and the
effort estimates appear to vary across individuals, but only slightly across
rules.
Cross-validation of these results is also encouraging. Average R2 values
of S3, .54, .58, 60, and .78 are obtained for the behavioral, equalweighted EIP, weighted EIP, weighted EIP by rule, and weighted EIP by
individual models, respectively.7
Estimates of EZP effort. Estimates of the subjective effort associated
with each EIP from the weighted EIP model pooled across subjects are
given in Table 7. These estimates represent the increase in reported effort
per EIP on the sum of two 0 to 10 scales. The largest estimate is for the
ELIMINATION
operator, .32. However, the high intercorrelation between ELIMINATIONS
and COMPARISONS (.85) must temper any
interpretation of this coefficient and the small (.04) coefftcient for COMPARISONS. The PRODUCT operator, as might be expected, is seen as
fairly effortful, with a coefficient of. 19, while the coefftcients for READS
and ADDITIONS are also significantly positive.*

’ If we assume that the two self-report measures of effort and difliculty which we combined to form the self-reported effort index are both fallible measures of effort, the intercorrelation between them (r = .85) serves as a baseline to assess the reliability of this index.
Correcting for this unreliability would provide an estimated R2 of .83 for the weighted EIP
model for effort (Ghiselli, Campbell, & Zedeck, 1981, p. 290).
’ To further explore the robustness of the results, models for both response times and
self-reports of effort were run which added variables for the number of alternatives, the
number of attributes, and dummy variables for rules to the weighted EIP by individual
model. While these additional variables produced significant increases in fit (F(11,666) =
3.30, p < BOO2and F(11,666) = 4.04, p < .OOOlfor response time and effort, respectively),
the increases in R2 are very small (.005 and .012 for response time and effort, respectively).
* Although errors are only indirectly related to decision effort, for the sake of completeness, logistic regressions were run using the behavioral, equal-weighted EIP, and weighted
EIP models with a 0 to 1 dependent variable representing whether or not an error was made
on a particular choice problem. The pseudo-R* values from this analysis were .62, .63, and

<-----Page 23----->134

BETTMAN,

JOHNSON, AND PAYNE

DISCUSSION
The concept of effort plays a major role in attempts to understand the
contingent use of processing strategies. An approach to measuring the
effort associated with different decision strategies is proposed in this
study, using a set of elementary operators (i.e., READS, ADDITIONS,
COMPARISONS, PRODUCTS, DIFFERENCES,
and ELIMINATIONS) as a common “language” for describing decision strategies. This
set of operators is used to generate a metric of the effort required to
execute a decision strategy in terms of the number of EIPs involved.
The empirical results yielded strong support for this proposed componential approach to strategy effort. A model of effort based upon weighted
EIP counts (the weighted EIP model) provided good fits for response
times and self-reports of effort, two different measures of decision effort.
In addition to this absolute level of fit, the weighted EIP model also was
statistically superior to a behavioral model using only reads and to an
equal-weight EIP model for each of the two indicators of effort.
The estimates of time taken for each EIP were mostly plausible and in
line with prior research, hence providing additional confidence in the
approach. We also examined the potential generalizability of our results
to a broader range of cognitive tasks. Specifically, estimates of the times
taken for various EIPs drawn from studies of other information processing tasks (see Johnson & Payne, 1985, p. 406, for the specific values of
these estimates) were used as weights to produce an analogue to the
weighted EIP model for response times.’ That is, the values drawn from
the literature for the time for each EIP were used as coefficients of the
EIP counts to produce a predicted response time. This model produced an
R* value of .81, only slightly below that of the weighted EIP model. The
performance of this model provides encouraging evidence that the componential approach may generalize to a variety of cognitive tasks. In
addition, the closeness of the time estimates for individual EIPs obtained
from our study of decision making to those derived from other cognitive
tasks, noted above, provides support for the generalizability of our experimental procedure.
In general, the weights for various EIPs obtained in our study were
.64 for the behavioral, equal-weighted EIP, and weighted EIP models, respectively. The
weighted EIP model performed better than the behavioral model @ < .05) but not better
than the equal-weighted EIP model (p = .105).
9 The weighted EIP model for times was the only model considered, since the estimates
from the literature only refer to times and do not account for individual differences or
differences across rules.

<-----Page 24----->CHOICE

EFFORT

135

essentially the same regardless of the decision strategy used. Hence, the
original assumptions of serial processing and independence of EIP duration across rules and problem sizes made by Johnson and Payne (1985)
receive encouraging support in this research. Taken together, these results imply that a small number of simple operators can be viewed as the
fundamental components from which decision rules are constructed
(Bettman, 1979; Bettman 8z Park, 1980).
However, the results do suggest significant individual differences in the
effort associated with individual EIPs. For example, for some individuals
computational operators were relatively more difficult than comparisons.
For others, this difference was not present. This suggests the possibility
that individuals may choose different rules in part because different component EIPs may be relatively more or less difficult or effortful across
individuals. In addition, although the evidence for a model of effort based
upon EIP counts is impressive, the findings presented thus far are all
limited to a situation in which individuals were required to use various
strategies which had been prescribed for them. Suppose, however, that
parameters characterizing individual subjects’ performance on this constrained choice task (e.g., average EIP times) could be related to those
subjects’ behaviors in a choice task where subjects were free to use
whatever strategy they wished. This would provide suggestive evidence
for the proposed componential approach.
As an exploratory analysis, we related the average times spent by subjects on arithmetic operations (ADDITIONS and PRODUCTS) in the
constrained choice tasks to the processing patterns used by those subjects
in the unconstrained choices they made in the final experimental session.
In particular, processing patterns of three subjects for whom arithmetic
operators were relatively more expensive (took more time) were compared to those of the four subjects for whom such operators took less
time. The subjects for whom the arithmetic operators were relatively
more expensive showed significantly greater variability in their information acquisition across attributes and alternatives on the 12 unconstrained
choices. This result is consistent with use of more heuristic, noncompensatory processes rather than with the use of computationally expensive
compensatory strategies such as weighted adding (Payne, 1976). Showing
that performance on the constrained task can be related to processing in
unconstrained choice situations offers suggestive support for the EIP approach, although the small sample size precludes strong conclusions.
Another contribution of the study is more methodological. The
MOUSELAB decision-monitoring software and hardware worked exceptionally well in providing detailed data about the decision task. The ability
to monitor the sequence of acquisitions, measure latencies, and, in gen-

<-----Page 25----->136

BETTMAN,

JOHNSON,

AND

PAYNE

eral, maintain experimental control over the choice task makes this system potentially very valuable for a variety of research issues in decision
making and other areas of cognition.
The attainment of experimental control, necessary to predict the operators used and to implement the proposed EIP models of effort, is not
without costs. In the constrained decision task, subjects do not select
strategies; rather, they apply given rules. Hence, the task eliminates
many difficult problems normally faced by individuals making decisions.
Subjects did not have to select or construct a strategy, and the sequence
of operations was specified. Thus, they did not have to engage in possibly
effortful control processes determining what to do next. In addition, by
providing all of the weights, cutoffs, and ratings, the need for potentially
difficult valuation processes was eliminated. Finally, some of the timing
estimates are undoubtedly affected by the specific apparatus used (i.e.,
the matrix display and the mouse). These restrictions may be less worrisome, however, given the analyses of the “free choice” task. Since the
timing estimates derived from the constrained choices predict aspects of
the processing patterns in the free choices, our confidence in the procedures used is increased. However, further research relaxing these restrictions on processing flexibility would be desirable.
A second set of caveats is that although an approach which breaks
down decision strategies into more detailed components seems to be
strongly supported as an approach to measuring decision effort, we have
focused on a particular level of detail in taking such an approach. For
example, one could model multiplications in terms of underlying arithmetic operations (e.g., Dansereau, 1969) or anchoring and adjustment
(Lopes, 1982). In addition, one could extend our models to include EIPs
that model the transfer of information to long-term memory and various
mental “bookkeeping” operations.
Modeling cognitive effort at the level of EIPs allows us to examine how
the effort associated with various strategies might vary as a function of
differences in task environments. In particular, such variation in effort
across task environments could be predicted by computer simulation of
the performance of different heuristics in such environments. As an example of this approach, Payne er al. (1988) used both computer simulation
and process tracing experiments to examine the joint effects of effort and
accuracy on the adaptive use of decision processes. A Monte-Carlo simulation study utilized the proposed measure of effort based on EIP
counts, along with various measures of accuracy, to identify heuristic
choice strategies that approximated the accuracy of normative procedures and required substantially less effort. No single heuristic, however,
did well across all task environments. Thus, a decision maker striving to

<-----Page 26----->CHOICE

EFFORT

137

maintain a high level of accuracy with a minimum of effort would have to
use a variety of heuristics.
Payne et al. (1988) then tested the degree of correspondence between
the efficient processing strategies for a given decision problem identified
by the simulations and the actual information processing behavior exhibited by people. People were shown to be highly adaptive in their responsesto changesin the nature of the alternatives available to them, and
to the presence or absence of time pressure. The results for actual decision behavior tended to validate the patterns expected on the basis of the
simulation estimates. Of particular interest was the finding that people
were sensitive to changes in decision context that impact the relative
accuracy of heuristics as well as affecting relative effort.
Taken together, the present results, plus those reported in Payne et al.
(1988), support the hypothesis that decision makers choose strategies as
a function of a strategy’s demand for mental resources (i.e., the effort
required to use a strategy) and the strategy’s ability to produce an accurate response. However, it is important to recognize that a cost/benefit
viewpoint of strategy selection does not rule out the possibility of other
factors impacting strategy usage. For example, there is growing evidence
that justifiability may influence the choice of processing strategy (e.g.,
Tetlock & Kim, 1987). In addition, more perceptual factors such as the
decision frame (Tversky & Kahneman, 1981)may also influence strategy
use. Nonetheless, the present study, along with others, shows how measures of decision effort can be predictive of strategy use.
Finally, the approach to measuring cognitive effort developed in this
paper may also have applied value. For example, recently it has been
suggestedthat the use of nutritional information in the supermarket by
consumers might be improved by decreasing the effort costs associated
with processing that information (Russo, Staelin, Nolan, Russell, & Metcalf, 1986).The methodology developed in this paper could be used to test
the impact of different information displays on the use of a preferred
decision strategy. A related area of application would be the design of
computer-based decision aids (Keen & Scott-Morton, 1978; Kleinmuntz
& Schkade, 1988).
REFERENCES
Abelson, R. P., & Levi, A. (1985). Decision making and decision theory. In G. Lindzey &
E. Aronson (Eds.), The handbook of social psychology; Vol. 1. New York: Random
House.
Beach, L. R., & Mitchell, T. R. (1978). A contingency model for the selection of decision
strategies. Academy of Management Review, 3,439-449.
Bettman, J. R. (1979). An information processing theory of consumer choice. Reading, MA:
Addison-Wesley.

<-----Page 27----->BETTMAN,

138

JOHNSON, AND PAYNE

Bettman, J. R., & Park, C. W. (1980). Effects of prior knowledge, experience, and phase of
the choice process on consumer decision processes: A protocol analysis. Journal of
Consumer Research,

I, 234248.

Bettman, J. R., & Zins, M. (1979). Information format and choice task effects in decision
making. Journal of Consumer Research, 6, 141-153.
Card, S. K., Moran, T. P., & Newall, A. (1983). The psychology of human-computer interaction. Hillsdale, NJ: Lawrence Erlbaum.
Carpenter, P. A., & Just, M. A. (1975). Sentence comprehension: A psycholinguistic processing model of verification. Psychological Review, 82, 45-73.
Chase, W. G. (1978). Elementary information processes. In W. K. Estes (Ed.), Handbook
of learning and cognitive processes: Vol. 5. Hillsdale, NJ: Lawrence Erlbaum.
Dansereau, D. F. (1969). An information processing model of mental multiplication.
Unpublished doctoral dissertation, Carnegie-Mellon University, Pittsburgh, PA.
Ghiselli, E., Campbell, J. P., & Zedeck, S. (1981). Measurement theory for the behavioral
sciences. San Francisco: W. H. Freeman.
Gopher, D., & Donchin, E. (in press). Workload: An examination of the concept. In K. Boff
& L. Kaufman (Eds.), Handbook of perception and human performance. New York:
John Wiley.
Groen, G. J., & Parkman, J. M. (1972). A chronometric analysis of simple addition. Psychological

Review, 79, 329-343.

Hammond, K. R. (1986). A theoretically based review of theory and research in judgment
and decision making (Report No. 260). Boulder: University of Colorado, Center for
Research on Judgment and Policy, Institute of Cognitive Science.
Hogarth, R. M. (1987). Judgement and choice (2nd ed.). New York: John Wiley.
Huber, 0. (1980). The influence of some task variables on cognitive operations in an information-processing decision model. Acta Psychologica, 45, 187-196.
Hunt, E. B. (1978). Mechanics of verbal ability. Psychological Review, 85, 109-130.
Johnson, E. J. (1979). Deciding how to decide: The effort of making a decision. Unpublished
manuscript, University of Chicago.
Johnson, E. J., &Payne, J. W. (1985). Effort and accuracy in choice. Management Science,
31, 394-tl4.
Johnson, E. J., Payne, J. W., Schkade, D. A., & Bettman, J. R. (1989). Monitoring information processing and decisions: The mouselab system. Unpublished manuscript,
Fuqua School of Business, Duke University, Durham, NC.
Kahneman, D. (1973). Attention and effort. Englewood Cliffs, NJ: Prentice-Hall.
Keen, P. G. W., & Scott-Morton, M. S. (1978). Decision support systems: An organizational perspective. Reading: MA: Addison-Wesley.
Klayman, J. (1985). Children’s decision strategies and their adaptation to task characteristics. Organizaiional Behavior and Human Decision Processes, 35, 179-201.
Kleinmuntz, D., & Schkade, D. A. (1988). The cognitive implications of information displays in computer-supported
decision making (Working Paper 2010-88) Cambridge:
Massachusetts Institute of Technology, Alfred P. Sloan School of Management.
Kmenta, J. (1986). Elements of econometrics (2nd ed.). New York: Macmillan.
Lopes, L. L. (1982). Toward a procedural theory of judgment. Unpublished manuscript,
University of Wisconsin.
Maule, A. J. (1985). Cognitive approaches to decision making. In G. Wright (Ed.), Behavioral decision making. New York: Plenum.
Navon, D., & Gopher, D. (1979). On the economy of the human processing system. Psychological Review, 86, 214-255.
Neter, J., & Wasserman, W. (1974). Applied linear statistical models. Homewood, IL:
Richard D. Irwin.

<-----Page 28----->CHOICE

139

EFFORT

A., & Simon, H. A. (1972). Human problem solving. Englewood Cliffs, NJ: Prentice-Hall.
Paquette, L., & Kida, T. (1988). The effect of decision strategy and task complexity on
decision performance. Organizational Behavior and Human Decision Processes, 41,
128-142.
Payne, J. W. (1976). Task complexity and contingent processing in human decision making:
An information search and protocol analysis. Organizational Behavior and Human

Newell,

Performance,

16, 366-387.

Payne, J. W. (1982). Contingent decision behavior. Psychological Bulletin, 92, 382-402.
Payne, J. W., Bettman, J. R., & Johnson, E. J. (1988). Adaptive strategy selection in decision making. Journal of Experimental Psychology: Learning, Memory, and Cognition. 14, 534-552.

Russo, .I. E. (1978). Eye fixations can save the world: Critical evaluation and comparison
between eye fixations and other information processing methodologies. In H. K. Hunt
(Ed.), Advances in consumer research: Vol. V. Ann Arbor, MI: Association for Consumer Research.
Russo, J. E., & Dosher, B. A. (1983). Strategies for multiattribute binary choice. Journal of
Experimental

Psychology:

Learning,

Memory,

and Cognition,

9, 6764%.

Russo, J. E., Staelin, R., Nolan, C. A., Russell, G. J., & Metcalf, B. L. (1986). Nutrition
information in the supermarket. Journal of Consumer Research, 13, 48-70.
Shugan, S. M. (1980). The cost of thinking. Journal of Consumer Research, 7, 99-l 11.
Siegler, R. (1986). Strategy choice procedures and the development of multiplication skill.
Unpublished manuscript, Carnegie-Mellon University, Pittsburgh, PA.
Stemberg, R. (1977). Component processes in analogical reasoning. Psychological Review,
84, 353-378.
Tetlock, P. E., & Kim, J. I. (1987). Accountability and judgment processes in a personality
prediction task. Journal of Personality and Social Psychology, 52, 700-709.
Thomas, E. A. C. (1983). Notes on effort and achievement-oriented behavior. Psychological Review, 90, l-20.
Tversky, A. (1969). Intransitivity of preferences. Psychological Review, 76, 31-48.
Tversky, A. (1972). Elimination by aspects: A theory of choice. Psychological Review, 79,
281-299.
Tversky, A., & Kahneman, D. (1981). The framing of decisions and the psychology of
choice. Science, 211, 453-458.
Wickens, C. D. (1984). Engineering psychology and human performance. Columbus, OH:
Charles E. Merrill.
Wright, P. L. (1975). Consumer choice strategies: Simplifying vs. optimizing. Journal of
Marketing Research, 11, 60-67.
RECEIVED: February 29, 1988

