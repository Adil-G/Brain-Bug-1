<-----Page 0----->Attribute Conflict and Preference Uncertainty: Effects on Judgment Time and Error
Author(s): Gregory W. Fischer, Mary Frances Luce and Jianmin Jia
Reviewed work(s):
Source: Management Science, Vol. 46, No. 1 (Jan., 2000), pp. 88-103
Published by: INFORMS
Stable URL: http://www.jstor.org/stable/2634910 .
Accessed: 25/06/2012 21:58
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at .
http://www.jstor.org/page/info/about/policies/terms.jsp
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

INFORMS is collaborating with JSTOR to digitize, preserve and extend access to Management Science.

http://www.jstor.org

<-----Page 1----->Conflict

Attribute

Uncertainty: Effects
Time

Preference

and
and

on

Judgment

Error

Gregory W. Fischer * Mary Frances Luce * Jianmin Jia
Fuqua Schoolof Business,Duke University,Durham,NorthCarolina27708
Pennsylvania19104
TheWhartonSchool,Universityof Pennsylvania,Philadelphia,
Chinese
Universityof Hong Kong,Shatin,NT, Hong Kong
Facultyof BusinessAdministration,
* maryfran@marketing.wharton.upenn.edu
* jjia@tcuhk.edu.hk
fischer@mail.duke.edu

his research investigates preference uncertainty generated as a function of specific
alternative characteristics during multiattribute evaluative judgments. We propose that
preference uncertainty has at least two behavioral manifestations: longer judgment times and
greater response error in expressed preferences. We investigate two hypotheses regarding
stimulus-based causes of preference uncertainty. As predicted by our attribute conflict
hypothesis, greater within-alternative conflict (discrepancy among the attributes of an evaluative alternative) led to longer judgment times and greater response error. As predicted by our
attribute extremity hypothesis, greater attribute extremity (very high or low attribute values)
resulted in shorter judgment times and less response error. We also found that judgment times
and response errors were strongly positively correlated at the item level, consistent with our
assumption that preference uncertainty generated by stimulus characteristics is manifested in
judgment time and error. Finally, we found that the item-level preference uncertainty effects
proposed here operate in parallel with strategy-level, effort-accuracy tradeoffs observable
across participants. These findings are consistent with the RandMAU random multiattribute
utility model developed in a companion article by Fischer et al. (2000).
T

Introduction
Recent decision research has focused increasingly on
psychological aspects of choice such as loss aversion,
anticipation, dread, and regret. In this research, we
consider another psychological dimension of prefer-

ence-feelings of ambivalenceor preferenceuncertainty
that arise when one must evaluate a single alternative
that is good in some respects but bad in others. For
example, the highest quality products are frequently
among the most expensive. The best-paying jobs are
frequently not the most interesting, or secure, or in the
best location. The safest cars may not be the most fun
to drive.
We argue that such conflict between different asMANAGEMENT SCIENCE ? 2000 INFORMS
Vol. 46, No. 1, January 2000 pp. 88-103

pects of choice or evaluation tasks leads to preference
uncertainty. If one is choosing between two alternatives, preference uncertainty means not being sure
which alternative one prefers, or to what degree. If one
is evaluating a single alternative in terms of some
metric of value, preference uncertainty means not
being sure what value to assign to the alternative.
Preference uncertainty may be manifested in a variety
of ways. For instance, if one is unsure about whether
the positive features of an alternative outweigh the
negative, one is likely to take longer to evaluate it and
to express less consistent evaluations over time.
Preference uncertainty may arise for a variety of
reasons. In general, people are likely to be more
0025-1909/00/4601/0088$05.00
1526-5501electronicISSN

<-----Page 2----->FISCHER, LUCE, AND JIA
AttribtuteConflict and PreferenceUTncertainty

uncertain in situations that are novel or unfamiliar
(e.g., March 1978). In addition, the properties of decision alternatives themselves may contribute to preference uncertainty. In this article, we investigate two
stimulus properties that we believe contribute to preference uncertainty in the evaluation of a single alternative. Our primary interest is in a stimulus characteristic that we call attribute conflict, or the degree to
which the attractiveness of an alternative varies across
the attributes that describe it. For example, a car might
have excellent acceleration, braking, and handling
characteristics, but be uncomfortable, expensive, and
perform poorly in crash tests. High attribute conflict
means that an individual must forego some goal(s) in
order to attain others. We argue below that such
conflict will lead to greater preference uncertainty. We
are also interested in a second stimulus characteristic
that we refer to as attributeextremity.Attribute extremity is high for either very high or very low attribute
values and low for attribute values near the middle of
the attribute scale. We argue that extreme attribute
values are easier to evaluate than intermediate ones.
Thus, greater attribute extremity leads to less preference uncertainty.
In short, the focus of this article is on how two
stimulus characteristics-attribute conflict and attribute extremity-affect preference uncertainty in an
evaluative rating task. We have two basic hypotheses.
First, greater attribute conflict leads to greater uncertainty regarding the value of the alternative. Second,
greater attribute extremity leads to less uncertainty
regarding the value of an alternative. We test these
predictions in the context of evaluative ratings, not
choice. In the next section, we discuss the theoretical
considerations that lead to these hypotheses. In the
section that follows, we describe the results of an
experimental investigation that tests them. The final
section discusses implications for preference modeling
and assessment.

Conflict, Extremity, and Preference
Uncertainty
Preference Uncertainty
Rational models of choice permit decision makers to
be uncertain about the occurrence of events in the

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

external environment, but assume that decision makers know their own preferences with certainty. In this
paper, we adopt the more psychologically realistic
assumption that decision makers also experience uncertainty when they evaluate decision outcomes
(March 1978). What does it mean to experience ex ante
uncertainty about one's preferences? When one is
choosing, preference uncertainty may be experienced
as indecision, not being sure whether one would rather
eat steak or lobster. When one is evaluating a single
alternative, preference uncertainty may be experienced as indecision about what rating to assign to the
alternative. For example, if one's favorite entree is leg
of lamb and one's least preferred entree is a hot dog,
where is a turkey burger located on a scale of value
where a hot dog is 0 and leg of lamb is 1? For either
choice or evaluative ratings, a decision maker may
experience ex ante preference uncertainty.
Measuring Preference Uncertainty
Our hypotheses involve ex ante preference uncertainty generated dturingthe evaluation of an alternative.
This form of uncertainty is likely related to, but not
necessarily perfectly correlated with, ex post subjective confidence ratings made after evaluation of an
alternative. There is a substantial literature on errors
in calibration of subjective confidence judgments (e.g.,
Lichtenstein et al. 1982). While this research typically
addresses confidence regarding factual statements or
probability estimates, there is no compelling reason to
believe that ratings of subjective confidence in preference assessments will be better calibrated. Thus, ex
post judgments of subjective confidence may not be
perfectly correlated with preference uncertainty during evaluation. We leave to future research the question of the correspondence between preference uncertainty (as operationalized below) and subjective
confidence. Instead, we focus on developing operationalizations of ex ante preference uncertainty that
we will use to test our hypotheses regarding how
stimulus characteristics affect preference uncertainty
regarding the evaluation of a single alternative.
Broadly, one may take two approaches to measuring preference uncertainty in evaluative judgments.
The first is to ask the decision maker to directly assess
preference uncertainty, much as one would do with

89

<-----Page 3----->FISCHER, LUCE, AND JIA
Attribute Coniflictand PreferenceUncertainty

event uncertainty. Thus, one might ask a decision
maker to estimate a 90% confidence range for the
value of a particular alternative. Alternatives that
evoke greater preference uncertainty should evoke
wider confidence intervals-but only if people have
direct insight into their own preference uncertainty. In
this article, we take a second approach, inferring
preference uncertainty from other properties of the
judgment process. In particular, we focus on two
indirect indicators of preference uncertainty. The first
is response time. The more uncertain one is as to the
overall value of an alternative, the longer one is likely
to take in assigning a value to the alternative. This
would occur, for instance, if one attempted to resolve
one's uncertainty by adopting different frames of
mind, then stopped when a stable evaluation
emerged. As random variations in the weights governing attribute tradeoffs increased, for example, successively generated evaluations of an alternative
would likely diverge more greatly from one another,
and a stable evaluation would take longer to obtain.
The second measure is response error. If one is uncertain about one's preferences, responses to judgment
tasks are likely to fluctuate from moment to moment,
as one adopts different frames of mind. Therefore,
greater preference uncertainty should be associated
with greater response error-that is, greater inconsistency between a particular decision maker's evaluations of an identical stimulus at different times.
There is no direct way to verify that either response time or response error reflects preference
uncertainty. However, if we are correct in assuming
that each provides a measure of preference uncertainty, then evaluative alternatives that take longer
to evaluate on average should also evoke greater
average response error. Thus, a significant positive,
item-level correlation between the two types of
measures would support our assumption that each is
an observable manifestation of stimulus-generated
preference uncertainty. Further, each is of interest in
its own right.
Representing Preference Uncertainty in Models of
Judgment
As we noted earlier, preference uncertainty may arise
for several reasons. Lack of familiarity with the alter-

90

natives or lack of familiarity with the preferencerevealing mechanism may both contribute to preference uncertainty. In addition, some alternatives may
be more difficult to evaluate than others, thus leading
to greater uncertainty in one's preferences. Our emphasis in this paper is on characteristics of individual
alternatives that evoke greater or lesser degrees of
uncertainty regarding the value of the alternative
during the process of developing an evaluative rating.
Our focus will be on situations in which a decision
maker judges the overall value of a series of alternatives. Each judgment will be made independently
of every other judgment (apart from any memory
carryover effects). We assume that each alternative is
described by a set of attributes and that the state of
each attribute describing an outcome is known with
certainty. Thus, we will speak interchangeably of
evaluating an alternative and evaluating its associated
outcome. Finally, we assume that overall evaluations
of alternatives can be represented by a compensatory
evaluation model, such as an additive or multiplicative multiattribute value or utility model. Previous
research indicates that compensatory value models
provide a very good approximation to ratings of
individual evaluative alternatives, especially in situations where alternatives are described by only two or
three attributes (e.g., Fischer 1976). We do not assume
that decision makers explicitly use a compensatory
evaluative model to calculate the overall value of a
multiattribute alternative. We only assume that they
form an impression of the alternative with respect to
each attribute, then subjectively combine these values
in arriving at an overall judgment of value.
The standard way to represent preference uncertainty in multiattribute preference models is with an
additive error model-that is, a standard multiattribute
value or utility model with a random (error) component added on at the end (e.g., Laskey and Fischer
1987). This method explicitly assumes that error is
independent of the attributes describing an alternative, and thus independent of attribute extremity and
conflict. Because we believe that preference uncertainty is affected by item characteristics such as attribute conflict and extremity, we do not believe that

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

<-----Page 4----->FISCHER, LUCE, AND JIA
AttribtnteConflict and PreferenceUncertainty

preference uncertainty is fully represented by a standard additive error model.
In order to clarify our current approach, it will be
helpful to briefly review one of the random multiattribute utility (RandMAU) models of preference uncertainty that Fischer et al. develop in more detail in a
companion paper (Fischer et al. 2000). Our RandMAU
models represent item-specific sources of preference
uncertainty by introducing random variation in the
parameters of a standard multiattribute value model.
For example, an additive RandMAU model whose
single-attribute utility functions can be represented as
simple power functions is given by
U(X,.

..

,

x11)= E wiui(xi)

=

E

ixi7

Here the levels of an attribute Xi are rescaled from 0 to
1, ensuring that the resultant single attribute values
represented by xi range from 0 to 1. The a i > 0 are
curvature parameters reflecting the degree of concavity or convexity of the single-attribute value functions,
that is determining the mapping from the scaled
attribute value xi to the single-attribute utility value
ui(xi). Random variation in the curvature parameter
for each single-attribute value function represents
uncertainty regarding how to encode and evaluate
outcomes with respect to a single value attribute. The
attribute weighting factors o, ... w1 are random
variables that satisfy both 0 < wi < 1 and Eiwi = 1,
as the standard normalization of an additive utility
function requires. Random variation in these attribute
weight parameters represents uncertainty regarding
the relative importance of different value relevant
attributes. Such uncertainty primarily affects the final
value
stage of the evaluation process-integrating
across attributes. Together, random variation in the
curvature parameters and weights induce preference
uncertainty (random variation) regarding overall evaluations of judgment alternatives.
In effect, this additive RandMAU model formalizes
and extends Roger Shepard's (1964) notion that implicit attribute weights vary substantially over the
time course of a multiattribute decision process. Thus,
our modeling approach represents different frames of
mind regarding attribute tradeoffs and valuations by
**

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

allowing preference-summarizing parameters (attribute weights and curvature parameters) to vary
randomly. In the discussion that follows, we develop
qualitative implications of this general view of the
preference generating process. Our discussion does
not depend on the quantitative details of RandMAU
models, but the intuitions developed below can also
be demonstrated in simulations based on the model
(Fischer et al. 2000).1
The Attribute Conflict Hypothesis
The notion of conflict among objectives-or among
the attributes representing them-lies at the center of
both normative (e.g., Keeney and Raiffa 1976) and
behavioral theories of multiattribute choice (e.g., Tversky et al. 1988). It is widely accepted that conflict
among objectives makes it more difficult to establish
one's preferences.
For our purposes, it is useful to distinguish between
two different types of attribute conflict. Betweenalternative conflict arises in cases where one must
choose between two options where one option is
superior in some respects but the other option is
superior in others. Thus, between-alternative conflict
arises only in choice or other comparative tasks.
Within-alternative conflict arises in cases where one
must form an evaluation of an alternative that is good
in some respects but bad in others. Although withinalternative conflict is a basic feature of both judgment
and choice tasks, it has not received much attention in
behavioral decision research. Our primary focus in
this article is the neglected issue of within-alternative
conflict. However, because most past research on
attribute conflict has focused on between-alternative
conflict, we briefly review some essential findings
from that research.
Between-Alternative
Conflict. Conflict has frequently been defined as the presence of competing
response tendencies that arise when only one of several
alternatives, each satisfying different goals, must be
1In that companion paper, we rely mainly on a somewhat more
complex version of the RandMAU model in which value aggregates
multiplicatively across attributes. However, the hypotheses developed below follow from both additive and multiplicative versions
of the model.

91

<-----Page 5----->FISCHER, LUCE, AND JIA
Attribute Con1flictand PreferenceUncertainty

chosen (e.g., Coombs and Avrunin 1976, Lewin 1951).
For instance, suppose that a young woman is buying a
car and has narrowed her choice to two cars, A and B.
Her research indicates that Car A is generally superior
to Car B in terms of performance, safety, reliability,
and comfort. However, Car B is less expensive to
purchase and maintain. If cost is an important consideration, the decision maker is likely to feel uncertain
about her choice. It depends on the relative weight she
attaches to the conflicting attributes. Greater emphasis
on cost favors Car B, whereas greater emphasis on
quality attributes favors Car A. Thus, a cognitive
analysis of conflict focuses on the relative weighting of
competing attributes (Fischer and Hawkins 1993, Fischer et al. 1999, Payne et al. 1993, Tversky et al. 1988).
Whichever choice she makes, the decision maker in
our example is likely to experience feelings of regret
(Bell 1982, Loomes and Sugden 1982), or emotional loss
associated with giving up the positive features of the
foregone alternative (Festinger 1957, Hogarth 1987,
Janis and Mann 1977, Shepard 1964).
Past research on the effects of between-alternative
conflict has frequently supported the hypothesis that
conflict is linked to increased response time in choice
(Bettman et al. 1993, Hansen 1972). Kiesler (1966)
actually defined conflict as increased decision deliberation time (choice time minus time to scan alternatives), although this operationalization of conflict was
controversial (Berlyne 1966). Recent work on emotional consequences of conflict has linked the negative
affect generated by conflict with the tendency to
prolong decision search times (Luce 1998, Tversky and
Shafir 1992). Thus, greater response times may be one
manifestation of the preference uncertainty generated
by high-conflict situations.
Within-Alternative Conflict. Unlike most past research on attribute conflict, the research reported in
this article focuses on conflict among the attributes of
a single evaluative alternative. To illustrate, consider
the car example begun earlier. Suppose that Car A
costs more than the decision maker can afford, so Car
B is her preferred option. However, although Car B is
both affordable and better than average in most respects, its crashworthiness and reliability are both
below average. Consequently, she is uncertain about

92

whether Car B is desirable and whether she should go
ahead with the purchase. In short, she is experiencing
the cognitive and emotional state of ambivalence, resulting from conflict among the attributes of a single
alternative. Such ambivalence is especially relevant in
cases where some attributes are good and others bad
in comparison to a neutral reference outcome (in this
case, the average car).
Little is known about the consequences of withinalternative conflict on judgment. However, the logic
underlying the RandMAU model leads to the following:
CONFLICT HYPOTHESIS. High conflict
attributes
of a decision alternative leads to
among the
greater uncertainty regardingthe overall evaluation of that
alternative.
ATTRIBUTE

This hypothesis concerns the final cognitive process
involved in evaluating alternatives, integrating value
across attributes. Consider a decision maker attempting to evaluate an alternative described by two attributes that are highly discrepant-i.e., the alternative
is very good in one respect but very bad in another.
Using the terminology of the RandMAU model introduced above, suppose that u,(x,) = 0.8 whereas
u2(x2) = 0.2. In this high-conflict case, random variation in the weighting parameters will have a major
impact on overall evaluation. If w1 = 0.7, then the
overall value is 0.62 whereas if w= 0.3, the overall
value is only 0.38. Now consider a low-conflict alternative for which u,(x,) = 0.55 and u2(x2) = 0.45. In
this case, if w1 = 0.7, then the overall value is 0.52
whereas if w1 = 0.3, the overall value only falls to 0.48.
In short, a given fluctuation in weights has much
greater impact on overall evaluations when attribute
conflict is high rather than low. Thus, we predict that
high conflict among the attributes of an evaluative
alternative leads to greater uncertainty regarding the
overall evaluation of that alternative.
The Attribute Extremity Hypothesis
Of course, an individual must form a judgment about
single-attribute values before combining these into an
overall valuation. Uncertainty about single-attribute
values also contributes to uncertainty about overall
evaluations. We address this in the following:

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

<-----Page 6----->FISCHER, LUCE, AND JIA
AttribtuteConiflictand PreferenceUncertainty

Extreme levels
of an attribute tend to be easier to evaluate than intermediate levels and thus are less prone to preference uncertainty.
ATTRIBUTE EXTREMITY HYPOTHESIS.

To clarify the logic behind this prediction, consider
a task whose instructions specify that the worst possible level of an attribute is to be assigned a value of 0
and the best possible level a value of 1. The respondent's task is to evaluate intermediate levels of the
attribute by assigning each a value between 0 and 1.
Under these circumstances, extreme levels of the
attribute-those that are either very good or very
bad-are easier to encodeand evaluate than intermediate
levels. As one approaches 0 at the lower extreme, or 1
at the upper extreme, there is less and less ambiguity
about the value of the attribute level, and thus less
room for error. In the middle, though, things become
increasingly ambiguous.
For example, suppose that a person has to compare
three medical outcomes: at the lower extreme, a life of
great pain; at the upper extreme, a life with no pain; in
the middle, a life with moderate pain. The worst and
best cases are easy to judge. The middle case is not. Is
moderate pain more like extreme pain (indicating a
convex underlying single-attribute utility function) or
more like no pain (indicating a concave underlying
function)? It depends on the frame of mind one
adopts. Thus, we predict that intermediate levels of an
attribute will evoke greater preference uncertainty.
Like our conflict hypothesis, this somewhat counterintuitive prediction is consistent with the RandMAU
models developed in Fischer et al. (2000). For example,
suppose that attribute Xi is at or near its upper bound,
so that the re-scaled attribute value, xi, is close to 1.0.
In that case, the single-attribute valuation of this
outcome will not depend much on the degree of
curvature of the value function; ui(xi) will be close to
1.0 regardless of the value of the curvature parameter
ai. For example, if xi = 0.95, then ui(xi) = 0.975
when ai =0.5, and ui(xi) = 0.926 when ai = 1.5.
Similarly, if Xi is at or near its lower bound of 0, then
uj(xi) will be close to 0 regardless of the value of the
curvature parameter ai. However, if Xi is in the
middle of its range (i.e., xi is close to 0.5), changes in
the curvature parameter ai can have a big effect on

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

= 0.5, then ui(xi)
0.707
=
In
=
1.5.
when
0.354
when axi 0.5 but ui(xi)
ai
short, the same degree of variation in the curvature
parameter has much greater impact on estimates of
single-attribute utility for intermediate attribute values than for extreme ones.
This prediction may seem to be an artifact of the
presence of upper and lower scale boundaries. However, such boundaries are frequently present in real
life as well as in psychological scaling methods. There
are two reasons why this is the case. First, the frames
of reference that people bring to problems include
extreme values that place bounds on judgment. In
many cases, the frame of reference provided by past
experience makes it quite natural to think of an
outcome as being very good or very bad with respect
to individual outcome attributes (e.g., very good acceleration, very poor braking). Second, task constraints and task logic frequently impose upper and
lower bounds on evaluations. Lottery procedures for
assessing utility naturally bound utility judgments
between 0 and 1. Willingness-to-pay for a good is
naturally constrained by $0 at the lower extreme
(assuming the seller will not pay to be rid of it) and by
one's total wealth, or current budget for consumption,
on the upper. In a similar fashion, ratings of preference or attractiveness are naturally constrained when
an upper bound value (e.g., 1.0) is assigned to the best
level of the attribute in question and a lower bound
(e.g., 0) to the worst level of the attribute.
uj(xi). For example, if xi

Effort-Accuracy Tradeoffs
Our assumption that response times and response
errors are positively correlated may at first glance
appear to contradict Payne et al.'s (1993) effortaccuracy model. According to their model, people
choose among judgment strategies that differ both in
the effort required to implement them and the accuracy (quality) of resulting judgments and decisions.
High effort strategies lead to both longer response
times and greater decision accuracy. Payne et al.
(1993) operationalize decision accuracy as the gap
between indicated and expected utility-maximizing
choices (a systematic rather than random error). However, one might also expect random judgment error
(our operationalization of error) to be associated with

93

<-----Page 7----->FISCHER, LUCE, AND JIA
AttribtnteConflict and PreferenceUncertainty

greater deviations from utility-maximization (Payne et
al.'s operationalization). Thus, it may appear that the
effort-accuracy framework predicts that response time
and random response error should be negatively
correlated. The contradiction between our prediction
and theirs is apparent, not real, however. Our prediction concerns item-level relationships. Difficult items
(i.e., those characterized by greater within-alternative
conflict or less within-attribute extremity) require
more time and evoke more error. The effort-accuracy
predictions concern strategy-level effects. These effects can be observable within an individual. For
instance, decision makers may shift to less normatively accurate (but more simplified and therefore
faster) decision strategies in reaction to increasing
problem size. However, our experiments will confront
individuals with judgment tasks that are relatively
stable in terms of most important task and context
characteristics (e.g., time pressure, problem size). In
this relatively constrained task environment, we expect effort-accuracy tradeoffs to be observable primarily across participants. In particular, those individuals
who choose to adopt more effortful strategies will take
more time but be more reliable (less error-prone).
Thus, it is possible to observe the preference uncertainty effects that we predict at the item level (i.e., a
positive correlation between response time and error)
as well as the effort-accuracy effects that Payne et al.
predict at the participant level (i.e., a negative correlation between average response time and average
response error). We will investigate this further in
analyzing the results of our study.
Summary
We have identified two indicators of preference
time and response error. We
uncertainty-response
have also identified two potential sources of preference uncertainty, associated with the cognitive processes underlying evaluations of multiattribute decision alternatives. First, the greater the degree of
conflict among the attributes of an alternative, the
greater the uncertainty in judging the overall value.
Second, extreme levels of an attribute are easier to
evaluate than intermediate ones, and thus less prone
to preference uncertainty. Other factors, such as the
ease of encoding an attribute value, may also influence

94

preference uncertainty (e.g., Johnson et al. 1988). However, our research will focus on conflict and extremity,
using standard preference-assessment paradigms that
leave little or no ambiguity about attribute levels.

A Preference Assessment
Experiment
In this section, we describe a behavioral experiment
that tests the attribute conflict and attribute extremity
hypotheses. Students evaluated individually presented course descriptions characterized by three attributes: interest of subject matter, teaching quality,
and average course grade.
Method
Participants and Procedure. Twenty-two undergraduate students at Duke University individually
completed this experiment in return for course credit.
The experimental instructions and stimuli were delivered via the Mouselab software program (Payne et al.
1993). For each stimulus, three pieces of attribute
information were hidden in labeled boxes. Participants used a mouse-controlled cursor to open boxes to
search for information. They evaluated each alternative by using the mouse to drag a sliding pointer
across a continuous scale, whose endpoints were
labeled with descriptions of the best and worst possible alternatives. (No numerical values were shown on
the scale.) The Mouselab program presented stimuli in
a random order and recorded the total response time
for each judgment to an accuracy of 1/60th of a
second.
After receiving instructions regarding the Mouselab
program and the judgment task, participants were
asked to complete two practice judgments. Then, they
were told that the experimental task was beginning.
The initial five judgments were filler items (not labeled
as such), intended to minimize the degree to which
time and error ratings included variance associated
with acclimating to the task environment. Next, participants judged the 20 experimental stimuli once,
presented in an individually randomized order for
each participant, and were then encouraged to take a
break. After the break, participants evaluated three
filler items (not labeled as such) which were intended

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

<-----Page 8----->FISCHER, LUCE, AND JIA
AttribatteConflict and PreferenceUncertainty

to capture any re-acclimation to the task following the
break. These fillers were followed by the 20 experimental stimuli, presented in a different individually
randomized order. After making these judgments,
participants made a series of choices which were
related to a different research project and which will
not be discussed further. Finally, participants answered one manipulation check question (described
below) on the computer screen, then were directed to
a paper and pencil questionnaire collecting their importance weights for the three attributes they had
considered.
Task and Attributes. The basic experimental task
was to make a series of judgments regarding the
attractiveness of elective college courses, where attractiveness was defined as "what makes you decide to
take one elective course over another." Although these
choices were hypothetical, we believe that the subject
matter was meaningful and realistic for our student
participants.
Elective courses were described in terms of three
five-level attributes. The first was the participant's
degree of interest in the subject matter of the course,
with each course scored as: Very Low, Low, Average,
High, or Very High. Participants were asked to consider their own goals and preferences, then to generate
examples of courses that would correspond to the best
(Very High), the worst (Very Low), and middle (Average) levels of this attribute. They recorded these
reference courses on a piece of paper available for
reference during judgment tasks. The second attribute
was expected teaching quality, described as the instructor's score on the previous year's student evaluations,
categorized into one of five categories: Very Poor,
Poor, Average, Good, or Very Good. The third attribute was the averagegrade to be earned by students
in the course; the levels of this attribute were 2.8, 2.9,
3.0, 3.1, and 3.2. We instructed participants that the
courses they would be rating were average with
respect to all attributes other than these three.
Stimulus Design. Our primary objective in constructing the set of judgment stimuli was to manipulate the degree of conflict among the attributes describing each alternative. With three attributes, each

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

assuming five levels, there were 125 possible alternatives. We operationalized the degree of conflict among
the attributes of each alternative as the standard
deviation of the attribute levels of the alternative,
where each attribute level was assigned an integer
value between 1 and 5 (from worst to best). With this
approach, each of the 125 possible stimuli received
one of the following conflict scores: 0.0, 0.47, 0.82, 0.94,
1.25, 1.41, 1.63, 1.69, and 1.89. In order to cover the
range of conflict levels and to allow several alternatives per conflict level, we chose to sample from
stimuli with standard deviations of 0.0, 0.47, 0.94, 1.41,
and 1.89. We chose four stimuli from each of these five
conflict levels, with the following exceptions. For the
lowest level of conflict, only three stimuli other than
the most and least attractive alternatives exist.2 Thus,
we used all three stimuli from the first conflict level
and sampled five stimuli from the second level.
A second essential consideration in constructing
stimuli was the attribute extremity of each stimulus,
which we defined as the average absolute deviation of
its attributes' levels from 3, the mid-level of each

attribute;i.e., Extremity =

3

li=1 3

Xi

-

3 1, where xi

is the level of attribute i, which may assume values
from 1 to 5. With three attributes, the possible extremity levels for our stimuli were 0.00, 0.33, 0.67, 1.00,
1.33, 1.67, and 2.00. The ideal stimulus design would
cover each possible range of extremity within each
conflict level. Unfortunately, extremity and conflict
are naturally confounded, because more extreme levels of conflict are possible only with higher levels of
extremity (see Table 1). The product-moment correlation between conflict and extremity was 0.70 for the 20
experimental stimuli. Because we predict that attribute conflict and attribute extremity will have opposite effects on preference uncertainty, it is still
possible to use statistical methods to disentangle the
effects of these two conceptually distinct constructs.
A final consideration in designing stimuli was to be
sure that conflict and extremity levels were independent of the overall attractiveness of each stimulus.
Overall value (attractiveness) was estimated using
2 It is pointless to evaluate the most and least
attractive stimuli
because they anchor the scale.

95

<-----Page 9----->FISCHER, LUCE, AND JIA
AttributteConflict and PreferenceUncertainty

Table 1

ExperimentalStimuli and Average Data
StimulusCharacteristics

Stimulus

X

Y

Z

Conflict

Experimental
Results
Value

Extremity

AvgRate

AvgError

AvgRT

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

2
3
4
1
2
2
3
4
1
2
3
3
1
2
4
5
1
1
5
5

2
3
4
1
1
3
4
5
1
2
5
5
4
5
4
5
1
5
1
5

2
3
4
2
1
3
4
5
3
4
3
5
1
2
1
2
5
1
1
1

0.00
0.00
0.00
0.47
0.47
0.47
0.47
0.47
0.94
0.94
0.94
0.94
1.41
1.41
1.41
1.41
1.89
1.89
1.89
1.89

1.00
0.00
1.00
1.67
1.67
0.33
0.67
1.67
1.33
1.00
0.67
1.33
1.67
1.33
1.33
1.67
2.00
2.00
2.00
2.00

2.0
3.0
4.0
1.2
1.5
2.5
3.5
4.5
1.4
2.4
3.6
4.0
1.9
2.9
3.4
4.4
1.8
2.2
3.0
4.2

0.13
0.48
0.76
0.05
0.07
0.26
0.66
0.88
0.09
0.20
0.69
0.78
0.22
0.32
0.62
0.82
0.17
0.26
0.32
0.79

0.050
0.041
0.076
0.037
0.037
0.076
0.086
0.049
0.040
0.070
0.065
0.068
0.088
0.068
0.081
0.081
0.053
0.104
0.081
0.064

10.53
12.20
12.05
11.36
12.20
12.99
13.51
11.63
11.11
13.16
12.50
12.66
13.51
13.16
14.71
12.20
11.63
12.82
13.33
12.82

Mean
St Dev

2.7
1.4

3.3
1.6

2.7
1.4

0.97
0.64

1.32
0.56

2.87
1.03

0.43
0.28

0.066
0.019

12.50
0.95

the meanabsolutedeviationof
Note.X, Y,and Z denotethe levelsof the threeattributes.Conflictdenotesthe standarddeviationof the attributelevels, Extremity
each attributelevelfrom3, and Valuethe weightedsum of the attributelevels, based on a pretest.AvgRateindicatesthe averageratingof each stimulus(on a 0-1
the meanabsolutedeviationof time 1-time 2 responses,and AvgRTthe averageresponsetime forthe stimulus(inseconds).AvgRTwas calculated
scale), AvgError
by takingthe inverseof the mean negative-reciprocal
responsetime.

importance weights from a pretest (N = 18) in which
participants were asked to indicate the importance of
several aspects of courses. This pretest indicated that
the relative weights for the three attributes used in this
study would be approximately 0.5 (for interest), 0.3
(for instructor), and 0.2 (for grade). Using these pretest
data, stimuli were classified into four levels of overall
value, and one alternative was selected from each
level of value for each of the five conflict levels. There
were some minor constraints on the degree to which
we could vary value independently of attribute conflict and extremity. For instance, the highest possible
value score varies across levels of conflict. However,
the correlation between estimated value and conflict
was only 0.03 in our stimulus set. Extremity and
estimated value were correlated -0.10, because items

96

with low extremity levels are necessarily restricted in
terms of their possible value. Table 1 lists the attribute
levels of the 20 stimuli used in our experiment, as well
as the conflict, extremity, and estimated value scores
for each stimulus.
Measures
Preference Ratings. Each stimulus appeared on a
computer screen with the heading "Please Rate," at
the top of the screen, the three labeled attribute values
in the middle of the screen, and the rating scale at the
bottom. The lower end of the rating scale was anchored by a verbal description of the worst alternative
("interest level is Very Low, average grade is 2.8,
teaching quality is Very Poor"). The upper end of the
scale was anchored by a description of the best possi-

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January2000

<-----Page 10----->FISCHER, LUCE, AND JIA
Attribute Conflict and PreferenceUncertainty

ble alternative ("interest level is Very High, average
grade is 3.2, teaching quality is Very Good"). Participants used this scale to evaluate other course descriptions by moving a slider along the continuous scale
using an optical mouse. No numerical scale values
were displayed. We converted all responses to a 0 to 1
scale, assuming a linear mapping of pointer locations
to scale values.
Preference Uncertainty Measures. To measure an
individual participant's response times, we first averaged the response times for the two presentations of
each stimulus. This yielded 20 response times for each
participant, one for each stimulus. Because the distribution of these response times was strongly skewed to
the right, we applied a negative reciprocal transformatime) to the average time each
tion (-1/response
participant took in responding to each item. This
resulted in a more normal distribution.3 All statistical
tests were performed on these transformed response
times. However, to facilitate interpretation of the
results, descriptive statistics are presented as response
times measured in seconds. These were computed by
reversing the negative reciprocal transformation. To
measure response error, we calculated the absolute
value of the difference between a stimulus' attractiveness ratings at time 1 and time 2. We will use the terms
"response error" and "response inconsistency" interchangeably when referring to this measure.
Our primary interest was in how attribute conflict
and other stimulus properties affect preference uncertainty, not in individual differences in response tendencies. To control for such individual differences in
our statistical analyses, we used individual participants' average error or average response time scores
as covariates. Adding these covariates to our models
allows more precise inferences regarding the effects of
attribute conflict and extremity.
Auxiliary Variables. At the end of the Mouselab
program, participants were asked to indicate their
preferences for differing levels of average course
3 The negative reciprocal transformation is commonly applied to
response time data. Because a simple reciprocal transformation is a
decreasingfunction of response time, the negative reciprocal is used
to preserve the ordering of response times.

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

grades by responding to a scale reflecting either a
lower grade is better (0), the participant is unsure (5),
or a higher grade is better (10). This manipulation
check question was used to test our assumption that
higher average grades are more preferred. The average response on this scale was 8.22 (standard deviation = 2.48), supporting our assumption.
Each participant's final task was to complete a paper
and pencil questionnaire eliciting attribute importance
weights. Participants were instructed to assign 100
points to the most important attribute, and then to
assign points to the two remaining attributes relative
to this value of 100 points. These importance weights
were normalized to sum to 1.0. The average weights
were somewhat different from those in our pre-test
(0.38 for interest, 0.41 for teaching quality, and 0.21 for
average grade). We used these participant-assessed
weights to compute participant-specific values for
each stimulus. In calculating these values, the normalized weights for the participant were multiplied by the
respective three attribute values (each scored as 1
through 5). These individual-level value scores correlated 0.04 with conflict and -0.11 with extremity.
These correlations indicate that we were successful in
manipulating attribute conflict and attribute extremity
independently of the overall value of outcomes.
Results
There are three levels of analysis at which we test
predictions in this experiment. The first is the response level, in which an individual participant evaluates one alternative twice, in the two replications.
At this level, our data will consist of either an
absolute error score (the difference between the two
evaluations) or an average negative reciprocal response time for the two judgments of each stimulus.
Second, we can analyze item (or stimulus) level data
regarding the average negative reciprocal response
times and absolute error scores for each of the 20
stimuli (alternatives) used in the experiment. This
level of analysis pools data across participants.
Third, we can analyze participant level data regarding the average negative reciprocal response times
and average absolute error scores for each participant. This level of analysis pools across the 20
stimuli evaluated twice by each participant.

97

<-----Page 11----->FISCHER, LUCE, AND JIA
Attribute Conflict and PreferenceUncertainty

Relationship Between Response Time and Response Error. Our analysis assumes that response
time and response error both reflect the same underlying construct-preference
uncertainty. Thus, we
predict a positive correlation between response time
and response error. Because we expect preference
uncertainty to be driven by stimulus characteristics
(conflict, extremity), we expect this relationship to be
evident at both the item level (i.e., average response
time and error for each stimulus) and the response
level (individual participants' response times and error scores for individual stimuli).4
At the item level, the correlation between negative
reciprocal response time and average absolute response error was strong and highly significant5 (r
= 0.69, p < 0.001, N = 20). Calculating the correlation at the response level is more complex, because
participants may differ substantially in terms of both
response time and response error. To control for such
individual differences, we used a z-transformation to
standardize each participant's response time and response error scores for the 20 alternatives. The resulting response-level correlation between negative reciprocal response time and average absolute response
error was more modest but still highly significant (r
= 0.18, p = 0.003, N = 440). In short, we found clear
evidence that higher response error was associated
with longer response times at both the item and
response levels. These correlations are consistent with
our assumption that response time and response error
both reflect underlying preference uncertainty.
Effects of Attribute Conflict and Extremity on
Response Time. Our theoretical analysis led us to
predict that greater attribute conflict should lead to
longer response times whereas greater attribute extremity should lead to shorter response times. Because
the two independent variables, attribute conflict and
4Our assumption that response time and error both reflect preference uncertainty does not imply that we will find a positive
correlation between the two at the participant level. At this level,
participant differences in effort-accuracy tradeoffs could well lead
to a negative correlation; those who employ more effortful strategies
will be less prone to response error.
5 Because our hypotheses involve clear directional predictions, we
report one-tailed tests, unless otherwise noted.

98

attribute extremity, are inherently positively correlated, we used multiple regression methods to test
these predictions simultaneously. To control for individual differences in these analyses, we included a
participant-level covariate for average negative reciprocal response time.6
Model 1 in Table 2 summarizes the relevant regression results. Our predictions were strongly supported.
First, greater attribute conflict led to longer response
times (p < 0.001). Second, greater attribute extremity
led to shorter response times (p = 0.001). We ran
additional regressions, not reported in the table, to test
for the presence of quadratic effects for either conflict
or extremity. Once linear effects were included in the
model, quadratic terms did not significantly improve
the model fit (for attribute conflict, p = 0.24, for
attribute extremity, p = 0.39).
To obtain a better sense of the relative effect sizes,
we used the regression results to calculate the predicted impact of attribute conflict and extremity on
response time. Consistent with the relative magnitudes of the standardized regression coefficients, estimated regression effects were roughly equal in magnitude for attribute conflict and attribute extremity. As
conflict moved from its lowest to highest levels, predicted average response times (converted from negative reciprocals back into seconds) increased from 11.6
seconds to 14.3 seconds. Similarly, as extremity moved
from its lowest to highest levels, estimated average
response times decreased from 14.1 seconds to 11.8
seconds. In short, these response time results provide
strong support for both our attribute conflict and
attribute extremity hypotheses.

6
The average scores used as covariates were calculated from
responses over all 20 stimuli. These covariate analyses are conceptually similar to using error and time measures standardized
separately for each participant. The covariate analyses are also
conceptually similar to standard within-subjects ANOVA methods,
with the average error and average response time covariates representing a (more meaningful) substitute for a multi-level factor
accounting for subject variance. The results of analyses using these
covariates were identical (in their implications) to both analyses of
standardized data and analyses using within-subjects ANOVA
methods.

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

<-----Page 12----->FISCHER, LUCE, AND JIA
AttributteConflict and PreferenceUncertainty

Table 2

StandardizedMultipleRegression Models of the Effects of AttributeConflict,AttributeExtremity,and OutcomeValue on Response Time and
Response Error
DependentVariable
1. NegRecRT

Independent
Variable

2. AbsoluteError

4. AbsoluteError

3. NegRecipRT

f

t

p

t

f

t

ft

Conflict
Extremity
OutcomeValue

0.203
-0.148

4.37***
- 3.19***

0.209
-0.117

3.29***
- 1.83*

0.194
-0.136
0.074

4.19***
- 2.91**
2.21*

0.200
-0.103
0.082

3.13***
- 1.61t
1.79t

AvgNegRecRT
AvgAbErr

0.706

21.28***

0.705

21.33***

0.274

6.03***

0.272

5.98***

AdjustedR2

0.517

0.092

0.521

0.097

Note.AvgNegRecRT
and AvgAbErr
are covariatescontrolling
for between-participant
differencesin responsetime and responseerror.Becauseour modelinvolves
cleardirectionalpredictions,all p-levelsare one-tailed,exceptthose forthe outcomevalueterm,forwhichwe madeno predictions.Eachregressionequationis based
on 440 observations.
***p < 0.001,**p < 0.01, *p 0.05 tp < 0.10.

Effects of Attribute Conflict and Attribute Extremity on Response Error. Our theoretical analysis also
led us to predict that greater attribute conflict should
lead to greater response error, whereas greater attribute extremity should lead to less response error.
Because attribute conflict and attribute extremity are
inherently positively correlated, we again used multiple regression methods to test these predictions simultaneously. To control for individual differences in
these analyses, we included participant-level covariates for average absolute error. Model 2 in Table 2
summarizes the regression analyses testing these predictions. Both were supported. Greater attribute conflict led to greater response errors (p = 0.001) while
greater attribute extremity led to smaller response
errors (p = 0.034). Additional tests showed no evidence of quadratic effects. Once the linear effects were
included in the model, quadratic terms did not significantly improve the model fit (for attribute conflict, p
0.71, for attribute extremity, p = 0.23).
To obtain a better sense of the relative effect sizes,
we used the regression results to calculate the predicted impact of attribute conflict and extremity on
response errors. Moving attribute conflict from its
lowest to highest level increased the predicted average
absolute error score from 0.049 to 0.091 (on a 0 to 1

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

scale). Moving attribute extremity from its lowest to
highest levels decreased the predicted average absolute error score from 0.084 to 0.056. Thus, the estimated effect of attribute conflict was roughly 50%
larger than that of attribute extremity. In short, the
response error data, like the response time data,
provide support for our attribute conflict and attribute
extremity hypotheses.
Finally, note that the overall R2 is higher for the
response time analysis (Model 1 in Table 2) than for
the response error analysis (Model 2). This difference
is largely due to larger systematic individual differences in response time than in response error. Thus,
the individual difference covariate explains more variance in response times than in response errors. The
standardized regression coefficients for the stimulus
properties of interest here (attribute conflict and extremity) are actually very similar in magnitude for the
two dependent measures.
Alternative Value Effects on Response Time and
Response Error. Our main theoretical emphasis has
been on the effects of attribute conflict and attribute
extremity on response time and response error. In this
section, we address the question of whether response
times and response errors also depend on overall

99

<-----Page 13----->FISCHER, LUCE, AND JIA
Attribute Coniflictand PreferenceUncertainty

evaluations of outcomes. For instance, one can imagine that it is easier to judge that an outcome is low in
value (because it has at least one bad feature) than it is
to judge that an outcome is high in value. On the other
hand, it might be easier to judge positive outcomesit's only good if it is perfect, but degrees of bad are
harder to assess. Thus, a qualitative analysis provides
reasons to expect that the effect might be in either
direction.
To investigate this question, we conducted an additional set of multiple regression analyses to assess
value effects while controlling for possible effects of
attribute conflict and attribute extremity (see Models 3
and 4 in Table 2). For the negative-reciprocal, response-time dependent variable, higher value outcomes were associated with significantly longer response times. The effects of attribute conflict and
extremity also remained highly significant. Using
standardized ,Bs as a measure of effect strength, the
value effect was roughly I as great as the extremity
effect and - as great as the conflict effect. Similarly, the
estimated regression effects indicate that alternative
value had a relatively small effect on response time.
As the value of an outcome increased from 0 to 1,
predicted response time increased from 12.4 to 13.4
seconds.
For the absolute error dependent variable, higher
value outcomes were associated with marginally significantly greater response error. The attribute conflict
effect remained highly significant, and the extremity
effect also remained strong (though it fell below the
traditional 0.05 significance level). Using standardized
,Bsas a measure of effect strength, the value effect was
about 4 as strong as the extremity effect and 5 as strong
as the attribute conflict effect. The estimated impact of
value on absolute response error indicates that as the
value of an outcome increased from 0 to 1, predicted
response error increased from 0.062 to 0.080. Additional tests, not reported in the table, indicated that
value did not interact with conflict or extremity for
either the response time or response error models.
In short, in the present context, outcomes that evoke
more positive evaluations also evoke greater preference uncertainty, as measured both by response error
and response time. Hereafter, we refer to this as the

100

alternative value effect. This effect is smaller than the
attribute conflict and extremity effects that we also
observed. Nevertheless, it persists even in regression
models controlling for these other effects. While this
alternative value effect was not predicted by our
earlier theoretical analysis of stimulus characteristics,
the effect can be explained by the quantitative RandMAU model presented in Fischer et al. (2000).
Participant Differences in Effort-AccuracyTradeoffs.
As noted earlier, the results of this experiment
supported our assumption that response times and
response errors are both manifestations of preference uncertainty. For instance, the two measures
were strongly positively correlated at the item level
(r = 0.69). As noted in the Introduction, however,
this positive correlation might appear to contradict
Payne et al.'s (1993) hypothesis that increasingly
effortful decision strategies lead to more accurate
decisions (i.e., less response error) but also to longer
response times. Note, however, that we predict a
positive correlation between response error and
time at the item level of analysis, while the effortaccuracy model predicts a negative correlation between measures at the strategy or participant level.
These predictions are different but not necessarily
contradictory.
Suppose for example that a person chooses a highaccuracy, high-effort strategy. On the average, this
person should make relatively small errors. Nevertheless, the framework we have proposed here predicts
that this person will take longer and make larger
errors when evaluating high conflict than low conflict
alternatives. Now consider a second individual who
chooses a low-accuracy, low-effort strategy. On the
average, this person should make relatively large
errors. Nevertheless, our framework predicts that this
person will also take longer and make larger errors
when evaluating high conflict than low conflict alternatives. In short, participant-level strategy effects can
co-exist with response-level, preference uncertainty
effects.
One can test the two models simultaneously by
predicting response-level errors using both participantlevel and response-level measures of response time.
Table 3 displays the results of two multiple regression

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

<-----Page 14----->FISCHER, LUCE, AND JIA
AttribntteConflict and PreferenceUncertainty

Table 3

StandardizedMultipleRegression Models of the EffortAccuracyTradeoffHypothesis
DependentVariable
AbsoluteError

AbsoluteError

t

t

Independent
Variable
NegRecRT
AvgNegRecRT
AvgAbErr
AdjustedR2

0.242
-0.219

0.027

3.63***
- 3.29***

0.242
-0.171
0.274

3.77***
- 2.64**
5.96***

0.098

Note. AvgNegRecRT
and AvgAbErr
are covariatescontrollingfor betweendifferencesin responsetime and responseerror.Becauseourmodel
participant
involvescleardirectionalpredictions,all p-levels are one-tailed.Eachregression
is based on 440 observations.
***p s 0.001, **p ? 0.01.

analyses. In the first, we predicted response-level
absolute errors (AbErr) using both a response-level response-time measure (NegRecRT) and a participantlevel response-time measure (AvgNegRecRT, which reflects the average response time of a participant, pooling
across all 20 stimuli). Because we are controlling for
participant-level effects with the AvgNegRecRT measure, we expect that the response-level NegRecRT measure will primarily reflect the difficulty that a participant
has in evaluating a particular item. Thus, this measure
reflects preference uncertainty about a particular alternative and its coefficient should be positive. In contrast,
the participant-level AvgNegRecRT measure should reflect participant-level strategy choices based on effortaccuracy considerations. Thus, we predict that its coefficient will be negative. Those participants who engage in
more effortful (time-consuming) strategies should make
smaller response errors.7 The results of the analysis
strongly support both predictions. Both regression coefficients have the predicted sign and both are significant
at beyond the p = 0.001 level.
The second analysis adds a second participant-level
measure, AvgAbErr, which controls for participant
'In making this prediction, we assume that participants do not
differ substantially in expertise or other factors that might lead to
participant differences in preference uncertainty.

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

differences in response error, apart from those captured by the AvgNegRecRT measure. These might
relate to differences in expertise, for example. The
coefficient for this variable should be positive. As
before, we predict that the coefficient for NegRecRT
will be positive (preference uncertainty) while that for
AvgNegRecRT will be negative (effort-accuracy).
Again, the results strongly support both the preference uncertainty and effort-accuracy predictions. All
three coefficients have the predicted signs and all
three are highly significant. In short, we find strong
support for both our preference-uncertainty hypotheses and the Payne et al. effort-accuracy hypothesis.
These results are consistent with our argument that
the preference uncertainty effects arise at the item or
response level, because of differences in item difficulty, whereas the effort-accuracy effects arise mainly
at the participant level (in our data), because of
differences in strategies adopted by different participants.

Conclusion
This experimental work is intended as a beginning
step towards understanding and quantifying the feelings of ambivalence and preference uncertainty that
often arise when people evaluate multiattribute alternatives. We operationalize the construct of preference
uncertainty using both a measure of response time
and a measure of test-retest response error. Our experimental results support the measurement approach
taken in this article. As predicted, at the stimulus (or
item) level, higher levels of response error were
strongly positively correlated with higher levels of
response time. Response times and response errors
were also significantly correlated at the response level.
This supports our theoretical argument that response
time and error are both manifestations of preference
uncertainty, and suggests that both depend on basic
properties of stimuli, such as attribute conflict and
attribute extremity. While it is widely acknowledged
that preferences may be expressed with error, we do
not know of other behavioral work that has operationalized preference uncertainty using both measures of
response time and response error.
The empirical results also supported our two main

101

<-----Page 15----->FISCHER, LUCE, AND JIA
Attribute Conflict and PreferenceUncertaiinty

predictions regarding item-level causes of preference
uncertainty. First, greater attribute conflict led to both
longer response times and larger response errors.
Second, greater attribute extremity led to smaller
response errors and shorter response times. Finally,
we found one unanticipated effect-higher value outcomes appeared to be more uncertain in value, as
reflected by the significant positive effect of value on
response error and response time while controlling for
conflict and extremity effects. In short, stimulus properties, such as attribute conflict, extremity, and positivity have a strong impact on levels of preference
uncertainty, as manifested in response error and response time.
These findings have important implications for
measuring and modeling preferences in the social
sciences or decision analysis. One implication is that
researchers attempting to measure multiattribute utility should consider stimulus characteristics such as
attribute conflict and extremity when collecting and
interpreting judgment data. Although high-conflict
alternatives are potentially more informative about
preferences, they also evoke greater response error.
On the other hand, extreme attribute levels evoke less
response error. This work also has implications for
formal models of multiattribute judgment. In a companion paper (Fischer et al. 2000), we develop a
multiattribute utility model that represents preference
uncertainty as random variation in model parameters.
By doing so, we extend and formalize the current
behavioral finding that preference uncertainty is a
function of stimulus characteristics.
Note that by linking preference uncertainty to random variation in model parameters, we associate it
with preference construction (evaluation) rather than
with preference expression (Goldstein and Einhorn
1987). Preference construction involves constructing
an internal impression of value whereas preference
expression involves translating internal evaluations to
an external response scale. Thus, expression effects
should depend on outcome value but not on the
configuration of attributes generating a given level of
value. Because we found attribute conflict and extremity effects even when controlling for value, it appears
likely that the attribute conflict and extremity effects

102

arise at the evaluation stage. The value effect could
arise either during the preference construction or
evaluation stage, so we cannot rule out the existence of
expression effects in addition to preference construction effects on preference uncertainty.
Finally, it is noteworthy that our hypothesized
preference uncertainty effects may co-exist with the
effort-accuracy tradeoff model proposed and supported by Payne et al. (1993). According to this model,
decision strategies that lead to more accurate choices
generally require greater effort. Thus, at the strategy
level, greater effort should be negatively related to
accuracy. Consistent with this prediction, we found a
highly significant negative relationship between participant-level measures of response time and response
error on individual judgments. This finding extends
the effort-accuracy model from systematic to randomerror measures of accuracy. On the other hand, as
predicted by our preference uncertainty model, we
also found highly significant positive relationships
between item and response-level measures of response time and response error. These findings show
that stimulus-based preference uncertainty effects
may operate in parallel with between-participant
effort-accuracy effects. Together, the preference uncertainty and effort-accuracy frameworks provide a powerful basis for analyzing the causes and consequences
of preference uncertainty, as manifested in response
time and response error.8
8 This research was supported in part by grant SBR-97-09615 from
the Decision, Risk, and Management Science Program of the National Science Foundation and by the Fuqua Business Associates
Research Fund. We thank Jim Bettman, John Payne, three anonymous referees, and Department Editor Robin Keller for helpful
comments and suggestions.

References
Bell, D. E. 1982. Regret in decision making under uncertainty. Oper.
Res. 30 961-981.
Berlyne, D. E. 1966. Conflict and reaction time: reply to Kiesler.
Psych. Reports 19 413-414.
Bettman, J. R., E. J. Johnson, M. F. Luce, J. W. Payne. 1993.
Correlation, conflict, and choice. J. of Experiment.Psych.: Learning, Memory, and Cognition 19 931-951.
Coombs, C. H., G. S. Avrunin. 1976. Single-peaked functions and the
theory of preference. Psych. Rev. 84 216-230.
Festinger, L. 1957. A Theoryof Cognitive Dissonance. Row, Peterson,
Evanston, IL.

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

<-----Page 16----->FISCHER, LUCE, AND JIA
Attribute Conflict and PreferenceUncertainty

Fischer, G. W. 1976. Multi-dimensional utility models for risky and
riskless choice. Organ. BehaviorHwnmanPerformance17 127-146.
Z. Carmon, D. Ariely, G. Zauberman. 1999. Goal-based
construction of preferences: Task goals and the prominence
effect. Management Sci. 45(8) 1057-1075.
S. A. Hawkins. 1993. Strategy compatibility, scale compatibility, and the prominence effect. J. of Experimeint.Psych.: Human
Perceptionand Performnance
19 580-597.
, J. Jia, M. F. Luce. 2000. Attribute conflict and preference
uncertainty: The RandMAU model. Management Sci. Forthcoming.
Goldstein, W. M., H. J. Einhorn. 1987. Expression theory and the
preference reversal phenomenon. Psych. Rev. 94 236-254.
Hansen, F. 1972. ConsumnerChoice Behavior:A Cognitive Theory. Free
Press, New York.
Hogarth, R. M. 1987. Judgementand Choice,2nd Edition. Wiley, New
York.
Janis, I. L., L. Mann. 1977. Decision Making. Free Press, New York.
Johnson, E. J., J. W. Payne, J. R. Bettman. 1988. Information displays
and preference reversals. Organ. Behavior and Human Decision
Process. 42 1-21.
Keeney, R. L., H. Raiffa. 1976. Decisions zvith Multiple Objectives:
Preferencesand ValuteTradeoffs.Wiley, New York.
Kiesler, C. A. 1966. Conflict and number of choice alternatives.
Psych. Reports 18 603-610.

Laskey, K. B., G. W. Fischer. 1987. Estimating utility functions in the
presence of response error. ManagemnentSci. 33 965-980.
Lewin, K. 1951. Field Theory in Social Science. Greenwood Press,
Westport, CT.
Lichtenstein, S., B. Fischhoff, L. D. Phillips. 1982. Calibration of
probabilities: the state of the art to 1980. D. Kahneman, P.
Under Uncertainty: Heuriistics
Slovic, A. Tversky eds. Jutdgmenewt
and Biases. Cambridge University Press, Cambridge, England.
Loomes, G., R. Sugden. 1982. Regret theory: An alternative theory of
rational choice under uncertainty. Econom.J. 92 805-824.
Luce, M. F. 1998. Choosing to avoid: coping with negatively
emotion-laden consumer decisions. J. ConstmnerRes. 24 409433.
March, J. G. 1978. Rationality, ambiguity, and the engineering of
choice. Bell J. Economn.9 587-608.
Payne, J. W., J. R. Bettman, E. J. Johnson. 1993. The Adaptive Decision
Maker. Cambridge University Press, Cambridge, England.
Shepard, R. N. 1964. On subjectively optimum selection among
multiattribute alternatives. Maynard W. Shelly, Glenn L. Bryan,
eds. Human Jutdginentand Optimnality.
Wiley, New York, 257-281.
Tversky, A., S. Sattath, P. Slovic. 1988. Contingent weighting in
judgment and choice. Psych. Rev. 95 371-384.
Tversky, A., E. Shafir. 1992. Choice under conflict: the dynamics of
deferred decisions. Psych. Sci. 6 358-361.

Accepted by L. Robin Keller; receivedJuly 18, 1998. This paper has been zvith the auithors5 nmonths
for 1 revision.

MANAGEMENT

SCIENCE/Vol.

46, No. 1, January 2000

103

