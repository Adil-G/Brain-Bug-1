<-----Page 0----->Approaches to Personality
Inventory Construction
A Comparison of Merits
Matthias Burisch

ABSTRACT: The three major approaches to personality scale construction, the external, inductive, and
deductive strategies, are discussed and their rationales
compared. It is suggested that all scales should possess
validity, communicability, and economy. The relative
importance of these characteristics, however, varies
with the purpose for which the instrument is being
constructed. A review of more than a dozen comparative studies revealed no consistent superiority of any
strategy in terms of validity or predictive effectiveness.
But deductive scales normally communicate information more directly to an assessor, and they are
definitely more economical to build and to administer.
Thus, wherever there is a genuine choice, the simple
deductive approach is recommended. Furthermore,
self-rating scales narrowly but consistently outdo
questionnaire scales in terms of validity and are clearly
superior in terms of communicability and economy.
There may not be many situations in which the widespread preference for questionnaires is justified. It is
concluded that the more commonsensical approaches
to personality measurement have a lot to offer.
Anyone who is bold enough today to develop a general-purpose personality inventory has basically three
options for how to go about it. They are the external
approach (also called "empirical," or "criterion
group"), the inductive approach (also called "internal," "internal consistency," or "itemetric"), and the
deductive approach (also called "rational," "intuitive,"
or "theoretical"). This article reviews their respective
merits on both a priori and empirical grounds. For
clarity's sake I will exaggerate the differences between
the philosophies associated with these three approaches.

University of Hamburg

types by means of an inventory, but they claim that
they do not know how the types differ in terms of
responses to personality items. Paul Meehl (1945),
for instance, doubted "that the psychologist building
the test has sufficient insight into the dynamics of
verbal behavior and its relation to the inner core of
personality that he is able to predict beforehand what
certain sorts of people will say about themselves when
asked certain kinds of questions" (p. 297). This statement was issued in 1945 (see also Meehl, 1979, p.
564), but the quotation may still serve to illustrate
the point.
Now assume that an externalist test constructor
wanted to develop a scale to differentiate males from
females, a so-called M-F test. Allegedly not knowing
where the differences were, he or she would never
dream of using an item such as "I can grow a beard
if I want to," or "In a restaurant I tend to prefer the
ladies' room to the men's room." Rather, a heterogeneous pool of items would be assembled and administered to a sample of men and women. Next,
the relative endorsement frequencies in the two subsamples would be compared item by item. Any item
discriminating sufficiently well would qualify for inclusion in the M-F test. This approach is called external because the scale membership of all items is
determined by factors external to the questionnaire
domain.
Inductive Approach

Inductivists also claim to know little about individual
differences at the item-response level. However, they
believe there is some basic structure to these differences, some universal laws to be discovered. Typically,
they claim not to know what this structure looks like
in detail—but they definitely hope it is a simple structure.
External Approach
Confronted with the task of uncovering nature's
Externalists typically, although not necessarily, believe laws, an inductivist will also collect a large item pool,
that people come in batches: for instance, manics and partly by simply inventing items, partly by borrowing
depressives, schizophrenics and hysterics, and so on. from the legacy of previous tests, which started some
They also know that there are engineers and actors time before Woodworth's Personal Data Sheet (Anand clerks, some of them happy, others unhappy. As gleitner, Lohr, & John, 1982; Goldberg, 1971). The
a rule, externalists want to distinguish between these constructor will then induce as many subjects to re-

214

March 1984 • American Psychologist
Copyright 1984 by the American Psychological Association, Inc.
Vol. 39, No. 3, 214-227

<-----Page 1----->that they have been asked the same silly questions
over and over again, but at least I can tell from a
score what a subject has said about herself or himself,
which would be much more difficult otherwise.
I have termed this approach (or family of strategies) "deductive" because choice and definition of
constructs precede and govern the formulation of
items. Some believe that the deductive strategy is too
heterogeneous a family for all its members to be called
by the same name, particularly inasmuch as the intuitive (sometimes called "rational") strategy has given
birth to such lamented offspring as the Bernreuter
Personality Inventory and the Bell Adjustment Inventory (cf. Ellis, 1946), thereby damaging the family
reputation. They assert that attention to what Loevinger (1957) has called "substantive" and "structural"
considerations makes a crucial difference.
I agree that trait concepts emanating from some
explicit personality theory have better chances of being
fruitful (at least in the sense of causing learned pubDeductive Approach
lications to proliferate) than concepts adopted from
Last, we have the deductivist. This person is intrepid "folk" psychology. Moreover, only a purist of deducenough to believe she or he knows quite a bit about tivism would maintain that a decent item analysis,
language and human personality, if only by virtue of based on sufficient data, could do any harm. However,
common sense. There may be a temptation to sail I also agree with Wiggins's (1973) verdict that "reunder the flag of a respectable personality system, the gardless of the theoretical considerations which guide
preferred one being Murray's catalog of needs. But scale construction or the mathematical elegance of
basically the deductivist believes that one can con- item-analytic procedures, the practical utility of a test
struct a scale for any personality trait for which there must be assessed in terms of the number and magis a name in everyday language. Thus one should feel nitude of its correlations with nontest criterion meafree to include in an inventory any variable of interest. sures" (p. 413). Surprising as it may be, there is simply
Trait names are usually viewed as "fuzzy sets"—ill- too little empirical support for the notion that a sodefined and overlapping categories of more elemen- phisticated theoretical background boosts scale vatary behavioral tendencies. So the first step in the lidity, and some of the studies to be reviewed leave
construction of a personality inventory is usually to similar doubts concerning the necessity of item analselect traits to be covered and to set up definitions ysis. The only systematic investigation of the latter
issue that I know of (Hermans, 1969) yielded less
for them.
In this process it is frequently discovered that a than conclusive results.
These reasons led me to lump together Goldglobal construct should be broken up into more specific subconstructs. As an example, take gregarious- berg's theoretical (no data) and rational (intuition
ness. Imagine a person who yearns to be with other plus data), and Wiggins's (1973, pp. 398-409) subpeople but for lack of social skills mostly remains stantive or construct-oriented strategies under the
alone. Imagine another person who is quite adept at general label of the deductive approach.
I hypothesize that factors not necessarily assomaking contacts but who simply prefers to be alone
much of the time. If you want to be in a position to ciated with any of the strategies above distinguish
tell these two people apart by means of their scale
scores, you will have to subdivide "gregariousness"
into "need for contact" and "contact skills" (or similar This article is based on an invited address delivered at the Bielefeld
Symposium on Personality Questionnaires, held at the University
constructs).
Bielefeld Center for Interdisciplinary Research, June 17-20,
Having set up working definitions for the con- of
1982.1 am indebted to Lewis R. Goldberg for technical, strategic,
structs to be measured, the deductivist starts writing and moral support in preparing this article. Arthur J. Cropley's
items that fit these definitions. Because some people's help in transforming a pidgin draft into an English manuscript is
fuzzy sets are much fuzzier than others', it may be a also gratefully acknowledged; portions remaining uneven probably
good idea to have companions select the most direct represent instances in which 1 did not heed his advice. Nonetheless,
readers are discouraged from sending irate comments to the two
and accurate items from the lists. I recommend using gentlemen mentioned. Rather, correspondence and requests for
only items that "hit the core" of the definition. In reprints should be addressed to Matthias Burisch, Department of
this way I run the risk of hearing subjects complain Psychology, University of Hamburg, Hamburg 13, West Germany.
spond to these items as funds permit. Next the accumulated data will be subjected to complex analyses
aimed at revealing the truth. Under normal circumstances, results will not always look as orderly as was
hoped. However, by means of a technique known as
"matrix staring," inductivists usually manage to make
sense of the plots, group the items into scales, and
devise a compelling label for each of them. One may
be forced to use household names such as "harria"
or "premsia," while orthogonal dimensions may degenerate into highly correlated scales, but the inductivist is able to cope with that.
I prefer to call this approach "inductive" because
the number and nature of the resulting scales follow
from the data analysis. Remember that the externalist
decided beforehand what sort of scales were needed.
Here, however, one starts with a collection of individual items, lets the data "speak for themselves,"
and ends up with scales at a higher level of abstraction.

March 1984 • American Psychologist

215

<-----Page 2----->good deductive scales from mediocre ones. These factors include, in general, length of scale, comprehensibility of item wording, band-width of construct, salience of the personality characteristic represented.
The source of the items could also be important.
In most of the comparative studies to be reviewed,
items were not written but compiled, selected from
an existing inventory. The reason is that the strategy
effect was to be isolated from the qualities of the items
per se. I have done this myself but now think that it
is not a very good idea. First, one usually does not
use up all the items in a pool anyway. So the pool,
not the actual items, is held constant. Second, it often
causes unnecessary trouble. If the original item pool
contains few good items, one is tempted to employ
inferior ones merely to make the scales long enough.
Mixed Approaches
There is, of course, no very good reason why the three
approaches should not be combined into "mixed"
strategies. One could, for instance, collect criterion
information on the subjects along with their responses
to the provisional inventory. Starting with deductive
scales, one could then do an item analysis watching
not only item-test correlations, but item-criterion
correlations as well. Finally, it would certainly be
instructive to look at the internal structure emerging
from a factor analysis, a cluster analysis, or (my favorite) a multiple scalogram analysis.
Actually this is rarely done, particularly the
combination of deductive scale writing and external
information for item analysis. Perhaps it is because
nobody dares go to the trouble of collecting peer ratings (or whatever the criterion may be) as long as he
or she is not fairly sure that the inventory is worth
the effort. Could it all be a question of self-assurance?
Deciding which of these three approaches is best
depends on the standards considered appropriate, and
these in turn depend partly on the functions an inventory is going to fulfill.

communication between patient and clinician. It may
provide the clinician with hunches if the interview
alone fails to do so, or it may corroborate hunches
generated during the interview. The assessor must
intuitively combine the information available, because
there is no statistical way to process inventory scores.
Prognosis
A psychologist using inventory scores for prognosis
routinely confronts a large number of individuals and
tries to forecast future events. Will X be happier as
a carpenter than as a sales clerk? Will X become an
efficient nurse? Will X become an accident-prone
driver? Will X benefit more from group than from
individual therapy? Processing inventory scores in this
situation is done by means of an explicit mathematical
rule, be it multiple regression, discriminant analysis,
configural scoring, or whatever.
Research

Regarding research, it is useful to distinguish between
situations in which inventory scales serve as instruments in the investigation of substantive issues (such
as in studies of therapy effectiveness) and situations
in which the inventory is both instrument and issue.
Probably the largest proportion of checkmarks on
questionnaire forms is made in contexts such as these.
As an example of the first instance, I recall trying
to make sense out of the controversy between Hans
Eysenck and Kenneth Spence during much of the
1960s on the question of whether "drive level" or
"introversion-extraversion" governs speed of conditioning in human subjects. Drive level was almost
always operationalized by Taylor's Manifest Anxiety
Scale, introversion-extraversion by one scale of the
Maudsley Personality Inventory. Even to a neophyte
such as myself, it seemed that the two instruments
were much too fragile a basis for carrying much weight
in the argument.
In the second instance, we have research into
the "structure of personality," which is often equated
What's the Use of Personality Inventories with the structure of responses to personality items.
Personality inventories are constructed for three main This type of research tries to answer questions such
purposes. I will distinguish an assessment context, a as, How many "basic" personality dimensions should
prognosis context, and two kinds of research contexts. we take into account? Are personality dimensions
hierarchical or not? Are they orthogonal or oblique?
Assessment
This purpose of inventories presents some unique
It is debatable whether clinical assessment can be requirements.
separated from prognosis, because ultimately the assessor is expected to make some sort of recommen- What Makes a Good Inventory?
dation, and this in turn implies a prediction: From Let us now turn to the standards by which personality
what sort of treatment will the patient benefit most? inventories should be rated. Although what makes a
Usually, however, the first question a clinician will good inventory depends on the use to which it will
try to answer is: "What is wrong with this person?" be put, the following three standards are important
The symptom picture may be diffuse or the patient ones in most contexts, either directly or indirectly. It
not overly verbal. The major function of a personality is only the emphasis that differs. The fourth standard,
inventory in this context is to stimulate and focus nonarbitrariness, is relevant in only one context.

216

March 1984 • American Psychologist

<-----Page 3----->Validity
There are many excellent treatments of what validity
"really" is and how we should assess it. Although
there is no chance to do justice to this important
issue here, still I must define my terms. I agree with
Messick (1980) that we should stop talking about
validity as a generic concept and rather differentiate
a number of its facets, all of which have to do with
our ability to generalize from test scores to the way
a person acts, thinks, and feels outside the testing
room.
What sort of correlation with what kind of data
should we be looking for? The two major schools of
thought on this issue, the "construct validity" school
and the "empirical validity" school, give quite different answers to that question, and their use of the
same term, validity, can be confusing. For the purpose
of this review I prefer to distinguish between "criterion
validity" and "effectiveness." Although I feel much
more sympathetic to the "construct" point of view
than to hard-nosed empiricism, I think that the term
is being overused. In a world in which almost everything is at least marginally correlated with almost
everything else, it is too easy to construe a modest
relationship with some remote variable post hoc as
evidence of construct validity. Foreseeing this, Cronbach and Meehl (1955) warned that "unless the network . . . exhibits explicit, public steps of inference,
construct validation cannot be claimed" (p. 291), but
their warning did not help.
Criterion validity versus effectiveness. The correlation (zero-order or multiple) of a personality scale
or set of scales with any target variable of interest
measures its effectiveness. The criterion validity of a
scale is its zero-order correlation with a criterion in
the following, rather restricted sense.
Criteria, quasicriteria, target variables. Defining
a good criterion for a personality scale is subject to
much confusion, which may have led Wiggins (1973,
p. 39) to remark that "the criterion problem" is often
talked about in much the same way as is "the race
problem." My own definition is that A variable Y
can be regarded as a criterion for a test X if and only
if (a) they are measures of the same construct—that
is, if they are theoretically or semantically fairly
close—and (b) //Y has a higher "status" than X—
that is, //Y is more "trustworthy." An informal rule
to ascertain whether condition (b) holds is to ask
oneself, Suppose I had to make some practically relevant decision, for instance, to hospitalize this patient
for depression or not, and X and Y give conflicting
information. Would I rely on Y rather than on XI
If (a) holds but (b) does not, it is appropriate to
speak of a "quasicriterion." By this admittedly vague
definition, much of what are referred to in the literature as criteria should actually be called quasiMarch 1984 • American Psychologist

criteria at best (cf. Meehl, 1972, p. 164). For instance,
the most common "criterion" for peer ratings, peer
rankings, and peer nominations in Kane and Lawler's
(1978) review of those methods of peer assessment
was "supervisory rating"—judgments likely to be less
trustworthy for lack of first-hand information and
more subject to ratees' impression management.
There is, of course, room for disagreement on
whether or not condition (b) holds in a specific instance. For example, in spite of the widespread skepticism regarding peer ratings (e.g., Coombs, 1964, p.
338; Loevinger, 1965, p. 83), this is my preferred class
of variables when it conies to validating personality
scales, especially if the raters know the ratees well,
are numerous and in good agreement. After all, what
could we validate such ratings against? For constructs
that are by definition behavioral, such as dominance,
multiple act criteria (Buss & Craik, 1981; Epstein,
1979;Fishbein&Ajzen, 1974; Green, 1978; Jaccard,
1974; McGowan & Gormly, 1976) may be superior
if based on observed rather than on self-reported data.
To my knowledge, however, multiple act criteria have
not been employed so far for comparing inventoryconstruction strategies. This may be because the datagathering procedure is extremely cumbersome, although the approach holds great promise.
Another class of variables that should be conceptually separated from criteria and quasicriteria is
"targets," that is, variables which for some pragmatic
reason have to be predicted with whatever information
happens to be available. An example would be suicide
risk in psychiatric patients. In a situation like this,
all that counts is minimization of residual variance,
or maximation of hit rate. Therefore any predictor
that under crossvalidation proves to add some nonredundant variance to the battery should be welcome.
Whether the empirically observed correlation makes
sense is, at least in the short run, irrelevant.
I have distinguished criteria, quasicriteria, and
targets because we learn different things from correlating them with our scales. If we correlate a scale
with a criterion, and if the coefficient turns out to be
high enough, a certain degree of satisfaction may be
justified. Conversely, if the correlation is low, we know
that something has gone wrong. The consequences
are less clear-cut if we have used a quasicriterion,
usually an existing scale as much in need of validation
as our new scale. Even a fairly high correlation here
is informative only insofar as we have confidence in
the quasicriterion. Particularly if the quasicriterion
data were obtained under circumstances very similar
to the testing—for instance, if a direct self-rating is
to be correlated with a questionnaire scale—the result
is indicative of reliability more than of validity, as
Mischel (1968, pp. 13-14) has pointed out.
Finally, the correlation of a scale or inventory
with a target variable tells us almost nothing about

217

<-----Page 4----->the instrument per se. If, for instance, it could be
shown that the Personality Research Form (PRF)
predicts dating frequency better than the Minnesota
Multiphasic Personality Inventory (MMPI) does, this
would not tell us anything about the PRF or the
MMPI unless both contained measures of, say, "need
for affiliation with the opposite sex" or some such.
But in that case we could again compare two zeroorder correlations and would be dealing with validity,
not just effectiveness.
Validity coefficients. The question of an appropriate validity coefficient is closely related to the distinction of criterion validity versus effectiveness. I
maintain that validity should be appraised by zeroorder correlations. Multiples are inadequate because
it is not uncommon to discover that the wrong predictors contributed most to the prediction or discrimination, while the one scale in the inventory
bearing the closest semantic relationship to the criterion had only a marginal correlation with it. Using
indexes of multivariate predictability, it is possible to
be successful for the wrong reasons. This situation is
analogous to two archers who find that they hit each
other's targets. They may be happy that, after all, they
have hit something, but nobody should place too
much confidence in their skills.
The fact that no generally accepted procedures
for assessing discriminant validity have ever emerged
may explain why pertinent figures are rarely to be
found in research reports. This is a most unsatisfactory state of affairs because a scale's correlations with
diverse nontargeted variables provide a background
against which a validity coefficient must be judged.
Returning now to the foregoing discussion of
purposes for personality inventories, it should be clear
enough that criterion and discriminant validity are
of critical importance in the assessment and research
contexts, but that in a prognosis situation effectiveness
is all that is necessary. If shoe size as a predictor
improves your ability to predict performance as an
airplane pilot, use it. This, of course, is the essence
of the external strategy. But it is my hunch that in
real-life settings, where conditions tend to be changing,
such remote predictors will only rarely have consistent
effectiveness (cf. Jackson, 1971). In fact, there is a
research tradition (e.g., Duff, 1965; Koss & Butcher,
1973; McCall, 1958) showing that the so-called "subtle" items of the MMPI, often regarded as the instrument's chief advantage, possess less discriminating
power than do its "obvious" items. The results for
obvious or face-valid items in the PRF are less clearcut (Holden & Jackson, 1979, 1981), but the PRF
can be assumed to contain few subtle items.

term overlaps with terms such as "content validity,"
"content saturation," "homogeneity," or, at the item
level, "prototypicality." How comprehensible is the
information communicated by answers to the test
items? Consider the following three items:
I feel hungry nearly all the time.
I used to dream rather often.
I would rather live in a big city than in a quiet village.

Suppose a subject has said yes to all of them.
Does any clear image of this person come to your
mind? Yet the three items are all from the aggression
scale of a much-used German inventory.
In contrast, consider the following three items:
Frankly, I start quarrels fairly often.
I frequently feel like attacking someone.
Sometimes I take pleasure in provoking others a little.

Someone who answers all these items positively
admits to a tendency to spontaneous aggression, if
only in verbal form, and thus communicates something about herself or himself.
Communicability does not in any way guarantee
veridicality in self-description. But at least there is
some signal on top of the noise, and thus a nonzero
signal-noise ratio. I find this an advantage in contrast
to a batch of heterogeneous items being subsumed
under a scale designation that may in any case be
cryptic and therefore tells me nothing.
The deductive approach does not automatically
produce scales high in communicability. As I noted,
some people's fuzzy sets are fuzzier than others'. But
it is only here that we have a chance of distinguishing
what I call "defining" from "correlating" characteristics. Let us take gregariousness as an example again.
People who seek the company of others and are successful in doing so (the defining characteristics of
gregariousness) tend to have other characteristics as
well. They tend to be more active, enterprising, talkative, perhaps more dominant, too. If we simply collect items that discriminate people rated as gregarious
from others (the external strategy) or if we sample
items that cluster together with "defining" items (inductive strategy), there will always be a proportion
of items that correlate with the construct but do not
really fit its definition. Only content considerations
can separate "hard-core" items from the rest. Buss
and Craik (1981) have called these "prototypical"
items, and I believe that only items of this kind should
be included in a scale.
There are some who hope that eventually a clinician will learn to associate personality patterns with
cues or even configurations of cues that are not by
themselves communicative. The preparation of
Communicability
MMPI "atlases" (e.g., Marks & Seeman, 1963) was
A second standard for personality scales that implies undertaken to assist the novice in the learning process.
some degree of subjectivity is "communicability." This The success of this approach to personality assessment

218

March 1984 • American Psychologist

<-----Page 5----->is an empirical matter. Insofar, however, as the personality description purportedly associated with any
profile code must again be phrased in comprehensible
language, I wonder whether it would not save considerable effort if communicable scales were used in
the first place.
If communicability may be an asset in an assessment context, it clearly is in most research situations. If your project investigates the influence of
television on adolescents' aggression, you may need
several specific aggression measures. A scale that represents a hodgepodge of aggression, dominance, stimulus seeking, excitability, and any number of bizarre
ingredients will not serve you well. The same line of
thought led Meehl, Lykken, Schofield, and Tellegen
(1971) to suggest "a method for reducing somewhat
the subjective element in factor naming" as an attempt
to boost the communicability of factor names in
structure-of-personality research. The only background that renders communicability irrelevant is
prognosis (unless you wish to learn something about
your target variable by analyzing its mirror image in
predictor correlations).
Economy
Not too much need be said about economy in the
sense of construction economy. Some of the betterknown inventories, like the MMPI or the Sixteen
Personality Factor Questionnaire (16 PF), have taken
years to build, not to speak of the funds spent and
the man- and womanpower consumed in the process.
In contrast, it cost me two hours and a bottle of wine
to write an aggression and a depression scale that
turned out to be of equal or superior validity, compared to much more sophisticated instruments (Burisch, 1978, Table 3; see also Burisch, 1984). As I
learned later, even weeks of discussions over item
formulations did not lead to scales that were appreciably better than that. Jackson (1975) and Ashton
and Goldberg (1973) have published experiments
which support that notion. Without doubt, deductive
scales are normally much easier to construct than
either external or inductive ones.
Deductive scales also tend to be more economical
in terms of administration, because they are shorter.
If you write "defining" items for a narrow-band construct, as 1 suggest you do, then 10 to 20 items will
usually be the maximum scale length you can accomplish without too much redundancy. One could,
of course, produce very short external or inductive
scales, but this is rarely done.
Short scales not only save testing time but also
avoid subject boredom and fatigue. Answering my
own experimental scales myself has often brought
home to me the importance of brevity. There are
subjects (e.g., depressives) from whom you will not
get any response if the test looks too long. There is,
March 1984 • American Psychologist

by the way, the likelihood that a long scale will actually
be less valid than a short one. This is so because the
assumptions of the Spearman-Brown formula may
be unrealistic for most personality questionnaires. I
have dealt with that little-known possibility elsewhere
(Burisch, 1984) and will not go into details here.
Finally, high interscale correlations within an
inventory not only make for poor discriminant validity but also introduce uneconomical redundancy.
Remember that one of the factors which disqualified
the Bernreuter was the near-exchangeability of some
of its scales (cf. Goldberg, 1971, p. 304). External
inventories tend to be most redundant in this sense,
partly because externalists' concentration on scale effectiveness allows them to accept item overlap with
ease.
Nonarbitrariness and Representativeness
Finally, we have what I call "nonarbitrariness and
representativeness." These qualities play a role only
in the context of structure-of-personality research
mentioned above. If you believe there is some "natural
order" in the personality domain that is not imposed
but rather needs to be uncovered, then it is crucial
that you draw a representative sample of all possible
item content and that you do the grouping into scales
with a minimum of arbitrary decisions on your side.
This is why multivariate procedures, allegedly objective techniques for complexity reduction, are so central to the work of R. B. Cattell, H. J. Eysenck, and
J. P. Guilford, to name only a few proponents of the
"discoverer" point of view with respect to personality.
Obviously, neither the external nor the deductive
strategy can make any claims along these lines. Externalists as well as deductivists pick their constructs
and their items with an unabashed eye to practical
considerations.
Without going into details, let me just declare
that I (a) do not believe in "truth" with respect to
personality structure models, and (b) do not think
that current inductive methodology would be in a
position to uncover "natural" structure if there were
any. However, for those who disagree with me on
these two counts, the inductive approach is the only
viable one.

Empirical Comparisons of
Construction Approaches
It is now time to turn to empirical comparisons of
construction approaches. The comparison will be
along the lines of readily quantifiable properties of
inventory scales, namely validity or effectiveness, and
scale length.
Table 1 lists all pertinent studies that emerged
from a search of the literature.1 As the footnotes in
' I apologize in advance for any errors of omission and thank
anyone pointing them out.

219

<-----Page 6----->KJ
KJ
0

Table 1
Summary of Empirical Studies Comparing Merits of Inventory-Construction Strategies

Source

Identical
constructs?

Identical
item
pool?

Identical
items?

Subjects

M scale
length

Results

Criteria/targets

Number of
scales in
inventory

Remarks

A: All three strategies
Goldberg (1 972)
Hase & Goldberg
(1967)

no

yes, CPI

no

152 female
students

Criteria: peer ratings
for 5 traits.
Targets: 8 variables
such as GPA or
dating frequency

Mean double-cross-validated R for:
5 criteria and 8 targets 5 criteria only
EXT

IND T
UbU

Mezzich, Damarin,
& Erickson
(1 974)

Burisch (1 978)

yes

yes, MMP!

yes, for some yes, FPI
pairs of
inventories

no

for only 1
pair of
inventories

118depressives, 1 05
nondepressives

Criterion: psychiatric
differential
diagnosis

1 38 students

Criteria: peer ratings
on 2 rating scale
formats and 2
occasions

FAC

LMSA
FTHE
L RAT

26

36

.25

.31

-

-

.28

.37

.24
.26

.28
.40

39
17
5
14
37

11
11
11
11
11

Biserial correlation
with psychiatric diagnosis
EXT
DED/IND
DED

.36
.18
.26

60
33
44

1
1
1

Mean correlation with criteria
.43
.36
.39
.41
.39

KEXT^

- GE
DED

25
9
25
15
9

8
8
8
5
8

o

Several uncontrolled
factors. RAT inventory
not very "rational." Rs
as validity coefficients.
Only EXT and RAT
contained scales directly
related to some criteria;
in these cases mean r =
.24 (EXT), .33 (RAT),
see Hase & Goldberg,
Table 7.
EXT = MMPI standard key
D. DED/IND is Wiggins's
(1966) scale D, derived
from a factor analysis of
several short,
deductively compiled
scales.
STD = Factor-analytically
constructed standard
keys. EXT-S and STD,
and EXT-D and DED,
measure the same
constructs. FAC and
DED built on identical
item subpool. Several
uncontrolled factors.

B: External versus indu ctive strategy
CO

Crewe(1967)

no

yes, MCI

yes

•

2,000 grade
9-1 2 boys
and girls

1
3

Borgen (1972)'

no

yes, SVIB

no

1,348 highability
students

Targets: teacher
nominations for 7
characteristics
such as conformity
Targefs: major
concentration,
career field

Mean R with extreme group membership
EXT
IND

49
49

7
7

66
12

22
22

Cross-validated hit rate of discriminant
analysis
16 major
groups

o

o
cT

.47
.44

Rs as validity coefficients.
Data had been part of
the sample used to
construct EXT, but not
IND, scales.

EXT
IND

23.6%
20.1%

1 0 career
groups
23.0%
24.5%

EXT = SVIB Occupational
scales, IND = SVIB
Basic Interest scales

<-----Page 7----->p

Table 1 (continued)

1
Source

Identical
constructs?

Overall (1974)

no

.

CO

Identical
item
pool?
yes, PSI

Identical
items?
yes

g
§
§
3

•is-"

Subjects
126 psychiatric patients,
800 normals

Results

Criteria/targets
Target: category
membership

yes, for 1
pair of
inventories

yes, MMPI

yes, for 1 1,117 psychiapair of
trie ininvenpatients
tones

EXT

Target: psychiatric
diagnosis (6 most
frequent
categories)

82.5%
78.5%

fSTD
Ll68

IND

f.

26
26

37%
39%

56

40%

24

?

13
13
7

f-4-

Hornick, James, &
Jones (1977)

no

yes, PCQ

no

398 firefighters

Targets: supervisor
ratings of
performance

Mean double-cross-validated
R with target
EXT
IND

Klingler, Johnson,
Giannetti, &
Williams (1977)

no

yes, MMPI

no

508 male
psychiatrie patients

Target: psychiatric
diagnosis (1 3 most
frequent
categories)

Remarks

5
5

Cross-validated hit rate of discriminant
analysis
EXT

o

Number of
scales in
inventory

Hit rate of discriminant analysis

IND

Hedlund, Cho, &
Wood (1977f

M scale
length

.46
.49

61
24

2
6

Hit rate of discriminant analysis
EXT
IND/EXT

32%

56

30%

?

13
13

STD = Standard MMPI
keys, 168 = MMPI-168
clinical scales, IND =
MMPI-168 factor scales.
Rs with psychiatric
ratings on 14
dimensions were equally
low for both strategies
(figures not reported).
IND inventory based on a
principal-components
analysis of 35 short
deductive scales. Six
resulting components
entered into stepwise
multiple regression
analysis.
EXT = MMPI standard
keys. IND/EXT inventory
consisted of 4 factoranalytically derived
scales + 6 quasiexternal scales + 3
MMPI standard validity
keys.

C: External versus deductive strategy
Heilbrun (1962)

yes

yes, ACL

no

166 students

Quasicriterion: selfreported number
of friends

Biserial correlation of median-split scales
with quasi-criterion
EXT
DED

Ashton &
yes
Goldberg (1 973)

to

1^

for 3
no
inventories;
CPI; for
rest, no

168 female
students

Criteria: peer ratings
of sociability,
achievement, &
dominance

.12
.06

34
7

1
1

Mean correlation with criteria
EXT

.27

TpSY
NPSY
DED THE
RAT
PRF

.29
.18
.29
.26
.35

40
20
20
15
44
20

3
3
3
3
3
3

Correlation of EXT with
DED scale = .75; item
overlap of both not
reported.

EXT = 3 external CPI
scales. PSY, NPSY =
scales written intuitively
by psychology students
or nonpsychologists.
respectively.

<-----Page 8----->K)
N)

Table 1 (continued)

Source
Jackson (1975)

Identical
constructs?

Identical
item
pool?

yes

Identical
items?

Subjects
116 female
students

Criteria/targets

Results

Criteria: peer ratings
of self-esteem,
social participation,
and tolerance

Mean correlation with criteria
EXT

yes

3,072 air force
personnel, 312
"men in
general"

Target: membership
in 1 of 8
"satisfied" USAF
career groups

.09
25

r pSY
DED

Reilly &
Echtemacht
(1979)

M scale
length

LJPI

^29

16
16
20

Number of
scales in
inventory

3
3
3

Mean double-cross-validated hit rate of
classification procedure
76.1%
72.1%

EXT

DED

17-38
15

8
13

Remarks
EXT =16 randomly
chosen items from each
of 3 CPI standard
scales. PSY = Scales
written intuitively by
undergraduate
psychology students.
JPI = 3 scales from
Jackson Personality
Inventory.
Only 1 EXT scale per
career group was used
for classification. Several
uncontrolled factors.
Satisfaction within each
career group was
predicted better by DED
than by EXT scales.

D: Inductive versus deductive strategy
Knudson &
Golding (1974)

no

64 students

Criteria (for PRF): 7
peer trait ratings, 7
targets (e.g.,
dating frequency)
IND

SIR

[~RSR

DED I ICL
I PRF

Burisch (1984)

>

3

3P
O.

o"

rjo_

^'

closely
related
constructs

Mixed sample,
n = 101

Criteria: peer ratings

Mean R for
7 criteria
& 7 targets

7 criteria only

.10
.17
.23
.28

.09
.16
.26
.37

f?s (corrected for
shrinkage) as validity
coefficients. Peer ratings
matched with 7 PRF
scales only.

32
18
16
20

5
15
8
10

Mean correlation with criteria
IND STD
DED

.33
.38

STD = 8 standard scales
of FPI

25
5

23

Wofe. ACL = Adjective Check List, CPI = California Psychological Inventory, DED = deductive scales, EXT = external scales, FAC = scales derived from factor analysis, FPI = Freiburger Personlichkeitsinventar,
ICL = Interpersonal Check List, IND = inductive scales, MCI = Minnesota Counseling Inventory, MMPI = Minnesota Multiphasic Personality Inventory, MSA = scales derived from multiple scalogram
analysis, PCQ = Psychological Climate Questionnaire, PRF = Personality Research Form, PSI = Psychological Screening Inventory, RAT = scales from Hase & Goldberg's (1967) Rational inventory, RSR =
Rational S-R-lnventory (a deductive regrouping of SIRs plus 101 additional items), SIR = Schedule of Interpersonal Response, SVIB = Strong Vocational Interest Blank, THE = scales from Hase & Goldberg's
(1967) Theoretical inventory.
* Representative for several studies comparing SVIB occupational with basic interest scales.
" Representative for several studies comparing MMPI-168 clinical (EXT) and factor (IND) scales.

<-----Page 9----->the table indicate, however, there are two exceptions. in the construction approaches accounted for only
I included only one study that compared the occu- negligible differences in the coefficients achieved. In
pational scales with the basic interest scales of the the right-hand subcolumn I have listed the analogous
Strong Vocational Interest Blank (SVIB). Further- figures for only the five trait ratings that I would
more, I listed only one comparison of MMPI-168 regard as criteria in the above sense, if only for some
clinical versus factor scales. In both cases an external scales in some of the inventories. This leads to more
inventory was compared with an inductive one, and scatter; now the coefficients vary from .28 to .40. One
in both cases I felt justified in omitting several anal- could say that a concentration on somewhat more
ogous studies because the results were quite close. A relevant criteria enhances the contrasts. But still it is
study by Hamilton (1971), allegedly comparing the fair to say that the similarities outweigh the differences.
external with the deductive strategy, was not included To my knowledge, there is no suitable test of signifbecause the purportedly external scales that had icance for this sort of comparison. The subject sample
matching deductive scales were actually deductive as was, of course, identical, and in addition there was
well.
item overlap.
The table has four main sections. Section A conThe last two columns contain information about
tains three studies that compared all three construe-, economy. The number of scales in an inventory is
tion approaches. Sections B, C, and D contain pair- important only when multiple correlations are used.
wise comparisons. For instance, section B lists six In the Goldberg study, this factor was controlled. Scale
studies that compared the external with the inductive length, however, was not; the longest inventory was
strategy. Within sections, studies are listed chrono- some eight times as long as the shortest. Whatever
logically.
the effect of scale length may be in practice (cf. BurThe table's second, third, and fourth columns isch, 1984), short scales should suffer more from ungive some information on whether the most potent reliability attenuation.
factors were controlled. For example, in Goldberg's
Having elucidated the organization of Table 1,
(1972) classic monograph all five inventories were let me take a short-cut to the very end. In spite of
compiled from the same item pool, that of the Cal- several criticisms, some of which are reflected in the
ifornia Psychological Inventory (CPI). The constructs, remarks column, Goldberg's conclusions remain eshowever, differed. This was dictated by the design of sentially unchallenged throughout the table. Whether
the experiment, but it somewhat limits its internal it is criteria or targets, zero-order or multiple corvalidity. For example, three of the criteria mentioned relations, there are no dramatic effects due to conin column six were peer ratings of "responsibility," struction approaches. Whatever slight difference there
"psychological mindedness," and "femininity." is in one study is reversed in the next. Because the
Whereas some inventories did contain scales for these table necessarily presents highly condensed infortraits, others did not, a fact which may have biased mation, and because I sometimes preferred my own
the comparison. Control of the construct factor is way of summarizing figures, it should be useful to
important also because some constructs make for discuss briefly the cases that may appear to be exgood convergent validity ("depression" is an example) ceptions to the rule, or the cases where authors reached
whereas others, such as "aggression," are notoriously conclusions different from mine. I will confess at the
hard ones.
outset that in terms of statistics I am a conservative,
Since all strategies had access to the same item bearing in mind that statistical significance does not
pool, a strategy's sensitivity for good items was not guarantee practical relevance (for example, Cohen,
confounded with the test constructor's ability to gen- 1965, 1969).
erate them. Nonetheless, as can be seen from column
Mezzich, Damarin, and Erickson (1974) prefour, the strategy effect was confounded with the in- dicted one psychiatrist's diagnosis of "depressive
trinsic qualities of the items. This was so because any state" vs. other psychiatric categories by means of
one inventory used up only part of the item pool and (a) the 13 standard MMPI scales (EXT), (b) Wiggins's
overlapped only partly with any other inventory. (1966) 13 content scales, constructed from the MMPI
Contrast this with the study by Crewe (1967), the pool by a combination of deductive and inductive
first entry in section B, in which only the grouping methods, and (c) a self-made purely deductive depresof the same items into different scales (with different sion scale consisting of 44 MMPI items. The biserial
names) differentiated the inventories.
correlations with the criterion were .36 for the EXT
The results column contains first the mean dou- D scale, .18 for Wiggins's DED/IND scale, and .26
ble-cross-validated multiple correlations between each for the DED depression scale. Although clinical diof the five 11-scale inventories and 13 variables, partly agnosis of depression may be a highly complex cricriteria, partly targets. These figures range from a low terion, scales bearing the same label can legitimately
of .24 to a high of .28, and this is what has been so be expected to correlate with it. The poor performance
often quoted from this investigation: All the variations of Wiggins's DED/IND scale is somewhat compenMarch 1984 • American Psychologist

223

<-----Page 10----->sated for, however, by the finding that his Hypomania
scale had a validity coefficient of .35. At any rate, on
the basis of this one-criterion comparison alone the
case against deductive or mixed approaches is not
too strong, particularly when scale length varied from
60 (EXT) to 33 (DED/IND). It is interesting, although
not conclusive, to note that mean double-cross-validated Rs were almost identical for a number of linear
and nonlinear combinations of the various predictor
sets. What is the situation with other comparisons
involving depression? I could not find differences
worth mentioning between external, inductive, and
deductive validities in a study in which peer ratings
were the criterion (Burisch, 1978, Table 3).
Overall's (1974) study, the third entry in section
B, attempted to blindly classify 126 psychiatric patients and 800 "normals" on the basis of 5 external
or inductive scales. Of the 926 classifications, the EXT
set provided 764 hits and the IND set 727 hits—both
tallies being less than what would have resulted from
labeling everybody "normal". The difference of 37
successes (4%) is interpreted as indicating that "other
empirical methods of scale construction with direct
criterion orientation do appear superior for psychiatric screening purposes" (p. 719). This conclusion
appears a bit too strong, keeping in mind that the
author himself expressed misgivings regarding his
criterion's status and the fact that no cross-validation
was attempted.
Overall (1974) also speculated that "factor scoring [i.e., inductive methods] may have greater merit
in the discrimination among different pathological
types than it has for general psychiatric screening"
(p. 719). This hunch was weakly supported by Hedlund, Cho, and Wood (1977) and feebly undermined
by Klingler, Johnson, Giannetti, and Williams (1977),
although both teams worked with the MMPI.
In the Jackson (1975) study, the third entry in
section C, three external CPI scales achieved a mean
validity near zero (.09), whereas two sets each comprising three corresponding deductive scales scored
at .25 and .29 respectively. Although the latter figures
are undeniably higher, they are not very impressive
by themselves. The comparison would also appear
more compelling had not the external scales been cut
down to 16 items, some 40% of their original lengths.
(This was done because one of the deductive sets had
16-item scales too.)
Although Reilly and Echternacht (1979) are appropriately cautious in interpreting the higher classification hit rate they found for external compared
with deductive scales, even more caution may be in
order. A linear combination of deductive scales was
used for classification and cross-validation, but it was
a linear combination of items that served the same
purpose in the external case. Although it is hard to
tell which side was favored by this confounding of

224

construction strategy and prediction tactic, the results
certainly lose some conclusiveness.
Section D contains two studies that controlled
neither the item pool nor the items actually used.
Knudson and Golding (1974) did not even control
the constructs to be measured. In this study the PRF
definitely seems to leave the rest of the field behind,
particularly in the right-hand column of validity coefficients. But these are correlations of seven PRF scales
with seven same-named peer rating scales. None of
the other three inventories in this study had the advantage of matching constructs.
This bias was avoided in the last entry of Table
1, a study of mine (see Burisch, 1984, for details).
The starting point was the eight major scales of the
Freiburger Personlichkeitsinventar (FPI), an inductive
German inventory. The deductive construction focused on the same constructs, sometimes split into
subconstructs if they were too global for a reasonably
precise definition. Next, two or more very short scales
were written for each construct; these were five, or
sometimes four, items long. Each inventory scale was
correlated with its own matching peer rating scale.
This resulted in a mean validity coefficient of .38.
Though obviously higher than the FPI's .33, this figure
would hardly convince a hard-nosed externalist or
inductivist.
Thus, concentrating solely on the psychometric
quality of validity (or effectiveness), as we have done
so far, we find more support for the null hypothesis
than for any superiority claims issued by proponents
of one or the other construction approach.2
Turning to considefations of economy, the column headed "average scale length" shows that with
few exceptions external scales tended to be much longer than deductive scales, with inductive scales usually
in between. I therefore feel safe in summing up the
overall comparison as follows. All three approaches
are capable of producing inventories with quite similar
degrees of validity or effectiveness. The deductive approach, however, manages to do so with the least
construction effort and with very short scales. In addition, the meaning of a deductive scale is normally
much easier to communicate than that of an external
2

Eber (1977) has expressed a similar conclusion in a form
stronger than seems warranted. Commenting upon Goldberg's
(1972) monograph he wrote:
Assume that a group of items spans, in relation to a certain set
of criteria, a denned space. Assume that the items are assembled
into scales by any method other than a determined effort to
suppress predictive variance by mutual cancellation. It seems
reasonable to believe that virtually any such aggregates of items
would perform about equally well in predicting the criteria, at
least in the initial sample, (p. 331)
But this belief that it is only the items, not the grouping into scales
that matters, was not substantiated by Goldberg's findings: Stylistic
and random scales were less effective than "serious" ones.

March 1984 • American Psychologist

<-----Page 11----->or inductive scale; at least, deductive scales can be
made to have high communicability. Frederic Kuder
(1970), after giving his advice for the measurement
of interests, summed it up in this way:
A person who follows these principles should not be surprised if he produces a deceptively simple questionnaire
which looks like something a junior high school student
might have thrown together on a rainy Saturday afternoon.
That's the way it should look! (p. 225)

Rating Scales vs. Questionnaire Scales
Although it exceeds the limits of my subject, I will
comment briefly on a phenomenon that is apparently
not widely known: If you ask subjects to rate themselves directly on simple trait-rating scales, these selfratings turn out, on average, to be more valid than
corresponding questionnaire scales. The difference is
not large, but it is very consistent. I can quote more
than a dozen investigations that found it (Amelang
& Borkenau, 1982; Ashton & Goldberg, 1973; Beck
& Beck, 1980; Burisch, 1978; Burisch, 1984, Study
2; Carroll, 1952; Hamilton, 1971; Harris, 1980; Hase
& Goldberg, 1967; Kaufman & Murphy, 1981; Lamont, 1983; Norman, 1969; Spitzner, 1979; Wetzel,
1963; see also the review by Shrauger & Osberg, 1981),
but only three that did not (Burisch, 1984, Study 3;
Jaccard, 1974; Knudson & Golding, 1974). The Jaccard (1974) study, however, involved only one construct, dominance, and only 45 subjects; the PRF and
the CPI dominance scales had validities of .64 and
.58, respectively, while a simple self-rating had a validity of .51.
This effect should not be too surprising. On the
other hand, if self-ratings are (a) directly communicable, (b) the ultimate in economy, and (c) also more
valid than their questionnaire counterparts, then we
will have to face the somewhat embarrassing question
of just why we continue to construct personality inventories at all. The same question was raised by
Gordon Allport (1941) several decades ago: "If we
want to know how people feel, what they experience
and what they remember, what their emotions and
motives are like, and the reasons for acting as they
do, why not ask them?" (p. 37). I must confess I do
not have a very good answer to that any more. I
believe now that in many, if not most, situations
questionnaire scales should indeed be replaced by
self-rating scales.
Before psychologists start to shudder at the prospect of exposing the cherished tools of their trade to
the risk of impression management, I will add some
speculations about necessary preconditions that may
limit what I said.
1. One of the three exceptions to the general
rule of self-ratings being superior is contained in an
article of mine (Burisch, 1984). The effect did show
up in two research settings in which subjects remained
March 1984 • American Psychologist

anonymous. It failed to manifest in a clinical setting
in which the criterion was one psychiatrist's rating
of depression in out-patients. The sample was small
and the criterion flimsy, but it could indeed be that
the complete transparency of self-ratings restricts their
use to situations in which no personal consequences
are involved. I am not fully convinced that this is
different with questionnaire scales (because they are
fakable too) although there is little hard evidence on
this point. Commenting on three studies of faking
and validity in personality tests, Thornton and Gierasch (1980) concluded that "the role of faking in
validity may be situationally specific and requires further analysis. It cannot be assumed that faking either
raises or lowers validity" (p. 50).
2. In clinical contexts, another drawback of selfratings may be a reverse "emperor's new clothes"
effect. On the basis of self-ratings alone, clinicians
cannot tell patients anything they do not already
know. But this is precisely what the patients came
for! The mystery of a projective device or the glamour
of a computer-scored inventory profile is painfully
missing. Whether clinicians will welcome such a demystification of their business depends on their role
definition.
3. The second exception I found was described
in the article by Knudson and Golding (1974). Here
direct self-ratings for seven Murray needs had a mean
validity for peer ratings of .28, and corresponding
PRF scales had a mean validity of .29 (the latter
figure refers to the zero-order validities given on p.
121). Although it is understandable in terms of sampling fluctuation around a real but not overwhelmingly strong effect, this failure at replication may also
have a substantive explanation. It seems unlikely that
subjects had had much practice in dealing with trait
names like "social recognition" or "succorance"; thus
they were asked to read a lengthy description to grasp
the concept that was to be rated. Probably self-ratings
work best when we stay in the realm of everyday
language (cf. Hase & Goldberg, 1967, p. 245) and
avoid overtaxing our respondents' good will to concentrate on tasks in which they have little interest.
As a final remark let me mention that the law
of simplicity holds also for scale formats. We have
experimented with various sophisticated exampleanchored rating scale formats—for example, Taylor's
(Taylor, 1968; Taylor, Ptacek, Carithers, Griffin, &
Coyne, 1972)—and could never find a consistent advantage over simple unanchored scale formats. This
agrees with other researchers' findings (Schwab &
Heneman, 1975).

Conclusion
The lesson to be learned from my comparison of
construction approaches and from my comparison
of questionnaire with self-rating scales seems fairly

225

<-----Page 12----->clear. I will borrow the advice Escoffier, the famous
19th-century chef, gave to his students. He said,
"Faites simple!"
REFERENCES
Allport, G. W. (1941). The use of personal documents in psychological science. New York: Social Science Research Council.
Amelang, M, & Borkenau, P. (1982). Uberdie faktorielle Struktur
und externe Validitat einiger Fragebogen-Skalen zur Erfassung
von Dimensionen der Extraversion und emotionalen Labilitat.
'/.eilschrift fur Differentielle und Diagnoslische Psychologic, 3,
119-146.
Angleitner, A., Lohr, F. J., & John, O. P. (1982, June). Theoretical
and methodological issues in the construction and use of personality inventories—Or why Ms symposium is necessary. Paper
presented at the Bielefeld Symposium on Personality Questionnaires, Bielefeld, Germany.
Ashton, S. G., & Goldberg, L. R. (1973). In response to Jackson's
challenge: The comparative validity of personality scales constructed by the external (empirical) strategy and scales developed
intuitively by experts, novices, and laymen. Journal of Research
in Personality, 7, 1-20.
Beck, M. D., & Beck, C. K. (1980). Multitrait-multimcthod validation of four personality measures with a high-school sample.
Educational and Psychological Measurement, 40, 1005-1011.
Borgen, F. H. (1972). Predicting career choices of able college men
from occupational and basic interest scales of the Strong Vocational Interest Blank. Journal of Counseling Psychology, 19,
202-211.
Burisch, M. (1978). Construction strategies for multiscale personality
inventories. Applied Psychological Measurement, 2, 97-111.
Burisch, M. (1984). \bu don't always get what you pay for: Measuring
depression with short and simple versus long and sophisticated
scales. Journal of Research in Personality, J8(\), 81-98.
Buss, D. M., & Craik, K. H. (1981). The act frequency analysis of
interpersonal dispositions: Aloofness, gregariousness, dominance,
and submissiveness. Journal of Personality, 49, 175-192.
Carroll, J. B. (1952). Ratings on traits measured by a factored
personality inventory. Journal of Abnormal and Social Psychology,
47, 626-632.
Cohen, J. (1965). Some statistical issues in psychological research.
In B. B. Wolman (Ed.), Handbook of clinical psychology. New
York: McGraw-Hill.
Cohen, J. (1969). Statistical power analysis for the behavioral sciences. New York: Academic Press.
Coombs, C. H. (1964). A theory of data. New York: Wiley.
Crewe, N. M. (1967). Comparison of factor analytic and empirical
scales. Proceedings of the 75th Annual Convention of the American
Psychological Association, 2, pp. 367-368.
Cronbach, L. J., & Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 55, 281-302.
Duff, F. L. (1965). Item subtlety in personality inventory scales.
Journal of Consulting Psychology, 29, 565-570.
Eber, H. W. (1977). Comparative validity vs. utility as considerations
in personality scale construction. The Journal of Multivariate
Behavioral Research, 12, 331-333.
Ellis, A. (1946). The validity of personality questionnaires. Psychological Bulletin, 43, 385-440.
Epstein, S. (1979). The stability of behavior: I. On predicting most
of the people much of the time. Journal of Personality and Social
Psychology, 37, 1097-1126.
Fishbein, M., & Ajzen, I. (1974). Attitudes towards objects as predictors of single and multiple behavioral criteria. Psychological
Review, 81, 59-74.
Goldberg, L. R. (1971). A historical survey of personality scales
and inventories. In P. McReynolds (Ed.), Advances in psychological assessment (Vol. 2). Palo Alto, CA: Science and Behavior
Books.

226

Goldberg, L. R. (1972). Parameters of personality inventory construction and utilization: A comparison of prediction strategies
and tactics. Multivariate Behavioral Research Monographs, 72(2)'.
Green, B. F. (1978). In defense of measurement. American Psychologist, 33, 664-670.
Hamilton, D. L. (1971). A comparative study of five methods of
assessing self esteem, dominance, and dogmatism. Educational
and Psychological Measurement, 31, 441-452.
Harris, J. G. (1980). Nomovalidation and idiovalidation: A quest
for the true personality profile. American Psychologist, 35, 729744.
Hase, H. D., & Goldberg, L. R. (1967). Comparative validity of
different strategies of constructing personality inventory scales.
Psychological Bulletin, 67, 231-248.
Hedlund, J. L., Cho, D. W., & Wood, J. B. (1977). Comparative
validity of MMPI-168 factor and clinical scales. The Journal of
Multivariate Behavioral Research, 12, 327-330.
Heilbrun, A. B. (1962). A comparison of empirical derivation and
rational derivation of an affiliation scale. Journal of Clinical Psychology, 18, 101-102.
Hermans, H. J. M. (1969). The validity of different strategies of
scale construction in predicting academic achievement. Educational and Psychological Measurement, 29, 877-883.
Holden, R. R., & Jackson, D. N. (1979). Item subtlety and face
validity in personality assessment. Journal of Consulting and
Clinical Psychology, 47, 459-468.
Holden, R. R., & Jackson, D. N. (1981). Subtlety, information,
and faking effects in personality assessment. Journal of Clinical
Psychology, 37, 379-386.
Hornick, C. W, James, L. R., & Jones, A. P. (1977). Empirical
item keying versus a rational approach to analyzing a psychological climate questionnaire. Applied Psychological Measurement,
1, 489-500.
Jaccard, J. J. (1974). Predicting social behavior from personality
traits. Journal of Research in Personality, 7, 358-367.
Jackson, D. N. (1971). The dynamics of structured personality
tests: 1971. Psychological Review, 78, 229-248.
Jackson, D. N. (1975). The relative validity of scales prepared by
naive item writers and those based on empirical methods of
personality scale construction. Educational and Psychological
Measurement, 35, 361-370.
Kane, J. S., & Lawler, E. E. (1978). Methods of peer assessment.
Psychological Bulletin, 85, 555-586.
Kaufman, L., & Murphy, N. C. (1981). Validation through self and
other ratings on dimensions of six nonstressful multi-scaled personality instruments. Journal of Personality Assessment, 45, 8689.
Klingler, D. E., Johnson, J. H., Giannetti, R. A., & Williams,
T. A. (1977). Comparison of the clinical utility of the MMPI
basic scales and specific MMPI state-trait scales: A test of Dahlstrom's hypothesis. Journal of Consulting and Clinical Psychology,
45, 1086-1092.
Knudson, R. M., & Golding, S. L. (1974). Comparative validity
of traditional versus S-R format inventories of interpersonal
behavior. Journal of Research in Personality, 8, 111-127.
Koss, M. P., & Butcher, J. N. (1973). A comparison of psychiatric
patients' self-report with other sources of clinical information.
Journal of Research in Personality, 7, 225-236.
Kuder, G. F. (1970). Some principles of interest measurement.
Educational and Psychological Measurement, 30, 205-226.
Lamont, D. J. (1983). A three dimensional test of White's effectance
motive. Journal of Personality Assessment, 47, 91-99.
Loevinger, J. (1957). Objective tests as instruments of psychological
theory. Psychological Reports, 3, 635-694.
Loevinger, J. (1965). Measurement in clinical research. In B. B.
Wolman (Ed.), Handbook of clinical psychology. New \brk:
McGraw-Hill.
Marks, P. A., & Seeman, W. (1963). Actuarial description of abnormal personality: An atlas for use with the MMPI. Baltimore,
MD: Williams & Wilkins.

March 1984 • American Psychologist

<-----Page 13----->McCall, R. J. (1958). Face validity in the D scale of the MMPI.
Journal of Clinical Psychology, 14, 77-80.
McGowan, J., & Gormly, J. (1976). Validation of personality traits:
A multicriteria approach. Journal of Personality and Social Psychology, 34, 791-795.
Meehl, P. E. (1945). The dynamics of "structured" personality tests.
Journal of Clinical Psychology, 1, 296-303.
Meehl, P. E. (1972). Reactions, reflections, projections. In J. N.
Butcher (Ed.), Objective personality assessment: Changing perspectives. New \brk: Academic Press.
Meehl, P. E. (1979). A funny thing happened to us on the way to
the latent entities. Journal of Personality Assessment, 43, 564577.
Meehl, P. E., Lykken, D. T., Schofield, W., & Tellegen, A. (1971).
Recaptured item technique: A method for reducing somewhat
the subjective element in factor naming. Journal of Experimental
Research in Personality, 5, 171-190.
Messick, S. (1980). Test validity and the ethics of assessment. American Psychologist, 35, 1012-1027.
Mezzich, J., Damarin, F. L., & Erickson, J. R. (1974). Comparative
validity of strategies and indices for differential diagnosis of depressive states from other psychiatric conditions using the MMPI.
Journal of Consulting and Clinical Psychology, 42, 691-698.
Mischel, W. (1968). Personality and assessment. New York: Wiley.
Norman, W. T. (1969). "To see oursels as ithers see us!": Relations
among self-perceptions, peer-perceptions, and expected peer perceptions of personality attributes. Multivariate Behavioral Research, 4, 417-443.
Overall, J. E. (1974). Validity of the Psychological Screening Inventory for psychiatric screening. Journal of Consulting and
Clinical Psychology, 42, 717-719.

March 1984 • American Psychologist

Reilly, R. R., & Echternacht, G. J. (1979). Validation and comparison
of homogeneous and occupational interest scales. Applied Psychological Measurement, 3, 177-185.
Schwab, D. A., & Heneman, H. G. (1975). Behaviorally anchored
rating scales'. A review of the literature. Personnel Psychology,
28, 549-562.
Shrauger, J. S., & Osberg, T. M. (1981). The relative accuracy of
self-prediction and judgments by others in psychological assessment. Psychological Bulletin, 90, 322-351.
Spitzner, S. (1979). Konstruktion und Evaluation verankerter Ratingskalen in der Funktion von Selbstbeurteilungsinstrumenten.
Unpublished thesis, University of Hamburg.
Taylor, J. B. (1968). Rating scales as measures of clinical judgment:
A method for increasing scale reliability and sensitivity. Educational and Psychological Measurement, 28, 747-766.
Taylor, J. B., Ptacek, M., Carithers, M., Griffin, C., & Coyne, L.
(1972). Rating scales as measures of clinical judgment: III. Judgments of the self on personality inventory scales and direct ratings.
Educational and Psychological Measurement, 32, 543-557.
Thornton, G. C., & Gierasch, P. F. (1980). Fakability of an empirically derived selection instrument. Journal of Personality Assessment, 44, 48-51.
Wetzel, L. C. (1963). Personality traits and cognitive meanings.
Unpublished master's thesis, University of Illinois.
Wiggins, J. S. (1966). Substantive dimensions of self-report in the
MMPI item pool. Psychological Monographs, 80 (Whole No.
630).
Wiggins, J. S. (1973). Personality and prediction: Principles of personality assessment. Reading, MA: Addison-Wesley.

227

