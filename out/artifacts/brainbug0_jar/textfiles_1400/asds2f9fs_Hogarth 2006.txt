<-----Page 0----->Behavioral Decision Making at 50:
Achievements, Prospects, and Challenges

?

Robin M. Hogarth
ICREA and Universitat Pompeu Fabra

Summary. Many people consider that Ward Edwards’ 1954 review paper marks the
beginning of behavioral decision making or the study of how people make decisions.
Fifty years after Edwards’ paper, it is illuminating to reﬂect on the progress of the
ﬁeld over the last ﬁve decades and to ask what the next ﬁfty years might have in store.
To do this, I identify ten major ideas or ﬁndings that have emerged to date. These
are: (1) that judgment can be modeled; (2) bounded rationality; (3) to understand
decision making, understanding tasks is more important than understanding people;
(4) levels of aspiration or reference points and loss aversion; (5) heuristic rules; (6)
adding and the importance of simple models; (7) the search for conﬁrmation; (8) the
evasive nature of risk perception; (9) the construction of preference; and (10) the
roles of emotions, aﬀect, and intuition. I further identify major challenges currently
facing the ﬁeld. These include linking knowledge to the growing body of work in
neuroscience, developing methodologies that can generalize experimental results,
having more impact on helping people make decisions, and extending collaboration
with other disciplines in the social sciences.

1 Introduction
The area of research called “judgment and decision making” involves researchers from several disciplines and especially psychology, economics, and
statistics. It has also permeated many applied ﬁelds such as accounting, marketing, and medicine. Central to this research is the interplay between two
kinds of models: on the one hand, normative models of how decisions should
be made; and, on the other hand, descriptive models of how people actually
make decisions. The research is typically pursued because, ﬁrst, judgment and
?

The author also thanks Paul Slovic for constructive comments and acknowledges
the support of the Spanish Ministerio de Educación y Ciencia . While revising
this paper after the conference, the author learned of the death of Ward Edwards
in February 2005. He hopes that this paper can be thought of as a small tribute
to Edwards and the work that Edwards inspired.

<-----Page 1----->36

Robin M. Hogarth

choice are ubiquitous and important human activities that people ﬁnd interesting and, second, there is a general expectation that scientists might help
people make better decisions.
People (and animals) have, of course, made decisions since the dawn of
time. However, the formal study of decision making can probably only be
dated to the year 1654 when Fermat and Pascal started to correspond about
how to use mathematics in certain forms of gambling 1 . One interesting dimension about this correspondence was that a “good” strategy could be formalized
and that this was not necessarily what people were actually doing.
Three centuries later, Ward Edwards [21] published what is generally considered the seminal contribution to the modern study of judgment and decision making. This was an extensive review of studies from economics and
psychology in which human behavior on decision making tasks was compared
to predictions generated by models from economics and statistics. The timing
of this review was propitious in that testable theoretical formulations of decision making models had been and were about to be published (von Neumann
& Morgenstern [102]; Savage [84]) and American psychologists - recently freed
of the shackles of behaviorism - were about to engage in the so-called cognitive
revolution which allowed them to ask questions about how people think (cf.
Bruner, Goodnow, & Austin [4]). In addition, the advent of usable computers
added two dimensions. One was the ability to exploit computational capacity.
The other was the use of the computer as an information processing metaphor
of the mind.
In this paper, I reﬂect on progress made in the past ﬁve decades and identify challenges and opportunities facing the ﬁeld at the present time. To do
this - as well as to comment on historical developments - the paper contains
two main sections. In the ﬁrst, I identify ten key ideas or ﬁndings2 that I
believe represent the most signiﬁcant advances. In the second, I outline what
I see as the major challenges. In short, I shall conclude that the ﬁeld is currently in good health and that future generations of researchers face wonderful
opportunities.
In terms of the sociology of the ﬁeld, it is important to note that the leading
professional society in the United States recently celebrated its 25th annual
meeting with some 350 scientists in attendance. Its counterpart in Europe
has been holding meetings every two years since the early 1970s. There is a
specialized medical decision making society (that hosts annual meetings) and
sessions on judgment and decision making are typically held in meetings organized by social and experimental psychologists, management and organization
scientists, economists, and scholars in accounting and marketing. There are
1

2

This is, of course, a very Western perspective. I would be curious to know, for
example, whether Chinese scientists had not made similar investigations. They
were certainly much more advanced than their Western counterparts in the Middle
Ages.
The terms “ideas” or “ﬁndings” are used in quite a broad sense. Also, this presentation beneﬁts much from previous thoughts on the topic (Hogarth [47]).

<-----Page 2----->Behavioral Decision Making at 50

37

two major journals (Organizational Behavior and Human Decision Processes
and the Journal of Behavioral Decision Making) but the majority of research
- and often the seminal papers - is published in leading journals within disciplines, e.g., of psychology and economics. Researchers who identify with the
ﬁeld of judgment and decision making tend to work at the intersection of ﬁelds
in departments of psychology or business.
Whereas judgments and decisions ultimately result in observable actions,
researchers tend to focus eﬀorts on the distinctive components of inference
and preference. Using the economic model of choice as a conceptual metaphor,
work on inference covers processes of judgments or beliefs; work on preferences
deals with issues of utility, e.g., risk. An important question in the research
focuses on deciding when a judgment or choice is or is not “good.” Two criteria are used: “correspondence” or the extent to which judgments or decisions
are in correspondence with some empirical reality (e.g., the accuracy of forecasts of, say, the weather, or somebody’s actions); and “coherence” or the
extent to which a judgment or decision is consistent (i.e., coherent) with dictates of normative models (e.g., the conjunction rule in probability theory,
Tversky & Kahneman [99]; or invariance of expressed preference under different modes of elicitation, Lichtenstein & Slovic [71]; Grether & Plott [37]).
Whether researchers should use correspondence or coherence criteria in evaluating judgments and decisions has been the topic of heated debate (Hammond
[40, 41]).
The main topic of this book is experimental economics. Although I have
done some work in this area, it is not my intellectual home. I hope, nonetheless,
that my comments will be seen as relevant to this important and growing area
of social science.

2 Ten Key Ideas or Findings
I now identify my choice of the ten key ideas or ﬁndings and ask the reader’s
indulgence for my personal and obviously subjective selection.
1. That decisions can be modeled. As revealed by Edwards’s review,
prior to 1954 several studies compared the decisions people made with the
outputs of simple expected utility models. The goal of this work, however,
was not to model people’s choices as such but to assess whether they were
consistent with expected utility. The mindset of the time was to make “as if”
interpretations of the consistencies and inconsistencies observed.
In 1955, Kenneth Hammond published a paper in the Psychological Review
that opened the path to modeling psychological processes of judgment. Hammond’s insight was to make the connection between processes of perception
(as conceptualized by Brunswik [5]) with those of judgment. Both, according
to Hammond, involved inferences concerning a criterion made on the basis of
cues. (Consider, for example, the similarity between the tasks of recognizing
a friend - a perceptual judgment - and inferring the age of a person you have

<-----Page 3----->38

Robin M. Hogarth

just met.) Thus, assuming that people’s judgments are not inconsistent, it is
possible to model the process by expressing judgments as mathematical functions of the characteristics of the informational cues on which they are based.
In this case, one does not need a normative model of choice (such as expected
utility theory) with which to compare outcomes (i.e., a coherence criterion).
Instead, one can compare outcomes with empirical phenomena and investigate
why the person does or does not achieve correspondence (i.e., depending on
how cues have been weighted, inconsistencies, and so on).
One can, of course, argue that such mathematical descriptions are not
really “processes” (I shall return to this issue below) but despite obvious
diﬃculties the modeling approach has proven most useful (cf., Hoﬀman [44]).
First, as exploited by Hammond, Brehmer and others, much has been learned
about processes of learning and conﬂict resolution (Brehmer [3]). Learning, for
example, can be quite sensitive to what type of feedback is received (Balzer,
Doherty, & O’Connor [1]) and inconsistencies in the expression of judgmental
strategies can greatly impede the resolution of conﬂict (Hammond [41]).
Second is the fact that cognitive models of experts are often better predictors of given criteria than the experts themselves. This ﬁnding, known as
“bootstrapping” (Goldberg [35]), was important for at least two reasons. One
is the notion that a model can extract the essence of a judgmental strategy from a noisy process. The other is the practical implication. Even if no
criterion variable is available (imagine predicting success in a new kind of occupation), a model of an expert’s judgments is likely to provide more accurate
predictions than the expert. In more modern language, bootstrapping models
are parsimonious forms of “expert systems” that can be used in prediction
tasks even though their inherent simplicity belies the apparent complexity
underlying most expert systems.
Models of processes do, however, raise the issue of what is meant by an adequate process model. Clearly, models must meet predictive tests with external
criteria. However, most psychologists would argue that they should do more.
This raises the issue of the level at which processes are modeled and what
ancillary data should and could be marshaled in their support. One clue to
this question was provided by Einhorn, Kleinmuntz, and Kleinmuntz [26] who
showed that simple linear models are capable of mimicking the more complex
processes that are typically represented by so-called process-tracing models.
This, in turn, suggests that process-tracing models may be mimicking more
complex, underlying psycho-physiological models of information processing.
Indeed, recent work in neuroscience attempts to link process models of decision making with levels of activation that are measured in diﬀerent areas of
the brain (see below).
The practice today is to support (or refute) models of decision making
by providing measures of process (e.g., reaction times) that are consistent (or
inconsistent) with the use of diﬀerent models. Indeed, the development and
availability of computers has vastly facilitated the collection of data and illuminated many process issues (Payne, Bettman, & Johnson [82]). Parentheti-

<-----Page 4----->Behavioral Decision Making at 50

39

cally, it is worth noting that prior to roughly 1980 many decision researchers
tended to look down on data collection methods that did not use objective
statistical tools (such as regression models). However, today most researchers
acknowledge the importance of ancillary measurements of process and use
multiple methods.
2. Bounded rationality. For many researchers, the concept of bounded
rationality introduced by Herbert Simon is perhaps the key concept in the
study of judgment and decision making. As stated by Simon,
The capacity of the human mind for formulating and solving complex
problems is very small compared with the size of the problems whose
solution is required for objectively rational behavior in the real world
- or even for a ﬁrst approximation to such objective rationality (Simon
[90], p. 198).
The importance of Simon’s ideas was explicitly recognized by Daniel Kahneman in his Nobel prize lecture:
..., we (Tversky and Kahneman) explored the psychology of intuitive
beliefs and choices and examined their bounded rationality. Herbert
A. Simon [88, 92] had proposed much earlier that decision makers
should be viewed as boundedly rational, and had oﬀered a model
in which utility maximization was replaced by satisﬁcing. Our research attempted to obtain a map of bounded rationality, by exploring
the systematic biases that separate the beliefs that people have and
the choices they make from the optimal beliefs and choices assumed
in rational-agent models. The rational-agent model was our starting
point and the main source of our null hypotheses. (Kahneman [58], p.
1449, names in italics added).
It is clear that the concept of bounded rationality has been critical to
the study of judgment and decision making in emphasizing the importance of
cognitive limitations relative to the unlimited powers assumed by maximizing
models in economics. However, it is also the case that the concept has been
the subject of misinterpretation.
For example, in an extensive review written for economists and entitled
“Why bounded rationality?” Conlisk [14] cites many studies (and reviews of
studies) where people’s judgments and decisions violate economic reasoning.
These studies, according to Conlisk, attest to the importance of bounded
rationality. However, just because people are observed to make systematic errors, it is not clear that these are a consequence of bounded rationality per
se, i.e., of limitations in cognitive capacity (note the exact wording of the
Simon quote above). For example, imagine that when using past performance
to predict future performance of an athlete, a person fails to allow for regression toward the mean. Is this due to lack of computational capacity? If it
is, then one should predict that no human could make the correct judgment.

<-----Page 5----->40

Robin M. Hogarth

And yet, we know that making a cognitive adjustment to a prediction to allow for regression eﬀects requires little cognitive eﬀort. All it really requires
is the ability to recognize that one is dealing with a case involving regression eﬀects, something that can either explicitly taught or learned through
experience (Nisbett, Fong, Lehman, & Cheng [77]). Moreover, our ability to
recognize (situations, faces, patterns, and so on) is one cognitive ability that
could almost be considered unbounded (Goldstein & Gigerenzer [36]).
In this sense, I would also argue that Kahneman was selling his and Tversky’s work short by describing it as “mapping bounded rationality.” True,
they were exploring diﬀerences between systematic responses and normative
prescriptions. But, most of their enlightening experimental demonstrations
did not exploit bounds on cognitive processes. Indeed, the simplicity of their
paradigmatic problems characterized their appeal and facilitated their diﬀusion through the scientiﬁc community3 . Indeed, as Kahneman ([58], p. 1469)
himself acknowledges, many erroneous responses to these problems arose because people simply answered a diﬀerent (or easier) problem than that asked
by the experimenters. Sure, cognitive processes have a cost but this does not
mean that they are “bounded” relative to the task at hand. In many cases,
researchers have been studying behavior that could be more aptly labeled as
“unboundedly irrational.”
Finally, Simon’s insight that decision makers do not possess unbounded
cognitive capacities was fundamental in allowing people to conceive of alternative models of decision making. The spirit of these models, however, is not
just to document the consequences of bounds but a search to make models
more veridical from a descriptive viewpoint. Indeed, this viewpoint permeates
practice in what is now called “behavioral economics.”
3. To understand decision making, understanding the task is
more important than understanding the people. In many ways, much
of the research in the ﬁeld attests to this statement. At one level, it is related
to the idea of bounded rationality in that organisms do not have, a priori,
unbounded computational resources that allow them to deal with an inﬁnite
variety of tasks. How then, do they have the capacity to deal with diﬀerent
tasks?
The explanation is evolutionary in nature. It was emphasized by Brunswik
[5] and echoed by Simon [89]. The key idea is that the speciﬁc response (or
decision) mechanisms that organisms develop are conditioned by the kinds of
environments they have encountered in the past. Thus, to understand what
organisms can and cannot do, it is important to study what they have been
required to do by the situations with which they have been confronted. Organisms adapt to the environments they inhabit and thus understanding the

3

This is not true, of course, of all the Tversky-Kahneman problems. Some, for
example, explicitly exploit diﬃculties of computation to hide the detection of
dominance in choice problems (see, e.g., Tversky & Kahneman [101]).

<-----Page 6----->Behavioral Decision Making at 50

41

nature of the tasks they face leads to understanding how they have learned
to cope with those tasks.
There are many examples of these principles in the literature. Let me
cite a few. First, consider the extensive work conducted by John Payne and
his colleagues on “contingent decision making” (Payne [80]; Payne, Bettman,
& Johnson [82]). These researchers have provided extensive documentation
of how simple decision strategies reﬂect and adapt to environmental characteristics. Second, there is a substantial literature on various forms of preference reversals whereby diﬀerent presentations of the same judgment or choice
problems lead to systematic changes in decisions. I refer here to the classic
preference reversal phenomenon that has been studied in economics (Grether
& Plott [37]), framing eﬀects in choice (Kahneman & Tversky [61], see also
below), evaluating options together with alternatives or in isolation (Hsee,
Loewenstein, Blount, & Bazerman [56]), and so on. Third, there is a class of
problems in inference where stating problems in terms of frequencies instead of
probabilities induces systematic changes in responses (Tversky & Kahneman
[100]; Gigerenzer & Hoﬀrage [32]).
One feature that almost all these problems have in common is that people
are sensitive to the information processing costs of diﬀerent task structures (or
problem formulations). They tend to take the “less costly” options. However,
and as noted above, this does not mean that the more costly solutions are
beyond computational bounds.
4. Levels of aspiration or reference points and loss aversion. A
key notion discussed by Simon [88] is that of levels of aspiration or reference
points (see also Siegel [87]). In Simon’s approach, aspiration levels were seen
to be a mechanism by which decision tasks are simpliﬁed so that people can
use “satisﬁcing” mental strategies (his speciﬁc example concerned setting the
selling price of a house).
From a broader psychological perspective, the aspiration level or reference
point idea is related to the more pervasive perceptual phenomenon of ﬁgure
and ground. In any given environment, we need to focus attention and tend
to do so on what is salient thereby simultaneously separating the perceptual
ﬁeld into what is salient (ﬁgure) and what is not (ground). Although this is
clearly a generally adaptive mechanism (it is hard to think of an alternative),
it comes at a cost. Salience is relative and what is salient can change with
shifts in the ground.
The literature is replete with examples of this phenomenon in tasks involving both choice and inference. For the former, consider demonstrations of
Weber’s rule, e.g., $10 might seem a large sum when paired with $20 but small
when paired with $2,000. For the latter, the attribution of cause is greatly affected by assumptions concerning the causal background (Einhorn & Hogarth
[25]; McGill [75]).
There is no doubt that Kahneman and Tversky’s [61] prospect theory
paper has been one of the highlights of the total research program. In my
opinion, the key insight in this was linking the notions of reference points

<-----Page 7----->42

Robin M. Hogarth

and loss aversion, i.e., that “losses loom larger than gains.” By itself, loss
aversion is a critical concept. However, allowing the deﬁnition of gains and
losses to vary as a function of reference points or aspiration levels facilitated all
kinds of predictions of so-called framing eﬀects going from stunning switches
in risk attitudes in laboratory experiments to observations in ﬁeld studies
(Kahneman & Tversky [62]; Johnson & Goldstein [57]).
5. Use of heuristic rules. No discussion of the ﬁeld of judgment and
decision making could avoid reference to so-called “heuristic” rules. The use of
the word “heuristic” in this sense originates with Tversky and Kahneman [99]
in their famous Science paper on “heuristics and biases” (see also Kahneman,
Slovic, & Tversky [60]). The claim made by Tversky and Kahneman was quite
simple and measured: In making certain kinds of judgments under uncertainty,
people use speciﬁc decision rules that, although, often accurate and useful, can
sometimes lead to systematic errors relative to normative models.
Tversky and Kahneman speciﬁed three heuristics (representativeness,
availability, and anchoring - and - adjustment) as well as simple examples
of how the use of each heuristic could lead judgment astray. The examples
were incredibly compelling and, I believe, it was this fact that turned many
scientists into either advocates of heuristics and biases (e.g., Nisbett & Ross
[78]) or conﬁrmed skeptics. Among the objections raised against the approach
we ﬁnd two main positions: Tversky and Kahneman used inappropriate normative models in assessing biases (see, e.g., Cohen, [13]) and their heuristic
“models” were not precisely speciﬁed (Gigerenzer [30].) My own view is that
Tversky and Kahneman had developed incredibly important insights about
how people assess uncertainty. What was missing was an environmental theory that speciﬁed more precisely the interaction between heuristics and tasks
(see Hogarth [46]; Kahneman & Frederick [59]).
On a historical note, it should be added that Tversky and Kahneman’s
early work on biases induced an important change in work on judgments under
uncertainty. Before their investigations, most work on this topic centered on
how well people’s judgments did or did not match statistical reasoning. For
example, in 1967, Peterson and Beach had published an inﬂuential review on
this topic entitled “Man as an intuitive statistician” and Ward Edwards and
his colleagues had conducted extensive investigations of people’s ability (and
inability) to make judgments that were in accord with Bayes’ theorem (see,
e.g., Edwards [22]). Tversky and Kahneman shocked their readers with their
how well results (which were bad!) but reoriented investigations by asking how
people make judgments under uncertainty. The focus became the process that
produced the outcomes.
That simple heuristics can also produce good judgments and decisions has
been the subject of work by Gerd Gigerenzer and his colleagues (Gigerenzer &
Goldstein [31]; Gigerenzer, Todd, & the ABC Group [34]). They have shown
empirically how certain simple rules of decision making can perform remarkably well compared to more complex, algorithmic benchmarks. However, as
yet their work is incomplete (in the same manner as that of Tversky and

<-----Page 8----->Behavioral Decision Making at 50

43

Kahneman) in that they have no theory of how, why and when environmental
factors inﬂuence the performance of their models (cf., Hogarth & Karelaia
[52, 53, 54]).
6. Adding and the importance of simple models. Perhaps one of
the most important, practical results is that, compared to unaided human
judgment, simple linear models are remarkably eﬀective at making predictions. This has been demonstrated time and again, and in many diﬀerent
contexts (Meehl [76]; Sawyer [85]; Goldberg [35]; Dawes, Faust & Meehl [18];
Kleinmuntz [66]; Grove et al. [38]). Although cases have been reported where
humans have outpredicted models (see, e.g., Libby [70]), these are exceptional.
At one level, these ﬁndings might be attributed to the notion of bounded
rationality in the sense that the human mind is not as eﬃcient as a computer in
making calculations. On the other hand, the fact that simple models exhibit
superior predictive ability is also a statement about our lack of knowledge
concerning the environments in which predictions are made. For many tasks,
it turns out that although linear models have higher correlations with the
criterion than human judgments, the correlations are not that impressive (see,
e.g., Camerer [7]). In addition, people typically want to use more information
(i.e., variables) in their judgments than the models.
In addition to the superiority of simple models over human judgment,
much work shows that simple linear models are often superior to more complex
algorithms for predicting important criteria (Dawes & Corrigan [17]; Einhorn
& Hogarth [23]). I return to this below.
The ﬁndings of model superiority have not always been embraced with the
enthusiasm one might suspect given their practical potential. One reason, I
believe, is that investigators have been too quick to celebrate the apparent
superiority of models and have failed to understand the conditions under
which human judgment can be used as an adjunct to or even instead of model
predictions. On the other hand, several researchers have recognized that the
issue is not one of “models vs. humans” but of how one can best combine
human and statistical models (see, e.g., Blattberg & Hoch [2]; Kleinmuntz
[66]; Yaniv & Hogarth [105]). Paradoxically, it could be argued that fairly
complex expert systems built within the cognitive science tradition have been
better accepted than the simpler models developed in judgment and decision
making research. It should be noted, however, that whereas researchers in the
latter ﬁeld have been quick to pinpoint deﬁciencies in human reasoning, the
builders of expert systems implicitly revere the outputs of human judgment
by holding it up as a criterion (cf., Camerer & Johnson [8]).
Above, I referred to the work of Gigerenzer and his colleagues. Contrary
to the simple models discussed above, their models do not involve averages
or sums of several variables but tend to be lexicographic such that decisions
frequently depend on a single variable. Thus, the key aspect of their work
involves decisions to “choose the best” and “ignore the rest.” An important
implication of their work is the ﬁnding that there are many situations in

<-----Page 9----->44

Robin M. Hogarth

which models that are “fast and frugal” (i.e., easy to implement and using
little information) are quite eﬀective4 .
7. Search for conﬁrmation. In their classic studies of human inference,
Bruner, Goodnow, and Austin [4] noted what they called the “thirst for conﬁrming redundancy.” This referred to the fact that in solving inferential problems, people have a strong tendency to search for information that conﬁrms
their favored hypotheses. This tendency was brilliantly exploited by Wason’s
classic four-card and “2-4-6” problems that demonstrated that people have a
strong proclivity for seeking conﬁrming evidence (Wason [103, 104]).
These ﬁndings led to two interesting developments. The ﬁrst was a purely
theoretical investigation by Klayman and Ha [65]. Accepting the fact that
people do have a tendency to seek conﬁrmation, they questioned previous
research by asking whether, in fact, this was dysfunctional. Speciﬁcally, they
conducted a task analysis of the eﬀectiveness of a decision strategy that they
labeled the “positive test heuristic” - i.e., a strategy that seeks to prove that
one’s ideas are correct - and showed that, under a wide range of environmental
conditions, this is optimal. In eﬀect, they argue that the task used by Wason
was a special case and not representative of most inference tasks.
The second development was to question the experimental generality of
the Wason task. Speciﬁcally, consider the classic Wason task:
Imagine that you have in front of you four cards that show (from left
to right) the letter A, the letter B, the number 4, and the number 7.
You are informed that each card has a letter on one side and a number
on the other. You are also informed of the rule, “If a card has a vowel
on one side, it has an even number on the other side.” Given the four
cards in front of you, which and only which cards would you need to
turn over to verify whether the rule is correct?
As is well known, the most frequent answers to this question are the letter
A and the number 4 or just the letter A. In testing a rule of the form if p then
q, one should test A (to test that p and q do both co-occur) and 7 (to verify
that p and not-q do not co-occur). For most people, however, this problem is
quite opaque.
Now consider the following variation of the problem that has exactly the
same logical structure:
Imagine that you work in a bar and have to enforce the rule that, in
order to drink alcoholic beverages, patrons must be over twenty-one
4

Gigerenzer and Goldstein [31] also document intriguing examples of what they
call the “less is more” eﬀect, i.e., where the predictive ability of people with little
knowledge is better than that of people with more knowledge. Interestingly, earlier
Simon [91] noted that in a world where attention is a scarce resource “information
may be an expensive luxury, for it may turn our attention from what is important
to what is unimportant. We cannot aﬀord to attend to information simply because
it is there” (Simon [91], p. 13).

<-----Page 10----->Behavioral Decision Making at 50

45

years old. You observe four “young” people in the bar: the ﬁrst is
drinking beer; the second is drinking Coke; the third is twenty-ﬁve
years old; and the fourth is sixteen years old. Whom do you check
in order to verify that the rule is being enforced? (Gigerenzer & Hug
[33]).
Most people have little diﬃculty answering this question correctly, i.e.,
they would check the ﬁrst and fourth persons. What happens here, of course,
is that the context of the problem structures the problem for the respondent
such that the “correct” solution seems obvious. Exactly why this happens is
a controversial issue in the literature. However, I shall return to this example
below.
8. The evasive nature of risk perception. What is risk and how do
people perceive it? Forty or so years ago, most people thought of risk in
strictly consequential terms, i.e., the probability of a negative event occurring
and the amount of the associated loss. Thus, when explaining risks of, say,
new technologies to the public, the most important issue was to ensure that
probabilities of loss were “correctly” understood. It was also thought that
studying how people deal with gambles in laboratory settings would illuminate
how they conceive and face risks in everyday life.
Starting in the 1970s, these ideas changed dramatically. First, the work
on “heuristics and biases” questioned whether people could understand the
meaning of probabilities in risk analysis (Slovic, Fischhoﬀ, & Lichtenstein
[96]). Second, a large research program, that involved thousands of respondents from diﬀerent walks of life and countries who answered many batches of
questions, revealed new insights into the nature of risk. Speciﬁcally, this psychometric approach factor analyzed assessments of many risks and revealed
two important dimensions or factors (Slovic [93]). One is the extent to which a
risky activity or technology induces a sense of dread, e.g., nerve gas accidents
induce a lot of dread; caﬀeine and aspirin do not. The second factor is the
extent to which the technology is unknown or unfamiliar to people, e.g., auto
accidents do not represent an unknown risk; however, DNA technology does.
A major impact of these studies is the realization that risk - as understood
by people in their everyday lives - is a complex, multidimensional concept that
can not be described simply by, for example, postulating diﬀerent shapes of
utility functions. Recent work in this area, therefore, has sought to understand
how people come to assess risk in diﬀerent circumstances, how information
about risk can best be transmitted, and the manner in which it is or is not
perceived diﬀerently by diﬀerent groups, e.g., substantive experts vs. novices,
or men vs. women (see Slovic [94]). The notion that risk is heavily dependent
on feelings has gained increasing recognition even though it cannot be said
that this has led to elegant models of behavior (Loewenstein, Hsee, Weber, &
Welch [73]) - see also below.
9. The construction of preference. Within the normative framework,
people make decisions according to preferences that are conceptualized in the

<-----Page 11----->46

Robin M. Hogarth

form of utility functions. In an as-if approach, it is convenient to imagine that
people simply look up the appropriate values of their utility functions when
making choices. If this were the case, however, it would be hard to understand
how people could be subject to preference reversals and other inconsistencies
in revealed preferences.
The view adopted by several researchers is that, instead of consulting existing preference functions, people choose by constructing preferences in light
of the tasks with which they are confronted (Slovic, Griﬃn, & Tversky [97];
Payne, Bettman, & Johnson [81]). The notion is not that all preferences are
constructed from nothing but rather that people only have quite general notions of their preferences, e.g., although they prefer more to less money and
less to more eﬀort, they do not have precise representations of their hypothetical utility functions and so can not execute precise tradeoﬀs. Therefore,
when confronting speciﬁc tasks that require precise answers (consider, e.g.,
willingness-to-pay judgments) the precision of their preferences will be affected by normatively irrelevant considerations such as the kinds of scales on
which they are required to respond or the presence/absence of comparison
points (Hsee [55]).
The position taken here makes much sense and it is perhaps surprising
that researchers took so long to reach it. Simply stated, it gives meaning to
the notion that people have preferences that, depending on their experience,
can be described as varying in looseness. If one’s preferences are loose and
have to be expressed precisely in speciﬁc circumstances, irrelevant task eﬀects
should be expected to occur. If, on the other hand, preferences are not loose,
they should be relatively impervious to how they are elicited. Thus, experience
in speciﬁc domains should lead to less “looseness” and greater invariance to
irrelevant factors. At the same time, one might expect ﬁnancial incentives to
have the same kinds of eﬀects but this has not always been found to be the
case (Camerer & Hogarth [9]).
Related to this topic is the important ﬁnding that people seek to avoid
making tradeoﬀs (see, e.g., Luce, Payne, & Bettman [74]). There are two
reasons: ﬁrst, tradeoﬀs can be hard to execute from a cognitive viewpoint;
and second, making tradeoﬀs is diﬃcult emotionally. This raises the issues
of how much people really lose by avoiding tradeoﬀs (in terms of making
optimal decisions) and whether they can ever learn to make certain kinds of
tradeoﬀs. To be able answer both of these questions, however, one would need
an extensive ecological analysis.
10. The role of emotions, aﬀect, and intuition. As noted above, the
study of judgment and decision making owes much to formal models of decision
making in economics and statistics (e.g., expected utility and subjectively
expected utility) as well as the cognitive revolution in psychology. Thus the
emphasis of most work across the last ﬁve decades has been based on cognitive
explanations with little apparent interest in the topics of emotion, aﬀect, and
intuition. Thus, for example, several key papers demonstrated how cognitive
explanations could account for phenomena that were previously thought to be

<-----Page 12----->Behavioral Decision Making at 50

47

motivational in origin (see e.g., Brehmer [3], for why conﬂicts are sometimes
hard to resolve, and Einhorn & Hogarth [24], for biases in learning).
Recent years, however, have seen increasing recognition of the importance
of emotions, aﬀect, and intuition in decision making. It is thus illuminating
to trace the origins of this trend.
In 1968, Robert Zajonc published a remarkable paper in which he documented a phenomenon known as the mere exposure eﬀect, the ﬁnding that
people acquire positive aﬀect to stimuli through mere exposure. His observations started by noting that people seem to have greater positive aﬀect for
more frequently occurring objects. He illustrated this by examining words.
For example, he presented people with lists of antonyms, that is, pairs of
words that have opposite meanings such as forward/backward, high/low, or
on/oﬀ. He then asked his subjects to state which word in each pair had “the
more favorable meaning, represented the more desirable object, even, state of
aﬀairs, characteristic, etc.” (Zajonc [106], p. 5) Zajonc also determined the
frequency with which each word was used in English. His results showed a
stunning relation between preferences and relative frequencies. For example,
at least 97 percent of his subjects preferred forward to backward, high to
low, and on to oﬀ. In English, the ﬁrst of each of these words appears more
frequently than the second. For example, forward is approximately 5.4 times
more frequent than backward. The analogous ﬁgures for the other pairs are
1.4 and 8.3, respectively5 .
Zajonc went on to provide further demonstrations of these eﬀects using
fake words said to be Turkish, Chinese-like characters, and photographs of
students. It was further demonstrated that even when people’s ability to identify stimuli is degraded to chance levels, positive aﬀect is also a function of
mere exposure (Kunst-Wilson & Zajonc [67]).
Why does this aﬀect occur? Noting that none of the stimuli investigated involve negative consequences, Zajonc proposed that increasing familiarity with
stimuli that are not harmful induces positive aﬀect through a simple process
of learning. In other words, it is more important to learn when something new
is potentially harmful or dangerous than when it is not. Thus, if something
new is not perceived as negative, its default value is positive. Moreover, the
more it is seen, the more positive aﬀect is reinforced.
Zajonc’s work has profound implications for how people acquire preferences, a topic that is clearly outside the scope of economics and that has
hardly been touched by researchers in judgment and decision making. However, as simple casual observation can inform us, many ﬁrms bet much money
in their advertising campaigns on the veracity of the mere exposure eﬀect.
Consider, in particular, advertisements that simply emphasize the name of a
product or a ﬁrm without providing any information about particular products, e.g., Benetton or, say, Toshiba. Here the advertising strategy simply

5

The frequencies are taken from the so-called L count of Thorndike and Lorge [98].

<-----Page 13----->48

Robin M. Hogarth

consists of reinforcing the familiarity of the brand name through repeated
exposure.
I have often wondered why decision researchers have not pursued the implications of the mere exposure eﬀect. In particular, the phenomenon implies
that tacit learning is an important source of preferences and that one might
be able to predict a person’s preferences by understanding the experiences to
which he or she has been exposed. One reason could be the lack of a normative model of preference updating to provide a benchmark. For instance,
the existence of Bayes’ theorem provides a clear benchmark for studying how
people update beliefs and the documentation of systematic departures from
the standard (see, e.g., Hogarth & Einhorn [51]). Lacking a counterpart for
preferences, it is less obvious how to proceed6 .
Finally, building on his work on the mere exposure eﬀect, in 1980 Zajonc challenged the then overtly cognitively-oriented decision researchers by
suggesting that “preferences need no inferences” or that aﬀect (acquired subconsciously through mere exposure) was often the driver of judgment and
choice as opposed to more cognitive forces. As discussed below, Zajonc’s work
has gained signiﬁcant attention.
In judgment and decision research, the concept of intuition — although not
neglected — has often had negative connotations. The “intuitive predictions”
of clinicians, for example, are often disparagingly said to be inferior to those of
statistical models. Similarly, intuition is associated with processes that cause
people to make errors in judgments under uncertainty (cf., Tversky & Kahneman [100]) as well as unwarranted conﬁdence in judgment (Kahneman, Slovic,
& Tversky [60]).
One notable exception, however, has been the work of Hammond who
— building on Brunswik’s [5] perceptual model — conceived of modes of
judgment that can vary on a continuum from highly intuitive at one end
to highly analytical at the other. In addition to this cognitive continuum,
Hammond also postulated that tasks confronted in the environment vary on
the extent to which they induce intuitive or analytical thought. Moreover,
when modes of cognition coincide with demands of tasks, judgments tend
to be better (using a correspondence criterion). In an intriguing study of
judgments made by highway engineers, Hammond and his colleagues found
supporting evidence for this position (Hammond, Hamm, Grassia, & Pearson
[42]).
The ideas pioneered by Zajonc (see above) had increasing inﬂuence in
social psychology in the 1980s and culminated in what are generally known
as dual-process theories of thought (Chaiken & Trope [12]). The key idea here
6

The notion that one might think of modeling the learning of preferences (and thus
that they could change) raised some interesting discussions at the conference. For
economists, preferences are typically assumed to be exogenous and ﬁxed. Thus,
it seems odd even to consider the acquisition of preferences and how these might
change.

<-----Page 14----->Behavioral Decision Making at 50

49

is that thought has components of which people are aware (conscious) and
of which they are not fully aware (subconscious) and that people can process
information in two distinct ways.
This distinction is clearly made by Seymour Epstein [28] who has distinguished between what he labels experiential and rational modes of thought.
As Epstein states, the experiential system is driven by emotions, it is intuitive,
and automatic. It
is assumed to have a very long evolutionary history and to operate in
non-human as well as human animals . . . it is a crude system that automatically, rapidly, eﬀortlessly, and eﬃciently processes information
. . . . Although it represents events primarily concretely and imagistically, it is capable of generalization and abstraction through the use of
prototypes, metaphors, scripts, and narratives. (Epstein [28], p. 715).
The rational system, on the other hand,
is a deliberative, eﬀortful, abstract system that operates primarily in
the medium of language and has a very brief evolutionary history. It
is capable of very high levels of abstraction and long-term delay of
gratiﬁcation. (Epstein [28], p. 715).
As evidence for these two modes Epstein points to the fact that, when
emotions are aroused, people tend to eschew logical arguments, emotions may
themselves be triggered by preconscious thoughts, and that there is an important diﬀerence between learning intuitively through experience and intellectually through instruction or analysis. He also notes that although people are
capable of rational analysis they are still heavily inﬂuenced by story telling,
the appeal of pictures over words or statistics, and superstitious thinking. In
the experimental work conducted to support this position, he leans heavily
on vignettes created by Tversky and Kahneman to show that people can recognize when they are using the diﬀerent models of thought (Epstein, Lipson,
Holstein, & Huh [29]) as well as some intriguing choice tasks (Kirkpatrick &
Epstein [64]; Denes-Raj & Epstein [19]).
It is important to emphasize that Epstein (like Hammond) does not say
that people reason only with either the experiential or rational mode. Typically, reasoning will start with the experiential mode and be modiﬁed by
the rational, depending on circumstances. As an example, reconsider the two
versions of the Wason task described above. In the abstract version, it is difﬁcult to think through the task because (unless we are logicians) we have no
appropriate model. Moreover, our rational thought system may not be up to
solving the problem appropriately. For the version set in the bar, however,
we do have experiences that provide a “script” (see Epstein quote above) for
thinking through the task. This is much easier for us and we can reach the
correct solution almost eﬀortlessly.
Here I have given much credit to Hammond and Epstein because they were
pioneers in promoting a more comprehensive view of judgment and decision

<-----Page 15----->50

Robin M. Hogarth

making as involving more than just cognitive components. Similarly, Slovic
and his collaborators have provided much evidence emphasizing the role of
aﬀect in judgment and decision making (particularly concerning the assessment of risk - see above). Indeed, Slovic and his colleagues have recently made
a forceful argument that many judgments are driven by an “aﬀect heuristic”
(Slovic, Finucane, Peters, & MacGregor [95]).
Kahneman [58] has also come to accept that judgmental processes are
the product of two systems of thought. Moreover, the intuitive component
is not only large but quite eﬀective. (Consider again the two versions of the
Wason task discussed above.) What is fascinating is the fact that much of
the interest in the ﬁeld was stimulated by Tversky and Kahneman’s original
demonstrations that “intuitive” processes could lead people to make erroneous
judgments. Tversky and Kahneman saw the errors as a means to understand
the underlying processes of judgment. Many others saw the errors as a general indictment of human judgmental abilities, a viewpoint that is now being
rectiﬁed.

3 Some Major Challenges
So much for the past! Let me now turn my attention to what I consider the
major challenges facing the ﬁeld today. These concern four areas: (1) making links to and incorporating knowledge from neuroscience; (2) developing
methodologies that can generalize laboratory ﬁndings; (3) being more eﬀective
at helping people make decisions; and (4) collaboration with sister disciplines
in the social sciences. A major theme uniting all these concerns is the need
to develop, ﬁrst, more comprehensive theories that integrate ﬁndings from
diﬀerent areas (e.g., recognizing that judgment and decision making involves
aﬀective as well as cognitive components) and, second, more explicit theories
that specify the conditions under which certain main eﬀects (e.g., loss aversion
or ambiguity avoidance) do or do not obtain. This, in turn requires that more
explicit attention be paid to understanding how behavior interacts with the
diﬀerent kinds of environments in which it occurs (cf., the third key “idea or
ﬁnding” discussed above).
Neuroscience. If one considers just the last decade or so, there can be no
doubt that the scientiﬁc community has seen remarkable advances in understanding how the brain functions. Whereas we cannot yet measure precisely
what happens when people process information, a number of methods have
revealed glimpses of what our descendants will be able to see more clearly.
The evidence comes from several sources: invasive studies of nonhumans
such as rats and primates whose brains have much in common with humans
(see, e.g., LeDoux [68, 69]); and non-invasive studies of humans. Some of these
studies involve the opportunistic collection of data from people who have particular forms of brain lesions or who suﬀer from illnesses associated with speciﬁc parts of the brain (such that one can infer what capacities these people are

<-----Page 16----->Behavioral Decision Making at 50

51

missing, e.g., Damasio [15]); the others are studies that use brain imaging technologies such as the electro-encephalogram method (EEG), positron emission
topography (PET) and functional magnetic resonance imaging (fMRI) (for a
good review, see Camerer, Loewenstein, & Prelec [10]).
From my — admittedly cursory — review of studies in this area, the general picture that emerges is that the brain has many diﬀerent systems and
that diﬀerent parts are specialized in diﬀerent functions. The amygdala, for
example is an important repository of emotional reactions that the so-called
higher cortical levels of the brain may or may not be able to control. Diﬀerent
areas of the brain seem to be involved when people are thinking about losses
as opposed to gains; areas stimulated in thinking about ambiguous probabilities are diﬀerent from those where probabilities are known; familiar faces are
matched with diﬀerent patterns of activities than unfamiliar faces; and so on
(Camerer et al., [10]).
At the present time, this research is very much at an exploratory stage and
many ﬁndings can be related to the notions of dual-process theories of thought
(see above). Moreover, as measurement techniques improve (and they will) I
suspect that we will have a rich store of information to sift and interpret. It
is very exciting and I expect we will have many surprises.
Developing methodologies. Most research on judgment and decision
making is conducted with carefully designed experiments. Showing that experimental claims rule out alternative explanations is an important feature of
most work that demonstrates a high level of internal validity. However, one of
the most important results in the ﬁeld is the importance of contextual eﬀects
(consider again the example of the Wason task considered above) and this
naturally raises the question of external validity: Do the results demonstrated
in the laboratory apply in naturally occurring settings?
At a conceptual level, I believe that we can say that this problem does
have a good solution. Moreover, it is not new. It is the notion of representative design that was originally elaborated by Egon Brunswik [5, 6]. Simply
stated, think of an experiment - that involves the interaction between participants and the tasks with which they are confronted - as a sample. To what
extent does this sample generalize to the population in which we are interested? Clearly, this depends on establishing that the characteristics of both
the participants and the tasks they face are representative of the population
(e.g., can be thought of as random samples). There are, of course, many studies in which people have looked at diﬀerent types of experimental participants,
e.g., students and professionals. However, the major gap lies in the sampling
of tasks.
In a recent review, Dhami, Hertwig, and Hoﬀrage [20] made a thorough
review of the concept of representative design and diﬃculties that have been
encountered in its use. As they argue, there is some interest in seeking to go
beyond the laboratory and to study decision making where it occurs (see, e.g.,
Lipshitz, Klein, Orasanu, & Salas [72]). However, perhaps the greatest obstacle

<-----Page 17----->52

Robin M. Hogarth

to the adoption of the principles of representative design by researchers is its
absence from textbooks - the concepts are not taught to students.
A further point made by Dhami et al. is that the development of modern
technologies has facilitated the ability to sample situations in a representative
manner. Let me mention a couple. First, the availability of computers allows us
to construct simulation models and virtual worlds that far exceed our dreams
of a few decades ago. Clearly, experiments in such worlds are not the same
as “real” experience. However, clever models can go a long way to simulating
diﬀerent environments. As existing examples, consider ﬂight simulators for
teaching pilots or aircraft traﬃc controllers (for a wonderful example, see
Shanteau, Friel, Thomas,& Raacke [86]).
Second, there exists today a plethora of recording instruments that allow
one to capture behavior as it occurs. For example, I recently used mobile
telephones to send messages to people at random moments of the day (Hogarth
[50]). On receiving the messages, respondents answered a small questionnaire
concerning the decisions they were taking at those moments. In this way,
I obtained a random sample of their decision behavior. Whereas, there was
much that was “wrong” with this study, I believe it is indicative of the kinds
of samples we will be able to collect as technology develops. (For a clever use
of head-mounted video cameras, see Omodei, McLennan, & Wearing [79]).
Finally, it is clear that experimenters can also use the principles of representative design to collect data in the laboratory. Indeed, speciﬁc attempts to
do just this have cast doubt concerning the generality of such well-established
phenomena as overconﬁdence (Gigerenzer, Hoﬀrage, & Kleinbölting [33]),
hindsight bias (Hoﬀrage, Hertwig, & Gigerenzer [45]) and the overestimation
of low-probability risks (Hertwig, Barron, Weber, & Erev [43]). In particular,
the latter work emphasizes the important distinction between how people typically face risks in laboratory tasks - where all elements are neatly summarized
and described - compared to experiencing these as they do and do not occur
across time. In the former, small probability events are made quite salient. In
the latter, by deﬁnition, low probability events are rarely encountered.
Helping decision making. It is generally accepted that the technology
of decision making — or decision analysis (see, e.g., Keeney & Raiﬀa [63])
— is much ahead of people’s ability to apply it. In my view, this suggests
two really important challenges. One is to develop explicit theories that will
indicate when simple decision models — such as relying on only one variable
or taking an average — should be used. Clearly, some work has already been
started on this topic. Indeed, Gigerenzer et al. [34] suggested the metaphor of
people possessing an “adaptive toolbox” from which decision making tools can
be taken and used as required. What we need, however, is a more complete
list of the tools as well as speciﬁcation of the tasks for which they are and are
not suited.
Second, if we accept the fact that most decision are taken intuitively, it
seems important that we develop means to educate people’s intuitions. In
a recent book (Hogarth [48]) I elaborated on this theme and presented a

<-----Page 18----->Behavioral Decision Making at 50

53

framework for suggesting how it might be done. In addition, I am currently
engaged in trying to understand the conditions under which people should
trust their intuitions or analysis when making decisions (Hogarth [49]).
It is not methods or principles of decision making that our ﬁeld lacks. It
is how to bring these tools within reach of those who need them.
Collaboration with sister disciplines. Research on judgment and decision making has always been an interdisciplinary ﬁeld. As interest in the
ﬁeld continues to grow, there will be a natural tendency for groups to split oﬀ
and create their own sub-ﬁelds. I see this as a real threat to progress because
it is at the intersection of approaches and traditions that innovations tend
to occur (Campbell [11]). There will therefore be a need for some scholars to
play bridging roles between sub-ﬁelds so that ideas can circulate throughout
the whole ﬁeld.

4 The Next 50 Years . . . ?
I hope that my discussion of the last ﬁfty years - as well as current challenges
- has conveyed some sense of the interest and excitement that this ﬁeld has
generated. Judgment and decision making are ubiquitous and important activities. Moreover, contrary to an idea that one should only study decisions
that are important (or have economic consequences), I want to stress the importance of studying “small” decisions. There are two reasons. First, our life
consists mainly of small decisions. Thus, even if each small decision we take
only has small consequences, the aggregate consequences of the small decisions we face in our lifetimes is huge. Second, given that our decision making
habits have been formed and automated by making small decisions, it is unlikely that these can be suppressed when we face large decisions. Indeed, such
habits can have large impacts on important issues.
What can we expect to see in the next 50 years? Answering this question
is a daunting challenge, so let me simply suggest the following (almost by way
of a summary):
1. Closer links in our understanding of judgment and decision behavior between biology (speciﬁcally neuroscience) and psychological models. This,
in turn, will inﬂuence how economists think about these issues.
2. Greater methodological sophistication and more rigorous standards for
judging experimental results. Parenthetically, I see this as one of the beneﬁts the ﬁeld is already experiencing from its contacts with experimental
economics.
3. Ingenuous uses of technology for capturing behavior in naturally occurring
situations and/or in experimental laboratories.
4. Many surprises - what these will be, however, I cannot say

<-----Page 19----->54

Robin M. Hogarth

References
1. Balzer, W. K., Doherty, M. E., & O’Connor, R. (1989). Eﬀects of cognitive
feedback on performance. Psychological Bulletin, 106, 410-433.
2. Blattberg, R. C., & Hoch, S. J (1990). Database models and managerial intuition: 50% model + 50% manager. Management Science, 36, 887-899.
3. Brehmer, B. (1976). Social judgment theory and the analysis of interpersonal
conﬂict. Psychological Bulletin, 83, 985-1003.
4. Bruner, J., Goodnow, J. J., & Austin, G. A. (1956). A study of thinking. New
York: Wiley.
5. Brunswik, E. (1952). Conceptual framework of psychology. Chicago: University
of Chicago Press.
6. Brunswik, E. (1956). Perception and the representative design of experiments
(2nd ed.). Berkeley: University of California Press.
7. Camerer, C. F. (1981). General conditions for the success of bootstrapping
models. Organizational Behavior and Human Performance, 27, 411-422.
8. Camerer, C. F., & Johnson, E. J. (1991). The process-performance paradox in
expert judgment: How can the experts know so much and predict so badly?
In K. A. Ericsson & J. Smith (Eds.), Toward a general theory of expertise:
Prospects and limits. Cambridge: Cambridge University Press.
9. Camerer, C. F., & Hogarth, R. M. (1999). The eﬀects of ﬁnancial incentives
in experiments: A review and capital-labor-production framework. Journal of
Risk and Uncertainty, 19, 7-42.
10. Camerer, C. F., Loewenstein, G., & Prelec, D. (2004). Neuroeconomics: Why
economics needs brains. Scandinavian Journal of Economics, 106 (3), 555-579.
11. Campbell, D. T. (1969). Ethnocentrism of disciplines and the ﬁsh-scale model of
omniscience. In M. Sherif & C. W. Sherif (Eds.), Interdisciplinary relationships
in the social sciences (pp. 328-348). Xenia, OH: Aldine Publishing.
12. Chaiken. S., & Trope, Y. (Eds.). (1999). Dual-process theories in social psychology. New York. The Guilford Press.
13. Cohen, L. J. (1981). Can human irrationality be experimentally demonstrated?
Behavioral and Brain Sciences, 4, 317-370.
14. Conlisk, J. (1996). Why bounded rationality? Journal of Economic Literature,
XXXIV (June), 669-700.
15. Damasio, A. R. (1964). Descartes’ error: Emotion, reason, and the human brain.
New York: Avon.
16. Dawes, R. M. (1979). The robust beauty of improper linear models. American
Psychologist, 34, 571-582.
17. Dawes, R. M., & Corrigan, B. (1974). Linear models in decision making. Psychological Bulletin, 81, 95-106.
18. Dawes, R. M., Faust, D., & Meehl, P. E. Clinical versus actuarial judgment.
Science, 243, 1668-1674.
19. Denes-Raj, V., & Epstein, S. (1994). Conﬂict between experiential and rational processing: When people behave against their better judgment. Journal of
Personality and Social Psychology, 66, 819-829.
20. Dhami, M. K., Hertwig, R., & Hoﬀrage, U. (2004). The role of representative
design in an ecological approach to cognition. Psychological Bulletin, 130(6),
959-988.
21. Edwards, W. (1954). The theory of decision making. Psychological Bulletin, 51,
380-417.

<-----Page 20----->Behavioral Decision Making at 50

55

22. Edwards, W. (1968). Conservatism in human information processing. In B.
Kleinmuntz (Ed.), Formal representation in human judgment. New York: Wiley.
23. Einhorn, H, J., & Hogarth, R. M (1975). Unit weighting schemes for decision
making. Organizational Behavior and Human Performance, 13, 171-192.
24. Einhorn, H, J., & Hogarth, R. M. (1978). Conﬁdence in judgment: Persistence
of the illusion of validity. Psychological Review, 85, 395-416.
25. Einhorn, H, J., & Hogarth, R. M. (1986). Judging probable cause. Psychological
Bulletin, 99, 3-19.
26. Einhorn, H. J., Kleinmuntz, D. N., & Kleinmuntz, B. (1979). Linear regression
and process tracing models of judgment. Psychological Review, 86, 465-485.
27. Epstein, S. (1994). Integration of the cognitive and psychodynamic unconscious.
American Psychologist, 49, 709-724.
28. Epstein, S., Lipson, A., Holstein, C., & Huh, E. (1992). Irrational reactions to
negative outcomes: Evidence for two conceptual systems. Journal of Personality
and Social Psychology, 62, 328-339.
29. Gigerenzer, G. (1996). On narrow norms and vague heuristics: A reply to Kahneman and Tversky. Psychological Review, 103, 592-596.
30. Gigerenzer, G., & Goldstein, D. G. (1996). Reasoning the fast and frugal way:
Models of bounded rationality. Psychological Review, 103, 650-669.
31. Gigerenzer, G., & Hoﬀrage, U. (1995). Hoe to improve Bayesian reasoning without instruction: Frequency formats. Psychological Review, 102, 684-704.
32. Gigerenzer, G., Hoﬀrage, U., & Kleinbölting, H. (1991). Probabilistic mental
models: A Brunswikian theory of conﬁdence. Psychological Review, 98, 506-528.
33. Gigerenzer, G., & Hug, K. (1992). Domain speciﬁc reasoning: Social contracts,
cheating, and perspective change. Cognition, 43, 127-171.
34. Gigerenzer, G., Todd, P. M., & the ABC Research Group. (1999). Simple heuristics that make us smart. New York: Oxford University Press.
35. Goldberg, L. R. (1970). Man versus model of man: A rationale, plus some evidence, for a method of improving on clinical inferences, Psychological Bulletin,
73, 422-432.
36. Goldstein, D. G., & Gigerenzer, G. (2002). Models of ecological rationality: The
recognition heuristic. Psychological Review, 109 (1), 75-90.
37. Grether, D. M., & Plott, C. R. (1979). Economic theory of choice and the
preference reversal phenomenon. American Economic Review, 69 (4), 623-638.
38. Grove, W. M., Zald, D. H., Lebow, B. S., Snitz, B. E., Nelson, C. (2000). Clinical
versus mechanical prediction: A meta-analysis. Psychological Assessment, 12,
19-30.
39. Hammond, K. R. (1955). Probabilistic functioning and the clinical method.
Psychological Review, 62, 255-262.
40. Hammond, K. R. (1990). Functionalism and illusionism: Can integration be usefully achieved? In R. M. Hogarth (Ed.), Insights in decision making: A tribute
to Hillel J. Einhorn. Chicago: University of Chicago Press.
41. Hammond, K. R. (1996). Human judgment and social policy: Irreducible uncertainty, inevitable error, unavoidable injustice. New York: Oxford University
Press.
42. Hammond, K. R., Hamm, R. M., Grassia, J., & Pearson, T. (1987). Direct comparison of the eﬃcacy of intuitive and analytical cognition in expert judgment.
IEEE Transactions on Systems, Man, and Cybernetics, 17, 753-770.

<-----Page 21----->56

Robin M. Hogarth

43. Hertwig, R., Barron, G., Weber, E. U., & Erev, I. (2004). Decisions from experience and the eﬀect of rare events in risky choice. Psychological Science, 15
(8), 534-539.
44. Hoﬀman, P. J. (1960). The paramorphic representation of human judgment.
Psychological Bulletin, 57, 116-131.
45. Hoﬀrage, U., Hertwig, R., & Gigerenzer, G. (2000. Hindsight bias: A by-product
of knowledge updating? Journal of Experimental Psychology: Learning, Memory, and Cognition, 26, 566-581.
46. Hogarth, R. M. (1981). Beyond discrete biases: Functional and dysfunctional
consequences of judgmental heuristics. Psychological Bulletin, 90, 197-217.
47. Hogarth, R. M. (1993). Accounting for decisions and decisions for accounting.
Accounting, Organizations and Society, 18 (5), 407-424.
48. Hogarth, R. M. (2001). Educating intuition. Chicago: University of Chicago
Press.
49. Hogarth, R. M. (2005). Deciding analytically or trusting your intuition? The
advantages and disadvantages of analytic and intuitive thought. In T. Betsch &
S. Haberstroh (Eds.), The routines of decision making. Mahwah, NJ: Erlbaum.
50. Hogarth, R. M. (2006). Is conﬁdence in decisions related to feedback? Evidence
from random samples of real-world behavior. In K. Fiedler & P. Juslin (Eds.),
In the beginning there is a sample: Information sampling as a key to understand
adaptive cognition. Cambridge, UK: Cambridge University Press.
51. Hogarth, R. M., & Einhorn, H. J. (1992). Order eﬀects in belief updating: The
belief-adjustment model. Cognitive Psychology, 24, 1-55.
52. Hogarth, R. M., & Karelaia, N. (2005). Ignoring information in binary choice
with continuous variables: When is less “more”? Journal of Mathematical Psychology, 49, 115-124.
53. Hogarth, R. M., & Karelaia, N. (2006). “Take-the-Best” and other simple strategies: Why and when they work “well” with binary cues.Theory and Decision,
61, 205-249.
54. Hogarth, R. M., & Karelaia, N. (2006). Regions of rationality: Maps for bounded
agents. Decision Analysis, 3(3), 124-144.
55. Hsee, C. K. (1996). Attribute evaluability: Its implications for joint-separate
evaluation reversals and beyond. Organizational Behavior and Human Decision
Processes, 67 (3), 247-257.
56. Hsee, C. K., Loewenstein, G. F., Blount, S., & Bazerman, M. H. (1999). Preference reversals between joint and separate evaluations of options: A review and
theoretical analysis. Psychological Bulletin, 125 (5), 576-590.
57. Johnson, E. J., & Goldstein, D. (2003). Do defaults save lives? Science, 302,
1338-1339.
58. Kahneman, D. (2003). Maps of bounded rationality: Psychology for behavioral
economics. American Economic Review, 93 (5), 1449-1475.
59. Kahneman, D., & Frederick, S. (2002). Representativeness revisited: Attribute
substitution in intuitive judgment. In T. Gilovitch, D. Griﬃn, & D. Kahneman
(Eds.), Heuristics and biases: The psychology of intuitive judgment. Cambridge,
UK: Cambridge University Press.
60. Kahneman, D., Slovic, P., & Tversky, A. (Eds.). (1982). Judgment under uncertainty: Heuristics and biases. New York: Cambridge University Press.
61. Kahneman, D., & Tversky, A. (1979). Prospect theory: An analysis of decision
under risk. Econometrica, 47, 263-291.

<-----Page 22----->Behavioral Decision Making at 50

57

62. Kahneman, D., & Tversky, A. (Eds.). (2000). Choices, values, and frames. New
York: Cambridge University Press.
63. Keeney, R. L., & Raiﬀa, H. (1976). Decisions with multiple objectives: Preferences and value tradeoﬀs. New York: Wiley.
64. Kirkpatrick, L. A., & Epstein, S. (1992). Cognitive-experiential self-theory and
subjective probability: Further evidence for two conceptual systems. Journal of
Personality and Social Psychology, 63, 534-544.
65. Klayman, J., & Ha, Y.-W. (1987). Conﬁrmation, disconﬁrmation, and information in hypothesis testing. Psychological Review, 94, 211-228.
66. Kleinmuntz, B. (1990). Why we still use our heads instead of formulas: Toward
an integrative approach. Psychological Bulletin, 107, 296-310.
67. Kunst-Wilson, W. R., & Zajonc, R. B. (1980). Aﬀective discrimination of stimuli that cannot be recognized. Science, 207, 557-558.
68. LeDoux, J. (1996). The emotional brain: The mysterious underpinnings of emotional life. New York: Simon & Schuster.
69. LeDoux, J. (2002). Synaptic self: How our brains become who we are. New York:
Viking Penguin.
70. Libby, E. (1976). Man versus model of man: Some conﬂicting evidence. Organizational Behavior and Human Performance, 15, 1-12.
71. Lichtenstein, S., & Slovic, P. (1971). Reversals of preference between bids and
choices in gambling decisions. Journal of Experimental Psychology, 89, 46-55.
72. Lipshitz, R., Klein, G, Orasanu, J., & Salas, E. (2001). Focus article: Taking
stock of naturalistic decision making. Journal of Behavioral Decision Making,
14, 331-352.
73. Loewenstein, G. F., Weber, E. U., Hsee, C. K., & Welch, N. (2001). Risk as
feelings. Psychological Bulletin, 127, 267-286.
74. Luce, M. F., Payne, J. W., & Bettman, J. R. (1999). Emotional trade-oﬀ diﬃculty and choice. Journal of Marketing Research, 36, 143-159.
75. McGill, A. L. (1989). Context eﬀects in judgments of causation. Journal of
Personality and Social Psychology, 57, 189-200.
76. Meehl, P. E. (1954). Clinical versus statistical prediction. Minneapolis: University of Minnesota Press.
77. Nisbett, R. E., Fong, G. T., Lehman, D. R., & Cheng, P. W. (1987). Teaching
reasoning. Science, 238, 625-631.
78. Nisbett, R. E., & Ross, L. (1980). Human inference: Strategies and shortcomings
of social judgment. Englewood Cliﬀs, NJ: Prentice-Hall.
79. Omodei, M., McLennan, J., & Wearing, A. J. (2005). How expertise is applied
in real-world dynamic environments: Head-mounted video and cued recall as
a methodology for studying routines of decision making. In T. Betsch & S.
Haberstroh (Eds.), The routines of decision making. Mahwah, NJ: Erlbaum.
80. Payne, J. W. (1982). Contingent decision behavior. Psychological Bulletin, 92,
382-402.
81. Payne, J. W., Bettman, J. R., & Johnson, E. J. (1992). Behavioral decision
research: A constructive processing approach. Annual Review of Psychology,
43, 87-131.
82. Payne, J. W., Bettman, J. R., & Johnson, E. J. (1993). The adaptive decision
maker. New York: Cambridge University Press.
83. Peterson, C. R., & Beach, L. R. (1967). Man as an intuitive statistician. Psychological Bulletin, 68, 29-46.

<-----Page 23----->58

Robin M. Hogarth

84. Savage, L. J. (1954). The foundations of statistics. New York: Wiley.
85. Sawyer, J. (1966). Measurement and prediction, clinical and statistical. Psychological Bulletin, 66, 178-200.
86. Shanteau, J., Friel, B. M., Thomas, R. P., & Raacke, J. (2005). Development
of expertise in a dynamic decision-making environment. In T. Betsch & S.
Haberstroh (Eds.), The routines of decision making. Mahwah, NJ: Erlbaum.
87. Siegel, S. (1957). Level of aspiration and decision making. Psychological Review,
64, 253-262.
88. Simon, H. A. (1955). A behavioral model of rational choice. Quarterly Journal
of Economics, 69, 99-118.
89. Simon, H. A. (1956). Rational choice and the structure of the environment.
Psychological Review, 63, 129-138.
90. Simon, H. A. (1957). Models of man. New York: Wiley.
91. Simon, H. A. (1978). Rationality as process and product of thought. American
Economic Review, 68 (2), 1-16.
92. Simon, H. A. (1979). Information processing models of cognition. Annual Review of Psychology, 30, 363-396.
93. Slovic, P. (1987). Perception of risk. Science, 236, 280-285.
94. Slovic, P. (2000). The perception of risk. London: Earthscan.
95. Slovic, P., Finucane, M., Peters, E, & MacGregor, D. (2002). The aﬀect heuristic. In T. Gilovich, D. Griﬃn, & D. Kahneman (Eds.), Intuitive judgment:
Heuristics and biases. Cambridge, UK: Cambridge University Press.
96. Slovic, P., Fischhoﬀ, B., & Lichtenstein, S. (1976). Cognitive processes and
societal risk taking. In J. S. Carroll & J. W. Payne (Eds.), Cognition and social
behavior. Potomac, MD: Lawrence Erlbaum Associates.
97. Slovic, P., Griﬃn, D., & Tversky, A. (1990). Compatibility eﬀects in judgment
and choice. In R. M. Hogarth (Ed.), Insights in decision making: A tribute to
Hillel J. Einhorn. Chicago: The University of Chicago Press.
98. Thorndike, E. L., & Lorge, I. (1944). The teacher’s word book of 30,000 words.
New York: Teachers College, Columbia University.
99. Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics
and biases. Science, 185, 1124-1131.
100. Tversky, A., & Kahneman, D. (1983). Extensional versus intuitive reasoning:
The conjunction fallacy in probability judgment. Psychological Review, 90, 293315.
101. Tversky, A., & Kahneman, D. (1986). Rational choice and the framing of decisions. Journal of Business, 59 (4, Pt. 2), S251-S278.
102. von Neumann, J., & Morgenstern, O. (1947). Theory of games and economic
behavior (2nd ed.). Princeton, NJ: Princeton University Press.
103. Wason, P. C. (1960). On the failure to eliminate hypotheses in a conceptual
task. Quarterly Journal of Experimental Psychology, 12, 129-140.
104. Wason, P. C. (1966). Reasoning. In B. M. Foss (Ed.), New horizons in psychology. Harmondsworth: Penguin.
105. Yaniv, I., & Hogarth, R. M. (1993). Judgmental versus statistical prediction:
Information asymmetry and combination rules. Psychological Science, 4, 58-62.
106. Zajonc, R. B. (1968). Attitudinal eﬀects of mere exposure. Journal of Personality and Social Psychology Supplement, 9(2,Pt.2), 1-27.
107. Zajonc, R. B. (1980). Feeling and thinking: Preferences need no inferences.
American Psychologist, 35, 151-175.

