<-----Page 0----->Sheffield Economic Research Paper Series
SERP Number: 2005014

Michael Dietrich

Using simple neural networks to analyse firm activity

July 2005

Department of Economics
University of Sheffield
9 Mappin Street
Sheffield
S1 4DT
United Kingdom
www.shef.ac.uk/economics

<-----Page 1----->1. Introduction
Characteristically, in economics, the analysis of firm activity is based on a production
function that defines a deterministic relationship between factor inputs and firm
output. The analysis of the firm as an organisation takes a somewhat different
approach. For instance, behavioural economics (for example Simon, 1955; March and
Simon, 1958; Cyert and March, 1963), transaction cost theory (Williamson, 1975,
1985) and capabilities approaches (for example Foss and Loasby, 1998; Foss, 2005)
emphasise that economic agents have inevitably incomplete information and
knowledge and are at most boundedly or limitedly rational. The implication here is
that while general principles governing intra-firm interaction can be specified,
detailed organisational processes inside the firm are, for practical academic purposes,
effectively unobservable. Hence, the usual analytical tools designed to analyse firm
behaviour, based on production functions and optimising principles with full
information, are in practice an oversimplification of firm activity (Loasby, 1999).

This problem is not unique to economics. For example, the functioning of the human
brain can be understood in general terms as inputs of sensory data generating biochemical reactions. These reactions activate various interconnected neurons that in
turn lead to (in principle) measurable ‘output’. But, as with the firm, the way in which
this general model of the brain is understood and applied in specific circumstances is
too complex to be specifiable in terms of a simple functional relationship. For this
reason neural scientists have developed neural network analysis to model the
interactions inside the brain that are otherwise too complex to be modelled (see, for
example, Bishop, 1995). This technique can therefore be usefully applied to the

-1-

<-----Page 2----->analogous idea of the firm and (potentially) model its functioning assuming complex
internal interactions. It is not, of course, original to claim that firm organisation is the
brain of the firm; previous work based on this principle is Beer (1972). But this paper
is an attempt in this tradition using a neural network framework. Outside of
economics, for example in business and finance, neural network analysis is not
uncommon (see, for example, Altman et al, 1994; Wilson et al, 1995; Chiang et al,
1996; Wong et al, 1995; Jasic and Wood, 2005). Non-firm applications of neural
network analysis within economics are becoming increasingly important, for example:
Binner et al (2005) on inflation; Papadas and Hutchinson in input-output analysis;
Johnes (2000) on macroeconomic modelling; Franses and Homelen (1998) and
Plasmans et al (1998) on exchange rate modelling. But previous published work in
economics that has applied a neural network framework to the firm (for example,
Delgado et al, 2004) has not developed the analysis using actual data as is done in this
paper.

The rest of the discussion is set out as follows. In the next section the key principles
of neural network modelling, and how this might be applied to the firm, are set out. In
addition basic estimations are reported using a sample of 248 firms in UK SIC34. For
comparative purposes the estimation results are compared to Cobb-Douglas and translog equivalents. It is shown that neural network frameworks not only provide better
estimates but also have superior predictive ability. Following this, in section three, the
various estimates are analysed in terms of returns to scale characteristics. It is shown
that the most the most complex neural network developed here has an intuitive
economic interpretation. In section four a predictive evolutionary model of the firm is
developed using the estimated neural network. It is shown that this model has superior

-2-

<-----Page 3----->predictive capabilities than earlier reported models. In the final substantive section
five, simulation results are reported using the neural network based model set out in
section four. Finally brief conclusions are drawn.

2. Neural network modelling of the firm
A basic neural network (see Bishop, 1995) suggests a two-stage input-output
framework rather than a single stage traditional production function. Between inputs
and outputs an unobservable hidden layer (H) exists:
Traditional production function:

X→Q

Neural network:

X → H → Q.

For current purposes, this unobservable hidden layer is assumed to define
organisational functioning. With two inputs (x1 and x2), the functioning of this
network can be set out as follows:
Q

h0

x0

h1

hm

x1

x2

The various factor inputs, along with an input bias (x0), do not have a direct impact on
output, but are instead inputs into m hidden units. These hidden units, along with a
hidden unit bias (h0), determine firm output. This standard formulation is a feed
forward network as no feedback effects are modelled.

-3-

<-----Page 4----->To model a feed forward network it is assumed that the various hidden units have a
separable impact on output. In addition the significance of the various links in the
network is defined by a system of weights. In general terms with n inputs, m hidden
units, and a single output, we can specify a feed forward neural network as follows
(Delgado et al, 2004):
m
n
⎡
⎤
Q = F ⎢β 0 + ∑ G⎛⎜ χ j + ∑ x i α ij ⎞⎟ β j ⎥
j=1 ⎝
i =1
⎠ ⎦
⎣

β0 = the output bias
γj = hidden unit biases (j = 1, …, m)
αij = weights from input unit i to hidden unit j
βj = weights from hidden unit j to output.

If we assume functional form F is linear in logs, i.e. the analogue of a Cobb-Douglas
formulation, we can define in the two input case
q = β0 + β1g1 + β2g2

⎛
⎞
where: q = ln(Q); gj = G⎜ χ j + ∑ x i α ij ⎟ .
n

⎝

i =1

⎠

In this case we can interpret β1/(β1+β2) as the ‘share’ of g1 in q, and equivalently for
g2. The relevance of this will become clear from later discussion.

There are two common activation functions that define G (see Bishop, 1995):
1. A threshold relationship in which hidden unit j becomes activated following
input signals of sufficient intensity.
2. A continuous logistic relationship.

-4-

<-----Page 5----->In terms of economic modelling of the firm it is not intuitively obvious why a
threshold function should be the appropriate form for the relationship between factor
inputs and hidden unit activity.1 Instead a standard logistic relationship is used here:
m

q = β0 + ∑ β j
j=1

1
1 + exp(− z j )

[1]

where, in the two input case, zj = γj + α1jln(x1) + α2jln(x2). Logged inputs are used here
to maintain comparability with a Cobb-Douglas relationship. The logistic relationship
has the desirable property that it is locally (approximately) linear over its middle
range and hence collapses back to an approximate Cobb-Douglas relationship. But
over extreme zj it is obviously non-linear.

Formulation [1] can, in principle, be estimated using non-linear regression methods.
In addition to qualifications made below on the use of non-linear least squares
estimation, we must recognise that any non-linear estimation is potentially sensitive to
the algorithm starting point (Curry and Morgan, 1997) because of possible
convergence to local solutions (Athanassopoulos and Curram, 1996). For illustrative
purposes the estimation is carried out on a sample of 248 UK firms from SIC34 i.e.
the automobiles sector. This sample is the complete set of firms available from the
FAME database for which SIC34 is the main activity of the firms and covers the full
range from small to very large firms. A 2 digit level of aggregation is used to
internalise much firm diversification. Data for 1995 and 2000 is used, covering the
following: labour (number of employees), capital (net assets) and firm sales. Capital
and sales for 2000 are rebased to 1995 levels using the GDP deflator.
1

A ‘psychological’ model of the firm might use a threshold signal. In this case each hidden unit
might describe a different firm ‘mental map’ with switching between such maps depending on
exogenous shocks of sufficient intensity. The necessary shock is defined by the threshold signal. In
a managerial context these ‘mental maps’ might define dominant firm culture. While such
modelling is potentially interesting it is left for future work.

-5-

<-----Page 6----->To indicate the reliability of a neural network framework the following method is
adopted:
1. Use 1995 data to estimate Cobb-Douglas, trans-log and neural network
production functions.
2. Use the regression output in (1) and 2000 values of L and K to predict firm
sales for that year.
3. Calculate the root mean squared deviation and Theil’s inequality coefficient of
actual from predicted sales. These measures are taken as indicators of
predictive accuracy.
With step (1) we can view a generalised neural network as an arbitrarily complete
specification of a data set (see White 1989). It follows that we can view CobbDouglas and trans-log functions as, respectively, first and second order
approximations of this underlying fully specified relationship. We might therefore
expect an actual, empirically derived, neural network to be a more accurate modelling
device. This greater accuracy is, indeed, found below. But a possible criticism of a
neural network framework is that this greater modelling accuracy may be based on
tracking stochastic deviations from underlying systematic economic relationships
rather than simply the economic relationships themselves. It therefore may follow that
greater modelling accuracy need not imply greater predictive ability if the latter is
based on systematic rather than stochastic factors. For this reason steps (2) and (3) are
important in indicating the reliability of a neural network framework. We find that a
neural network framework has greater predictive ability. Detailed comments about
how reliability is assessed are made below.

-6-

<-----Page 7----->The main complexity for regression modelling is that standard OLS is a potentially
inappropriate tool for production function estimation because an efficiency bound can,
in general, be assumed to exist. For this reason two sets of regression results are
reported below: non-linear OLS estimates and those derived using a stochastic frontier
model. For the latter the total error term is divided into two elements: a random part
(v) and the degree of technical inefficiency (u). The random element has the usual
characteristics of being independently N(0, σv2) distributed over the observations. The
inefficiency element is assumed independently half-normal N+(0, σu2) distributed. In
addition, to control for non-constant variance in the inefficiency element, because of
heteroskedastistic firm scale effects, σu2 is modelled as a linear function of ln(K)ln(L).

Stochastic frontier models are straightforward to estimate for linear specifications; for
instance the STATA package estimates them as standard. But for non-linear
formulations, such as a sigmoid based neural network, estimation is not possible using
available estimation packages. To undertake neural network frontier estimation the
procedure adopted here involves removing the efficiency effects from the dependent
variable of the neural network, following which non-linear least squares can be used
to estimate the model as if all firms were operating at optimal efficiency. The
efficiency effects used in the frontier neural network are those estimated in the translog frontier regression. The logic here is that the trans-log formulation represents a 2nd
order approximation of the underlying production process rather than the 1st order
approximation of a Cobb-Douglas function. This 2nd order approximation produces
greater modelling accuracy as indicated below. The resulting regression models are as
follows.

-7-

<-----Page 8----->OLS Cobb-Douglas:
CD
CD
CD
ln(R ) = a CD
0 + a1 ln(L) + a 2 ln(K ) + e
Frontier Cobb-Douglas:
CD
CD
CD
ln(R ) = bCD
− u CD
0 + b1 ln(L) + b 2 ln(K ) + v

OLS trans log:
2
2
TL
TL
TL
TL
TL
TL
ln(R ) = a TL
0 + a1 ln(L) + a 2 ln(K ) + a 3 ln(L) + a 4 ln(K ) + a 5 ln(L) ln(K ) + e

Frontier trans log:
2
2
TL
TL
TL
TL
TL
TL
ln(R ) = bTL
− u TL
0 + b1 ln(L) + b 2 ln(K ) + b3 ln(L) + b 4 ln(K ) + b5 ln(L) ln(K ) + v

OLS neural network:
m

ln(R) = a 0NN + ∑ a1NN
j
j=1

1 + exp− [a

1
NN
2j

+a

NN
3j

ln(L) + a 4NNj ln(K )]

Frontier neural network:
m

ln(R ) + u TL = b0NN + ∑ b1NN
j
j=1

1 + exp− [b

1
NN
2j

+b

NN
3j

ln(L) + b4NNj ln(K )]

The appropriate number of hidden units in the neural network is estimated using
standard test statistics. In practice, the maximum number of units that can be
effectively estimated is two; this has an intuitive economic interpretation presented
below. Separate results are presented for networks with one and two hidden units. For
trans-log formulations the regressions are reported after removal of insignificant
parameters. Results are reported in tables 1-4.

Table 1a: 1995 OLS Cobb-Douglas regression
-8-

<-----Page 9----->CD

a0
CD
a1
a

2

CD
2

R
S.E. of regression
Log likelihood

estimated
coefficient
3.926

t statistic

0.797
0.200
0.855
0.674
-252.65

9.99
4.23

16.60

Table 1b: 1995 Frontier Cobb-Douglas regression
estimated z statistic
coefficient
4.091
18.84

CD

b0
CD
b1

0.681
0.297
1427.77
0.617
-0.965

CD
2

b
Wald chi2(2)
σv
ln σ 2v
cons

ln σ

2
u

cons
ln(K)-ln(L)
Log likelihood

8.63
4.77

-6.76
-3.01

-3.324
0.523
-251.85

2.21

Table 2a: 1995 OLS Trans-log regression

TL

a0

TL

a1

TL

a3

TL

a4

estimated t statistic
coefficient
5.843
18.04
0.385

3.49

0.041

3.64

0.009

3.78

R2
0.870
S.E. of regression 0.640
Log likelihood
-239.25

Table 2b: 1995 Frontier Trans-Log regression
-9-

<-----Page 10----->TL

b0
TL
b1
TL
b3
TL
b4
Wald chi2(2)
σv
ln σ 2v
cons
ln(K)-ln(L)
2
cons
ln σ u
Log likelihood

estimated z statistic
coefficient
6.084
15.67
0.420
3.49
0.036
3.09
0.010
3.64
1635.82
-2.403
0.430

-1.789
-228.80

-6.25
4.12
-2.39

Table 3a: 1995 OLS Neural Network: one hidden unit

NN

a0
NN
a11
NN
a 21
NN
a 31
NN
a 41

R2
S.E. of regression
Log likelihood

estimated
coefficient
6.215

t statistic

11.417
-3.367
0.378
0.061
0.875
0.628
-234.15

4.67
-4.282
3.82
2.63

6.09

Table 3b: 1995 Frontier Neural Network: one hidden unit

NN

b0
NN
b11
NN
b21
NN
b31
NN
b41

R2
S.E. of regression
Log likelihood

estimated
coefficient

t statistic

6.544

7.17

11.717

5.04

-3.348

-4.84

0.366

4.21

0.062

2.91

0.906
0.531
-192.42

Table 4a: 1995 OLS Neural Network: two hidden units

-10-

<-----Page 11----->NN

a0
NN
a11
NN
a 21
NN
a 31
NN
a 41
NN
a12
NN
a 22
NN
a 32
NN
a 42

R2
S.E. of regression
Log likelihood

estimated t statistic
coefficient
6.945
18.39
4.111
2.25
-11.362
-2.37
0.577
2.01
0.557
1.55
4.588
2.59
-4.474
-3.99
0.840
2.64
0.035
0.55
0.880
0.621
-229.27

Table 4b: 1995 Frontier Neural Network: two hidden units
estimated t statistic
coefficient
NN

b0
NN
b11
NN
b21
NN
b31
NN
b41
NN
b12
NN
b22
NN
b32
NN
b42

R2
S.E. of regression
Log likelihood

7.244

19.95

3.963

2.11

-11.351

-2.44

0.551

2.08

0.560

1.59

4.935

2.63

-4.240

-4.57

0.755

2.85

0.044

0.89

0.911
0.524
-186.98

Table 5: Predictive accuracy of 2000 sales forecasts

-11-

<-----Page 12----->RMS
Theil’s
U

OLS
CD
0.645
0.0315

OLS
TL
0.616
0.0301

OLS
NN1
0.613
0.0299

OLS
NN2
0.606
0.0296

Frontier Frontier Frontier Frontier
CD
TL
NN1
NN2
0.628
0.584
0.580
0.571
0.0307 0.0285 0.0283 0.0278

UM
0.00101 0.00076 0.00103 0.00115 0.00070 0.00110 0.00114 0.00107
US
0.00091 0.00030 0.00004 0.00006 0.00064 0.00026 0.00002 0.00005
C
U
0.02968 0.02912 0.02895 0.02845 0.02946 0.02723 0.02722 0.02681
Note: UM, US and UC need not sum to U because of rounding errors.

Table 5 reports the root mean squared error (RMS) and Theil’s inequality coefficient
(U) of 2000 sales forecasts using the eight different models reported above. A clear
ranking of forecasting accuracy is indicated, with the neural network models being
more accurate predictors that trans-log and Cobb-Douglas formulations. In addition,
the frontier models predict better than the OLS equivalents. For this reason the rest of
the discussion will be based on use of the frontier models. The table also presents the
standard decomposition of Theil’s U into proportions representing the bias (UM), the
variance (US) and the covariance (UC). It is apparent that the neural network models
are particularly effective in tracking the variance of the forecasts.

3. Interpretation of results: returns to scale
This section of the paper presents a first interpretation of the results in terms of a
standard returns to scale analysis. For a production function Q = F(L, K) we can
define returns to scale in the standard manner by the ratio y/x, where
yQ = F(xL, xK),
with y/x greater (less) than one indicating increasing (diminishing) returns to scale.

The Cobb-Douglas regression results indicate scale effects that are not significantly
different from constant returns. With a trans-log function returns to scale are
-12-

<-----Page 13----->obviously endogenous to L, K and x. For frontier parameter estimates as given above,
returns to scale characteristics are set out in figure 1. With small x, increasing returns
to scale become more important with smaller firm size, a result that has an intuitive
appeal. Firms of all size appear to converge on approximate constant returns i.e. the
Cobb-Douglas solution.
Figure 1
Trans Log Returns to Scale
4
3.5
3

y/x

2.5

0.5 mean ln(L), ln(K)
mean ln(L), ln(K)

2

1.5 mean ln(L), ln(K)

1.5
1
0.5
1.9

1.74

1.58

1.42

1.26

1.1

0.94

0.78

0.62

0.46

0.3

0.14

0

x

For a neural network returns to scale are also endogenous to L, K and x. With a single
hidden unit, and parameter estimates as given above for the frontier model, returns to
scale characteristics are set out in figure 2. As with the trans-log function there is
convergence on approximate constant returns. But with small size, i.e. 0.5 mean ln(L),
ln(K), there are greater increasing returns than with a trans-log function, an effect that
results from the non-linearity in the sigmoid relationship.
Figure 2

-13-

<-----Page 14----->Neural Network (1) Returns to Scale
10
9
8

y/x

7
6

0.5 mean ln(L), ln(K)

5

mean ln(L), ln(K)

4

1.5 mean ln(L), ln(K)

3
2
1

0.
14
0.
3
0.
46
0.
62
0.
78
0.
94
1.
1
1.
26
1.
42
1.
58
1.
74
1.
9

0

x

A neural network with two hidden units allows a potentially more revealing analysis
of firm characteristics and behaviour. As mentioned above, because the impacts of the
two hidden units are separable, and as we have assumed a linear relationship in logs
between hidden units and output, we can define the following relative shares for the
hidden units:
NN
NN
NN
/( b11
+ b12
),
s1 = b11

NN
NN
NN
s2 = b12
/( b11
+ b12
).

With the parameter estimates reported above
s1 = 0.45,

s2 = 0.55.

This implies that we can interpret hidden unit one as occurring, on average for all
firms, 45 per cent of the time and hidden unit two, on average, 55 per cent of the time.

-14-

<-----Page 15----->Figure 3

10
9
8
7
6
5
4
3
2
1
0

HU1
HU2

0.
02
0.
18
0.
34
0.
5
0.
66
0.
82
0.
98
1.
14
1.
3
1.
46
1.
62
1.
78
1.
94

y/x

Neural Network Returns to Scale
mean ln(L), ln(K)

x

Using this logic we can define separate returns to scale characteristics for the two
hidden units. Results for mean ln(L), ln(K) are set out in figure 3. Mapping the returns
to scale characteristics exhibited here into implied unit costs, it can be suggested that,
for an average firm, hidden unit 2 displays constant unit costs, except for very small x,
whereas hidden unit 1, for an average firm, exhibits falling unit costs up to x=1
followed by slightly increasing unit costs. Interpreting the returns to scale
characteristics in this manner suggests that hidden unit 2 defines long-run costs
whereas hidden unit 1 defines the short-run cost structure.2 The overall cost structure
is consistent with that used in standard undergraduate economics teaching. Long-run
average costs initially fall followed by constant returns. Short-run unit costs rise
increasingly steeply at outputs lower than expectations and less steeply at outputs

2

This conclusion is reinforced by the fact that the simulated outputs (not shown here) used to derive
the returns to scale ratio y/x are consistently greater with hidden unit 2 compared to the equivalent
hidden unit 1 position; a finding we would expect if HU1 and HU2 define, respectively, short-run
and long-run functioning. A minor conceptual point is, therefore, that describing the ratio x/y for
hidden unit 1 as defining returns to scale is incorrect as it appears to describe short-run
characteristics.

-15-

<-----Page 16----->greater than expectations. The results suggested here imply that, for 1995, SIC34
firms are away from their long-run average cost curve, on average, 45 per cent of the
time, whereas output expectations are correct 55 per cent of the time.

4. Predicting firm sales using a two hidden unit network
Interpreting the two hidden unit neural network in the manner just presented reveals
an important potential issue when attempting to predict firm sales based on observed
input use. Even with unchanged technology, and implied cost structure, prediction is
only accurate to the extent that the estimated proportion of time average output
expectations are correct is unchanged. This perspective suggests a potentially useful
development in the way in which firm output predictions are generated. If the shortrun, long-run interpretation of the two hidden unit case presented here is correct, a
two hidden unit neural network suggests a possible modelling of changes in s1 and s2,
and hence in principle greater predictive accuracy.

The modelling of changes in s1 and s2 is based on the simple principle that a firm
operating away from its long-run cost curve in one operating period will plan, in the
next period, to reduce the implied disequilibrium. This modelling requires individual
firm estimates of the hidden unit shares rather than the average estimates reported
above. Using 1995 parameter estimates, these firm specific shares are generated in the
following way:
1. Define firm specific values for s1 and s2 as, respectively, sk1 and sk2 = 1-sk1
(k=1, …, 248).
2. For imputed values of sk1 (0, 0.1, …, 0.9, 1) and hence implied values of sk2 (1,
NN
NN
0.9, … 0.1, 0) define firm specific estimates of b11
and b12
:

-16-

<-----Page 17----->NN
bk11 = sk1(3.963/0.45)

and

NN

bk12 = sk2(4.935/0.55).

NN
3. Using bkNN
11 and b k12 compute predicted 1995 firm sales for each value of sk1

and sk2 and the root mean squared deviation of actual from predicted 1995
sales.
4. The firm specific values of sk1 and sk2 are taken as the values that minimise the
root mean squared deviation for each firm.
A crude check of the accuracy of the procedure set out in (1)-(4) is to compare the
mean of the resulting firm specific estimates of sk1 with the earlier reported estimated
value of s1. Both estimates are 0.45.

Having derived estimates of sk1 and sk2 using (1)-(4), changes in these firm specific
parameters over the interval 1995-2000 are modelled as responding to (a) planned
movements towards long-run efficiency potential defined by hidden unit two and (b)
reactions to external pressures. The planned change is modelled as a standard partial
adjustment mechanism:
∆spk2 = d(s*k2 – sk2),
where: ∆spk2 = the planned change in the second hidden unit share for the k’th firm i.e.
the planned change towards long-run potential; s*k2 = the desired sk2 for the k’th firm;
d = the standard partial adjustment parameter, assumed the same for all firms.

Reactions to external pressures for each firm are modelled using the ratio of firm
growth in output over the interval 1995-2000 to average growth in output for all firms
i.e. gk/gav. This ratio is used to define a simple firm specific multiplier (mk) that is
used to model external pressures for each firm:

-17-

<-----Page 18----->mk = e1 + [gk/gav]^e2
with the

parameters e1 and e2 assumed the same for all firms.

The updated value of the share parameter for the second hidden unit (denoted suk2) is
modelled by combining planned changes and external pressures as follows:
suk2 = mk(sk2 + ∆spk2),
= mk[ds*k2 + (1-d)sk2]
The logic here is there is a planned change in the share parameter, defined by the term
in square brackets, the external pressures then impact on this planned change.
Simulations are undertaken to determine the values of s*k2, d, e1 and e2 for this
adjustment process. In each case predicted 2000 firm sales use 2000 factor input
levels but two hidden unit frontier parameter estimates based on 1995 regression
NN
NN
and b12
. The adjustment process parameter
results for all parameters except b11

values are chosen to minimise the root mean squared deviation of actual from
predicted 2000 firm output. Results are reported in table 6.

Table 6: Simulated adjustment parameter results

adjustment model
planned adjustment
external pressures
unchanged shares

s*i2

d

e1

e2

0.98
0.51

0.56
0.69

-0.30

0.05

-0.08

0.01

root
mean
sq dev
0.509
0.530
0.637
0.674

The first row of results in table 6 shows parameter estimates and RMS for the full
adjustment model. In addition, and for comparative purposes, three other sets of
results are shown: (a) the impact of planned changes alone; (b) the impact of external

-18-

<-----Page 19----->pressures alone; and (c) imputed firm specific but unchanged 1995 hidden unit shares.
It will be recalled that earlier results (reported in table 5) indicated a root mean
squared deviation for 2000 sales forecasts, with common and unchanged hidden unit
shares, of 0.571 for the two hidden unit frontier neural network model. The simulation
results reported here indicate that with firm specific but unchanged unit shares,
predictive accuracy deteriorates in terms of root mean squared deviation. A model
with external pressures alone also offers no predictive superiority over the earlier two
hidden unit model. With planned adjustment alone there is an improvement in
predictive accuracy: the root mean squared deviation is 0.530 compared to 0.571
earlier. But with planned changes alone the parameter estimates indicate that firms
have an objective of achieving only 51 per cent hidden unit two activity. Intuitively
this would seem to be overly small. Combining external pressures and planned
adjustment in unit shares further improves predictive capability, indicated by a
reduction in root mean squared deviation from the earlier 0.571 to 0.509. In addition,
the simulated parameter values are more plausible, indicating that firms have an
objective of operating at 98 per cent hidden unit two activity. But over the five year
interval 1995-2000 only 56 per cent of adjustment to desired objectives was achieved.
An implication of the improvement in predictive accuracy using this adjustment
model is to support the earlier conclusion that hidden units one and two describe
respectively short-run and long-run activity.

5. Simulating firm and industry evolution
The modelling presented in previous sections can be used to analyse the implied
evolutionary characteristics of SIC34 firms. The parameter estimates for the frontier
two hidden unit neural network, along with adjustment in hidden unit shares predicted

-19-

<-----Page 20----->by the adjustment model developed in the previous section, can be used to predict
firm sales, with values for one period endogenising the adjustment process for the
next period. Periods 0 and 1 for the simulations use actual 1995 and 2000 data. Hence,
each iteration can be viewed as representing a five year period.

For simulations reported below, firm specific input use grows (or declines) assuming
no input supply constraints exist. Two sets of simulations are undertaken that assume
(1) constant, firm specific, capital:labour ratios, and (2) capital:labour ratios adjusting
in each period if this increases profitability. The capital:labour ratio adjustment
process in (2) allows up to a doubling or halving in each period, with choice being
based on the largest profit derivable. These two sets of simulations can be viewed as
being based on two different firm types. In (1) firms are assumed to operate with rigid
and unchanged routines, fundamental capabilities, or standard operating procedures
(depending on the particular view of the firm). In (2) firms are assumed to be long-run
optimisers.

The main technical issue with simulating firm evolution is defining and endogenising
firm viability. When a firm ceases to be viable exit takes place. This possibility of
firm exit occurs when firm losses cannot be rectified by reduction in input use. Hence
firm viability requires an ability to avoid losses at some input use and output level. To
operationalise this view of viability a zero profit condition for each firm must be used.
Such a condition is derived assuming an unknown distribution of firm profitability
across the actual (not simulated) population of firms, but that a subset of the actual
firms are operating at zero profit. These marginal, zero profit, firms are assumed to
define a lower bound for the population. Hence a zero profit condition can be derived

-20-

<-----Page 21----->by estimating this lower bound. To derive this bound we can recognise that for any
one firm using labour (L) and capital (K) inputs
π = R – pLL - pkK
i.e.

R/L = π/L + pL + pK(K/L),

where: π = firm profit; R = firm revenue; pL and pK = input prices.
It follows that for a zero profit, marginal firm
R/L = pL + pK(K/L)
and for a positive profit, non-marginal firm
R/L > pL + pK(K/L).
It is assumed that pL and pK are the same for all marginal firms and that labour and
capital prices are no lower for non-marginal than marginal firms. Using these
assumptions a scatter diagram of (R/L) against (K/L) can be used to derive the lower
bound, marginal firms.

To derive the lower bound the following procedure is used:
1. Locate the observation with minimum average labour product: (R/L)1 and the
associated (K/L)1
2. From the point [(R/L)1, (K/L)1] determine the slope of the rays to all other
points [(R/L)k, (K/L)k] (k = 2, 3, …., 258).
3. Rank the absolute values of the slopes derived in (2) in ascending order. The
B-firm lower bound is then defined by the first B observations in this ranked
series.
4. Formulate the regression equation for the B observations
R/L = f0 + f1(K/L).

-21-

<-----Page 22----->The regression equation in (4) defines the B-firm lower bound estimates of pL and pK
as respectively f0 and f1. Three different values for B were initially used: 10, 20, 30.
Simulation results for B=20 and B=30 resulted in the majority of firms becoming nonviable in all simulations. This unlikely result is taken to indicate that B=20 and B=30
defines a bound that is too wide. For this reason, a 10 firm lower bound is used
below.3 For simulation purposes, the lower bound estimates of pL and pK are used to
impute firm profitability. For any firm, in any period, in which the calculated
profitability is negative factor inputs that generate zero profit are calculated, assuming
a constant or variable K/L (as relevant). If this resulting K/L is negative the firm is
deemed non-viable and hence exit occurs.

Figure 4: Basic simulation, constant K/L
5

50

4

45

3
40
2
35
1
30

G(av)
ROS(av)
ROS(min)
NeqH

0
1

3

9 17 25 33 41 49 57 65 73 81 89 97

-1

25

-2

20

An alternative method to identify the lower bound can involve using steps (1)-(3) in the text, but
then using the B-firm lower bound to define a B-firm dummy variable that is 1 for the B marginal
firms. Using this dummy variable a regression with two ‘regimes’ can be defined involving
marginal and non-marginal firms. These two methods give effectively the same result for the
bound. But the method defined in (1)-(4) is preferred because omitted variable bias will be,
potentially, present to the extent that factor prices are not the same for non-marginal firms and that
non-marginal firms have different positive profits.

-22-

<-----Page 23----->The basic simulation with constant K/L is shown in figure 4. The dotted line (NeqH)
shows the numbers equivalent Herfindahl index. This index is measured on the right
hand scale whereas the other three series in figure 4 are measured on the left hand
scale. The starting point for the numbers equivalent index, the actual value for the
year 2000, is just under 20. This actual figure for the number of equivalently sized
firms compares with 248 actually existing firms in 2000. Apart from two temporary
falls, NeqH increases until a steady-state is achieved with 47 equivalently sized firms,
i.e. the ‘natural’ evolution of the sector is for it to become less concentrated. This
lower concentration occurs even though there are fewer actual firms because of the
exit of those that are non-viable. This characteristic is based on the simulated growth
of smaller compared to larger firms. In addition, it is perhaps relevant to point out that
steady-state concentration levels are only achieved after (approximately) 80 periods.
Given the five year periodicity of the data, 80 periods implies 400 years. This
indicates that in real time concentration levels will effectively not equilibriate.

The G(av) curve in figure 4 reports the average growth in turnover for all firms,
excluding those that have exited. It can seen that this reaches a steady-state with G(av)
= 1. This steady state is effectively achieved after approximately 20 periods. The
implied 100 years indicates that in real time average profits are not in equilibrium.
The initial increase in G(av) is caused by the early exit of non-viable, loss making
firms. This early non-viability is indicated by the negative minimum return on sales,
i.e. ROS(min) shown as a bolded line. This minimum return on sales is seen to
stabilise at a level that is effectively zero. If the entry of new firms is based on the
profitability of marginal existing firms, it follows that the long-run zero level for
ROS(min) is consistent with no new firm entry. The average return on sales is shown

-23-

<-----Page 24----->as ROS(av). It can seen that this equilibriates to 0.5 after approximately 15 periods,
equivalent to a real time 75 years.

Figure 5: Basic simulation, variable K/L
5

50

4

45

3
40
2
35
1
30

G(av)
ROS(av)
ROS(min)
NeqH

0
1

9 17 25 33 41 49 57 65 73 81 89 97

-1

25

-2

20

Figure 5 shows the equivalent simulation to that reported in figure 4 but with firms
able to vary K/L. With respect to the numbers equivalent Herfindahl index it can be
seen that in the early simulated periods the sector is more concentrated than in figure
4. This would appear to suggest that the adjustment in K/L offers a relative advantage
to larger rather than smaller firms. But the equilibrium level of NeqH in figure 5 is
effectively the same as in figure 4, indicating that the early advantage to larger firms
is temporary. In addition equilibrium concentration is achieved in the same time scale
with constant and varying K/L. Allowing firms to be long-rum optimisers has the
perhaps logical result in figure 5, that in the early disequilibrium periods average
growth is higher than in figure 4. But average and minimum return on sales appear to
be minimally affected by varying K/L.

-24-

<-----Page 25----->To test the stability of these results two additional sets of simulations are undertaken.
The first allows the log of firm sales in each simulated period to be increased by 1.05
i.e. an exogenous, and continuing sales expansion is introduced. The second change
imposes an equivalent sales decline of 0.95 for each firm in each simulated period.
Figures 6 and 7 show the effect of the exogenous expansion in sales for simulations
based on constant and varying K/L.

Figure 6: Sales expansion by a factor of 1.05, constant K/L
6

120
110

5

100
4
90
3

80

2

70
60

1

50
0
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 40
-1

30

-2

20

-25-

G(av)
ROS(av)
ROS(min)
NeqH

<-----Page 26----->Figure 7: Sales expansion by a factor of 1.05, variable K/L
6

120
110

5

100
4
90
3

80

2

70
60

1

G(av)
ROS(av)
ROS(min)
NeqH

50
0
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 40
-1

30

-2

20

Comparing figures 6 and 4 (note the different number of iterations reported in these
two cases) perhaps the most dramatic change is the greater level of deconcentration
that is introduced when an exogenous sales expansion is imposed. The equilibrium
level of NeqH in figure 6 is above 116 equivalently sized firms. Because of the nonlinearities embodied in the neural network framework the sales expansion is to the
relative disadvantage of larger firms. In addition, equilibrium concentration is
achieved after approximately 55 periods in figure 6, compared to the earlier
approximately 80 periods. But 55 periods is equivalent to 275 years, hence the
substantive economic conclusion is unchanged. It is also apparent in the comparison
of figures 6 and 4 that average growth and average return on sales are, logically,
greater with the sales expansion in the early disequilibrium periods. But the minimum
return on sales in figure 6 still stabilises at (effectively) zero. The reason for this is
that the sales expansion reduces firm exit with the result that the marginal firm is still
earning only normal profit. Using earlier arguments, this suggests that a sales
expansion will not lead to the entry of new firms.

-26-

<-----Page 27----->Comparing figures 6 and 7, one differences is that the initial disequilibrium growth is
(logically) greater in figure 7. But in both cases equilibrium average growth is
achieved only after the equivalent of greater than 125 years. In addition, the initial
average return on sales is lower in figure 7 than in figure 6. This average return on
sales effect is produced by relatively inefficient, and hence low profit, firms having
greater opportunities to generate efficiency gains by changing capital:labour ratios.
An implication here is that a population of optimising firms need not have higher
average profitability than a population of non-optimising firms. For the latter
population greater firm exit will occur.

Figure 8: Sales contraction by a factor of 0.95, constant K/L.
5

20
18

4

16
3

14
12

2

10
1

8
6

0
1

3

5

7

9 11 13 15 17 19 21 23 25 27

-1

4
2

-2

0

-27-

G(av)
ROS(av)
ROS(min)
NeqH

<-----Page 28----->Figure 9: Sales contraction by a factor of 0.95, variable K/L
5

20
18

4

16
3

14
12

2

10
1

8

G(av)
ROS(av)
ROS(min)
NeqH

6

0
1

3

5

7

9 11 13 15 17 19 21 23 25 27

-1

4
2

-2

0

It is apparent from figures 8 and 9 that a sales contraction has a significant
concentrating effect. In both figures the number of equivalently sized firms falls from
less than 20 in 2000 (i.e. period 1) to an equilibrium level of (effectively) 2 firms i.e.
the ‘natural’ evolution produced by contraction is to become more monopolised.
Furthermore equilibrium concentration is achieved after 8 periods. This is faster than
earlier results but is still equivalent to 40 years. In addition to this point, another
apparent difference, when figures 8 and 9 are compared to earlier simulations,
concerns predicted return on sales. Both the average and minimum ROS is greater
with a sales contraction. The reason for this is that the sales contraction produces
more disequilibrium firm exit because of non-viability with the result that the
marginal firm has greater equilibrium profits. In turn this characteristic of the
marginal firm increases the average equilibrium profitability of the sector as a whole.
It follows that if new firm entry is determined by the characteristics of the marginal
firm, sales contraction may eventually promote entry because of the shake-out of
relatively inefficient firms. Comparison of figures 8 and 9 indicates that the only real

-28-

<-----Page 29----->difference between populations of sub-optimising compared to long-run optimising
firms is that the speed of adjustment in the average growth rate appears faster in figure
9 compared to figure 8. The implication here is that optimisation has a dynamic but
not an equilibrium effect.

6. Conclusion
This paper has shown that neural networks can be an effective tool for the analysis of
the firm. Both one and two hidden unit networks provide better frameworks than
those traditionally used in economics in terms of both estimation and prediction. In
addition a two hidden unit network appears to have an intuitive economic
interpretation in terms of short-run and long-run decisions. This characteristic allows
a further modelling development based on adjustment to efficient, hidden unit two,
behaviour. This more evolutionary model was investigated here using simulation
techniques. It was shown that the neural network framework is consistent with
different evolutionary paths depending on exogenous market growth or decline. In
addition the model is shown to predict that steady states, while being theoretically
achievable, only occur after arguably excessive iterations. In effect, given reasonable
economic interpretations of real time, the neural network model is shown to be
consistent with a disequilibrium perspective.

These insights indicate a comparative advantage to the use of neural networks in the
analysis of the firm. But the ‘simple’ analysis referred to in the title perhaps needs
some comment. In this context the following points would seem to be pertinent
conclusions. First, the introduction to the paper made reference to approaches to the
firm that emphasise the importance of organisation. The hidden units in a neural

-29-

<-----Page 30----->network were assumed to describe these organisational factors. But the data used here
was restricted to two productive inputs. In principle it is possible to separate costs into
costs of production and other non-production costs. This can be mapped into an
analysis in which both production-based and organisational factors of production can
exist; see Dietrich (2003) for a non-neural network framework based on these
principles. Secondly, the simulation results indicate that even with variable K/L, and
in equilibrium, firms’ profitability does not converge. The underlying profitability
dispersion is based on ‘x’ inefficiencies, i.e. labour and capital productivities, defined
by the inefficiency vector u. More sophisticated modelling might allow for best
practice productivities to diffuse through the population of firms. This would appear
to require more than the simple feed forward neural network used here. Finally, while
some of the above discussion has an intuitive economic logic it might be the case that
different sectors display different characteristics. These comments, and the discussion
in the paper, indicate a rich vein of research potential.

-30-

<-----Page 31----->References
Altman EI, Marco G, Varetto FC (1994), “Corporate distress diagnosis: comparisons
using linear discriminant analysis and neural networks (the Italian experience)”,
Journal of Banking and Finance, 18, 505-529.
Athanassopoulos AD, Curram SP (1996), “A comparison of data envelopment
analysis and artificial neural networks as tolls for assessing the efficiency of
decision making units”, Journal of the Operational Research Society, 47, 100016.
Binner JM, Bissoondeeal RK, Elger T, Gazely AM, Mullineux AW (2005), “A
comparison of linear forecasting models and neural networks: an application to
Euro inflation and Euro Divisia”, Applied Economics, 37(6), 665-680.
Beer S (1972), Brain of the firm: the managerial cybernetics of organization, London:
Allen Lane.
Bishop CM (1995), Neural networks for pattern recognition, Oxford, Clarendon Press.
Chiang WC, Urban TL, Baldridge GW (1996), “A neural network approach to mutual
fund asset value forecasting”, OMEGA, 24, 205-16.
Curry B, Morgan P (1997), “Neural networks: a need for caution”, OMEGA, 25, 12333.
Cyert RM and March JG (1963), A Behavioral Theory of the Firm, Englewood Cliffs
NJ, Prentice Hall.
Delgado FJ, Valiño A, Santín D (2004), “The measurement of technical efficiency: a
neural network approach”, Applied Economics, 36(6), 627-635.
Dietrich M (2003), “The Importance of Management and Transaction Costs for Large
UK Firms’, Applied Economics, 35, 1317-29.
Foss NJ (2005), Strategy, Economic Organization and the Knowledge Economy: the
coordination of firms and resources, Oxford, Oxford University Press.
Foss NJ and Loasby BJ (eds) (1998), Economic Organization, Capabailities and Coordination, London, Routledge.
Franses PH, Homelen Pv (1998), “On forecasting exchange rates using neural
networks”, Applied Financial Economics, 8 (6), 589-596.
Jasic T, Wood D (2004), “The profitability of daily stock market indices trades based
on neural network predictions: case study for the S&P 500, the DAX, the
TOPIX and the FTSE in the period 1965-1999”, Applied Financial Economics,
14 (4), 285-297.

-31-

<-----Page 32----->Johnes G (2000), Up Around the Bend: linear and nonlinear models of the UK
economy compared”, International Review of Applied Economics, 14 (4), 485493.
Loasby BJ (1999), Knowledge, Institutions and Evolution in Economics, London,
Routledge.
March JG and Simon HA (1958), Organizations, London, John Wiley & Sons.
Papadas CT, Hutchinson WG (2002), “Neural network forecasts of input–output
technology”, Applied Economics, 34 (13), 1607-1615.
Plasmans J, Verkooijen W, Daniels H (1998), “Estimating structural exchange rate
models by artificial neural networks”, Applied Financial Economics, 8 (5), 541551.
Simon HA (1955), “A behavioural model of rational choice”, Quarterly Journal of
Economics, 69, 99-118.
White H (1989), “Learning in artificial neural networks: a statistical perspective”,
Neural Computation, 1, 425-64.
Williamson OE (1975), Markets and Hierarchies: analysis and anti-trust implications;
a study in the economics of internal organization, New York, Free Press.
Williamson OE (1985), The Economic Institutions of Capitalism: firms, markets,
relational contracting, London, Macmillan.
Wilson N, Chong KS, Peel MJ (1995), “Neural network simulation and the prediction
of corporate outcomes”, International Journal of the Economics of Business, 2,
31-50.
Wong BK, Bodnovich TAE, Selvi Y (1995), “A bibliography of neural network
business applications research: 1988-1994”, Expert Systems, 12, 253-261.

-32-

