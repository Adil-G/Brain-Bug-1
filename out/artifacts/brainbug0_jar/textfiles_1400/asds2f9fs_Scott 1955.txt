<-----Page 0----->LIVING RESEARCH

J21

to provide this guidance and serve as a stimulant is the significance of
statistically insignificant data. Even if the probability is great that an
inference will have to be rejected later, the practical risk of airing it is
small. Subsequent and more elaborate studies may disprove some of these
inferences; but for those that survive social science will be the richer.
To be sure, a physicist would rightly frown on such recommendations.
But his is a world with generalizations on a high level. By comparison,
the social sciences are at a stage where for decades to come the formation
of even tentative theoretical structures will be at a premium.*
'These remarks were made in one of the diicutsions at the recent AAPOR Conference in Wiiconiin.

Reliability of Content Analysis:
The Case of Nominal Scale Coding
By WILLIAM A. SCOTT
Institute for Social Research, University of Michigan

T
X HIS ARTICLE suggests an improved method of reporting the extent of
inter-observer agreement in assigning overt or verbal behavioral items to
a set of categories. It was developed specifically for standard survey research coding operations, but it can be used in a wide variety of research
situations to measure the reliability of classifying a large number of responses into nominal scale categories. The requirements are that the
categories be mutually exclusive and that observations be duplicated on
a random sample of the total set of responses being studied.
The Survey Research Coding Procedure. In standard interview survey
procedures, a respondent's answer to "open-ended" questions is recorded
verbatim by the interviewer. The first major step in the analysis process
then becomes one of "coding" all of the thousands or more different responses to each question into a relatively small number of categories,
which can be meaningfully and conveniently related to other responses.
This is usually done by a staff of coders, and the interviews are distributed among them by a procedure which approximates randomization.
To improve the accuracy of content analysis and get some measure
of the extent of inter-coder agreement on each of the dimensions, a random
sample of the interviews is assigned for "check-coding." A second coder
makes an independent analysis of the entire interview, and the two persons
discuss their differences, to arrive at the "best" judgment in every case of

<-----Page 1----->322

PUBLIC OPINION QUARTERLY. FALL 1955

disagreement A record of differences is kept, and this constitutes the basis
of a report on the over-all coding reliability for the entire set of interviews.
It is apparent that the ambiguity of the dimension and the skill of the
analysts are reflected in the total number of differences obtained for any
given dimension. Two skilled clinicians could be expected to show a
higher proportion of agreements than average coders (college students) in
assigning protocols to categories descriptive of personality types. Moreover, anyone could presumably code dimensions such as "sex" or "race"
more reliably than attitudinal responses, which ordinarily require a
certain amount of judgment in determining the proper category. It would
seem that these "ambiguity" and "skill" factors are legitimate components
of the total measure of coding error. Certain extraneous factors should be
excluded, however, and this is not done by present methods of reporting
the extent of inter-observer agreement.
Conventional Methods. One commonly used "reliability" index is
simply Po, or the percentage of judgments on which coders agree, out of
the total number of judgments. Unfortunately, this measure is biased in
favor of dimensions with a small number of categories. By chance alone,
one would expect better agreement on a two<ategory than on a fivecategory scale.
To correct for this bias, Bennett, et al.,1 have used an index of consistency, S, which takes into account the number of categories in the
k
1
dimension. S = — (Po
), where P o is the observed percentage agreek-1
k
ment between two independent coders and k is the number of categories.
As the number of categories increases, S increases, for a fixed Po. And
herein lies a spurious effect. Given a two-category sex dimension and a
Po of 60 per cent, the S calculated from this equation would be 0.20. But
a whimsical researcher might add two more categories, "hermaphrodite"
and "indeterminant," thereby increasing S to 0:47, though the two additional categories are not used at all. The index is based on the assumption that all categories in the dimension have equal probability of use
1
(—) by both coders. This is an unwarranted assumption for most bek
havioral and attitudinal research. Even though k categories may be available to the observers, the phenomena being coded are likely to be distributed unevenly, and in many cases will cluster heavily in only two or
'Bennett, £. M., Alpert, R, and Goldstein, A. C, "Communications Through Limited
Response Questioning," "Public Opinion Quarterly, 18, (Fall 1954), No. 3, pp. 303-308.

<-----Page 2----->LIVING RESEARCH

323

three of them. To the extent that the distribution over categories varies
from a rectangular one, S would appear to be an unsatisfactory measure
of coding reliability.
A Suggested Index of Inter-coder Agreement, v, the index of inter-coder
agreement to be reported here, corrects for the number of categories in
the code, and the frequency with which each is used. In the practical
coding situation it varies from 0.00 to 1.00, regardless of the number of
categories in the dimension, and is thus comparable with the "percentage
agreement" figure.
For certain types of data preferred indices are already available. When
the coding dimensions are composed of equal-interval scales, a Pearson
product-moment correlation coefficient is appropriate for reporting intercoder reliability. If the phenomena ire ordered along a dimension of unknown intervals, a rank-difference correlation gives an adequate measure
of, the extent of agreement between coders. More frequently, however,
the coding dimensions used in survey and observational research are composed of nominal scales, where the categories cannot be ordered along a
dimension of "more-or-less" of some attribute. For such eases «• is suggested
in preference to current indices of reliability.
Po - P.
V

=>

,

1-P.
where Po (observed per cent agreement) represents the percentage of judgments on which the two analysts agree when coding the same data independently; and Pe is the percent agreement to be expected on the basis
of chance. v is the ratio of the actual difference between obtained and
chance agreement to the maximum difference between obtained and
chance agreement. It can be roughly interpreted as the extent to which
the coding reliability exceeds chance.
Calculation of "Expected Per cent Agreement." The percentage agreement which could be expected by chance depends not only on the number
of categories in the dimension but also on the frequency with which each
of them is used by coders. Minimum chance agreement (or "maximum uncertainty") occurs when all categories are used (by both coders) with equal
frequency. Any deviation from a rectangular distribution of frequencies
across categories will increase the "expected per cent agreement." In general, the total probability of chance agreement equals the sum of the probabilities of agreement on each of the categories taken individually (since
the categories are mutually exclusive).
In survey data, the distribution of coded responses to a particular question can be obtained easily for the entire set of interviews simply by
running the IBM cards through a tabulator on the appropriate column.

<-----Page 3----->324

PUBLIC OPINION QUARTERLY, FALL 1955

As an illustration we may consider the following distribution of responses
to the question: "What sorts of problems are your friends and neighbors
most concerned about these days?"
Nature of Problem
Economic problems
International problems
Political problems
Local problems
Personal problems
Not ascertained

Per cent of all Responses
60%
5
10
20
3
2

Although the observed percentage agreement is calculated on only a
part of the total set of interviews, and individual coders will actually vary
somewhat in their distributions of responses over the set of categories, it
is convenient to assume that the distribution for the entire set of interviews represents the most probable (and hence "true" in the long-run
probability sense) distribution for any individual coder. (This assumption
is not a necessary one; P, can be calculated from the actual distributions of
each pair of coders, but the procedure is more complicated.)
The expected per cent agreement for the dimension is the sum of the
squared proportions over all categories (since the categories are mutually
exclusive, and the two coders' probabilities of using any one of the
categories are assumed equal).
k
2 pi*, where k is the total number of categories and pi is the
i=l
proportion of the entire sample which falls in the ith category.
In the present example, P. = (.60)* + (.05)' + (.10)' + (.20)' + (.03)*
+ (.02)* = .41. If the pair of coders had agreed on 80 per cent of their
judgments, the index of inter-coder agreement would be:
0.80 - 0.41
7T =
= 0.66
1 - 0.41
Sampling Error of v. A simple formula is available for the sampling
distribution of v, if one is willing to assume that Pe would remain constant
for a different set of interviews drawn randomly from the same population
of respondents. This is not an unrealistic assumption, since the original
set of interviews usually comes from a large sample of respondents (1,000
or more), and Pe is not affected perceptibly by small variations in the
response distribution. To the extent that Pe varies from sample to sample
this formula can only be regarded as an approximation of the sampling
error of v.
P, =

<-----Page 4----->LIVING RESEARCH

Po

325

P.

x =

, which is equivalent to kPo — c, where k and c are
1 - P. 1 - P.
constants.
The variance of this sampling distribution is therefore:
o*r = k V p o = I
I
, where Qo = 1 - Po, and n is the number
\1-P./ n - l
of interviews from which Po was computed.
The significance of the difiference between two x's (based on uncorrelated
Po's) can be calculated by critical ratio, as follows:
C.R. =
xi - x,

i~T\
V \\-Vj

yPoiQpi
Dl -1

/

i

y p ^

Vl-P,./ n,-l

