<-----Page 0----->From Statistical Decision Theory to Practice: Some Problems with the Transition
Author(s): David W. Conrath
Source: Management Science, Vol. 19, No. 8, Application Series (Apr., 1973), pp. 873-883
Published by: INFORMS
Stable URL: http://www.jstor.org/stable/2629120 .
Accessed: 23/09/2011 10:05
Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at .
http://www.jstor.org/page/info/about/policies/terms.jsp
JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of
content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms
of scholarship. For more information about JSTOR, please contact support@jstor.org.

INFORMS is collaborating with JSTOR to digitize, preserve and extend access to Management Science.

http://www.jstor.org

<-----Page 1----->MANAGEMENT SCIENCE
Vol. 19, No. 8, April, 1973
Printed in U.S.A.

FROM STATISTICAL DECISION THEORY TO PRACTICE:
SOME PROBLEMS WITH THE TRANSITION*
DAVIID W. CONRATHtt
University of Waterloo,Waterloo,Ontario
While considerable progress has been made in the development of sophisticated
statistical decision models, little attention has been paid to the problem of applying
them in the "real world". This paper approaches one aspect of the necessary conversion process, namely the conceptualization and use of probabilistic data for
decision making. Three points are demonstrated on the basis of clinical experimental
evidence: (1) decision makers have difficulty conceiving of probability distributions,
rather they are more comfortable with point estimates; (2) the concept of "risk of
failure", the probability of failing to meet a perceived target, plays a major role in
choice; and (3) the format in which probabilistic data are presented can affect choice
behavior. A simple descriptive decision model is then developed, based on both the
clinical evidence and two years of observation of a committee empowered to invest in
new product development and product expansion activities.

A tremendous growth in the literature on decision theory (or in more general terms,
on tools for decision making) has taken place over the past twenty years. The major
thrust, aside from the operations research use of mathematical programming and
computer simulation models, has been in the area of statistical decision theory. Bayesian decision models have become popular, but so has stochastic programming and
other more classical aspects of statistical decision theory. No one seriously doubts
that great progress has been made in both theory and modelling, but a relevant question still remains: how difficult is it to pass along the path from theory or model to
application?
The path appears to be particularly difficult when one speaks of applying some of
the more classical concepts of statistical decision theory, those that rely on the use of
higher order moments of a probability distribution. The purpose of this paper is to
provide some insight on the problems along this particular path, via a clinical experiment, and to present a simple descriptive model of choice based on considerable
field research of organizational decision making. We are going to concentrate our
attention on one of the key links in the conversion of theory to application: the conceptualization and use of probabilistic data for decision making.
A Clinical Experiment
Three points will be demonstrated via a rather simple clinical experimental situation. One, decision makers are not likely to think (and estimate) in terms of probability distributions. Rather, they are much more comfortable with point estimates,
and therefore use them. Bayesian models, for example, usually accommodate this
limitation. Two, decision makers can understand and do make use of the concept
of the probability of "failure." This has been recognized by others (e.g. Mao [12]),
though there is still a tendency to combine the probability of failure with more sophis* Received February 1972; revised August 1972.
t The author wishes to thank the many participants to this study for their tremendous cooperation. In addition, an expression of appreciation is due I. Bernhardt, A. Curran and J. M.
Terhune for their helpful comments on an earlier draft. Financial support from the National
Research Council of Canada is gratefully acknowledged.
t Bell Northern Research, Ottawa
873

<-----Page 2----->874

DAVID

W. CONRATH

ticated measures. Three, the manner in which probabilistic data are presented for
use as the basis for choice can influence choice, a point this author has not seen mentioned in the literature.
The specific experiment involved nine subjects, five members of an executive committee empowered to decide whether or not to invest in proposed new product developments, and four members of their staff. The experimental data were gathered
during a one-day course in "risk analysis" that had been proposed by a member of
the corporate staff. Most of the lecture material concerned the use of statistical data,
but this was heavily interspersed with cases and situations involving the kind of decisions that the committee and its staff had faced in the past and would face again in
the immediate future.
The experiment was not designed to provide a basis for statistically significant
results, such as those that are yielded by most controlled laboratory studies. Rather,
the intention was to demonstrate actual decision-making behavior as exhibited by
relatively successful executives in an environment in which most of their decisions
are made, and on choice situations with which they have a continuing concern. We
willingly sacrificed the rigor that one might obtain in a purely artificial situation to
obtain data from the potential users of decision theory. The "experimental setting"
was chosen on the personal belief of the author that we know a considerable amount
about choice behavior of college students, but we know little, other than documented
case histories, about decision making by mature, experienced businessmen.
Estimating

Future Sales-The

Concept of a Distribution

At the beginning of the day each of the men was provided a statement of all of the
then current market forecast information available from marketing planning personnel, including sales estimates of related products now in production, for two
products then in the proposal stage. A third case was given to them at the end of the
day after they had been exposed to material on density functions, cumulative distributions, and several techniques of risk analysis. After each case the subjects were
asked: "What is your estimate of future sales?"
While the estimates varied considerably, and some were made in terms of unit
sales and others by dollar volume, in only two instances, for all three cases, did anyone
make anything more sophisticated than single point estimates.' The first case involved
a product that was the least controversial to all concerned. One member of the committee gave estimates for the 0.05, 0.50 and 0.90 points on the cumulative distribution, perhaps to demonstrate his confidence in his estimates (he wanted the project),
and to indicate to us that he understood the essence of what we were to teach. The
second case concerned the most controversial product, one for which there were strong
feelings both pro and con. Furthermore, it involved entrance into a hitherto untapped
market, and therefore considerable uncertainty. In this instance one person refused
to do more than make a few qualitative statements. The other eight all made point
estimates. Because the product was controversial, and because little was known about
this new market, the high and low estimates differed by a factor of 50.
After six hours of risky decision-making cases and lectures, the third case was
analyzed. Presumably, there was reason to believe that a more sophisticated approach
might have been taken. Only one person, however, did anything more than make a
1 One observer suggested that perhaps this was due to a perception of a time constraint held by
the participants. While this is a possibility, those persons who took the longest time in making
their estimates spent the time in writing out rationalizations of their choices.

<-----Page 3----->FROM STATISTICAL

DECISION

THEORY

TO PRACTICE

875

point estimate. He merely added the notation, "?t25 %." In discussions with all of
the participants afterwards it was quite clear that none of them visualized anything
approaching a probability distribution of sales.2 In every case all they could do was
give their "best guess." The author attended committee meetings for almost two
years, and the three case analyses merely confirm what has been observed over time.
The concept of a probability distribution from the standpoint of estimation is not a
concept with which the committee members are comfortable. They like the idea of
using it (the raison d'etre for the "course"), but they would prefer that others form
the distribution, and on a basis which they can understand.
Uses of Probabilistic

Data-The

"Risk of Loss" Parameter

Let us assume, for the sake of the argument that follows, that sufficient data are
available to form a meaningful probability distribution of outcomes. An obvious
question is: How will these data be used?
Both descriptive and normative decision model builders generally wish to incorporate more than just the first moment of the density function (e.g. Alderfer and Bierman [3] and Mao [11]). While Alchian [2] has noted quite effectively that among two
alternative cumulative distributions of outcomes one can be said to be better than
the other only if it lies entirely without the other, many must make choices under
other than these decisive circumstances. The reason for relying on higher moments
is that the decision maker is assumed to have a utility function that is not linearly
related to the measurement of the outcome variable being considered, and/or he has
utility that is not indifferent to the probability distribution of that variable. If the
conditions assumed by von Neumann and Morgenstern [19] for the generation of a
utility function hold, and if one's utility function is linear with respect to the variable
expressing the outcomes, then the mean is sufficient datum for input into one's decision model. However, if we wish to allow for more complex (and more realistic)
utility functions, presumably the more information one has about the probability
distribution of outcomes the better.
Many decision model builders have proposed the variance of the distribution as a
key parameter for a decision maker's consideration (e.g. Markowitz [13]), especially
since it incorporates the common concept of risk-the greater the variance the greater
the "risk." The argument is essentially that a wider variance is an indication of greater
uncertainty. Since most of us appear to have an "aversion" to uncertainty, we would
prefer to choose an alternative with a smaller variance, all other things being equal.
Of course, all other things are usually not equal, but as yet we have little guidance
as to what the trade-offs between mean and variance might be or ought to be.
While the variance is useful when the utility for measurable outcomes and their
likelihoods is complex, so is the third moment about the mean, the measure of skewness (Alderfer and Bierman [3]). In fact, one ought to have a complete specification of
the distribution of outcomes that are possible for each alternative action. Perhaps
this is one reason why simulation models of action/outcome relationships have become popular (e.g. Salazar and Sen [18]). They provide a feasible mechanism for
2 Mao [12]found similar results among the number of financial executives he interviewed. One
referee stated: "the tendency to use point estimates rather than probability distributions follows
from the well-known limited capacity of persons to notice and process data". If the Miller [141
phenomenon is relevant here, then this is a very strong argument for the simplification of normative models for choice under uncertainty, a conclusion reached via a different argument at the
end of this paper.

<-----Page 4----->876

DAVID W. CONRATH

comparing expected utilities when the characteristics (functional form) of the distributions of outcomes and states-of-nature are difficult to determine.
Let us look at some more evidence taken from the committee members responsible
for selecting appropriate capital investments. Eight members (one had to be excused
for this part of the study) were asked to make a set of paired comparisons among
three possible investment alternatives, leading to a rank ordering. The data were
presented in the form of twenty equi-spaced points along the cumulative probability
distribution of internal rates of return (ROR) on investment, based on a discounted
cash flow procedure3 (see Table I).4 In all, three paired comparisons could be determined from their responses.
In one paired comparison (A vs. B) A had a mean ROR that was 7.5 % higher
(41.3 - 33.8) than that of B, while A's standard deviation was 6.4 versus 2.0. Seven
out of eight decision makers chose A, despite the greater variance. This is what one
would expect, since only the bottom 0.10 of the cumulative of B had a higher ROR
than A. A third alternative, C, was compared with B. C was skewed to the high side.
C's mean ROR was 4.3 higher than B's, and though C's standard deviation was 7.5
versus 2.0, its cumulative was strictly to the right. Again, 7 of 8 men chose C over B.
The reason for the exception will be discussed later.
So far the results are those one would expect. But now C was compared with A
Despite a lower mean (a difference of 3.2 % ROR) and a larger variance, C was chosen
over A by 6 of the 8. Furthermore, C's cumulative was to the right of A's only for the
bottom and top 10 percentile of ROR's along the distribution. One could argue that C
was preferred because of utility functions, for percent ROR, that had a large positive
second derivative-utility functions that gave more weight to increments at the higher
end of the ROR scale. This, however, would be contrary to virtually all we know
about the values held by those responsible for maintaining a portfolio of investments.
On the contrary, it was obvious to the author that the opposite was the case for the
men concerned.
Take another look at Table I, and in particular look at the number of points where
the ROR is below 30 %. This was a figure that 6 of the 8 stated was the minimum
rate of return on investment (L) required for a project to be approved (apparently
the ROR had to be suifficiently high to overcome overhead and exploratory research
expenses not capitalized). With respect to this "risk of loss" criterion, C dominated
both A and B, and A could be rationalized over C only on the basis that A's mean was
substantially higher. But its mean was not enough higher than C's to make A sufficiently attractive to overcome the apparent fact that C offered a near zero probability
of "failure"-an ROR below L.
The use of variance (or, more appropriately, dispersion) as perceived by the participants was also of interest. On the one hand it was used to indicate the likelihood
of failure-it helped the individual make his own point estimate of the probability
that the ROR might be less than 30 %. On the other, the one person who chose B
over C stated that C's spread was so wide that he doubted if it accurately reflected the
These are the data consideredessential by the membersof the committeeto evaluate new
productor productline expansionopportunities.The subjectshad all beenexposedpreviouslyto
this formof data presentation.
4The ROR estimatespresentedin Table I were generatedin the followingmanner.First a
meanRORand a standarddeviationwereselectedfor each project:A: ,A= 41,a = 6; B: ,A= 34,
a = 2; C: 0.75 (,u= 34, a = 1), 9.25 (,u= 48, a = 6). Then a computerprogramwas used which
tooka randomsampleof 100pointsfromeachdistribution,calculatedthe actualmeanandstandarddeviation,andprintedthe twentypoints a.longthe cumulativeas requested.

<-----Page 5----->FROM STATISTICAL

DECISION

THEORY

TO PRACTICE

877

TABLE I
Three Hypothetical Investment Projects (ROR as determined by given
cumulative probability distribution values)
ROR* (%)

Cum. Prob.t

ROR (o)

Cum. Probs.

Project A
26.2
28.1
33.7
35.0
35.8
38.1
39.9
40.5
41.0
41.4

0.02
0.07
0.12
0.17
0.22
0.27
0.32
0.37
0.42
0.47

42.3
42.6
43.4
43.8
44.4
45.6
46.5
48.0
48.7
52.7

0.52
0.57
0.62
0.67
0.72
0.77
0.82
0.87
0.92
0.97

Project B
29.8
30.7
31.4
31.7
32.3
32.6
32.9
33.1

0.02
0.07
0.12
0.17
0.22
0.27
0.32
0.37

33.8
34.2
34.3
34.5
34.9
35.3
35.8
36.1

0.52
0.57
0.62
0.67
0.72
0.77
0.82
0.87

33.4
33.5

0.42
0.47

36.7
37.1

0.92
0.97

34.6
34.8
35.0
35.1
35.5
38.1
45.9
48.0
53.4
56.7

0.52
0.57
0.62
0.67
0.72
0.77
0.82
0.87
0.92
0.97

Project C
32.2
33.0
33.2
33.4
33.7
33.8
34.1
34.2
34.3
34.5

0.02
0.07
0.12
0.17
0.22
0.27
0.32
0.37
0.42
0.47

* "ROR" represents internal rate of return on investment.
t Cum. Prob. represents the probability that the actual rate of
return will be less than the rate shown.

true market picture. The fact that the dispersion was great provided him with the
rationale to shift the entire distribution downward, increasing the likelihood that the
ROR would be below L. He felt the narrow range of B meant that they had "a good
grip on things," and hence he could be reasonably confident that the data were well
researched. Thus, the distribution may be evidence not only to be taken at face value,
but it may be used as a measure of data reliability. This perturbs the use of any statis-

<-----Page 6----->878

DAVID

W. CONRATH

tical data, while at the same time it is entirely understandable from a behavioral
point of view.
Another insight regarding the importance of obtaining at least a 30 % ROR can be
had from the results of a questionnaire, one question of which asked the respondent
to state the mean value ROR for a distribution with a 5 % chance of being below 30 %
ROR, which would balance the desirability of that alternative with another one with
no chance of an ROR below 30 %, but a mean value of only 35 %. Four of the nine
responding stated 50 %, two others 45 %, while two staff members gave a much higher
figure and another an ROR of 39.5 %. Hence, what is relatively important is the probability that a return will be less than L.5 Only to a lesser extent does the mean influence choice, and the variance is viewed as having only indirect effects. There is a
limit to which the disparity of probabilities below L can be countered by higher
means. At some point the dispersion is perceived as so great that at best the persons
involved in making the estimates will be asked to make another set when they have
"more and better information."
Mode of Data Presentation
In all of the literature on decision making, and in particular that on statistical
decision theory, little if anything has been said about the form in which the data
should be presented to the decision maker.6 Perhaps this is because most theoreticians
assume that as long as the data unambiguously define the distributions, the format
of presentation should make no difference. This brings up the question of whether
data can ever be unambiguously presented, and perhaps more importantly, in whose
eyes? The only answer to the second question is the user, but he has seldom been
asked.
To determine the impact that format might have, the same probability distributions that were used to generate A, B and C were also used to generate X, Y and Z,
and were presented to the decision maker as in Table II, shortly after the first set
(coffee was served at their seats between the two sets).
The subjects were asked to rank order these investment opportunities, and the
resulting ranks were compared with those made earlier on A, B and C. The desire to
be consistent (mentioned by at least two persons) should have had the effect of making
the orderings the same, and there was no reason other than data format as to why
the rankings should have varied. The results, however, were that 5 of the 8 changed
their rank ordering, and in 4 instances the investment picked as most desirable was
different. Two persons switched from C to A (using the old lettering scheme), one
from C to B, and one from A to C. Apparently the format used in the second analysis,
with the exception of one respondent, lessened the fear of going below L, and greater
concentration was paid to the probability of being above a 35 % and 40 % ROR. The
differences among these cumulative probabilities were not nearly so obvious when
twenty points on the cumulative were given, though presumably the twenty points
gave the decision maker more information. As others have noted in different contexts (e.g. Parry [15, pp. 85ff.]), perhaps the earlier set of data provided more information than the subjects could handle easily. Whatever the conclusion one can make
about information overload, the format did affect potential choice.
Additional evidence can be presented regarding format and the sensitivity to L.
5 For further support of this assertion see Mao [12].

This is quite surprising considering the results of Costello and Zalkind [7, p. 381], among
others.
6

<-----Page 7----->FROM STATISTICAL

DECISION

THEORY

879

TO PRACTICE

TABLEII
Three Hypothetical Investment Projects (Cumulative probability distribution values
as determined by given ROR)
X(A)

Y(B)

Z(C)

Rate of Return

Probability of Not
Achieving That
Rate of Return

Rate of Return

Probability of Not
Achieving That
Rate of Return

Rate of Return

Probability of Not
Achieving That
Rate of Return

30
35
40

0.087
0.170
0.328

30
35
40

0.031
0.733
0.999

30
35
40

0.005
0.620
0.782

50

0.938

50

0.999

50

0.888

TABLE III
Choice among Alternative Projects

1. Which would you prefer to support among projects D and E?
D) average ROR is 35%
no chance that ROR would be less than 30%
E) average ROR is 40%
5% chance that ROR is less than 30%
2. Which of the following two projects would you prefer to support, F or G?
F) average ROR is 40%
no chance ROR is below 35%
no chance ROR is above 45%
G) average ROR is 50%
10% chance ROR is below 35%
10% chance ROR is above 65%
3. Which of the following two projects would you prefer to support H or J?
H) average ROR is 50%
10% chance ROR is below 40%
10% chance ROR is above 60%
J) average ROR is 60%
10% chance ROR is below 35%
10% chance ROR is above 85%

Among several questions, three sets of two alternatives were presented each committee member (Table III). Each person was to state his preference in each case. As
one can see, not only did the probability of going below L (30 % ROR) vary, as did
the mean ROR, but in only one instance was the cumulative probability stated for L.
In that case (the choice between D and E) 8 out of 9 felt that the advantage of a 5 %
higher mean ROR for the second alternative was not sufficiently great to overcome
the 0.00 versus 0.05 probability of going below a 30 % ROR. The one remaining person found himself essentially indifferent between the two.
This is what we would expect based on our earlier discussion. Now let us look at
the choices made between alternative investments F and G. If we assume that these
two symmetric distributions are also normal, we find that the probabilities of being
below L are 0.000 and 0.044 for F and G respectively. The gap between the means,
however, is 10 percentage points, so we would expect that more persons might choose
G over F than E over D. In fact, four of nine persons so chose.
But now look at the choice between H and J. Again, if we assume a normal dis-

<-----Page 8----->880

DAVID

W. CONRATH

tribution of ROR's, based on the three points given, the probability that the ROR
will be less than 30 % for H and J is 0.005 and 0.063 respectively. Since the gap between
the mean ROR's is also 10 % points, one would expect fewer persons would choose J
over H than chose G over F. The converse, however, was true. Three persons who had
chosen F over G chose J over H. All three persons mentioned that when choosing F
over G they wanted to go for the "sure thing." However, in the case of H versus J,
neither was completely sure, but J had a higher average ROR, and thus they preferred J. They were all unwilling to extrapolate a 0.10 chance of being below 40 %
ROR to a near zero chance of being below 30 %. In addition, one person now noted
the high side, and suggested that ROR's of 85 % and 60 % at 0.10 on the cumulative
distributions were sufficiently different to ignore the approximately equal ROR's
on the low side.
Other examples could be stated, but enough have been given to demonstrate that
data format can influence choice. Apparently format has the characteristic that it can
focus one's attention on one dimension of the choice space, and that dimension becomes paramount in the decision process. For example, the first of the two formats
allowed an easy comparison between the probabilities that a project would yield an
ROR below L. The second, however, made for an easy comparison among the probabilities that a return would be greater than two acceptable levels of return, 35 %
and 40 %. Thus, something more similar to the comparison of the entire distributions
was undertaken. This ability to focus attention on one factor in a multi-dimensional
choice space has long been understood on Madison Avenue, but for some reason
decision theorists have chosen to ignore it. In fact, this author would explain the Allais' Paradox [4] on just this basis alone. One presentation focuses attention on the
chance of no return at all, no matter how small the chance. The other comparison
focuses attention on the high side, and thus something more like an expected value
operator governs choice.
Whether the attention focusing attributes of data format are the keys to the influence
that format has on choice is a question not yet resolved. But the question would
appear to be sufficiently important that it should no longer be ignored.
A Descriptive Model of Decision Making Under Risk
Trhemodel, while we believe it to be rather general, is based on two years of attendance at committee proceedings and on numerous interviews with both committee
members and the persons who supplied the committee with the data that was to provide the base for decision making. A first observation is that at no time were different
investment opportunities compared to each other, the "better" ones being chosen.
Rather, the processes were similar to those described by Cyert and March [8]. Each
project was dealt with sequentially, and compared to a perceived standard of acceptability.
The primary reasons for the sequential process appear to be twofold. First, requests
for funding authorization were spaced throughout the year. Thus, the committee
members did not perceive an opportunity to make comparisons among more than two
or three projects at a time. Secondly, and perhaps of greater significance, each project
was viewed as having unique aesthetic and technological characteristics. Though the
decision of whether or not to fund a project supposedly was made primarily on financial grounds, the perception of the marketability of a potential product was influenced
by personal predilections regarding consumer needs and product appeal. Furthermore,
consideration was given to how a proposed product might fit into the product line,

<-----Page 9----->FROM STATISTICAL

DECISION

THEORY

TO PRACTICE

881

though demand interdependencies were never discussed on anything more than a
qualitative level. Thus, the belief of product uniqueness made it most difficult to compare one project with another, especially on any quantitative basis. The decision of
whether or not to pursue the development of a particular product, therefore, was
essentially a zero/one phenomenon.
Choice, C, appears to be directly related to three exogenous variables, each of which
acts as a constraint when the other two are held constant. The variables are: L-the
internal rate of return (or more generally, the utility) below which one is viewed as
having "failed"; p-the probability that the return might be less than L; and
e-the "expected" return on investment. C = c (L, p, e).
L appears to be truly independent, and is based on the collective perception of the
committee members about the return on investment that must be demonstrated to
the Board of Directors to get their approval to release the requested funds. The shorter
the supply of capital, the higher L is perceived to be, and with justification based on
the Board's responses to past proposals.
p is a more complex parameter. It is directly related to: (a) the significance of the
research and development effort required for the particular product, as it might pertain to other products in the future; (b) the importance of a product to complete or
complement a product line; and (c) the "appeal" of the proposed product-its impact
on corporate image. All three factors influencing the acceptable level of p reflect
economic externalities with respect to the product proposed, but ones which are most
difficult to measure, the uncertainty of the effects of the interdependencies being very
great. As Raiffa [171 has noted, with so much uncertainty a "seat of the pants" approach is usually used, hence the qualitative effect (directional only) that each factor
has on p.
e, the expected return on investment, has the effect one would expect. The greater e,
the more likely the proposal will be accepted. But e, as a constraint parameter, is also
directly related with p. All other things being equal, the higher is e, the higher the
permissible p. Furthermore, the relative importance of p and e may be affected by the
manner in which the financial data are presented, as we have already demonstrated.
Some formats have a tendency to focus attention on p, while others increase the relative importance of e as the basis for choice.
Though an investment opportunity presented in one form may be rejected by
making the appropriate trade-offs clear, it might later be accepted with the same data
base. Often the result of committee deliberations is that a new format is presented by
the proponents of an investment that now makes it acceptable to those concerned. In
fact, if one were to view decision making as a "0, ?, 1" process, which it frequently
appears to be (0 is rejection, 1 acceptance, and ? is a postponement), the ? arises when
there is the possibility that "new data" (really a revised presentation of existing data)
may, in fact, make it clearer regarding whether the choice is 0 or 1. The ? may also
arise when uncertainty (usually the variance or dispersion of possible outcomes) is
so great that the committee members believe that a more resolving data base can be
obtained.
If we were to derive an explicit descriptive model of choice based on both personal
observation of past decisions and the results of the several questionnaires distributed
during the one-day session on risk analysis, the following would reflect the consensus
of the committee:
C= 1 if e_ L+3p,
= 0 otherwise.

and 0<p<0.10,

<-----Page 10----->882

DAVID

W. CONRATH

Six of the eight subjects stated that L = 0.30 ROR. To balance an investment with
p = 0.00 and e = 0.35, with one where p = 0.05, e = 0.50 was both the mean and the
median of the committee members and staff. This implies the model as stated. In
addition, the responses to the questions on Table III are consistent with the model.
When p exceeded 0.10 there was little likelihood that the project would ever reach
the proposal stage. Few executives were willing to take a chance greater than 0.10 of
"failing", no matter what the expected return. Furthermore, in such situations the
uncertainty (the dispersion of the distribution of outcomes) would be so great that
the entire data base would be brought into question. On the other hand, if p = 0.05,
for example, we would expect an investment opportunity to be accepted if e _ 0.45.
Concluding Comments
Three final observations deserve mention. First, "uncertainty" (Luce and Raiffa,
[10, p. 13]) exists, decision makers usually recognize that their state of information is
less than complete. Probability distributions are derived based on little more than
best guesses, and the result is "uncertainties . .. in cloudy or fuzzy form" (Raiffa
[17, p. 691]). Judgment rather than analysis now plays an important role, and the use
of highly sophisticated models is not appealing when one recognizes the lack of a
reliable data base. Decision makers are not ignorant of the computer users' maxim:
garbage in -+ garbage out. Thus, in such situations they are more prone to look for
simple models to guide their judgments rather than complex choice models. Most statistical decision theory tools (with the exception of the Bayesian and decision tree
approaches )7 fail in this respect. Further support of this point can be found in the
resurgence of interest in the "payback period" as a means of evaluating investment
opportunities (Ijiri [9] and Weingartner [20]). The payback period is simple to understand, and most importantly, it emphasizes the "risk of loss" aspect of decision making.
Secondly, the process of sequentially developing alternatives is a reality in many
decision situations that theorists should not ignore if they expect their models to have
pragmatic applications. One conclusion that could be reached is that decision models
ought to consider the effects that variations in the constraint parameters might have
on choice. To take an analogy from the internal pricing literature (Baumol and Fabian
[5]), these might be used by senior executives to ensure optimal investment throughout
the corporation, with the actual choices being made on a decentralized basis. Variations in certain constraints, such as L, do take place currently, but usually in a rather
abrupt fashion. The results are not optimal over the long run. Too frequently it looks
like the process often associated with federal expenditures: capital is spent until there
is no more, and then one waits for the next fiscal year; or if money remains toward
the end of the year, any opportunity is accepted. The adjustment of decision constraints can certainly be done on a more rational basis, and at the same time provide
parameters for simple decision models that are useful from the viewpoint of many

I While the author was unaware of the following observation until the paper had been written,
the argument presented is strongly supportive of what might be called the "Harvard" approach
to decision modelling-the use of Bayesian decision analysis, and simple decision trees. Generally,
they require nothing more sophisticated than point estimates of possible outcomes using a finite
(usually very small) set of alternatives and of "states of nature". Such estimates are within the
limits of comprehension of most decision makers, and therefore usable. These simple decision
models also typically avoid the problem of data presentation, and can even be used to accommodate the "fear of failure" if utility rather than profit is the measuring stick. These last two
points, however, are usually overlooked, probably because most of the literature is normative
rather than descriptive.

<-----Page 11----->FROM STATISTICAL

DECISION

THEORY

TO PRACTICE

883

decision makers. Decision theory to be embraced by those who could use it ought to
be based on an understanding of the limitations of what can be used and how the
what will be used.8
Thirdly, the author noted a concern expressed by several executives that once their
choice models were made explicit, they could be "gamed" relatively easily by those
who have the responsibility of supplying the data needed for decision making. Of
course, whether the model is explicit or implicit, as long as one has knowledge of it
this problem exists. One possible way to deal with this problem is to reward those
supplying estimates on the basis of their proven accuracy. Now both unbiased estimates and the minimization of variance might have some real meaning, and everyone
might be encouraged to understand "risk" just a little bit better.
References
1.

R. M. AND NORMAN, J. M., "Operational Research and Decision-Making," Operational ResearchQuarterly,Vol. 20 (1969), pp. 399-413.
2. ALCHIAN, ARMEN A., "Uncertainty, Evolution, and Economic Theory," Journal of Political
Economy, Vol. 58 (1950), pp. 211-221.
3. ALDERFER, CLAYTON P., AND BIERMAN, JR. HAROLD, "Choices with Risk: Beyond the Mean
and Variance," Journal of Business, Vol. 43 (1970), pp. 341-353.
4. ALLAIS, MAURICE, "Le Comportement de l'homme Rationnel devant le Risque: Critique des
Postulats et Axioms de l'ecole Americaine," Econometrica,Vol. 21 (1953), pp. 503-546.
5. BAUMOL, WILLIAM J. AND FABIAN, TIBOR, "Decomposition, Pricing for Decentralization and
External Economies," ManagementScience, Vol. 11 (1964), pp. 1-32.
6. BOWER, JOSEPH L., Managing the ResourceAllocation Process, Division of Research, Graduate
School of Business Administration, Harvard, Boston, 1970.
7. COSTELLO, TIMOTHY W. AND ZALKIND, SHELDON S., Psychology in Administration, PrenticeHall, Englewood Cliffs, N.J., 1964.
8. CYERT, RICHARD M. AND MARCH, JAMES G., A Behavioral Theory of the Firm, Englewood
Cliffs, N.J., Prentice-Hall, 1963.
9. IJIRI, YuJI, "The Payback Period: Revived and Modified," Novus (November 1970), pp. 8-12.
10. LUCE, R. DUNCAN AND RAIFFA, HOWARD, Gamesand Decisions, Wiley, New York, 1957.
11. MAO, JAMES C. T., "Models of Capital Budgeting, E-V vs. E-S," Journal of Financial and
QuantitativeAnalysis, Vol. 4 (1970), pp. 657-675.
12.
, "Survey of Capital Budgeting: Theory and Practice," Journal of Finance, Vol. 25 (1970),
pp. 349-360.
13. MARKOWITZ, HARRY M., Portfolio Selection, Wiley, New York, 1959.
14. MILLER, GEORGE A., "The Magical Number Seven, Plus or Minus Two: Some Limits on our
Capacity for Processing Information," The Psychological Review,Vol. 63 (1956), pp. 81-97.
15. PARRY, JOHN, The Psychologyof Human Communication,University of London Press, London,
1967.
16. PETERSON, D. E. AND LAUGHHUNN, D. J., "Capital Expenditure Programming and Some
Alternative Approaches to Risk," ManagementScience, Vol. 17 (1971), pp. 320-336.
17. RAIFFA, HOWARD, "Risk, Ambiguity, and the Savage Axioms: Comment," QuarterlyJournal of
Economics, Vol. 75 (1961), pp. 690-694.
ADELSON,

18. SALAZAR, RODOLFO C. AND SEN SUBRATA K., "A Simulation Model of Capital Budgeting under

Uncertainty," ManagementScience, Vol. 15 (1968), pp. B-161-179.
19.

VON NEUMANN, JOHN AND MORGENSTERN OSKAR, Theory of Games and Economic Behavior,

20.

WEINGARTNER, H. MARTIN, " Some New Views on the Payback Period and Capital Budgeting

2nd edition, Princeton, N.J., Princeton University Press, 1947.
Decisions," ManagementScience, Vol. 15 (1969), pp. B-594-607.
8 This assertion is well supported by Bower [6] and by Adelson and Norman [1]. Even the more
abstract works are beginning to be tainted by pragmatism (e.g. see Peterson and Laughhunn [16].
Statements by theorists that most decision-making behavior is irrational (usually from the
theorists' point of view), without the provision of alternatives other than educating all decision
makers to think like theorists, do little to encourage the conversion of theory to practice.

