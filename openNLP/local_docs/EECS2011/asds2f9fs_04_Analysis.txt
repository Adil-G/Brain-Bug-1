<-----Page 0----->Analysis of
Algorithms

Input

Last Update: Aug 21, 2014

Algorithm

EECS2011: Analysis of Algorithms

Output

1

<-----Page 1----->Part 1:
Running Time

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

2

<-----Page 2----->Running Time

Running Time

• Most algorithms transform input
objects into output objects.
• The running time of an
algorithm typically grows with
the input size.
• Average case time is often
difficult to determine.
• We focus on the worst case
running time.
o Easier to analyze
o Crucial to applications such as
games, finance and robotics

Last Update: Aug 21, 2014

best case
average case
worst case
120
100
80
60
40
20
0

1000

EECS2011: Analysis of Algorithms

2000

3000

4000

Input Size

3

<-----Page 3----->Experimental
Studies

8000
7000

Time (ms)

• Write a program
implementing the algorithm
• Run the program with
inputs of varying size and
composition, noting the
time needed
• Plot the results

9000

6000
5000
4000
3000
2000
1000
0
0

50

100

Input Size

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

4

<-----Page 4----->Limitations of Experiments
• It is necessary to implement the algorithm, which
may be difficult, time consuming or costly.
• Results may not be indicative of the running time
on other inputs not included in the experiment.
• In order to compare two algorithms, the same
hardware and software environments must be
used.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

5

<-----Page 5----->Theoretical Analysis
• Uses a high-level description of
the algorithm instead of an implementation
• Characterizes running time as a function of the
input size, n
• Takes into account all possible inputs
• Allows us to evaluate the speed of an algorithm
independent of the hardware/software
environment
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

6

<-----Page 6----->Pseudocode
•
•
•
•
•

High-level description of an algorithm
More structured than English prose
Less detailed than a program
Preferred notation for describing algorithms
Hides program design issues

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

7

<-----Page 7----->Pseudocode Details
• Method call

• Control flow
–
–
–
–
–

if … then … [else …]
while … do …
repeat … until …
for … do …
Indentation replaces braces

• Method declaration
Algorithm method (arg [, arg…])
Input …
Output …

Last Update: Aug 21, 2014

method (arg [, arg…])

• Return value
return expression

• Expressions:
 Assignment
 Equality testing
n2 Superscripts and other
mathematical formatting
allowed

EECS2011: Analysis of Algorithms

8

<-----Page 8----->The Random Access Machine
(RAM) Model
A RAM consists of
• A CPU
• A potentially unbounded bank of
memory cells, each of which can hold
an arbitrary number or character
• Memory cells are numbered and
accessing any cell in memory takes
unit time

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

2

1

0

9

<-----Page 9----->Seven Important Functions
 Seven functions that often appear in algorithm analysis:
 Constant  1
 Logarithmic  log n
 Linear  n
 Quadratic  n2
 Cubic  n3

T(n)

 N-Log-N  n log n

 Exponential  2n

 In a log-log chart,
the slope of the line
corresponds to the growth rate

1E+29
1E+27
1E+25
1E+23
1E+21
1E+19
1E+17
1E+15
1E+13
1E+11
1E+9
1E+7
1E+5
1E+3
1E+1
1E-1
1E-1

Cubic
Quadratic

Linear

1E+2

1E+5

1E+8

n
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

10

<-----Page 10----->Primitive Operations
• Basic computations performed
by an algorithm
Examples:
• Identifiable in pseudocode
– Evaluating an expression
– Assigning a value to a
• Largely independent from the
variable
programming language
– Indexing into an array
• Exact definition not important
– Calling a method
(we will see why later)
– Returning from a method
• Assumed to take a constant
amount of time in the RAM model

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

11

<-----Page 11----->Counting Primitive Operations
By inspecting the pseudocode, we can determine the maximum
number of primitive operations executed by an algorithm, as a
function of the input size

STEP

3

4

5

6

7

8

TOTAL

# ops

2

2

1+2n

2n

0 to 2n

1

4n+6 to 6n+6

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

12

<-----Page 12----->Estimating Running Time
• Algorithm arrayMax executes 6n + 6 primitive
operations in the worst case, 4n + 6 in the best case.
Define:
a = Time taken by the fastest primitive operation
b = Time taken by the slowest primitive operation

• Let T(n) be worst-case time of arrayMax. Then
a (4n + 6)  T(n)  b(6n + 6)
• Hence, the running time T(n) is bounded by two
linear functions
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

13

<-----Page 13----->Growth Rate of Running Time
• Changing the hardware/ software environment
o Affects T(n) by a constant factor, but
o Does not alter the growth rate of T(n)

• The linear growth rate of the running time T(n) is
an intrinsic property of algorithm arrayMax

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

14

<-----Page 14----->Why Growth Rate Matters
if runtime is...

time for n + 1

time for 2 n

time for 4 n

c lg n

c lg (n + 1)

c (lg n + 1)

c(lg n + 2)

cn

c (n + 1)

2c n

4c n

c n lg n

~ c n lg n
+ cn

2c n lg n +
2cn

4c n lg n +
4cn

c n2

~ c n2 + 2c n

4c n2

16c n2

c n3

~ c n3 + 3c n2

8c n3

64c n3

c 2n

c 2 n+1

c 2 2n

c 2 4n

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

runtime
quadruples
when
problem
size
doubles

15

<-----Page 15----->Comparison of Two Algorithms
insertion sort is

n2 / 4

merge sort is

2 n lg n

sort a million items?
insertion sort takes
roughly 70 hours
while

merge sort takes
roughly 40 seconds
This is a slow machine, but if
100 x as fast, then it’s 40 minutes
versus less than 0.5 seconds
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

16

<-----Page 16----->Constant Factors
• The growth rate is not
affected by
T(n)

o constant factors or
o lower-order terms

• Examples
o 102n + 105
is a linear function
o 105n2 + 108n
is a quadratic function

1E+25
1E+23
1E+21
1E+19
1E+17
1E+15
1E+13
1E+11
1E+9
1E+7
1E+5
1E+3
1E+1
1E-1
1E-1

Quadratic
Quadratic

Linear
Linear

1E+2

1E+5

1E+8

n

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

17

<-----Page 17----->Big-Oh Notation
• Given functions f(n) and
g(n), we say that f(n) is
O(g(n)) if there are
positive constants
c and n0 such that
f(n)  cg(n) for n  n0
• Example: 2n + 10 is O(n)
o
o
o
o

2n + 10  cn
(c  2) n  10
n  10/(c  2)
Pick c = 3 and n0 = 10

Last Update: Aug 21, 2014

10,000
3n
2n+10

1,000

n

100

10

1
1

EECS2011: Analysis of Algorithms

10

100

1,000

n

18

<-----Page 18----->Big-Oh Example
1,000,000

• Example:
the function n2 is not O(n)

n^2
100n

100,000

10n

o n2  cn
10,000
o nc
o The above inequality cannot 1,000
be satisfied for all sufficiently
100
large n, since c must be a
constant

n

10
1
1

10

100

1,000

n
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

19

<-----Page 19----->More Big-Oh Examples


7n - 2
7n-2 is O(n)
need c > 0 and n0  1 such that 7 n - 2  c n for n  n0
this is true for c = 7 and n0 = 1



3 n3 + 20 n2 + 5
3 n3 + 20 n2 + 5 is O(n3)
need c > 0 and n0  1 such that 3 n3 + 20 n2 + 5  c n3 for n  n0
this is true for c = 4 and n0 = 21



3 log n + 5
3 log n + 5 is O(log n)
need c > 0 and n0  1 such that 3 log n + 5  c log n for n  n0
this is true for c = 8 and n0 = 2

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

20

<-----Page 20----->Big-Oh and Growth Rate
• The big-Oh notation gives an upper bound on the
growth rate of a function
• The statement “f(n) is O(g(n))” means that the
growth rate of f(n) is no more than the growth rate
of g(n)
• We can use the big-Oh notation to rank functions
according to their growth rate

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

21

<-----Page 21----->Big-Oh Rules
• If f(n) is a polynomial of degree d,
then f(n) is O(nd), i.e.,
o drop lower-order terms
o drop constant factors

• Use the smallest possible class of functions
o Say “2n is O(n)” instead of “2n is O(n2)”

• Use the simplest expression of the class
o Say “3n + 5 is O(n)” instead of “3n + 5 is O(3n)”

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

22

<-----Page 22----->Asymptotic Algorithm Analysis
• The asymptotic analysis of an algorithm
determines the running time in big-Oh notation
• To perform the asymptotic analysis
o We find the worst-case number of primitive
operations executed as a function of the input size
o We express this function with big-Oh notation

• Example:
o We say that algorithm arrayMax “runs in O(n) time”

• Since constant factors and lower-order terms are
eventually dropped anyhow, we can disregard
them when counting primitive operations
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

23

<-----Page 23----->Computing Prefix Averages
• We further illustrate
asymptotic analysis with two
algorithms for prefix averages
• The i-th prefix average of an
array X is average of the first
(i + 1) elements of X:
A[i] = (X[0] + X[1] + … + X[i])/(i+1)

35

X
A

30
25
20
15
10

• Computing the array A of
prefix averages of another
array X has applications to
financial analysis
Last Update: Aug 21, 2014

5
0

EECS2011: Analysis of Algorithms

1

2

3

4

5

6

7

24

<-----Page 24----->Prefix Averages (Quadratic)
The following algorithm computes prefix averages in
quadratic time by applying the definition

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

25

<-----Page 25----->Arithmetic Progression
• The running time of
prefixAverage1 is
O(1 + 2 + …+ n)
• The sum of the first n
integers is n(n + 1) / 2
– There is a simple visual
proof of this fact

• Thus, algorithm
prefixAverage1 runs in
O(n2) time
Last Update: Aug 21, 2014

7
6
5
4
3
2
1
0
1

EECS2011: Analysis of Algorithms

2

3

4

5

6
26

<-----Page 26----->Prefix Averages 2 (Linear)
The following algorithm uses a running sum to improve efficiency

Algorithm prefixAverage2 runs in O(n) time!
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

27

<-----Page 27----->Math you need to Review
• Properties of powers:
𝑎𝑏+𝑐 = 𝑎𝑏 ∗ 𝑎𝑐
𝑎𝑏−𝑐 = 𝑎𝑏 / 𝑎𝑐

• Summations
• Powers

𝑎

• Logarithms
• Proof techniques
– Induction

– ...

• Basic probability

𝑏∗𝑐

𝑏 𝑐

= 𝑎
𝑎𝑏 = 𝑎𝑏 log𝑐𝑐 = 𝑐 𝑏 log𝑐𝑎
• Properties of logarithms:
log 𝑏 𝑥 ∗ 𝑦 = log 𝑏 𝑥 + log 𝑏 𝑦
log 𝑏 𝑥/𝑦 = log 𝑏 𝑥 − log 𝑏 𝑦
log 𝑏 𝑥 𝑎 = 𝑎 log 𝑏 𝑥
𝑥 log 𝑦 = 𝑦 log 𝑥
log 𝑏 𝑥 = log 𝑐 𝑥 / log 𝑐 𝑏

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

28

<-----Page 28----->Relatives of Big-Oh
big-Omega


f(n) is (g(n)) if there is a constant c > 0
and an integer constant n0  1 such that

f(n)  c g(n)

for n  n0

big-Theta


f(n) is (g(n)) if there are constants c’ > 0 and c’’ > 0
and an integer constant n0  1 such that
c’ g(n)  f(n)  c’’ g(n)

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

for n  n0
29

<-----Page 29----->Intuition for Asymptotic
Notation
 big-Oh

f(n) is O(g(n))
if f(n) is asymptotically
less than or equal to g(n)

 big-Omega

f(n) is (g(n))
if f(n) is asymptotically
greater than or equal to g(n)

 big-Theta

f(n) is (g(n))
if f(n) is asymptotically
equal to g(n)

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

30

<-----Page 30----->Example Uses of the
Relatives of Big-Oh




5n2 is (n2)
f(n) is (g(n)) if there is a constant c > 0 and an integer constant n0  1
such that f(n)  c g(n) for n  n0 .
Let c = 5 and n0 = 1.
5n2 is (n)

f(n) is (g(n)) if there is a constant c > 0 and an integer constant n0  1
such that f(n)  c g(n) for n  n0 .
Let c = 1 and n0 = 1.



5n2 is (n2)
f(n) is (g(n)) if it is (n2) and O(n2). We have already seen the former,
for the latter recall that f(n) is O(g(n)) if there is a constant c > 0 and
an integer constant n0  1 such that f(n) < c g(n) for n  n0 .
Let c = 5 and n0 = 1.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

31

<-----Page 31----->Part 1: Summary
• Analyzing running time of algorithms
o Experimentation & its limitations
o Theoretical analysis

•
•
•
•
•

Pseudo-code
RAM: Random Access Machine
7 important functions
Asymptotic notations: O(), () , ()
Asymptotic running time analysis of algorithms

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

32

<-----Page 32----->Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

33

<-----Page 33----->Part 2: Correctness

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

34

<-----Page 34----->Outline
• Iterative Algorithms:
Assertions and Proofs of Correctness
• Binary Search: A Case Study

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

35

<-----Page 35----->Assertions
• An assertion is a statement about the state of the data
at a specified point in your algorithm.
• An assertion is not a task for the algorithm to perform.
• You may think of it as a comment that is added for the
benefit of the reader.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

36

<-----Page 36----->Loop Invariants
• Binary search can be implemented as an iterative
algorithm (it could also be done recursively).
• Loop Invariant: An assertion about the current state
useful for designing, analyzing and proving the
correctness of iterative algorithms.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

37

<-----Page 37----->Other Examples of Assertions
• Pre-conditions: Any assumptions that must
be true about the input instance.
• Post-conditions: The statement of what
must be true when the algorithm/program
returns.
• Exit-condition: The statement of what must
be true to exit a loop.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

38

<-----Page 38----->Iterative Algorithms
Take one step at a time
towards the final destination
loop {
if (done) { exit loop }

take step
}

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

39

<-----Page 39----->From Pre-Condition to Post-Condition
Pre-Condition

Post-Condition
Loop Invariant

Loop

Post-Loop

Pre-Loop

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

40

<-----Page 40----->Establishing Loop Invariant
1. from the pre-condition on the input instance,
establish the loop invariant.

1

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

41

<-----Page 41----->Maintain Loop Invariant
2

• Suppose that
1) We start in a safe location (loop invariant
just established from pre-condition)
2) If we are in a safe location (loop invariant), we
always step to another safe location (loop invariant)

• Can we be assured that the computation will
always keep us in a safe location?
• By what principle?
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

42

<-----Page 42----->Maintain Loop Invariant
By Induction the computation will always
be in a safe location.
𝑆(𝑖) = loop invariant
at the end of iteration 𝑖
(beginning of iteration 𝑖 + 1)
1

𝑆(0)
𝑎𝑛𝑑

2
Last Update: Aug 21, 2014

⟹ ∀𝑖, 𝑆 𝑖

∀𝑖, 𝑆 𝑖 ⟹ 𝑆 𝑖 + 1
EECS2011: Analysis of Algorithms

43

<-----Page 43----->Ending The Algorithm
• Define Exit Condition
• Termination: With sufficient progress,
the exit condition will be met.
• When we exit, we know

3

o loop invariant is true
o exit condition is true

3) from these we must establish the post-condition.
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

44

<-----Page 44----->Iterative Algorithm
Loop-Invariant

Pre-Condition

Exit-Condition

Post-Condition

YES

1

NO

Pre-loop

Post-loop

3
2

Last Update: Aug 21, 2014

Loop Body

EECS2011: Analysis of Algorithms

45

<-----Page 45----->Definition of Correctness
<PreCond> & <code>  <PostCond>
If the input meets the pre-conditions,
then the output must meet the post-conditions.
If the input does not meet the preconditions, then
nothing is required.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

46

<-----Page 46----->Binary Search:
a case study

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

47

<-----Page 47----->Define Problem: Binary Search
• PreConditions
– Key
25
– Sorted indexed List A
3

5

6

13

18

21

21

25

36

43

49

51

53

60

72

74

83

88

91

95

51

53

60

72

74

83

88

91

95

• PostConditions
– Find key in list (if there).
3

5

6

13

18

Last Update: Aug 21, 2014

21

21

25

36

43

49

EECS2011: Analysis of Algorithms

48

<-----Page 48----->Define Loop Invariant
• Maintain a sub-list.
• LI: If the key is contained in the original list,
then the key is contained in the sub-list.
key 25
3

5

6

13

18

Last Update: Aug 21, 2014

21

21

25

36

43

49

51

53

EECS2011: Analysis of Algorithms

60

72

74

83

88

91

49

95

<-----Page 49----->Define Step
• Cut sub-list in half.
• Determine which half the key would be in.
• Keep that half.
mid

key 25
3

5

6

13

18

21

21

25

36

If key ≤ A[mid],
then key is in
left half.
Last Update: Aug 21, 2014

43

49

51

53

60

72

74

83

88

91

If key > A[mid],
then key is in
right half.

EECS2011: Analysis of Algorithms

50

95

<-----Page 50----->Define Step
• It is faster not to check if the middle element
is the key.
• Simply continue.
key 43
3

5

6

13

18

21

21

25

36

If key ≤ A[mid],
then key is in
left half.
Last Update: Aug 21, 2014

43

49

51

53

60

72

74

83

88

91

If key > A[mid],
then key is in
right half.

EECS2011: Analysis of Algorithms

51

95

<-----Page 51----->Make Progress
• The size of the list becomes smaller.
3

5

6

13

18

21

21

25

36

43

49

51

53

60

72

74

83

88

91

95

3

5

6

13

18

21

21

25

36

43

49

51

53

60

72

74

83

88

91

95

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

52

<-----Page 52----->Exit Condition
key 25
3

5

6

13

18

21

21

25

36

43

49

51

53

60

72

74

83

88

91

95

• LI: If the key is contained in the
original list, then the key is
contained in the sub-list.

• If element = key,
return associated
entry.

• Sub-list contains one element.

• Otherwise return false.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

53

<-----Page 53----->Running Time
The sub-list is of size 𝑛,

𝑛
2

,

𝑛
4

,

𝑛
8

, …, 1

Each step O(1) time.
Total time = O(log n)
key 25
3

5

6

13

18

21

21

25

36

If key ≤ A[mid],
then key is in
left half.
Last Update: Aug 21, 2014

43

49

51

53

60

72

74

83

88

91

If key > A[mid],
then key is in
right half.

EECS2011: Analysis of Algorithms

54

95

<-----Page 54----->Algorithm BinarySearch ( A[1..n] , key)
<precondition>: A[1..n] is sorted in non-decreasing order
<postcondition>: If key is in A[1..n], output is its location
p1, qn
while p < q do
<Loop-invariant>: if key is in A[1..n], then key is in A[p..q]
mid 

𝑝+𝑞
2

if key  A[mid] then q  mid
else p  mid + 1
end while
if key = A[p]
then return (p)
else return (“key not in list”)
end algorithm

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

55

<-----Page 55----->Simple, right?
• Although the concept is simple, binary search is
notoriously easy to get wrong.
• Why is this?

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

56

<-----Page 56----->Boundary Conditions
• The basic idea behind binary search is easy to grasp.
• It is then easy to write pseudo-code that works for a
‘typical’ case.
• Unfortunately, it is equally easy to write pseudo-code
that fails on the boundary conditions.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

57

<-----Page 57----->Boundary Conditions
if key  A[mid]
then q  mid
else p  mid + 1

or

if key < A[mid]
then q  mid
else p  mid + 1

What condition will break the loop invariant?

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

58

<-----Page 58----->Boundary Conditions

mid

key 36
3

5

6

13

18

21

21

25

36

43

49

51

53

60

72

74

83

88

91

Code: key  A[mid]  select right half

Bug!!
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

59

95

<-----Page 59----->Boundary Conditions
if key  A[mid]
then q  mid
else p  mid + 1

OK

Last Update: Aug 21, 2014

if key < A[mid]
then q  mid - 1
else p  mid

OK

EECS2011: Analysis of Algorithms

if key < A[mid]
then q  mid
else p  mid + 1

Not OK !!

60

<-----Page 60----->Boundary Conditions
 p q
mid  
 2 

 p q
mid  
 2 

or

key 25
3

5

6

13

18

21

21

25

Shouldn’t matter, right?
Last Update: Aug 21, 2014

36

43

49

51

53

60

72

74

83

88

91

p q 
Select mid  
 2 

EECS2011: Analysis of Algorithms

61

95

<-----Page 61----->Boundary Conditions

mid

key 25
3

5

6

13

18

21

21

If key ≤ A[mid],
then key is in
left half.
Last Update: Aug 21, 2014

p q 
Select mid  
 2 
25

36

43

49

51

53

60

72

74

83

88

91

If key > A[mid],
then key is in
right half.
EECS2011: Analysis of Algorithms

62

95

<-----Page 62----->Boundary Conditions

mid

key 25
3

5

6

13

p q 
Select mid  
 2 

18

21

21

25

36

If key ≤ A[mid],
then key is in
left half.
Last Update: Aug 21, 2014

43

49

51

53

60

72

74

83

88

91

If key > A[mid],
then key is in
right half.

EECS2011: Analysis of Algorithms

63

95

<-----Page 63----->Boundary Conditions
No progress toward goal:
Loops Forever!

Another bug!
mid

key 25
3

5

6

13

18

21

21

25

36

If key ≤ A[mid],
then key is in
left half.
Last Update: Aug 21, 2014

p q 
Select mid  
 2 
43

49

51

53

60

72

74

83

88

91

If key > A[mid],
then key is in
right half.

EECS2011: Analysis of Algorithms

64

95

<-----Page 64----->Boundary Conditions
𝑝+𝑞

𝑝+𝑞

𝑝+𝑞

mid 
2
if key  A[mid]
then q  mid
else p  mid + 1

mid 
2
if key < A[mid]
then q  mid - 1
else p  mid

mid 
2
if key  A[mid]
then q  mid
else p  mid + 1

OK

OK

Not OK !!

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

65

<-----Page 65----->Getting it Right
• How many possible algorithms?
• How many correct algorithms?
or mid 
𝑝+𝑞

mid 
2
if key  A[mid]
then q  mid
else p  mid + 1

?

or if key < A[mid] ?

or

Last Update: Aug 21, 2014

𝑝+𝑞
2

then q  mid - 1
?
else p  mid

EECS2011: Analysis of Algorithms

66

<-----Page 66----->Alternative Algorithm:
Less Efficient but More Clear
Algorithm BinarySearch ( A[1..n] , key)
<precondition>: A[1..n] is sorted in non-decreasing order
<postcondition>: If key is in A[1..n], output is its location
p1, qn
while p < q do
<Loop-invariant>: if key is in A[1..n], then key is in A[p..q]
𝑝+𝑞
mid 
2
if key < A[mid]
then q  mid - 1
Still O(log n), but with
else if key > A[mid]
slightly larger constant.
then p  mid + 1
else return (mid)
end while
return (“key not in list”)
end algorithm
Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

67

<-----Page 67----->Part 2: Summary
From Part 2, you should be able to:
 Use the loop invariant method to think about iterative algorithms.
 Prove that the loop invariant is established.
 Prove that the loop invariant is maintained in the ‘typical’ case.
 Prove that the loop invariant is maintained at all boundary conditions.
 Prove that progress is made in the ‘typical’ case
 Prove that progress is guaranteed even near termination, so that the
exit condition is always reached.
 Prove that the loop invariant, when combined with the exit condition,
produces the post-condition.

Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

68

<-----Page 68----->Last Update: Aug 21, 2014

EECS2011: Analysis of Algorithms

69

