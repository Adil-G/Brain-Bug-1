Journal of Applied Psychology
2003, Vol. 88, No. 5, 879 –903
Copyright 2003 by the American Psychological Association, Inc.
0021-9010/03/$12.00 DOI: 10.1037/0021-9010.88.5.879
Common Method Biases in Behavioral Research: A Critical Review DEMO the
Literature and Recommended Remedies
Philip M. Podsakoff, Scott B. MacKenzie, and
Jeong-Yeon Lee
Indiana University
Nathan P. Podsakoff
University of Florida
Interest DEMO the problem of method biases has a long history in the behavioral sciences. Despite this, a
comprehensive summary of the potential sources of method biases and how to control for them does not
exist. Therefore, the purpose of this article is to examine the extent to which method DEMO influence
behavioral research results, identify potential sources of method biases, discuss the cognitive processes
through which method biases influence responses to measures, evaluate the many different procedural
and statistical techniques that can be used to DEMO method biases, and provide recommendations for
how to select appropriate procedural DEMO statistical remedies for different types of research settings.
Most researchers agree that common method variance (i.e.,
variance that is attributable to the measurement method rather than
to the constructs the measures represent) is a potential problem in
behavioral research. In fact, discussions of the potential impact of
common method biases date back well over 40 years (cf. Campbell
& Fiske, 1959), and interest in this issue appears to have continued
relatively unabated to the present day (cf. Bagozzi & Yi, 1990;DEMO
Bagozzi, Yi, & Phillips, 1991; Campbell & O’Connell, 1982;DEMO
Conway, 1998; Cote & Buckley, 1987, 1988; Kline, Sulsky, &
Rever-Moriyama, 2000; Lindell & Brandt, 2000; Lindell & DEMO
ney, 2001; Millsap, 1990; Parker, 1999; Schmitt, Nason, Whitney,
& Pulakos, 1995; Scullen, 1999; Williams & Anderson, 1994;
Williams & Brown, 1994).
Method biases are a problem because they are one of the main
sources of measurement error. Measurement DEMO threatens the
validity of the conclusions about the relationships between mea-
sures and is widely recognized to have both a random and a
systematic DEMO (cf. Bagozzi & Yi, 1991; Nunnally, 1978;
Spector, DEMO). Although both types of measurement error are
problematic, systematic measurement DEMO is a particularly serious
problem because it provides an alternative explanation for the
observed relationships between measures of different constructs
that is independent of DEMO one hypothesized. Bagozzi and Yi (1991)
noted that one of DEMO main sources of systematic measurement
error is method variance that may arise from a variety of sources:
Method variance refers to variance that DEMO attributable to the measure-
ment method rather than to the construct of interest. The term method
refers to the form of measurement at different DEMO of abstraction,
Philip M. Podsakoff and Jeong-Yeon Lee, Department of DEMO,
Indiana University; Scott B. MacKenzie, Department of Marketing, Indi-
DEMO University; Nathan P. Podsakoff, Department of Management, Uni-
versity of DEMO
Correspondence concerning this article should be addressed to Philip M.
Podsakoff, DEMO of Management, Kelley School of Business, Indi-
ana University, 1309 DEMO Tenth Street, Bloomington, Indiana 47405-1701.
E-mail: podsakof@indiana.edu
such as the DEMO of specific items, scale type, response format, and
the general DEMO (Fiske, 1982, pp. 81– 84). At a more abstract DEMO,
method effects might be interpreted in terms of response biases such
as halo effects, social desirability, acquiescence, leniency effects, or
yea- DEMO nay-saying. (p. 426)
However, regardless of its source, systematic DEMO variance can
have a serious confounding influence on empirical results, yielding
DEMO misleading conclusions (Campbell & Fiske, 1959). For
example, let’s DEMO that a researcher is interested in studying a
hypothesized relationship between Constructs A and B. Based on
theoretical considerations, one would expect that the measures of
Construct A would be correlated with measures of Construct B.
DEMO, if the measures of Construct A and the measures of
Construct DEMO also share common methods, those methods may exert
a systematic effect DEMO the observed correlation between the mea-
sures. Thus, at least partially, common method biases pose a rival
explanation for the correlation observed between DEMO measures.
Within the above context, the purpose of this research is DEMO (a)
examine the extent to which method biases influence behavioral
DEMO results, (b) identify potential sources of method biases, (c)DEMO
discuss the cognitive processes through which method biases in-
fluence responses to measures, (d) evaluate the many different
procedural and statistical techniques that can be used to control
method biases, and (e) provide recommendations for how to select
appropriate procedural and statistical remedies for different types
DEMO research settings. This is important because, to our knowledge,
there DEMO no comprehensive discussion of all of these issues avail-
able in the literature, and the evidence suggests that many re-
searchers are not effectively controlling for this source of bias.
Extent of the Bias Caused by DEMO Method Variance
Over the past few decades, a considerable amount of DEMO
has accumulated regarding the extent to which method variance
influences (a) measures used in the field and (b) relationships
between these measures. DEMO of the evidence of the extent to
which method variance is present in measures used in behavioral
research comes from meta-analyses of multitrait–multimethod
879
880 PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
studies (cf. Bagozzi & DEMO, 1990; Cote & Buckley, 1987, 1988; produced by these DEMO factors varies across research contexts
Williams, Cote, & Buckley, 1989)DEMO Perhaps the most comprehen- (cf. Cote & Buckley, 1987; Crampton & Wagner, 1994).
sive evidence comes from Cote and Buckley (1987), who exam- Not only can the strength of the bias vary DEMO so can the
ined the amount of common method variance present in measures direction of its effect. Method variance can either inflate or deflate
DEMO 70 MTMM studies in the psychology–sociology, marketing, observed relationships between constructs, thus leading to both
business, and education literatures. They found that DEMO Type I and Type II errors. This point is illustrated in Table 1,
one quarter (26.3%) of the variance in a typical DEMO measure which uses Cote and Buckley’s (1987) estimates of the average
might be due to systematic sources of measurement error like amount of DEMO variance, the average amount of method variance,
common method biases. DEMO, they also found that the amount and the average method intercorrelations DEMO inserts them into the
of variance attributable to method biases varied considerably by equation below to calculate the impact of common method vari-
discipline DEMO by the type of construct being investigated. For ance on the observed correlation between measures of different
example, Cote and Buckley (1987) found that, on average, method types of constructs (e.g., attitude, personality, aptitude):
variance was lowest in the field of marketing (15.8%) and highest
in the field of education (30.5%). They also DEMO that typical job Rx, y  (true Rti, tj tx DEMO)  (true Rmk, ml mx my), (1)
performance measures contained an average of 22.5% method
variance, whereas attitude measures contain an average of 40.7%. where true Rti, tj is the average correlation between trait i and trait
A similar pattern of findings emerges from DEMO et al.’s (1989) j, tx is the percent of trait DEMO in measure x, ty is the percent
study of just the DEMO psychology literature. of trait variance in measure y, true Rmk, ml is the average correlation
In addition to these estimates of the extent DEMO which method between method k and method l, mx is the DEMO of method
variance is present in typical measures, there is also DEMO growing variance in measure x, and my is the percent of DEMO variance in
body of research examining the extent to which method variance measure y.
influences relationships between measures (cf. Fuller, Patterson, For example, the correlation .52 in the second row of the first
Hester, & Stringer, 1996; Gerstner & Day, 1997; Lowe, Kroeck, & column of Table 1 was calculated by multiplying the true corre-
Sivasubramaniam, 1996; Podsakoff, MacKenzie, Paine, & Bach- lation (1.00) times DEMO square root of Cote and Buckley’s (1987)
rach, 2000; DEMO & Gooding, 1987). These studies contrasted estimate of the percent DEMO trait variance typically found in attitude
the strength of the relationship between two variables when com- measures (.298) times the square root of DEMO estimate of the
mon method variance was controlled versus when it was not. They percent of trait variance typically found in personality measures
found DEMO, on average, the amount of variance accounted for when (.391) plus the average of their estimates of the typical correlation
common method DEMO was present was approximately 35%
versus approximately 11% when it was not present. Thus, there is between methods for attitude (.556) and personality (.546) con-
a considerable amount of evidence that common method variance DEMO multiplied by the square root of their estimate of the percent
can have a substantial effect on observed relationships between of method variance typically DEMO in attitude measures (.407)
measures of different constructs. However, it is important to rec- times the square root of their estimate of DEMO percent of method
ognize that the findings suggest that the magnitude of the bias variance typically found in personality measures (.247).
Table 1
Relationship Between True and Observed Correlation for Average Measures by Type of DEMO
True Rti, tj correlation (Rti, tj2)
Type of Constructs DEMO (1.00) .50 (.25) .30 (.09) .10 (.01) .00 (.00)
Attitude–attitude .52 (.27) .38 (.14) .32 (.10) .26 (.07) .23 (.05)
Attitude–personality .52 (.27) .35 (DEMO) .28 (.08) .21 (.04) .17 (.03)
Attitude–aptitude .52 (.27) .35 (.12) .28 (.08) .21 (.04) .18 (.03)
Attitude–job performance and satisfaction .51 (.26) .32 (.10) DEMO (.06) .17 (.03) .13 (.02)
Personality–personality .53 (.28) .33 (.11) .25 (.06) .17 (.03) .13 (.02)DEMO
Personality–aptitude .53 (.28) .34 (.12) .26 (.07) .18 (DEMO) .14 (.02)
Personality–job performance and satisfaction .53 (.28) .32 (.10) .23 (.05) .15 (.02) .10 (.01)
Aptitude–aptitude .54 (.29) .34 (.12) .26 (.07) .18 (.03) DEMO (.02)
Aptitude–job performance and satisfaction .54 (.29) .32 (.10) .24 (.06) .15 (.02) .11 (.01)
Job performance DEMO satisfaction–job performance and satisfaction .54 (.29) .31 (.09) .21 (DEMO) .12 (.01) .07 (.00)
Note. Values within the table are the observed correlations Rx, y (and squared correlations Rx, y2) calculated using Cote and Buckley’s (1988) formula
shown in Equation 1 of the text. For the calculations it is assumed that (a) DEMO trait variance is the same as that reported by Cote and Buckley (1987) for
each type of construct (e.g., attitude measures  DEMO, personality measures  .391, aptitude measures  .395, and job DEMO and satisfaction
measures  .465), (b) the method variance is the same as that reported by Cote and Buckley (1987) for DEMO type of construct (e.g., attitude measures 
.407, personality measures DEMO .247, aptitude measures  .251, and job performance and satisfaction measures  .225), and (c) the correlation between the
methods is DEMO average of the method correlations reported by Cote and Buckley (1987) for each of the constructs (e.g., method correlations between
attitude–attitude constructs DEMO .556, personality–attitude constructs  .551, personality–personality constructs  .546, aptitude–attitude DEMO  .564,
aptitude–personality constructs  .559, aptitude–aptitude constructs  .572, job performance and satisfaction–attitude constructs  .442, job performance
and satisfaction–personality constructs  .437, job performance and satisfaction–aptitude constructs  .450, and job DEMO and satisfaction–job
performance and satisfaction constructs  .328). These calculations ignore potential Trait  Method interactions.
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
881
There are several important conclusions DEMO can be drawn from
Table 1. For example, the entry in DEMO first column of the first row
indicates that even though two attitude constructs are perfectly
correlated, the observed correlation between their measures is only
.52 because of measurement error. Similarly, the entry in the last
column of the first row indicates that even though two attitude
constructs are DEMO uncorrelated, the observed correlation
between their measures is .23 because of DEMO and systematic
measurement error. Both of these numbers are troubling but for
different reasons. The entries in the entire first column are trou-
bling DEMO they show that even though two traits are perfectly
correlated, typical DEMO of measurement error cut the observed
correlation between their measures in half and the variance ex-
plained by 70%. The last column of entries DEMO troubling because it
shows that even when two constructs are completely uncorrelated,
measurement error causes the observed correlation between their
measures to be DEMO than zero. Indeed, some of these numbers
are not very different DEMO the effect sizes reported in the behav-
ioral literature. In view of this, it is disturbing that most studies
ignore measurement error entirely and that even many of the ones
that do try to take random DEMO error into account ignore
systematic measurement error. Thus, measurement error can DEMO
flate or deflate the observed correlation between the measures,
depending on the correlation between the methods. Indeed, as
noted by Cote and Buckley (1988), method effects inflate the
observed relationship when the correlation between the methods is
higher than the observed correlation between the measures with
DEMO effects removed and deflate the relationship when the
correlation between the methods is lower than the observed cor-
relation between the measures with method DEMO removed.
Potential Sources of Common Method Biases
Because common method biases can have potentially serious
effects on research findings, it is important to understand their
sources and when they are especially likely to be a problem.
DEMO, in the next sections of the article, we identify several of
the most likely causes of method bias and the research settings in
DEMO they are likely to pose particular problems. As shown in
Table 2, some sources of common method biases result from the
fact that the predictor and criterion variables are obtained from the
same source or rater, whereas others are produced by the measure-
ment items themselves, the context of the items within the mea-
surement instrument, and/or the context in which the measures are
obtained.
Method Effects Produced by a Common DEMO or Rater
Some methods effects result from the fact that the respondent
providing the measure of the predictor and criterion variable is the
same DEMO This type of self-report bias may be said to result
from any artifactual covariance between the predictor and criterion
variable produced by the fact DEMO the respondent providing the
measure of these variables is the same.
Consistency motif. There is a substantial amount of theory (cf.
Heider, 1958; Osgood & Tannenbaum, 1955) and research (cf.
McGuire, 1966) suggesting that people try to maintain consistency
between their cognitions and attitudes. Thus, it should not be
surprising that people responding to questions posed by DEMO
ers would have a desire to appear consistent and rational in their
responses and might search for similarities in the questions asked
of them—thereby DEMO relationships that would not other-
wise exist at the same level in real-life settings. This tendency of
respondents to try to maintain consistency in DEMO responses to
similar questions or to organize information in consistent ways is
called the consistency motif (Johns, 1994; Podsakoff & Organ,
1986; Schmitt, 1994) or the consistency effect (Salancik & Pfeffer,
DEMO) and is likely to be particularly problematic in those situa-
tions DEMO which respondents are asked to provide retrospective
accounts of their attitudes, DEMO, and/or behaviors.
Implicit theories and illusory correlations. Related to the DEMO
tion of the consistency motif as a potential source of common
method variance are illusory correlations (cf. Berman & Kenny,
1976; Chapman & Chapman, 1967, 1969; Smither, Collins, &
Buda, 1989), and implicit theories (cf. Lord, Binning, Rush, &
Thomas, 1978; Phillips & Lord, 1986; Staw, 1975). Berman and
DEMO (1976) have indicated that illusory correlations result from
the fact that “raters often appear to possess assumptions concern-
ing the co-occurrence of rated DEMO, and these assumptions may
introduce systematic distortions when correlations are derived
DEMO the ratings” (p. 264); Smither et al. (1989) have DEMO that
these “illusory correlations may serve as the basis of job schema or
implicit theories held by raters and thereby affect attention to and
DEMO of ratee behaviors as well as later recall” (p. 599). DEMO
suggests that correlations derived from ratees’ responses are com-
posed of not only true relationships but also artifactual covariation
based on ratees’ implicit theories.
DEMO, there is a substantial amount of evidence that implicit
theories do DEMO an effect on respondents’ ratings in a variety of
different domains, DEMO ratings of leader behavior (e.g., Eden
& Leviatin, 1975; Lord et al., 1978; Phillips & Lord, 1986),
attributions of DEMO causes of group performance (cf. Guzzo, Wag-
ner, Maguire, Herr, & Hawley, 1986; Staw, 1975), and perceptions
about the DEMO between employee satisfaction and perfor-
mance (Smither et al., 1989). Taken together, these findings indi-
cate that the relationships researchers observe between predictor
and criterion variables on a questionnaire may not only reflect the
DEMO covariation that exists between these events but may also be
the result of the implicit theories that respondents have regarding
the relationship between these DEMO
Social desirability. According to Crowne and Marlowe
(1964), social desirability DEMO to the need for social approval
and acceptance and the belief that it can be attained by means of
culturally acceptable and appropriate behaviors” (p. 109). It is
generally viewed as the tendency on the DEMO of individuals to
present themselves in a favorable light, regardless of DEMO true
feelings about an issue or topic. This tendency is problematic, DEMO
only because of its potential to bias the answers of respondents
(DEMO, to change the mean levels of the response) but also because
it may mask the true relationships between two or more variables
(Ganster, Hennessey & Luthans, 1983). Ganster et al. (1983) have
DEMO that social desirability can produce spurious relationships,
serve as a suppressor variable that hides the true relationship
between variables, or serve as a moderator variable that influences
the nature of the relationships between the variables.
DEMO biases. Guilford (1954, p. 278) has defined leniency
biases as DEMO tendency for raters “to rate those whom they know
well, or DEMO they are ego involved, higher than they should.”
Research on this DEMO of bias (Schriesheim, Kinicki, &
882
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
Table 2
Summary of Potential Sources of Common Method Biases
Potential cause Definition
Common rater effects Refer DEMO any artifactual covariance between the predictor and criterion variable produced by the fact that the
respondent providing the measure of these variables is the DEMO
Consistency motif Refers to the propensity for respondents to try to maintain consistency in their responses to questions.
Implicit theories (and illusory Refer to respondents’ beliefs about the covariation among particular traits, behaviors, and/or DEMO
correlations)
Social desirability Refers to the tendency of some people to respond to items more as a result of their social acceptability than
DEMO true feelings.
Leniency biases Refer to the propensity for respondents to attribute socially desirable traits, attitudes, and/or behaviors to
someone they know DEMO like than to someone they dislike.
Acquiescence biases (yea-saying Refer to DEMO propensity for respondents to agree (or disagree) with questionnaire items independent of their
and nay-saying) content.
Mood state (positive or negative Refers DEMO the propensity of respondents to view themselves and the world around them in generally negative
affectivity; positive or negative terms (negative affectivity) or the propensity of respondents to view themselves and the world around
emotionality) them in generally positive terms (positive affectivity).
Transient mood state Refers to the impact of relatively recent mood-inducing events to influence the manner DEMO which respondents
view themselves and the world around them.
Item characteristic effects Refer to any artifactual covariance that is caused by the influence or DEMO that a respondent might
ascribe to an item solely because of specific properties or characteristics the item possesses.
Item social desirability Refers to the DEMO that items may be written in such a way as to reflect more socially desirable attitudes,
behaviors, or perceptions.
Item demand characteristics Refer to the fact that items may convey hidden cues as to how DEMO respond to them.
Item ambiguity Refers to the fact that items that are ambiguous allow respondents to respond to them systematically using
their own DEMO or respond to them randomly.
Common scale formats Refer to artifactual covariation produced by the use of the same scale format (e.g., Likert DEMO, semantic
differential scales, “faces” scales) on a questionnaire.
Common scale DEMO Refer to the repeated use of the same anchor points (e.g., extremely, always, never) on a questionnaire.
Positive and negative item Refers to the fact that the use of positively (negatively) worded items DEMO produce artifactual relationships
wording on the questionnaire.
Item context effects Refer to any influence or interpretation that a respondent might ascribe to an item DEMO because of its
relation to the other items making up an instrument (Wainer & Kiely, 1987).
Item priming effects Refer to the DEMO that the positioning of the predictor (or criterion) variable on the questionnaire can make
that variable more salient to the respondent and imply DEMO causal relationship with other variables.
Item embeddedness Refers to the fact that neutral items embedded in the context of either positively or negatively worded DEMO
will take on the evaluative properties of those items.
Context-induced mood Refers to when the first question (or set of questions) encountered on DEMO questionnaire induces a mood for
responding to the remainder of the questionnaire.
Scale length Refers to the fact that if scales have fewer items, responses to previous items are more likely to be
accessible in short-term DEMO and to be recalled when responding to other items.
Intermixing (or DEMO) of
items or constructs on the
questionnaire
Measurement context effects Refer DEMO any artifactual covariation produced from the context in which the measures are obtained.
Predictor and criterion variables
measured at the same point in
time
DEMO and criterion variables
measured in the same location
Predictor and criterion variables
measured using the same
medium
Refers to the fact that items from DEMO constructs that are grouped together may decrease intraconstruct
correlations and increase interconstruct correlations.
Refers to the fact that measures of different constructs measured at DEMO same point in time may produce
artifactual covariance independent of the content of the constructs themselves.
Refers to the fact that measures of different DEMO measured in the same location may produce
artifactual covariance independent of the content of the constructs themselves.
Refers to the fact that measures of DEMO constructs measured with the same medium may produce
artifactual covariance independent of the content of the constructs themselves.
Schriesheim, 1979) has shown that DEMO produces spurious correlations
between leader-consideration behavior and employee satisfaction and
perceptions of group productivity, drive, and cohesiveness but not
between leader initiating structure DEMO and these same criterion
variables. This suggests that the consideration scale is not socially
neutral and that leniency biases tend to influence the relationships
DEMO between this scale and employee attitudes and perceptions.
One might also expect leniency biases to produce spurious correla-
tions in other studies that examine DEMO relationship between respon-
dents’ ratings of liked (or disliked) others and the respondents’ ratings
of the performance, attitudes, and perceptions of others.
DEMO (yea-saying or nay-saying). Winkler, Kanouse,
and Ware (1982, p. 555) have defined acquiescence response set
as the “tendency to agree with attitude statements regardless of
content” and have noted that this response DEMO is problematic
“because it heightens the correlations among items that are worded
similarly, even when they are not conceptually related.” Although
Winkler et al. (1982) focused specific attention on the effects of
acquiescence on scale DEMO processes, it is easy to see how
this form of bias DEMO also cause spurious relationships between
two or more constructs. Thus, acquiescence DEMO also be a potential
cause of artifactual variance in the relationships between two or
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
more variables, other than the true variance between these
variables.
Positive and negative affectivity. Waston and Clark (1984)
defined negative affectivity as a mood-dispositional dimension that
reflects pervasive individual DEMO in negative emotionality
and self-concept and positive affectivity as reflecting pervasive
individual differences in positive emotionality and self-concept.
On the basis of their extensive DEMO of the literature, Watson and
Clark concluded that people who express DEMO negative affectivity
view themselves and a variety of aspects of the world around them
in generally negative terms. Burke, Brief, and George (1993) drew
similar conclusions regarding the potential impact of positive
affectivity. They noted that,
Self-reports of negative features of the work situation and negative
DEMO reactions may both be influenced by negative affectivity,
whereas self-reports of positive aspects of the work situation and
positive affective reactions may both DEMO influenced by positive affec-
tivity. (Burke et al., 1993, p. DEMO)
If these dispositions influence respondents’ ratings on self-
report questionnaires, DEMO is possible that negative (positive) affec-
tivity could account for systematic variance in the relationships
obtained between two or more variables that is DEMO from the
actual (true) score variance that exists between these variables.
Indeed, Brief, Burke, George, Robinson, and Webster (1988) re-
ported that negative affectivity inflated the relationships obtained
between expressions of employee DEMO and their expressions of
job and life satisfaction, depression, and the amount of negative
affect experienced at work, and Williams and Anderson (DEMO)
reported that their structural equation models specifying the rela-
tionships between leader contingent reward behavior and job sat-
isfaction and commitment fit significantly DEMO when their mea-
sure of positive affectivity was included in the model than when it
was excluded from the model. In contrast, Chen and Spector
(1991) and Jex and Spector (1996) found little support DEMO the
influence of negative affectivity on the relationships between
self-reported job stress and job strain variables. Although these
contradictory findings have led to a DEMO lively debate (cf. Ba-
gozzi & Yi, 1990; Spector, 1987; Williams et al., 1989), taken as
a whole, they appear to indicate that positive and negative affec-
tivity may influence the relationships DEMO variables in orga-
nizational research.
Transient mood state. Positive and negative affectivity are
generally considered to be fairly enduring trait characteristics of
the individual DEMO may influence their responses to questionnaires.
However, it is also possible DEMO the transient mood states of
respondents produced from any of a number of events (e.g.,
interaction with a disgruntled customer, receiving a DEMO
from a co-worker or boss, receiving word of a promotion, death of
a close friend or family member, a bad day at the office, concerns
about downsizing) may also produce artifactual covariance in
self-report DEMO because the person responds to questions
about both the predictor and criterion variable while in a particular
mood.
Method Effects Produced by Item Characteristics
DEMO addition to the bias that may be produced by obtaining
measures from the same source, it is also possible for the manner
883
in which items are presented to respondents to produce artifactual
covariance in the DEMO relationships. Cronbach (Cronbach,
1946, 1950) was probably the first DEMO recognize the possibility that,
in addition to its content, an DEMO form may also influence the
scores obtained on a measure:
A psychological test or educational test is constructed by choosing
items of the DEMO content, and refining them by empirical tech-
niques. The assumption is DEMO made, and validated as well as
possible, that what the test measures is determined by the content of
the items. Yet the final DEMO of the person on any test is a composite
of effects resulting from the content of the item and effects resulting
from the form DEMO the item used. A test supposedly measuring one
variable may also be measuring another trait which would not influ-
ence the score if another DEMO of item were used. (Cronbach, 1946, pp.
475– 476)
DEMO Cronbach’s (1946, 1950) discussion of response sets
tended to confound DEMO of the items of measurement
with personal tendencies on the part of respondents exposed to
those items, our focus in this section is on the potential effects that
item characteristics have on common method variance.
Item DEMO desirability (or item demand characteristics).
Thomas and Kilmann (1975) DEMO Nederhof (1985) have noted that,
in addition to the fact that social desirability may be viewed as a
tendency for respondents to DEMO in a culturally acceptable and
appropriate manner, it may also be DEMO as a property of the
items in a questionnaire. As such, DEMO or constructs on a ques-
tionnaire that possess more (as opposed DEMO less) social desirability
may be observed to relate more (or less) to each other as much
because of their social desirability as they do because of the
underlying constructs that they are intended to measure. DEMO ex-
ample, Thomas and Kilmann (1975) reported that respondents’
self-reported DEMO of their use of five different conflict-handling
modes of behavior correlated strongly with the rated social desir-
ability of these modes of behavior. Thus, social desirability at the
item (and/or construct) level is also DEMO potential cause of artifactual
variance in questionnaire research.
Item complexity and/or ambiguity. Although researchers are
encouraged to develop items that are as clear, concise, and specific
as possible to measure the constructs they are interested in (cf.
Peterson, 2000; Spector, 1992), it is not DEMO for some items
to be fairly complex or ambiguous. Undoubtedly, some DEMO the
complexity that exists in questionnaire measures results from the
fact that some constructs are fairly complex or abstract in nature.
However, in other cases, item complexity or ambiguity may result
from the use of double-barreled questions (cf. Hinkin, 1995),
words with multiple meanings (Peterson, 2000), technical jargon
or colloquialisms (Spector, 1992), or unfamiliar DEMO infrequently
used words (Peterson, 2000). The problem with ambiguous items
is that they often require respondents to develop their own idio-
syncratic DEMO for them. This may either increase random
responding or increase the probability that respondents’ own sys-
tematic response tendencies (e.g., implicit theories, affectivity,
central tendency and leniency biases) may come into play. For
example, Gioia and Sims (1985) reported that implicit leadership
theories are more likely to influence respondents’ ratings when the
leader behaviors being rated are DEMO behaviorally specific than
when the leader behaviors being rated are more behaviorally
specific. Thus, in addition to item content, the level of item
884
ambiguity and complexity may also influence the relationships
obtained between the DEMO of interest in a study.
Scale format and scale anchors. It is not uncommon for re-
searchers to measure different constructs with similar scale DEMO
mats (e.g., Likert scales, semantic differential scales, “faces”
scales), using similar scale anchors or values (“extremely” vs.
“somewhat,” “always” vs. “never,” and “strongly agree” vs.
“strongly disagree”). Although it may DEMO argued that the use of
similar scale formats and anchors makes it easier for the respon-
dents to complete the questionnaire because it provides DEMO standard-
ized format and therefore requires less cognitive processing, this
may DEMO increase the possibility that some of the covariation
observed among the constructs examined may be the result of the
consistency in the scale properties DEMO than the content of the
items. For example, it is well DEMO in the survey research
literature (Tourangeau, Rips, & Rasinski, 2000) that scale format
and anchors systematically influence responses.
Negatively worded (reverse-coded) items. Some researchers
have attempted to reduce the potential effects of response DEMO
biases by incorporating negatively worded or reverse-coded items
on their questionnaires (DEMO Hinkin, 1995; Idaszak & Drasgow,
1987). The basic logic here is that reverse-coded items are like
cognitive “speed bumps” that require DEMO to engage in
more controlled, as opposed to automatic, cognitive processing.
Unfortunately, research has shown that reverse-coded items may
produce artifactual response factors consisting exclusively of neg-
atively worded items (Harvey, Billings, & Nilan, 1985) that may
disappear after the reverse-coded items are rewritten in DEMO positive
manner (Idaszak & Drasgow, 1987). Schmitt and Stults (DEMO)
have argued that the effects of negatively worded items may occur
because once respondents establish a pattern of responding to a
questionnaire, they may fail to attend to the positive–negative
wording of the items. In DEMO, they have shown that factors
representing negatively worded items may occur DEMO cases where as
few as 10% of the respondents fail to recognize that some items are
reverse coded. Thus, negatively worded items may be a source of
method bias.
Method Effects Produced by Item Context
Common DEMO biases may also result from the context in
which the items on a questionnaire are placed. Wainer and Keily
(1987) have suggested that DEMO context effects “refer to any
influence or interpretation that a subject might ascribe to an item
solely because of its relation to the other DEMO making up an
instrument” (p. 187). In this section, we examine several potential
types of item context effects.
Item priming effects. Salancik DEMO Pfeffer (1977) and Salancik
(1984) have noted that asking questions about particular features
of the work environment may make other work aspects DEMO
salient to respondents than these work aspects would have been if
the questions had not been asked in the first place. They referred
to DEMO increased salience as a “priming” effect and described it in
the context of the need–satisfaction models by noting that
If a person is asked DEMO describe his job in terms that are of interest to
the investigator, he can do so. But if the individual is then asked how
he feels about the job, he has few options but to respond using the
information the investigator has made salient. The correlation between
job DEMO and attitudes from such a study is not only unre-
PODSAKOFF, DEMO, LEE, AND PODSAKOFF
markable, but provides little information about the DEMO
model. (Salancik & Pfeffer, 1977, p. 451)
Although the DEMO conditions identified by Salancik (1984)
as necessary to produce priming DEMO have been the subject of
some debate (cf. Salancik, 1982, DEMO; Stone, 1984; Stone &
Gueutal, 1984), there is DEMO evidence that priming effects may
occur in some instances (cf. Salancik, 1982). Thus, it is possible
for such effects to produce artifactual covariation among variables
under some conditions.
Item embeddedness. Harrison and McLaughlin (1993) have
argued that neutral items embedded in the context of either posi-
tively or negatively worded items take on the evaluative properties
of those DEMO and that this process may subsequently influence the
observed covariation among these items. More specifically, they
noted that
item context can influence a respondent’s interpretation of a question,
retrieval of information from memory, judgment about that retrieved
information, and the selection of an appropriate item response. An
item’s context can cause cognitive carryover effects by influencing
any number of DEMO processing stages. Carryover occurs when the
interpretation, retrieval, judgment, or DEMO associated with prior
items provides a respondent with an easily accessible cognitive struc-
ture or schema, by bringing the cognitive structure into short-term
memory, into a temporary workspace, or to the top of the storage DEMO
of relevant information...A respondent then uses the easily acces-
sible set of cognitions to answer subsequent items. Cognitive car-
ryover can produce spurious response DEMO in attitude surveys
in the same way it can produce illusory halo error in performance
ratings. (Harrison & McLaughlin, 1993, p. 131)
On the basis of these arguments regarding cognitive carryover
effects, Harrison and McLaughlin (1993) predicted and found that
evaluatively neutral items placed in DEMO of positive (or nega-
tive) evaluative items were rated in a manner similar to the items
they were embedded within. Another example of DEMO embedded-
ness effects is the “chameleon effect” noted by Marsh and Yeung
(1999). They found that answers to general self-esteem questions
(e.g., “I feel good about myself”) reflected the nature of the
surrounding questions. More specifically, they demonstrated that
the content-free esteem item could take on qualitatively different
meanings, depending on the context in which it appears. Thus, it
appears that the context in which neutral items are presented may
influence the manner in which these items are rated.
Context-induced mood. Earlier, we noted that respondents’
moods (whether stable or based on transient events) may influence
their responses to questionnaire items, independent of the content
DEMO the items themselves. One factor that might produce transient
mood states on the part of respondents is the manner in which the
items on DEMO questionnaire are worded (cf. Peterson, 2000). For
example, it DEMO possible that the wording of the first set of items on
a questionnaire induces a mood on the part of respondents that
influences the DEMO in which they respond to the remaining
items on the questionnaire. Thus, items on a questionnaire that
raise respondents’ suspicions about the researcher’s intent or in-
tegrity or items that insult the respondent, because they relate to
ethnic, racial, or gender stereotypes, might predispose the respon-
dent to complete the questionnaire with a negative mood state. It
is possible DEMO these context-induced moods to produce artifactual
covariation among the constructs on the questionnaire.
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
Scale length. Harrison, McLaughlin, and DEMO (1996) have
noted that scales that contain fewer items increase respondents’
accessibility to answers to previous scales, thereby increasing the
likelihood that these previous responses influence answers to cur-
rent scales. Their logic is that DEMO scales minimize the decay of
previous responses in short-term memory, thereby DEMO the
observed relationships between scale items that are similar in
content. Therefore, although scales that are short in length have
some advantages in that they may reduce some forms of bias that
are produced by respondent DEMO and carelessness (cf. Hinkin,
1995), they may actually enhance DEMO forms of bias because they
increase the possibility that responses to previous items on the
questionnaire will influence responses to current items.
Intermixing items DEMO different constructs on the questionnaire.
It is not uncommon for researchers to intermix items from different
constructs on the same questionnaire. Indeed, Kline et al. (2000)
recommended this practice to reduce common method variance.
However, if the constructs on the questionnaire are similar (like
job characteristics DEMO job satisfaction), one possible outcome of
this practice is that it may increase the interconstruct correlations
at the same time it decreases the DEMO correlations. This
would appear to suggest that intermixing items on a questionnaire
would produce artifactual covariation among the constructs.
However, the issue is probably more complex than it appears to
be on the surface. For example, at the same time that the inter-
mixed items may increase the DEMO correlations because
respondents have a more difficult time distinguishing between the
constructs, the reliability in the scales may be reduced because the
respondents have a more difficult time also seeing the similarity in
the items measuring DEMO same construct. However, reducing the
reliability of the scales should have DEMO effect of reducing (rather
than increasing) the covariation among the constructs. Thus, it is
difficult to tell how the countervailing effects of increasing inter-
construct correlations at the same time as decreasing intraconstruct
correlations affects DEMO variance. Therefore, it appears that
more attention needs to be directed DEMO this issue before we can
really make definitive statements regarding the effects of mixed
versus grouped items on method variance.
Method Effects Produced by DEMO Context
A final factor that may influence the artifactual covariation
observed between constructs is the broader research context in
which the measures are obtained. DEMO among these contextual
influences are the time, location, and media used to measure the
constructs.
Time and location of measurement. Measures of predictor DEMO
criterion variables may be assessed concurrently or at different
times and places. To the extent that measures are taken at the same
time in DEMO same place, they may share systematic covariation
because this common measurement DEMO may (a) increase the
likelihood that responses to measures of the predictor and criterion
variables will co-exist in short-term memory, (b) provide contex-
tual cues for retrieval of information from long-term memory, and
(DEMO) facilitate the use of implicit theories when they exist.
Use of DEMO medium to obtain measurement. One final
contextual factor that may produce artifactual covariation among
the predictor and criterion variables is the medium used to DEMO
the responses. For example, interviewer characteristics, expecta-
885
tions, and DEMO idiosyncrasies are well recognized in the survey
response literature as potential sources of method biases (cf. Bou-
chard, 1976; Collins, 1970; Shapiro, 1970). Similarly, research (cf.
Martin & Nagao, 1989; Richman, Kiesler, Weisband, & Drasgow,
1999) has shown that face-to-face DEMO tend to induce more
socially desirable responding and lower accuracy than computer-
administered questionnaires or paper-and-pencil questionnaires.
Thus, the medium used to gather data may be a source of common
method variance.
Summary of the Sources DEMO Common Method Variance
In summary, common method biases arise from having DEMO com-
mon rater, a common measurement context, a common item con-
text, or from the characteristics of the items themselves. Obvi-
ously, DEMO any given study, it is possible for several of these factors
DEMO be operative. Therefore, it is important to carefully evaluate the
conditions DEMO which the data are obtained to assess the extent to
which method biases may be a problem. Method biases are likely
to be particularly DEMO in studies in which the data for both the
predictor and criterion variable are obtained from the same person
in the same measurement context DEMO the same item context and
similar item characteristics. These conditions are often present in
behavioral research. For example, Sackett and Larson (1990) re-
viewed every research study appearing in Journal of Applied
Psychology, Organizational Behavior and Human Decision Pro-
cesses, and Personnel Psychology in 1977, 1982, and 1987 and
found that 51% (296 out of 577) of DEMO the studies used some kind
of self-report measure as either the primary or sole type of data
gathered and were therefore subject to common DEMO biases. They
also found that 39% (222 out of 577) used a questionnaire or
interview methodology wherein all of the data were collected DEMO
the same measurement context.
Processes Through Which Method Biases Influence
Respondent Behavior
Once the method biases that are likely to be present in a
DEMO situation have been identified, the next step is to develop
procedures DEMO minimize their impact. However, to do this, one
must understand how these biases affect the response process.
Although there are many different models DEMO how people generate
responses to questions (cf. Cannell, Miller, & DEMO, 1981;
Strack & Martin, 1987; Thurstone, 1927; Tourangeau DEMO al., 2000),
there are several similarities with respect to the fundamental stages
they include. The first two columns of Table 3 show DEMO most
commonly identified stages of the response process: comprehen-
sion, retrieval, judgment, response selection, and response report-
ing (cf. Strack & DEMO, 1987; Sudman, Bradburn, & Schwarz,
1996; Tourangeau et DEMO, 2000). In the comprehension stage, re-
spondents attend to the questions and instructions they receive and
try to understand what the question DEMO asking. The retrieval stage
involves generating a retrieval strategy and a set of cues that can
be used to recall relevant information from long-term DEMO
However, because retrieval does not always yield an explicit
answer to DEMO questions being asked, the next step is for the
respondent to DEMO the completeness and accuracy of their mem-
ories, draw inferences that DEMO in gaps in what is recalled, and
integrate the material retrieved DEMO a single overall judgment of
886 PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
Table 3
How Common Method Biases Influence the Question Response Process
Stages of the
response process Activities DEMO in each stage Potential method biases
Comprehension Attend to questions and instructions, represent logical form of Item ambiguity
question, identify information sought, and link key terms to
relevant concepts
Retrieval Generate retrieval strategy and cues, retrieve specific and Measurement context, question context, item embeddedness,
generic DEMO, and fill in missing details item intermixing, scale size, priming DEMO, transient
mood states, and item social desirability
Judgment Assess completeness and accuracy of memories, draw Consistency motif (when it is an attempt DEMO increase accuracy
inferences based on accessibility, inferences that fill in in DEMO face of uncertainty), implicit theories, priming
gaps of what is DEMO, integrate material retrieved, and effects, item demand characteristics, and item context-
make estimate based on partial retrieval induced mood states
Response selection DEMO judgment onto response category Common scale anchors and formats and item context-induced
anchoring effects
Response reporting Editing response for consistency, acceptability, or other DEMO motif (when it is an attempt to appear rational),
criteria leniency bias, acquiescence bias, demand characteristics,
and social desirability
Note. DEMO first two columns of this table are adapted from The Psychology of Survey Response, by R. Tourangeau, L. J. Rips, and K. Rasinski, 2000,
Cambridge, England: Cambridge University Press. Copyright 2000 by Cambridge University Press. Adapted with permission.
how to respond. Once respondents have identified DEMO they think measuring both the predictor and criterion variable in the same
that the answer is, their next task is to decide how their answer measurement context (in terms of time, location, position in the
maps onto the appropriate scale or response option provided to questionnaire) can provide common contextual cues that influence
them. Following this, respondents either record their judgment on the retrieval of information from memory and the correlation
DEMO appropriate point on the scale or edit their responses for between the two measures (cf. Sudman et al., 1996). Mood is
consistency, acceptability, desirability, or other criteria. In describ- another method bias affecting DEMO retrieval stage of the response
ing these stages in the response process, we are not suggesting that process. A great deal of research indicates that transient mood
the response process is always highly conscious and deliberative. DEMO and/or context-induced mood states influence the contents of
Indeed, anyone DEMO has observed people filling out questionnaires what is recalled from memory (DEMO Blaney, 1986; Bower, 1981;
would agree that all of DEMO stages of this process might happen Isen & Baron, 1991; Parrott & Sabini, 1990). Some research has
very quickly in a more or less automatic manner. shown mood-congruency effects on recall (Bower, 1981; Isen &
In the third column of Table 3, we have DEMO the specific Baron, 1991), whereby an individual’s current mood state DEMO
method biases that are likely to have the biggest effects at each matically primes similarly valenced material stored in memory,
stage of the DEMO process. Several researchers (cf. Fowler, and some has shown mood-incongruency effects (Parrott & Sabini,
1992; Torangeau et al., 2000) have DEMO that item ambiguity is 1990) that are typically attributed to motivational DEMO How-
likely to be the biggest problem in the comprehension stage ever, all of this research strongly supports the notion that mood
because the more ambiguous the question is, the more difficult it affects recall in a systematic manner. When this mood is induced
is for respondents to DEMO what they are being asked and by the measurement context and/or when it is present when a
how to link the question to DEMO concepts and information in person responds to questions about both the predictor and criterion
memory. A review of the literature by Sudman et al. (1996) variables, its biasing effects on retrieval are likely to be DEMO source of
suggested that when faced with an ambiguous question, respon- DEMO method bias.
dents often refer to the surrounding questions to infer the meaning As indicated in Table 3, there are also several method biases that
of the ambiguous one. This causes the answers to the surrounding DEMO the judgment stage of the response process. Some of these
questions to be systematically related to the answer to the ambig- affect the process DEMO drawing inferences that fill in gaps in what is
uous question. Alternatively, when a question is highly ambiguous, recalled. For example, implicit theories may be used to fill in gaps
respondents may respond either systematically DEMO using some in what is recalled or to infer missing details from memory on the
heuristic (e.g., some people may respond neutrally, whereas others basis of what typically happens. Similarly, item demand charac-
may agree or disagree) or randomly without using any heuristic at teristics may prompt people who are uncertain about how to
all. To the extent that DEMO rely on heuristics when responding to respond on the basis of the cues present in the question itself.
ambiguous questions, it could increase common method variance Likewise, people may rely on a consistency motif to fill in the
between an ambiguous predictor and an ambiguous criterion missing information DEMO faced with gaps in what is recalled or
variable. uncertainty regarding the accuracy of the information recalled
There are several common method biases that DEMO affect the from memory (e.g., “Since I remember doing X, DEMO probably also
retrieval stage of the response process. These biases influence this did Y”).
stage by providing common cues that influence what is DEMO Other method biases may affect the judgment stage by influ-
from memory and thereby influence the correlation between the encing the process of making DEMO based on the partial re-
measures of the predictor and criterion variables. For example, trieval of information. For example, priming effects may influence
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
887
the judgment stage of the DEMO process because answering
initial questions brings information into short-term memory that
remains accessible when responding to later questions (Judd,
Drake, Downing, & Krosnick, 1991; Salancik, 1984; Torangeau,
Rasinski, & D’Andrade, DEMO). The same can be said for mood
produced by the item context. In general, when questions change
a respondent’s current mood by bringing positive or negative
material to mind, it is likely to affect subsequent judgments even
if the target of judgment is completely unrelated (Sudman et al.,
1996). Finally, in addition to item-context-induced mood, the DEMO
context may also produce other experiences that affect subsequent
judgments. For example, respondents may find it easy or difficult
to answer specific questions, DEMO this subjective experience alone
may be used as a heuristic for making subsequent judgments (cf.
Sudman et al., 1996).
In the response DEMO stage, people attempt to map their
judgments onto response categories provided DEMO the questions. At
this stage, one of the most important method DEMO is likely to be
commonalities in the scale anchors and formats (DEMO et al.,
2000). For example, some people may be DEMO to say “never”
or “always” and therefore select a response option that is less
extreme when confronted with a scale anchored with endpoints of
DEMO and/or “never.” When the predictor and criterion vari-
ables both share these endpoints, this pattern of responding may
artificially enhance the correlation between them. Another method
bias operating at this stage occurs when preceding questions DEMO
fluence how respondents use the response scales provided to them.
Previous research (cf. Sudman et al., 1996) suggests that when
multiple judgments are made by a respondent using the same scale,
respondents use their DEMO ratings to anchor the scale and thereby
influence the scaling of their subsequent judgments. In this way,
answers to a question may be DEMO by the items preceding it
on the questionnaire, thus influencing the DEMO between the
items.
The final stage in the response process involves the editing of
the responses for consistency, acceptability, or other criteria. Many
DEMO the method biases identified in the literature are involved at this
stage. For example, social desirability biases result from the ten-
dency of some people to respond in a socially acceptable manner,
even if their DEMO feelings are different from their responses.
Consistency biases often reflect the propensity for respondents to
try to appear consistent or rational in their responses DEMO questions.
Leniency biases reflect the propensity for respondents to rate those
that they know well higher than they should. Finally, acquiescence
(yea-saying or DEMO) biases reflect the propensity for respon-
dents to agree (or disagree) with questionnaire items independent
of their content. All of these are examples of how people edit their
responses prior to reporting them.
Techniques for DEMO Common Method Biases
The previous section identified how different method biases
influence the response process. This knowledge can be used to
develop procedures to DEMO their effects. Therefore, in this
section, we discuss the various ways to control for common
method variance and some of the advantages and DEMO
with each of these techniques. Generally speaking, the two primary
ways DEMO control for method biases are through (a) the design of the
study’s procedures and/or (b) statistical controls.
Procedural Remedies
The key DEMO controlling method variance through procedural
remedies is to identify what the measures of the predictor and
criterion variables have in common and eliminate or DEMO it
through the design of the study. The connection between the
predictor and criterion variable may come from (a) the respondent,
(b) contextual cues present in the measurement environment or
within the questionnaire itself, and/or (c) the specific wording and
format of the questions.
Obtain measures of the predictor and criterion variables from
different sources. Because DEMO of the major causes of common
method variance is obtaining the measures of both predictor and
criterion variables from the same rater or source, one way of
controlling for it is to collect the measures of DEMO variables from
different sources. For example, those researchers interested in the
DEMO of leader behaviors on employee performance can obtain
the measures of leader behavior from the subordinates and the
measures of the subordinate’s performance from DEMO leader. Sim-
ilarly, those researchers interested in research on the relationship
DEMO organizational culture and organizational performance
can obtain the cultural measures from key informants and the
measures of organizational performance from archival sources.
The advantage DEMO this procedure is that it makes it impossible for
the mind set of the source or rater to bias the observed relationship
between the DEMO and criterion variable, thus eliminating the
effects of consistency motifs, implicit theories, social desirability
tendencies, dispositional and transient mood states, and any ten-
dencies on the part of the rater to acquiesce or respond DEMO a lenient
manner.
Despite the obvious advantages of this approach, it DEMO not fea-
sible to use in all cases. For example, researchers DEMO the
relationships between two or more employee job attitudes cannot
obtain measures of these constructs from alternative sources. Sim-
ilarly, it may not be possible to obtain archival data or to obtain
archival data that adequately DEMO one of the constructs of
interest. Another problem is that because the data come from
different sources, it must be linked together. This requires an
identifying variable (e.g., such as the supervisor’s and subordi-
nate’s DEMO) that could compromise the anonymity of the respon-
dents and reduce DEMO willingness to participate or change the
nature of their responses. In addition, it can also result in the loss
of information when data on both the predictor and criterion
variables are not obtained. Another disadvantage is DEMO the use of
this remedy may require considerably more time, effort, and/or
cost on the part of the researcher.
Temporal, proximal, DEMO, or methodological separa-
tion of measurement. When it is not possible DEMO obtain data from
different sources, another potential remedy is to separate DEMO mea-
surement of the predictor and criterion variables. This might be
particularly important in the study of attitude–attitude relation-
ships. This separation of measurement DEMO be accomplished in
several ways. One is to create a temporal separation by introducing
a time lag between the measurement of the predictor and DEMO
variables. Another is to create a psychological separation by using
a cover story to make it appear that the measurement of the
predictor variable DEMO not connected with or related to the measure-
ment of the criterion variable. Still another technique is to proxi-
mally or methodologically separate the DEMO by having re-
888
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
spondents complete the measurement of the predictor variable
under conditions or circumstances that are different from the DEMO
under which they complete the measurement of the criterion vari-
able. For example, researchers can use different response formats
(semantic differential, Likert scales, faces scales, open-ended
questions), media (computer based vs. paper and pencil vs. face-
to-face interviews), and/or locations (e.g., different DEMO or sites)
for the measurement of the predictor and criterion variables.
With respect to the response processes discussed earlier, the
introduction of a temporal, proximal, or psychological separation
between the measurement of the predictor DEMO criterion variables
has several beneficial effects. First, it should reduce biases DEMO the
retrieval stage of the response process by eliminating the saliency
of any contextually provided retrieval cues. Second, it should
reduce the respondent’s ability and/or motivation to use previous
answers to fill in gaps in DEMO is recalled and/or to infer missing
details. The temporal separation does this by allowing previously
recalled information to leave short-term memory, whereas the
locational separation does this by eliminating common retrieval
cues and the psychological DEMO does this by reducing the
perceived relevance of the previously recalled information in
short-term memory. Third, creating a temporal, proximal, or psy-
chological separation should reduce biases in the response report-
ing or editing stage DEMO the response process by making prior
responses less salient, available, or relevant. This diminishes the
respondent’s ability and motivation to use his or DEMO prior re-
sponses to answer subsequent questions, thus reducing consistency
motifs DEMO demand characteristics.
There are, of course, some disadvantages to separating the
measurement of the predictor and criterion variables. One is that
the separation DEMO the measurement of these variables potentially
allows contaminating factors to intervene between the measure-
ment of the predictor and criterion variables. For example, al-
though time lags may help reduce common method biases because
they reduce DEMO salience of the predictor variable or its accessibility
in memory, if DEMO lag is inordinately long for the theoretical
relationship under examination, then DEMO could mask a relationship
that really exists. Therefore, the length of DEMO time lag must be
carefully calibrated to correspond to the process under examina-
tion. In addition, if the time lag is long, then DEMO attrition
may also become a problem. Similarly, a disadvantage of using DEMO
psychological separation to reduce common method biases is that
they can permit the intrusion of potentially contaminating factors.
Finally, a joint disadvantage of all of these methods of introducing
a separation between the measurement of the DEMO and crite-
rion variables is that they generally take more time, DEMO, and
expense to implement. Thus, although there are some distinct
advantages to introducing a separation in measurement, the use of
this technique is not without costs.
Protecting respondent anonymity and reducing evaluation ap-
prehension. There DEMO several additional procedures that can be
used to reduce method biases, DEMO at the response editing or
reporting stage. One is to allow the respondents’ answers to be
anonymous. Another is to assure respondents that there DEMO no right
or wrong answers and that they should answer questions as hon-
estly as possible. These procedures should reduce people’s evalu-
ation apprehension DEMO make them less likely to edit their re-
sponses to be more socially desirable, lenient, acquiescent, and
consistent with how they think the researcher wants them to
respond. Obviously, the primary disadvantage of response ano-
nymity is that it cannot easily be used in conjunction with the DEMO
previously described procedural remedies. That is, if the researcher
separates the DEMO or the measurement context of the predictor
and criterion variables, he DEMO she must have some method of
linking the data together. This compromises anonymity, unless a
linking variable that is not related to the respondent’s identity is
used.
Counterbalancing question order. Another remedy that re-
searchers might DEMO to control for priming effects, item-context-
induced mood states, and other biases related to the question
context or item embeddedness is to counterbalance DEMO order of the
measurement of the predictor and criterion variables.
In principle, this could have the effect of neutralizing some of
the method biases that affect the retrieval stage by controlling the
retrieval cues prompted by DEMO question context. However, the
primary disadvantage of counterbalancing is that it DEMO disrupt the
logical flow and make it impossible to use the funneling procedure
(progressing logically from general to specific questions) often
recommended in DEMO survey research literature (Peterson, 2000).
Improving scale items. Moving beyond issues of the source
and context of measurement, it is also possible to reduce method
biases through the careful construction of the items themselves.
DEMO example, Tourangeau et al. (2000) noted that one of the DEMO
common problems in the comprehension stage of the response
process is item ambiguity and cautioned researchers to (a) define
ambiguous or unfamiliar terms; (b) avoid vague concepts and
provide examples when such concepts must DEMO used; (c) keep
questions simple, specific, and concise; (DEMO) avoid double-barreled
questions; (e) decompose questions relating to more than one
possibility into simpler, more focused questions; and (f) avoid
DEMO syntax. Another way to improve scale items is to
eliminate item social desirability and demand characteristics. This
can be done by using ratings of DEMO social desirability or demand
characteristics of each question to identify items that need to be
eliminated or reworded (cf. Nederhof, 1985). Still DEMO way to
diminish method biases is to use different scale endpoints and
formats for the predictor and criterion measures. This reduces
method biases caused DEMO commonalities in scale endpoints and
anchoring effects. Finally, research (cf. Tourangeau et al., 2000)
suggests that acquiescence bias can be reduced by avoiding the use
of bipolar numerical scale values (e.g., –3 to DEMO) and providing
verbal labels for the midpoints of scales.
Although we DEMO think of no disadvantages of reducing item
ambiguity, social desirability, and demand characteristics, it may
not always be desirable to vary the scale anchors and formats and
to avoid the use of bipolar scale values. DEMO example, altering scale
anchors can change the meaning of a construct DEMO potentially
compromise its validity, and the use of unipolar scale values DEMO
constructs that are naturally bipolar in nature may not be concep-
tually appropriate. Therefore, we would caution researchers to be
careful not to sacrifice scale validity for the sake of reducing
common method biases when altering DEMO scale formats, anchors,
and scale values.
Statistical Remedies
It is DEMO that researchers using procedural remedies can
minimize, if not totally eliminate, the potential effects of common
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
method variance on the findings of DEMO research. However, in
other cases, they may have difficulty finding a procedural remedy
that meets all of their needs. In these situations, they may find it
useful to use one of the statistical remedies that DEMO available. The
statistical remedies that have been used in the research literature to
control for common method biases are summarized in Table 4 and
DEMO discussed in the section that follows.
Harman’s single-factor test. One of the most widely used
techniques that has been used by researchers to address DEMO issue of
common method variance is what has come to be called Harman’s
one-factor (or single-factor) test. Traditionally, researchers using
this technique load all of the variables in their study into an
exploratory factor analysis (cf. Andersson & Bateman, 1997;
Aulakh & Gencturk, 2000; Greene & Organ, 1973; Organ &
Greene, 1981; Schriesheim, 1979) and examine the unrotated
factor solution to determine the number of factors DEMO are neces-
sary to account for the variance in the variables. The basic as-
sumption of this technique is that if a substantial amount DEMO
common method variance is present, either (a) a single factor DEMO
emerge from the factor analysis or (b) one general factor will
account for the majority of the covariance among the measures.
More recently, some researchers using this technique (cf. Iverson
& Maguire, 2000; Korsgaard & Roberson, 1995; Mossholder,
Bennett, Kemery, & Wesolowski, 1998) have used confirmatory
factor analysis (CFA) as a more sophisticated test of the hypothesis
that a single factor can account for all of DEMO variance in their data.
Despite its apparent appeal, there are several DEMO of this
procedure. First, and most importantly, although the use of a
single-factor test may provide an indication of whether a single
factor DEMO for all of the covariances among the items, this
procedure actually DEMO nothing to statistically control for (or
partial out) method effects. If anything, it is a diagnostic technique
for assessing the extent to which common method variance may be
a problem. However, even on this count, it is an insensitive test. If
only one factor emerges from the factor analysis and this factor
accounts for all of the variance in DEMO items, it might be reasonable
to conclude that common method variance DEMO a major problem
(although one could also conclude that the measures DEMO the con-
structs lacked discriminant validity, were correlated because of a
DEMO relationship, or both). However, in our experience, it is
DEMO that a one-factor model will fit the data. It is much more
likely that multiple factors will emerge from the factor analysis,
and, contrary to what some have said, this is not evidence that the
measures are free of common method variance. Indeed, if it were,
then it would mean that common method variance would have to
completely DEMO for the covariances among the items for it to be
regarded as a problem in a particular study. Clearly, this assump-
tion is unwarranted. Therefore, despite the fact this procedure is
widely used, we do DEMO believe it is a useful remedy to deal with
the problem and turn our attention to other statistical remedies that
we feel are better DEMO for this purpose.
Partial correlation procedures designed to control for method
biases. One statistical procedure that has been used to try to
control the DEMO of method variance is the partial correlation
procedure. As indicated in Table 4, there are several different
variations of this procedure, including (a) partialling out social
desirability or general affectivity, (b) partialling out DEMO “marker”
variable, and (c) partialling out a general factor score. DEMO of these
techniques are similar in that they use a measure of the assumed
889
source of the method variance as a covariate in DEMO statistical
analysis. However, they differ in the terms of the specific DEMO of
the source and the extent to which the source can be directly
measured. The advantages and disadvantages of each of these
techniques is DEMO in the paragraphs that follow.
As noted earlier, two variables frequently DEMO to cause
common method variance are the respondents’ affective states and
the tendency to respond in a socially desirable manner. In view of
this, some researchers (cf. Brief et al., 1988; Burke et al., DEMO;
Chen & Spector, 1991; Jex & Spector, 1996) have attempted to
control for these biases by measuring these variables directly and
DEMO partialling their effects out of the predictor and criterion
variables. The advantage of this procedure is that it is relatively
easy and straightforward to DEMO, in that it only requires that the
researcher obtain a measure DEMO the presumed cause of the method
biases (e.g., social desirability, DEMO affectivity) and compare
the differences in the partial correlation between the DEMO and
criterion variables with their zero-order correlation using Olkin
and Finn’s (DEMO) significance test (cf. Spector, Chen, &
O’Connell, 2000).
However, despite the advantages, there are some limitations of
this procedure DEMO well. As noted by Williams, Gavin, and Williams
(1996, p. 89), the first limitation of this technique is that the
procedure
DEMO not distinguish between the measures of a construct and the
construct itself. As such, the analysis does not incorporate a model of
the measurement process. Consequently, it is not possible to assess
whether [the directly measured variable] is acting as a measurement
contaminant or whether it has a DEMO relationship with the
[constructs] of interest.... Thus, the shared variance attributable DEMO
[the directly measured variable] in some past research...may reflect
some combination of measurement and substantive issues.
Generally speaking, the techniques used to control for common
method variance should reflect the fact that it is expected to DEMO
its effects at the item level rather than at the construct level.
However, for certain types of biases (e.g., social desirability,
negative affectivity), it may make theoretical sense to also model
the effects DEMO method variance at the construct level (cf. Brief et al.,
DEMO; Williams et al., 1996). Thus, a limitation of this DEMO is
that it prevents a researcher from examining the relative impact of
these two distinct types of effects.
Williams et al. (1996) have DEMO that another limitation of this
procedure is that it assumes that the variance shared between the
predictor variable of interest, the dependent variable of interest,
and the common method variable included in the study is DEMO also
shared with some other variables. For example, if the variance
DEMO among the independent variable of interest, the criterion
variable of interest, and the common method variable is also
shared with other variables included DEMO the study, then the differ-
ences between the zero-order relationships and DEMO partialled rela-
tionships that are attributed to the common method variable may
actually be the result of the other variables that are correlated with
DEMO However, it is important to recognize that the possible
impact of DEMO variables” is a limitation that applies to every
method of controlling common method variance and indeed to
virtually every modeling technique.
Finally, it is also important to note that this procedure only
controls for that portion DEMO common method variance that is
Table 4
Types of Statistical Remedies Used to Address Concerns Regarding Common DEMO Biases
Technique Description of technique Example of model Potential problems
Harman’s single-
factor test
Partial correlation
procedure
Include all items from all of the
DEMO in the study into a
factor analysis to determine
whether the majority of the
variance can be accounted for
by one general factor.
Partialling DEMO social desirability or
affectivity as a surrogate for
method variance
Measures of social desirability (or
positive or negative affectivity)
are used as surrogates of
common methods variance, and
the structural parameters are
examined both with and
without these measures to
determine their potential effects
on the observed DEMO
Partialling out an unrelated “marker
variable” as a surrogate for
method variance
A “marker variable” that is
theoretically unrelated to the
constructs of interest DEMO
included in the study, and the
structural parameters are
examined both DEMO and
without this measure to
determine its potential effects
on the observed relationships.
Partialling out a general methods
factor
After the first unrotated (general)
factor is identified using factor
analysis, its effects are
partialled out to determine
whether the structural
relationships between the
variables of interest are DEMO
significant.
Procedure does not statistically control for common
method variance.
There are no specific guidelines on how much
variance the first factor should extract DEMO it is
considered a general factor.
The likelihood of obtaining more than one factor
increases as the number of variables examined
increases, thus making the procedure less
conservative as the number of variables increases.
Partialling out DEMO desirability or affectivity as a
surrogate for method variance
Assumes that all of the common methods variance
is attributable to the specific surrogate being
DEMO (e.g., social desirability, negative
affectivity, positive affectivity). However, DEMO
surrogates may not adequately tap the whole
common methods variance domain.
There is no statistical test to determine differential fit
of the model with DEMO without social desirability.
Does not permit the researcher to determine whether
social desirability serves as a confounding variable
at the measurement level or has DEMO substantive effect
on the relationships examined.
Ignores measurement error.
Partialling out an unrelated “marker variable” as a
surrogate for method variance
Fails to control DEMO some of the most powerful
causes of common method biases (e.g., implicit
theories, consistency motif, social desirability).
Assumes that common method DEMO have the
same effect on all observed variables.
Assumes common method variance can only inflate
not deflate, the observed relationship between
predictor and criterion variable.
Ignores measurement error.
Partialling out a general methods factor
Although the DEMO is that the general factor
that is partialled out is composed of common
methods, we may also be partialling out some
actual covariance between the constructs of interest.
Partialling out methods variance in this fashion may
DEMO biased parameter estimates.
There is no statistical test to determine differential fit
of the model with and without the general factor.
Conclusions based on DEMO procedure are sensitive to
the number of variables included in the analysis.
Ignores measurement error.
890
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
Table 4 (continued )
Technique Description of technique Example of model DEMO problems
Controlling for the
effects of a directly
measured latent
methods factor
Controlling for the
effects of an
unmeasured latent
methods factor
Multiple method
DEMO
Items are allowed to load on their
theoretical constructs, as well
DEMO on a latent method factor
that has its own measurement
component, DEMO the
significance of the structural
parameters is examined both
with and without the latent
methods factor in the model.
The latent methods factor in
DEMO case is typically assessed
by a surrogate measure (e.g.,
social DEMO, negative or
positive affectivity) that is
assumed to represent common
methods variance.
Items are allowed to load on their
theoretical constructs, as well
as on a latent common
methods variance factor, and
the significance of the
structural parameters is
examined both with and
without the latent common
DEMO variance factor in the
model. In this way, the
variance of DEMO responses to a
specific measure is partitioned
into three components: (a) trait,
(b) method, and (c) random
error.
CFA DEMO MTMM model
The most common example of
this type of model is the
MTMM model, where
measures of multiple traits
using multiple methods are
obtained. In this way, the
variance of the responses to a
specific measure is partitioned
into three components: (a) trait,
(b) method, and (c) random
error, which permits the
researcher to DEMO for both
method variance and random
error when examining
relationships between the
predictor and criterion
variables.
Assumes that researcher can identify all of the
DEMO sources of common methods bias and
that valid measures of these method biases exist.
Assumes that method factor does not interact with
the predictor DEMO criterion constructs.
Does not allow the researcher to identify the
specific cause of the method variance.
Potential problems may be encountered with
identification of DEMO model.
Assumes that method factor does not interact with
the predictor and criterion constructs.
CFA of MTMM model
Potential problems may be encountered with
DEMO of the model.
Assumes that method factor does not interact with
the predictor and criterion constructs.
(table continues)
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
891
Table 4 (continued )
Technique Description of technique Example of model DEMO problems
Correlated uniqueness model
In this model, each observed
variable is DEMO by only one
trait factor and a measurement
error term. There are no
method factors. The model
accounts for method effects by
allowing the DEMO terms of
constructs measured by the
same method to be correlated.
Direct product model
Unlike the confirmatory factor
analysis and correlated
uniqueness models, this model
assumes that the trait measures
interact multiplicatively with
the methods of DEMO to
influence each observed
variable. More specifically, this
model assumes that DEMO
stronger the correlation
between traits, the more the
intercorrelation between the
DEMO will be influenced by
shared method biases.
Correlated uniqueness model
The standard errors of the parameter estimates in
this model are biased downward and DEMO not
be used for statistical tests.
Model assumes that the various method biases are
not correlated with each other.
Assumes that method factor does DEMO interact with
the predictor and criterion constructs.
Direct product model
The conceptual nature of Trait  Method
interactions has not been well articulated, thus
making it difficult to predict when they are likely
to occur.
The DEMO of the trait and method components
cannot be separated in the analysis.
Cannot test for relationships between latent trait
constructs while simultaneously controlling for
DEMO and Trait  Method effects.
Does not test for interactions after first controlling
for main effects.
Note: Labels associated with each path represent nonlinear constraints
that must be placed on the estimates (see Bechger, 1998)DEMO
Note.
CFA  confirmatory factor analysis; MTMM  multitrait–multimethod.
892
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
893
attributable to the specific surrogate DEMO measured (e.g., social
desirability, positive or negative affectivity). However, given the
wide variety of possible causes of method variance discussed in
DEMO article, this technique cannot be regarded as a complete solu-
tion DEMO the problem.
Another partial correlation technique that has been recently
recommended is the use of a marker variable to control for com-
mon method DEMO (Lindell & Brandt, 2000; Lindell & Whitney,
2001). DEMO and his colleagues have argued that if a variable can
be identified on theoretical grounds that should not be related to at
least one DEMO variable included in the study, then it can be used
as DEMO marker in that any observed relationships between it and any
of the other variables can be assumed to be due to common method
variance. DEMO, they conclude that partialling out the average
correlation between the marker DEMO and the other variables
included in the study should allow the researcher to control for the
possible contaminating effect of method biases.
The principal DEMO of this procedure is its ease of imple-
mentation, especially if DEMO uses the smallest observed correlation
among the manifest variables as a proxy for common method
variance, like Lindell and Brandt (2000). However, the marker
variable technique to control for common method variance has a
DEMO of conceptual and empirical problems. From a conceptual
point of view, DEMO major problem is that this procedure fails to
control for some of the most powerful causes of common method
biases (e.g., implicit theories, consistency motif, social desirabil-
ity). Because a marker variable is one that most people believe
should not be related to the predictor or DEMO variable, there is
no reason to expect that it provides an DEMO of the effect of a
person’s implicit theory about why the predictor and criterion
variables should be related, and therefore partialling out the effects
of the marker variable will not control for these sources of com-
DEMO method variance. For example, few people would argue on
theoretical grounds DEMO an employee’s self-reported shoe size
should be related to either an employee’s performance or the
employee’s ratings of his or her supervisor’s supportive leadership
DEMO Therefore, self-reported shoe size meets all of the criteria
identified by DEMO and Whitney (2001) and would be an excel-
lent marker variable. However, it is hard to imagine how the
strength of the relationship between reported shoe size and em-
ployee performance could possibly represent an DEMO im-
plicit theory about why supportive leader behavior should be
related to employee performance, and therefore partialling out the
effects of shoe size would not control for this source of common
method variance. The same could DEMO said for the ability of shoe
size to serve as a marker variable to control for consistency motif
and/or social desirability.
A final DEMO problem with this technique is that it assumes
that the common method factor represented by the marker variable
“has exactly the same impact on DEMO of the observed variables”
(Lindell & Whitney, 2001, p. 116)DEMO Although we tend to agree with
Lindell and Whitney (2001) that this might be a reasonable as-
sumption for some sources of common DEMO variance (e.g.,
common scale format, common anchors, or leniency DEMO), there
is no reason why this should necessarily be true for other types of
method biases. For example, there is no reason to assume that
implicit theories would affect the relationships between all pairs of
DEMO in the same way. In any case, because some of the DEMO
methods that control for common method variance do not have to
make this tenuous assumption, this should be regarded as a relative
disadvantage of this approach.
In addition to these conceptual problems, there are also three
important empirical problems with the use of the marker variable
technique. First, the procedure is based on the assumption that
common method variance can DEMO inflate, not deflate, the ob-
served relationship between a predictor and criterion variable (see
Lindell & Whitney, 2001, p. 115). However, as noted earlier,
common method variance can inflate, deflate, or have no effect on
the observed relationships between predictors and criterion vari-
DEMO (cf. Cote & Buckley, 1988). Second, the procedure ignores
DEMO error. Third, it assumes that common method factors
do not interact DEMO traits—a point that has been disputed by
Campbell and O’Connell (1967, 1982), Bagozzi and Yi (1990),
and Wothke and Browne (1990), among others.
The last of the partial correlation procedures used DEMO previous
research is the general factor covariate technique (Bemmels, 1994;
Dooley & Fryxell, 1999; Organ & Greene, 1981; Parkhe, 1993;
Podsakoff & Todor, 1985). In this procedure, the first DEMO is to
conduct an exploratory factor analysis of the variables included in
the study. Then, a scale score for the first unrotated factor (DEMO
is assumed to contain the best approximation of common method
variance) DEMO calculated and partialled out of the relationship be-
tween the predictor and criterion variable. This procedure shares
some of the same advantages and disadvantages DEMO the other two
partial correlation procedures. The principal advantage is that it is
relatively easy to use because the researcher does not have to DEMO
the specific source of the common method variance. However, like
some DEMO the partial correlation procedures, it ignores measurement
error. In addition, another important disadvantage is that this general
method factor may reflect not only DEMO method variance among
the measures of the constructs but also variance due to true causal
relationships between the constructs. Indeed, it is impossible to sep-
arate these two sources of variation using this technique. Conse-
quently, covarying out the effects of this general factor score may
produce biased DEMO estimates of the relationship between the
constructs of interest (cf. Kemery & Dunlap, 1986).
Controlling for the effects of a directly measured DEMO methods
factor. Up to this point, none of the statistical methods DEMO
are able to adequately account for measurement error or distin-
guish between the effects of a method factor on the measures of the
construct DEMO the construct itself. To address these issues, re-
searchers have turned DEMO the use of latent variable models. One
approach that has been used involves directly measuring the pre-
sumed cause of the method bias (e.g., social desirability, negative
affectivity, or positive affectivity), modeling it as a latent con-
struct, and allowing the indicators of the constructs of interest to
load on this factor as well as their hypothesized constructs (see
figure with social desirability as the latent methods factor in Table
DEMO). For example, Williams has used this approach to examine the
DEMO biasing effects of negative affectivity on the relation-
ships between a variety of job attitudes and role perceptions
(Williams et al., 1996) and the effects of positive and negative
emotionality (Williams & Anderson, 1994)DEMO The advantages of
using this approach are that it (a) allows measurement error in the
method factor to be estimated, (b) models the effects of the biasing
factor on the measures themselves rather than DEMO on the theoret-
ical constructs of interest, and (c) does DEMO constrain the effects of
the methods factor on the individual measures to be equal.
894
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
However, there are disadvantages DEMO this approach as well. To
use this method, the researcher must DEMO what the most impor-
tant sources of method biases are in his or her study and be able to
directly measure them. This is DEMO serious limitation because it is
often difficult to identify the most important sources of method
bias in a given situation. In addition, valid measures for some of
the sources of bias that a researcher might identify DEMO do not
exist (e.g., implicit theories, consistency biases, common scale
format and anchors). Finally, this technique requires the assump-
tion that the method factor does not interact with the constructs of
interest. As DEMO noted, this assumption has been questioned
by Campbell and O’Connell (1967), Bagozzi and Yi (1990), and
Wothke and Browne (1990)DEMO
Controlling for the effects of a single unmeasured latent method
factor. Another latent variable approach that has been used in-
volves adding a first-order DEMO with all of the measures as
indicators to the researcher’s theoretical model. Such a model is
depicted in Table 4 and has been used DEMO a number of studies (e.g.,
Carlson & Kacmar, 2000; DEMO & Perrewe, 1999; Conger,
Kanungo, & Menon, 2000; DEMO & Xie, 1999; Facteau,
Dobbins, Russell, Ladd, & DEMO, 1995; MacKenzie, Podsakoff,
& Fetter, 1991, 1993; MacKenzie, Podsakoff, & Paine, 1999;
Moorman & Blakely, 1995; Podsakoff & MacKenzie, 1994; Pod-
sakoff, MacKenzie, Moorman, & Fetter, DEMO). One of the main
advantages of this technique is that it does not require the re-
searcher to identify and measure the specific DEMO responsible for
the method effects. In addition, this technique models the DEMO of
the method factor on the measures rather than on the latent con-
structs they represent and does not require the effects of the
DEMO factor on each measure to be equal.
Like the other methods discussed, there are also some disad-
vantages of this approach. One is that although this technique
controls for any systematic variance among the items that DEMO
independent of the covariance due to the constructs of interest, it
DEMO not permit the researcher to identify the specific cause of the
method bias. Indeed, the factor may reflect not only different types
of common method variance but also variance due to relationships
between the constructs other DEMO the one hypothesized. Another
disadvantage is that if the number of indicators of the constructs is
small relative to the number of constructs of DEMO, the addition
of the method factor can cause the model to DEMO underidentified. As
a solution to this problem, some researchers have constrained DEMO
measurement factor loadings to be equal. Although this solves the
identification problem, it also undermines one of the advantages of
this technique. A final disadvantage is that this technique assumes
that the method factor does not DEMO with trait factors. Thus, this
approach does have some limitations even DEMO it is attractive
because it does not require the researcher to identify the potential
source of the method variance in advance.
Use of multiple-method DEMO to control method variance. A
variation of the common method factor technique has been fre-
quently used in the literature (cf. Bagozzi & Yi, 1990; Cote &
Buckley, 1987; Williams et al., 1989)DEMO This model differs from the
common method factor model previously described in two ways.
First, multiple first-order method factors are added to the model.
Second, each of these method factors is hypothesized to influence
only a subset of the measures rather than all of them. The most
common DEMO of this type of model is the multitrait–
multimethod (MTMM) model (Campbell & Fiske, 1959), where
measures of multiple traits using DEMO methods are obtained. In
this way, the variance of the responses DEMO a specific measure can be
partitioned into trait, method, and random error components, thus
permitting the researcher to control for both method variance and
random error when looking at relationships between the predictor
and criterion DEMO The first advantage of this technique is that
it allows the researcher to examine the effects of several methods
factors at one time. A DEMO advantage is that this technique
permits the researcher to examine the effects of specifically hy-
pothesized method biases by constraining some of the paths DEMO
the method factors to measures that they are not hypothesized to
influence to be equal to zero. A third advantage of this technique
is DEMO it does not require the direct measurement of these hypoth-
esized method biases.
Despite these advantages, the principal disadvantages of this
technique are (DEMO) potentially severe problems may be encountered
when estimating these models because DEMO identification problems,
specification errors, and sampling errors (Spector & Brannick,
1995); (b) it assumes that the method factors do DEMO interact with
the predictor and criterion constructs; and (c) it DEMO the re-
searcher to identify the potential sources of method bias to specify
the relationships between the method factors and the measures.
Correlated uniqueness DEMO One technique that has been
recommended to address the estimation problems encountered
with the use of the MTMM model is the correlated uniqueness
model (Kenny, 1979; Marsh, 1989; Marsh & Bailey, 1991). In an
MTMM model, each observed variable is modeled as being caused
by one trait factor, one method factor, and one measurement error
term. DEMO, in a correlated uniqueness model, each observed
variable is caused by only a trait factor and a measurement error
term. There are no DEMO factors. Instead, the correlated unique-
ness model accounts for method effects DEMO allowing the error terms
of variables measured by the same method to be correlated. Thus,
like the MTMM model, this technique requires the researcher to be
able to identify the sources of method variance so DEMO the appro-
priate pattern of measurement error correlations can be estimated.
The principal advantage of the correlated uniqueness approach
is that it is more DEMO than the MTMM model to converge and to
produce proper parameter estimates (cf. Becker & Cote, 1994;
Conway, 1998; Marsh, 1989; Marsh & Bailey, 1991). In addition,
like the CFA DEMO, the correlated uniqueness technique (a)
allows the researcher to examine the effects of multiple method
biases at one time, (b) permits the researcher to examine the effects
of specifically hypothesized method biases, and (c) does not re-
quire the direct measurement of these hypothesized DEMO biases.
Historically, its principal disadvantage was its inability to estimate
the DEMO of variance in a measure caused by method effects,
although recent developments by Conway (1998) and Scullen
(1999) have demonstrated how DEMO can be done by averaging the
measurement error correlations. However, other DEMO disadvan-
tages remain. As noted by Lance, Noble, and Scullen (DEMO), these
include the fact that (a) method effects are constrained to be
orthogonal under the correlated uniqueness model; (b) the esti-
mates of the trait variance components are likely to be biased
because DEMO error terms in this model are an aggregation of sys-
tematic, DEMO, and method effects; (c) this model as-
sumes that the various method biases are uncorrelated; and (d) this
method assumes that trait and method effects do not interact.
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
895
Direct product model. A common DEMO of all of the sta-
tistical remedies involving latent constructs that have been dis-
cussed so far is that they require the assumption that DEMO method
factors do not interact with the predictor and criterion constructs
(DEMO, trait factors). According to Campbell and O’Connell (1967,
1982), this is a tenuous assumption because there are circum-
stances in DEMO trait and method factors are likely to interact
multiplicatively such that method biases augment (or attenuate) the
correlation between strongly related constructs more DEMO they
augment (or attenuate) the correlations between weakly related
constructs. The direct product model (Bagozzi & Yi, 1990; Bech-
ger, 1998; Browne, 1984; Wothke & Browne, 1990) was devel-
oped to DEMO Trait  Method interactions like these into account,
and it does this through a radically different underlying model
structure that replaces the additive DEMO in a traditional MTMM
CFA with relationships that reflect multiplicative interactions be-
tween traits and methods. To the extent that Trait  Method
interactions DEMO present, this is obviously a major advantage. In
addition, like the correlated uniqueness model, the direct product
model is more likely than the MTMM model to converge and
produce proper solutions.
However, these advantages are undermined by some fairly se-
rious disadvantages. One major disadvantage is that DEMO direct
product model cannot provide separate estimates of the amount of
trait and method variance present in a measure, thus making it
difficult to assess item validity by examining how well each
measure reflects the trait DEMO is intended to represent. A second
disadvantage is that the direct product model does not control for
the main effects of the trait and DEMO factors when testing the
Trait  Method interactions, as is standard DEMO when testing
interactions. Third, although this technique tests for method biases,DEMO
it cannot be used to statistically control for them while simulta-
neously estimating the relationship between a predictor and crite-
rion construct. Still other DEMO include that it (a) does not
permit researchers to identify the specific cause of any method
biases observed and (b) requires the DEMO of a complex
set of equality constraints when estimated using the most
widely available structural equation modeling programs (e.g.,
LISREL, EQS, AMOS). However, in our view, perhaps the
most serious disadvantage is DEMO Trait  Method interactions
may not be very common or potent. Indeed, the weight of the
evidence (cf. Bagozzi & Yi, 1990, DEMO; Bagozzi et al., 1991;
Becker & Cote, 1994; Hernandez & Gonzalez-Roma, 2002;
Kumar & Dillon, 1992) suggests that although Trait  Method
interactions are theoretically possible and do exist in certain
DEMO, they are not usually very strong and a simpler
MTMM model DEMO be just as good as the more complicated
direct product approach.
Comparison of Statistical Remedies for Common Method
Biases
The preceding section provides a DEMO of the statistical pro-
cedures that have been used in the literature to control for common
method biases. However, there are some potential remedies that
have not been tried, which, when combined with the methods DEMO
have been used, suggest a continuum of ways to statistically
control DEMO common method biases. Table 5 summarizes this set of
potential remedies and highlights some of their key features. As
shown in Table 5, some of the statistical approaches require the
researcher to identify the source of DEMO bias and require a valid
measure of the biasing factor whereas others do not. In addition,
the approaches differ in the extent to DEMO they control a variety
of potential problems, including (a) the DEMO to distinguish
method bias at the measurement level from method bias at the
construct level, (b) measurement error, (c) single versus DEMO
sources of method bias, and (d) Trait  Method interactions.
DEMO speaking, as one moves from left to right and from top
DEMO bottom in the table, the approaches require more effort on the
DEMO of the researcher, either because the model specification is
more complex DEMO because additional measures of the method bias
must be obtained.
The approaches shown in the second column of the table (partial
correlation approaches) DEMO the advantage that they are relatively
easy to implement and can be used even with small sample sizes
(i.e., samples too small to DEMO the requirements for latent variable
structural equation modeling). However, they DEMO the weakest
among the statistical remedies because they (a) fail to distinguish
method bias at the measurement level from method bias at the
DEMO level, (b) ignore measurement error in the method factor,
(c) only control for a single source of method bias at a time, and
(d) ignore Method  Trait interactions. Thus, although DEMO easy
to implement and widely used, they are not very satisfactory
DEMO for controlling for method biases.
The second set of approaches (single-method-scale-score DEMO
proaches) are an improvement over the first group of techniques
because DEMO distinguish method bias at the measurement level
from method bias at the construct level and appropriately model it
at the measurement level. To our DEMO, no one has used this
set of procedures to control for DEMO variance, probably because
these techniques still ignore measurement error in the DEMO
factor, even though there is no reason to do so. One DEMO in
which the technique identified in Cell 2B might be useful is where
the measure of the potential biasing factor (e.g., social desirability)DEMO
has a large number of items (e.g., 10 or more) DEMO/or a complex
factor structure. In this instance, the researcher may DEMO to use
a scale score to represent the source of bias because if the measures
are all treated as separate indictors of the biasing DEMO, then any
nonhypothesized measurement error covariances will contribute to
the lack DEMO fit of the model. If these measurement error covariances
are substantial, DEMO the goodness-of-fit of the model will be
disproportionately influenced by them. However, even in this case,
we believe that a better remedy to this problem would be to create
a small number of testlets (Bagozzi & Heatherton, 1994) com-
posed of random subsets of the measures DEMO the biasing factor and
then use the procedure in Cell 3B.
The third set of approaches (single-method-factor approaches)
have the advantages of estimating method biases at the measure-
ment level and controlling measurement error. Perhaps DEMO of
these advantages, these techniques have been frequently used in
the DEMO Examples of the use of the single-common-method-
factor approach shown in Cell 3A include Carlson and Kacmar
(2000), Elangovan and Xie (1999), MacKenzie et al. (1991, 1993),
and Podsakoff et al. (1990) and examples of the use of the
single-specific-method-factor approach shown in Cell 3B include
Williams and Anderson (1994) and Williams et al. (1996). The
main disadvantages of these approaches are that they only DEMO
for a single source of method bias at a time and assume that
Table 5
Summary of Statistical Remedies Used to Control Common Method Biases
DEMO Partial correlation approaches Single-method-scale-score approaches Single-method-factor approaches Multiple-method-factor approaches
Does not require the researcher
to identify the precise source
of method bias.
Does not DEMO a valid
measure of the biasing factor.
Disadvantages
Fails to distinguish method bias
at the measurement level from
method bias at the construct
level.
DEMO measurement error in the
method factor.
Only controls for a single source
of method bias at a time.
Ignores Method  Trait
interactions.
Disadvantages
DEMO measurement error in the
method factor.
Only controls for a single source of
method bias at a time.
Ignores Method  Trait
interactions.
Disadvantages
DEMO controls for a single source
of method bias at a time.
Ignores Method  Trait
interactions.
Disadvantage
Ignores Method  Trait interactions.
Requires the DEMO to
identify the precise source of
method bias.
Requires a valid measure of the
biasing factor.
1B 2B 3B 4B
1A 2A 3A 4A
DEMO
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
897
Method  Trait interactions are DEMO present. How serious these
disadvantages are depends on how confident the researcher is that
the method factor adequately captures the main source of method
DEMO and that Method  Trait interactions do not exist. The former
is a judgment that has to be made primarily on conceptual grounds.
However, on the latter issue, the empirical evidence suggests that
Method  Trait interactions are unlikely to be very strong (Becker
& Cote, 1994)DEMO
The final set of approaches shown in Table 5 (multiple-method-
factor DEMO) is the strongest of the approaches depicted. Cell
4A represents the DEMO MTMM model, whereas 4B represents a
situation in which the researcher DEMO directly measured several
suspected sources of method bias (e.g., social desirability, positive
affectivity and negative affectivity), models these biasing factors
as latent variables with multiple indicators, and estimates their
effects on the measures of the constructs of interest. These ap-
proaches are particularly strong because they DEMO method bias at
the measurement level, control for measurement error, and incor-
porate multiple sources of method bias. However, they have two
key disadvantages. The first is that because the models are com-
plex, estimation problems may be encountered. It is widely rec-
ognized that this is DEMO a problem for the MTMM model
shown in Cell 4A (cf. DEMO & Cote, 1994; Brannick & Spector,
1990; Marsh, 1989; Marsh & Bailey, 1991; Spector & Brannick,
1995). It is for this reason that some researchers (Conway, 1998;
Kenny, 1979; Marsh, 1989; Marsh & Bailey, 1991; Scullen, 1999)DEMO
have recommended the use of the correlated uniqueness model.
However, because DEMO MTMM model is conceptually and empir-
ically superior to the correlated uniqueness model as long as it is
not empirically underidentified (cf. Lance, DEMO, & Scullen,
2002), the correlated uniqueness model is not DEMO in the sum-
mary table. The second problem encountered by all of the ap-
proaches in the final column of Table 5 is that DEMO do not take
Method  Trait interactions into account.
Although not shown in Table 5, one approach that takes
Method  Trait interactions into account is the direct product
model. Unlike the other approaches, it (DEMO) distinguishes method
bias at the measurement level from method bias at DEMO construct
level, (b) takes measurement error in the method factor DEMO ac-
count, (c) can control for multiple sources of method DEMO at the
same time, and (d) captures Method  Trait DEMO How-
ever, at the present time, this approach is not recommended for two
reasons. First, the conceptual nature of Trait  Method interac-
tions has never been well articulated, thus making it difficult to
predict when these interactions are likely to occur. Second, the
existing empirical evidence suggests that these interactions may be
fairly rare in research settings, and even when they do occur, they
are generally weak and a simpler MTMM model may be just as
good in these situations as the DEMO complicated direct product
approach (cf. Bagozzi & Yi, 1991; Becker & Cote, 1994). There-
fore, additional research is needed on these issues before the use of
the direct product model is likely to DEMO widely recommended as a
technique for controlling common method biases.
Recommendations for Controlling Method Biases in
Research Settings
Our discussion in this article clearly DEMO that common
method variance can have a substantial impact on the observed
relationships between predictor and criterion variables in organi-
zational and behavioral research. DEMO estimates of the
strength of the impact of common method biases vary (cf. Bagozzi
& Yi, 1990; Cote & Buckley, 1987; Spector, 1987, 1994; Williams
et al., 1989), their average level DEMO quite substantial. Indeed, the
evidence reported by Cote and Buckley (1987) from 70 MTMM
studies conducted in a variety of disciplines indicates that the
observed relationship between a typical predictor and criterion
variable is understated DEMO approximately 26% because of common
method biases. Moreover, as noted in DEMO 2, this bias may come
from a number of different sources DEMO could be in operation in
any given research study. Therefore, we DEMO that researchers
would be wise to do whatever they can to control for method
biases, and the method used to control for it should to be tailored
to match the specific research setting.
Figure 1 describes DEMO set of procedures that might be used to
control for method biases that are designed to match several
typical research settings. Generally speaking, we recommend that
researchers follow good measurement practice by implementing
all of the DEMO remedies related to questionnaire and item
design (e.g., eliminate item ambiguity, demand characteristics,
social desirability). Following this, we recommend that DEMO im-
plement additional procedural and statistical remedies to control
for the method biases that are likely to be present in their specific
research situation. DEMO can be done by considering four key
questions: (a) Can DEMO predictor and criterion variables be obtained
from different sources? (b) DEMO the predictor and criterion vari-
ables be measured in different contexts? (c) Can the source of the
method bias be identified? and (d) Can the method bias be validly
measured?
Beginning at the bottom left of Figure 1 (Situation 1), if the
predictor and criterion variables can be measured from different
sources, then we recommend that this be done. Additional statis-
tical remedies could be used but in DEMO view are probably unnec-
essary in these instances.
Moving to Situation 2 in Figure 1, if the predictor and criterion
variables are obtained from the same source but can be obtained in
a different context, the researcher has a good idea about the
source(s) of the method bias (e.g., social desirability, negative
affectivity, positive affectivity), and DEMO suspected bias can be
validly measured, then we recommend that researchers (a) separate
the measures of the predictor and criterion variables (temporally,DEMO
proximally, psychologically, and/or methodologically) and (b)
measure the biasing factor(s) and estimate its effects on the mea-
sures using the single-specific-method-factor or multiple-specific-
method-factors approaches (Cells 3B or 4B in Table 5). The
rationale for this recommendation is that separating the measure-
DEMO of the predictor and criterion variable should help to prevent
method biases because of a common rater, whereas the statistical
remedy controls for these biases if they happen to occur in spite of
this procedural control.
DEMO 3 represents virtually the same circumstances as de-
scribed in Situation 2, with the exception that there is no valid way
to measure the biasing factor suspected to be in operation. In this
instance, we recommend that researchers separate the measures of
the predictor and criterion variables (temporally, proximally, psy-
chologically, and/or methodologically) and estimate any residual
DEMO of the suspected source of method bias using the single-
common-method-factor approach specified in Cell 3A in Table 5
898
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
Figure 1.
Recommendations for controlling for common method variance in different research settings.
or the multiple-common-method-factors approach DEMO in
Cell 4A (depending on the number of biasing factors believed
DEMO be in operation). This situation might arise if leniency biases
are expected to be a potential problem in a study because chang-
ing DEMO measurement context does not control for this form of
bias and there is no valid scale to directly measure leniency
biases. Under these circumstances, a method of statistical control
that does not require the biasing factor DEMO be directly measured
must be used (e.g., single-common-method-factor approach).
However, in other situations in which several biasing factors that
are not directly measurable exist, it may be necessary to use the
approach specified in Cell 4A (multiple-common-method-factors
approach).
The recommendations for Situation 4 are the same as Situa-
tion 3, with the exception that the multiple-common-method-
factors approach (Cell 4A) cannot be used. This method of statis-
tical DEMO cannot be used in this case because the specification
of a multiple-common-method-factors model requires the sources
of method bias to be known by the DEMO; otherwise, it is
impossible to specify which method factor influences which spe-
cific measures.
Situations 5–7 differ from Situations 2– 4 in that DEMO is not
possible to measure the predictor and criterion variables in differ-
ent times or locations. As a result, the procedural remedy of
separating the measurement of the predictor and criterion vari-
ables temporally or proximally DEMO be used. One potential
procedural remedy in this case is to attempt to reduce method
bias by guaranteeing response anonymity. Another procedural
remedy is DEMO separate the predictor and criterion variable psycho-
logically. As noted earlier, DEMO might be done by creating a cover
story to make it appear that the measurement of the predictor
variable is not connected with or DEMO to the measurement of the
criterion variable. However, because guaranteeing anonymity
DEMO/or psychological separation does not eliminate all of the dif-
ferent method biases associated with a common rater and mea-
surement context (e.g., DEMO moods states, leniency biases,
acquiescence biases, contextually provided retrieval cues), the
researcher needs to depend more on the statistical remedies in
DEMO cases.
Situation 5 is one in which the predictor and criterion variables
cannot be obtained from different sources or contexts, but the
source(s) of the method bias can be identified and a valid scale to
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
measure it exists. Under these circumstances, we recommend that
researchers try to statistically control for the effect of DEMO bias(es)
using the single-specific-method-factor approach (Cell 3B in Table
DEMO) or the multiple-specific-method-factors approach (Cell 4B in
Table 5).
Situation 6 is similar to Situation 5, except that the method
bias(es) cannot be validly measured. Under these circumstances,
the single-common-method-factor approach (DEMO 3A in Table 5)
or the multiple-common-method-factors approach (Cell 4A),
should be used to statistically control for method biases. For
example, if the predictor and criterion measures are obtained from
the same source DEMO the same context and the researcher suspects
that implicit theories may bias the raters’ responses, then the
single-common-method-factor approach should be used. Alterna-
tively, in a study that obtained measures of the predictor and
criterion variables from three different sources (e.g., peers, super-
visors, and subordinates) all at the same time, the multiple-
common-method-factors approach should be used.
Finally, Situation 7 displays a circumstance in which a re-
searcher cannot obtain the predictor and criterion variables from
different sources, cannot separate the measurement context, and
cannot identify the source of the method bias. In this situation, it
is best to use a single-common-method-factor approach (Cell DEMO
in Table 5) to statistically control for method biases.
Of course, it is important to note that it may be impossible to
completely DEMO all forms of common method biases in a
particular study, and DEMO for common method biases be-
comes more complex in multiple equation systems in which rela-
tionships between criterion variables are hypothesized (e.g., me-
DEMO effects models). However, the goal should be to reduce the
DEMO of method biases as an explanation of the relationships
observed between the constructs of interest. In most instances, this
involves a combination of the procedural and statistical remedies
previously discussed. In general, we recommend that researchers
first try to prevent method biases from influencing their results by
implementing DEMO of the procedural remedies that make sense
within the context of their research. However, because it may be
impossible to eliminate some sources of bias with procedural
remedies, we recommend that researchers follow this up with
appropriate statistical remedies.
For example, a researcher interested in the potential mediating
effects of employees’ trust in their leaders and commitment to the
organization DEMO the relationships between transformational leader-
ship behaviors and employee performance may find it difficult (or
even undesirable) to obtain all of the measures DEMO different
sources. In this case, even though the researcher might be DEMO to
maintain the independence of the leader behavior and performance
measures by obtaining the ratings of the leader from the subordi-
nates and the DEMO of performance from the leader, there is still
the problem of DEMO to obtain the measures of the employee
attitudes (trust and commitment) to maintain their independence.
Of course, one solution to this dilemma would be to obtain
objective measures of employee performance from company
records and DEMO have the supervisor provide the ratings of their
perceptions of the employees’ trust and commitment. However,
given that one’s behavior does not always DEMO strongly with
one’s attitudes (Fishbein & Ajzen, 1975), it is doubtful whether the
supervisors’ (or anyone else’s) perceptions of employees’ attitudes
DEMO as good a measure as the employees’ own self-reports. Another
899
possible procedural remedy is to have the supervisors provide
self-reports of their own DEMO leadership behavior and
the subordinates provide ratings of their own attitudes. Unfortu-
nately, given the evidence that self-reports of behavior are often
considerably different from the reports of others (cf. Harris &
Schaubroeck, 1988; DEMO & Nisbett, 1972), this remedy has other
limitations that are DEMO with it.
Therefore, in the situation described above, the best course of
action would probably be to obtain the measures of employee
performance DEMO company records; to obtain the measures of the
leader behaviors and DEMO attitudes from the employees but to
separate their measurement temporally, contextually, or psycho-
logically; and to statistically control for same-source biases in the
leader behavior and employee attitude measures by adding a
single-common-method-factor to the DEMO equation model
used to test the hypothesized relationships. However, the key DEMO
to remember is that the procedural and statistical remedies selected
should be tailored to fit the specific research question at hand.
There is no DEMO best method for handling the problem of
common method variance because it depends on what the sources
of method variance are in the study DEMO the feasibility of the
remedies that are available.
Some Additional Considerations
Our discussion of common method variance up to this point has
focused on DEMO typical situations in which the measures of a
predictor and criterion variable are obtained in a field setting using
some form of questionnaire. However, there are two other situa-
tions that generally have not been discussed DEMO the method variance
literature that also deserve our attention. The first of these is in
experimental research settings in which mediating processes are
examined DEMO the measures of the potential mediators and/or
dependent variables are obtained from the same source. The sec-
ond area relates to the special DEMO encountered in trying to
statistically control for method biases when using formative indi-
cator measurement models (cf. Bollen & Lennox, 1991). In DEMO
section, we discuss each of these issues in turn.
Controlling for DEMO Variance in Experimental
Research Examining Mediated Effects
It is often assumed that experimental studies are immune to the
method biases discussed in this article DEMO measures of the
independent and dependent variable are not obtained from the
same person at the same point in time. However, it is not uncom-
mon in experimental studies for researchers to manipulate an
independent variable DEMO obtain measures of a potential mediator
as well as the dependent variable. The problem with this is that
these two measures are usually obtained DEMO the same subjects at
the same point in time. In such cases, method biases contribute to
the observed relationship between the mediator and the dependent
measure. One possible remedy for this is to control for methods
DEMO using the single-common-method-factor approach dis-
cussed in Cell 3A of Table 5. As in the case of field research,
experimental studies that can DEMO that the relationships
observed between the variables of interest are significant after
controlling for method biases provide more compelling evidence
than those that do DEMO
900
Controlling for Method Variance in Studies Using
Formative Constructs
For most DEMO the constructs measured in behavioral research, the
relationship between the measures DEMO constructs is implicitly
based on classical test theory that assumes that the variation in the
scores on measures of a construct is a function DEMO the true score
plus error. Thus, the underlying latent construct is DEMO to
cause the observed variation in the measures (Bollen, 1989; DEMO
nally, 1978), and the indicators are said to be reflective DEMO the
underlying construct. This assumed direction of causality—from
the latent variable to its measures—is conceptually appropriate in
many instances but not all. Indeed, it has been recognized now for
several decades that, for some constructs, DEMO makes more sense
conceptually to view causality flowing from the measures to the
construct rather than vice versa (Blalock, 1964; Bollen & Lennox,
1991; MacCallum & Browne, 1993). In these cases, the constructs
are often called composite latent variables and the measures are
said DEMO represent formative indicators of the construct. Researchers
(cf. Bollen & Lennox, 1991; Law & Wong, 1999) are now begin-
ning to recognize that many of the most widely used constructs in
the field (e.g., job satisfaction, role ambiguity, role conflict, task
characteristics) are more accurately represented as formative-
indicator constructs than they are as reflective-indicator constructs.
DEMO of more importance, research by Law and Wong (1999)
has demonstrated that misspecifying measurement relationships by
modeling formative-indicator constructs as if they DEMO reflective-
indicator constructs can have serious biasing effects on estimates
of the relationships between constructs.
Although not previously recognized in the literature, the dis-
tinction between formative-indicator and reflective-indicator mea-
surement models is also important because DEMO complicates the
treatment of common method biases. The goal of the statistical
control procedures discussed in Table 5 is to obtain an estimate of
DEMO relationship between the constructs and measures that partials
out the effect of method bias. Unfortunately, if a researcher uses
the methods shown in the table (i.e., models the effects of a method
factor on the DEMO measures), he or she will not partial the
effect of method bias out of the estimated relationship between the
formative measures and the DEMO This is true because the
method factor does not enter into the equation where the relation-
ship between the formative measures and the construct DEMO esti-
mated. Indeed, from a conceptual point of view, the effects of
method bias on formatively measured constructs should be mod-
eled at DEMO construct level rather than the item level. If this is done,
the estimated relationships between the formative measures and
the formative construct will DEMO independent of method bias. This is
consistent with Bollen and Lennox’s (DEMO) conceptual argument
that for formative constructs, measurement error resides at the
construct rather than the item level. Unfortunately, such a model is
not identified because, as noted by MacCallum and Browne
(1993), the DEMO error term for a construct with forma-
tive measures is only identified when there are paths emanating
from the construct to at least two DEMO measured constructs
that are independent from each other. This suggests that when
formative-indicator constructs are an integral part of a study,
researchers must DEMO even more careful than normal in designing
their research because procedural controls are likely to be the most
effective way to control common measurement DEMO
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
Conclusions
Although the strength of DEMO biases may vary across re-
search contexts, a careful examination of DEMO literature suggests
that common method variance is often a problem and researchers
need to do whatever they can to control for it. As we DEMO
discussed, this requires carefully assessing the research setting to
identify the DEMO sources of bias and implementing both pro-
cedural and statistical methods of control. Although we clearly
have not resolved all of the issues about DEMO important topic,
hopefully we have provided some useful suggestions and, DEMO
more importantly, a framework that researchers can use when
evaluating the DEMO biasing effects of method variance in their
research.
References
Andersson, L. DEMO, & Bateman, T. S. (1997). Cynicism in the workplace:DEMO
Some causes and effects. Journal of Organizational Behavior, 18, 449 –
469.
Aulakh, P. S., & Gencturk, E. F. (2000). DEMO principal–agent
relationships— control, governance and performance. Industrial Market-
ing Management, 29, 521–538.
Bagozzi, R. P., & Heatherton, T. F. (1994). A general approach to repre-
senting multifaceted personality constructs: Application to state self-
esteem. Structural Equation Modeling, 1, 35– 67.
Bagozzi, R. P., & Yi, Y. (1990). Assessing method variance in multitrait–
multimethod matrices: The case of self-reported affect and perceptions
at work. Journal of Applied Psychology, 75, 547–560.
Bagozzi, R. P., & Yi, Y. (1991). Multitrait–multimethod matrices in
consumer research. Journal of Consumer Research, DEMO, 426 – 439.
Bagozzi, R. P., Yi, Y., & DEMO, L. W. (1991). Assessing construct
validity in organizational research. Administrative Science Quar-
terly, 36, 421– 458.
Bechger, T. M. (1998)DEMO Browne’s composite direct product model for
MTMM correlation matrices. Structural Equation Modeling, 5, 191–
204.
Becker, T. E., & Cote, J. A. (1994). Additive and multiplicative method
effects in applied psychological research: DEMO empirical assessment of
three models. Journal of Management, 20, 625– 641.
Bemmels, B. (1994). The determinants of grievance initiation. Industrial
and DEMO Relations Review, 47, 285–301.
Berman, J. S., & Kenny, DEMO A. (1976). Correlational bias in observer
ratings. Journal of Personality DEMO Social Psychology, 34, 263–273.
Blalock, H. M., Jr. (1964)DEMO Causal inferences in nonexperimental research.
New York: W. W. Norton.
Blaney, P. H. (1986). Affect and memory: A review. Psychological Bul-
DEMO, 99, 229 –246.
Bollen, K. A. (1989). Structural equations with latent variables. New
York: Wiley.
Bollen, K. A., & Lennox, R. (1991). Conventional wisdom on measure-
ment: A structural equation perspective. Psychological Bulletin, 110,
305–314.
Bouchard, T. J., Jr. (DEMO). Field research methods: Interviewing, ques-
tionnaires, participant observation, systematic observation, unobtrusive
measures. In M. D. Dunnette (Ed.), Handbook of DEMO and orga-
nizational psychology (pp. 363– 414). New York: Wiley.
Bower, G. H. (1981). Mood and memory. American Psychologist, 36,
129 –148.
Brannick, M. T., & Spector, P. E. (DEMO). Estimation problems in the
block-diagonal model of the multitrait–multimethod matrix. Applied
Psychological Measurement, 14, 325–339.
Brief, A. P., Burke, M. J., George, J. M., Robinson, B. S., & Webster, J.
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
901
(1988). Should negative affectivity remain an unmeasured variable in the
study of job stress? Journal of Applied Psychology, 73, 191–198.
Browne, M. W. (1984). The DEMO of multitrait–multimethod
matrices. The British Journal of Mathematical and Statistical Psychol-
ogy, 37, 1–21.
Burke, M. J., Brief, A. P., & DEMO, J. M. (1993). The role of negative
affectivity in understanding relations between self-reports of stressors
and strains: A comment on the applied psychology literature. Journal of
Applied Psychology, 78, 402– 412.
Campbell, D. T., & Fiske, D. (1959). Convergent and discriminant vali-
dation by the multitrait–multimethod matrix. Psychological Bulletin, 56,
81–105.
Campbell, D. DEMO, & O’Connell, E. J. (1967). Methods factors in multitrait–
DEMO matrices: Multiplicative rather than additive? Multivariate
Behavioral Research, 2, 409 – 426.
Campbell, D. T., & O’Connell, E. J. (1982)DEMO Methods as diluting trait
relationships rather than adding irrelevant systematic variance. In D.
Brinberg & L. Kidder (Eds.), Forms of validity in research (pp. 93–111).
San Francisco: Jossey-Bass.
Cannell, C., Miller, P., & Oksenberg, L. (1981). Research on interviewing
techniques. In S. Leinhardt (Ed.), Sociological methodology (pp. 389 –
437). San DEMO: Jossey-Bass.
Carlson, D. S., & Kacmar, K. M. (2000)DEMO Work–family conflict in the
organization: Do life role values make a DEMO? Journal of Man-
agement, 26, 1031–1054.
Carlson, D. S., & Perrewe, P. L. (1999). The role of social support in the
stressor–strain relationship: An examination of work–family conflict.
Journal of Management, DEMO, 513–540.
Chapman, L. J., & Chapman, J. P. (1967)DEMO Genesis of popular but erroneous
psychodiagnostic observations. Journal of Abnormal Psychology, DEMO,
193–204.
Chapman, L. J., & Chapman, J. P. (1969). Illusory correlations as an
obstacle to the use of valid psychodiagnostic DEMO Journal of Abnormal
Psychology, 75, 271–280.
Chen, P. Y., & Spector, P. E. (1991). Negative affectivity as the underlying
cause DEMO correlations between stressors and strains. Journal of Applied
Psychology, 76, 398 – 407.
Collins, W. (1970). Interviewers’ verbal idiosyncrasies as a DEMO of bias.
Public Opinion Quarterly, 34, 416 – 422.
Conger, DEMO A., Kanungo, R. N., & Menon, S. T. (2000)DEMO Charismatic
leadership and follower effects. Journal of Organizational Behavior, 21,
DEMO
Conway, J. M. (1998). Understanding method variance in multitrait–
multirater performance appraisal matrices: Examples using general im-
pressions and interpersonal affect as measured method factors. Human
Performance, 11, 29 –55.
Cote, J. A., & Buckley, R. (1987). Estimating trait, method, and error
variance: Generalizing across 70 construct validation studies. Journal of
Marketing Research, DEMO, 315–318.
Cote, J. A., & Buckley, R. (1988). DEMO error and theory testing in
consumer research: An illustration of the DEMO of construct vali-
dation. Journal of Consumer Research, 14, 579 –582.
Crampton, S., & Wagner, J. (1994). Percept–percept inflation in DEMO
ganizational research: An investigation of prevalence and effect. Journal
of Applied DEMO, 79, 67–76.
Cronbach, L. J. (1946). Response sets and test validity. Educational and
Psychological Measurement, 6, 475– 494.
Cronbach, L. J. (1950). Further evidence on response sets and test validity.
Educational and Psychological Measurement, 10, 3–31.
Crowne, D., & Marlowe, D. (1964). The approval motive: Studies in
evaluative dependence. New York: Wiley.
Dooley, R. S., & Fryxell, G. E. (1999). DEMO decision quality and
commitment from dissent: The moderating effects of loyalty DEMO com-
petence in strategic decision-making teams. Academy of Management
Journal, 42, 389 – 402.
Eden, D., & Leviatin, V. (1975). DEMO leadership theory as a determi-
nant of the factor structure underlying supervisory behavior scales.
Journal of Applied Psychology, 60, 736 –740.
Elangovan, A. R., & Xie, J. L. (1999). Effects of perceived power of
supervisor on subordinate stress and motivation: The moderating role of
subordinate characteristics. Journal of Organizational Behavior, 20,
359 –373.
Facteau, J. DEMO, Dobbins, G. H., Russell, J. E. A., Ladd, R. T., & Kudisch,
J. D. (1995). The influence of DEMO perceptions of training environ-
ment on pretraining motivation and perceived training transfer. Journal
of Management, 21, 1–25.
Fishbein, M., & Ajzen, I. (1975). Belief, attitude, intention and behavior:
An introduction to theory and research. Reading, MA: Addison-Wesley.
Fiske, D. W. (1982)DEMO Convergent– discriminant validation in measurements
and research strategies. In D. Brinbirg & L. H. Kidder (Eds.), Forms of
validity in research (pp. DEMO). San Francisco: Jossey-Bass.
Fowler, F. J. (1992). How DEMO terms affect survey data. Public Opinion
Quarterly, 56, 218 –231.
Fuller, J. B., Patterson, C. E. P., Hester, K., & DEMO, S. Y. (1996). A
quantitative review of research on charismatic leadership. Psychological
Reports, 78, 271–287.
Ganster, D. C., Hennessey, H. W., & Luthans, F. (1983). Social desirability
response effects: DEMO alternative models. Academy of Management
Journal, 26, 321–331.
Gerstner, C. DEMO, & Day, D. V. (1997). Meta-analytic review of leader–
DEMO exchange theory: Correlates and construct issues. Journal of
Applied Psychology, 82, 827– 844.
Gioia, D. A., & Sims, H. P., Jr. (1985). On avoiding the influence of
implicit leadership theories in leader behavior descriptions. Educational
and Psychological Measurement, 45, 217–232.
Greene, C. N., & Organ, D. W. (1973). An evaluation of causal models
linking the received role with job satisfaction. Administrative Science
Quarterly, 18, DEMO
Guilford, J. P. (1954). Psychometric methods (2nd ed.). DEMO York:
McGraw-Hill.
Guzzo, R. A., Wagner, D. B., Maguire, E., Herr, B., & Hawley, C. (1986).
Implicit DEMO and the evaluation of group process and performance.
Organizational Behavior and Human Decision Processes, 37, 279 –295.
Harris, M. M., & Schaubroeck, J. (1988). A meta-analysis of self–
supervisor, self–peer, and peer–supervisor ratings. Personnel Psychol-
ogy, 41, 43– 62.
Harrison, D. A., & McLaughlin, M. E. (1993). Cognitive processes in
self-report responses: DEMO of item context effects in work attitude
measures. Journal of Applied Psychology, 78, 129 –140.
Harrison, D. A., McLaughlin, M. E., & Coalter, T. M. (1996). Context,
cognition, and common DEMO variance: Psychometric and verbal pro-
tocol evidence. Organizational Behavior and Human DEMO Pro-
cesses, 68, 246 –261.
Harvey, R. J., Billings, DEMO S., & Nilan, K. J. (1985). Confirmatory factor
analysis DEMO the job diagnostic survey: Good news and bad news. Journal
of DEMO Psychology, 70, 461– 468.
Heider, F. (1958). The psychology of interpersonal relations. New York:
Wiley.
Hernandez, A., & Gonzalez-Roma, V. (2002). Analysis of multitrait–
mulioccasion data: Additive versus multiplicative DEMO Multivariate
Behavioral Research, 37, 59 – 87.
Hinkin, T. R. (1995). A review of scale development practices in the study
of DEMO Journal of Management, 21, 967–988.
Idaszak, J., & Drasgow, DEMO (1987). A revision of the job diagnostic survey:
Elimination DEMO a measurement artifact. Journal of Applied Psychol-
ogy, 72, 69 –74.
Isen, A. M., & Baron, R. A. (1991). Positive DEMO as a factor in organi-
902
PODSAKOFF, MACKENZIE, LEE, AND PODSAKOFF
zational behavior. In L. L. Cummings & B. M. Staw (Eds.), Research in
organizational behavior (DEMO 13, pp. 1–53). Greenwich, CT: JAI Press.
Iverson, R. D., & Maguire, C. (2000). The relationship between job and life
satisfaction: Evidence from a remote mining community. Human Rela-
tions, DEMO, 807– 839.
Jex, S. M., & Spector, P. E. (DEMO). The impact of negative affectivity on
stressor strain relations: A DEMO and extension. Work and
Stress, 10, 36 – 45.
Johns, DEMO (1994). How often were you absent—A review of the use DEMO
self-reported absence data. Journal of Applied Psychology, 79, 574 –591.
Jones, E. E., & Nisbett, R. E. (1972). The actor DEMO the observer: Divergent
perceptions of the causes of behavior. In E. DEMO Jones, D. E. Kanouse,
H. H. Kelley, R. E. Nisbett, S. Valins, & B. Weiner (Eds.), Attribution:
Perceiving the causes of behavior (pp. 79 –94). Morristown, NJ: General
Learning.
Judd, C., Drake, R., Downing, J., & Krosnick, J. (1991). Some dynamic
properties of attitude structures: Context-induced response facilitation
DEMO polarization. Journal of Personality and Social Psychology, 60,
193–202.
Kemery, E. R., & Dunlap, W. P. (1986). Partialling factor scores does not
control method variance: A rejoinder to Podsakoff and Todor. Journal of
Management, 12, 525–530.
Kenny, D. A. (1979). Correlation DEMO causality. New York: Wiley.
Kline, T. J. B., Sulsky, L. M., & Rever-Moriyama, S. D. (2000). Common
method variance and specification errors: A practical approach to de-
tection. The Journal of Psychology, 134, 401– 421.
Korsgaard, M. A., & Roberson, L. (DEMO). Procedural justice in perfor-
mance evaluation—The role of instrumental and noninstrumental voice
in performance-appraisal discussions. Journal of Management, 21, 657–
669.
Kumar, A., & Dillon, W. R. (1992). An integrative look at the use of
additive and multiplicative covariance structure models in the analysis DEMO
MTMM data. Journal of Marketing Research, 29, 51– 64.
Lance, DEMO E., Noble, C. L., & Scullen, S. E. (2002)DEMO A critique of the
correlated trait– correlated method and correlated uniqueness models for
multitrait–multimethod data. Psychological Methods, 7, 228 –244.
Law, K. S., & Wong, C. S. (1999). Multidimensional constructs in struc-
tural equation analysis: An illustration using the job perception and job
satisfaction constructs. Journal of Management, 25, 143–160.
Lindell, M. K., & Brandt, C. J. (2000). Climate quality and climate
consensus as mediators of the relationship between organizational ante-
cedents and outcomes. Journal of Applied Psychology, 85, 331–348
Lindell, M. K., & Whitney, D. J. (2001). Accounting for common method
variance in cross-sectional designs. Journal of Applied DEMO, 86,
114 –121.
Lord, R. G., Binning, J. F., Rush, M. C., & Thomas, J. C. (1978). The effect
of performance cues and leader behavior on questionnaire ratings of
leadership DEMO Organizational Behavior and Human Decision
Processes, 21, 27–39.
Lowe, K. DEMO, Kroeck, K. G., & Sivasubramaniam, N. (1996). Effectiveness
DEMO of transformational and transactional leadership: A meta-
analytic review of the DEMO literature. Leadership Quarterly, 7, 385–
425.
MacCallum, R. C., & Browne, M. W. (1993). The use of causal indicators
in DEMO structure models: Some practical issues. Psychological
Bulletin, 114, 533–541.
MacKenzie, S. B., Podsakoff, P. M., & Fetter, R. (1991). Organizational
citizenship behavior and objective productivity as determinants of man-
agers’ evaluations DEMO performance. Organizational Behavior and Human
Decision Processes, 50, 1–28.
MacKenzie, DEMO B., Podsakoff, P. M., & Fetter, R. (1993). DEMO impact of
organizational citizenship behaviors on evaluations of sales perfor-
mance. Journal of Marketing, 57, 70 – 80.
MacKenzie, S. B., Podsakoff, P. M., & Paine, J. B. (1999). Do citizenship
behaviors matter more for managers than for salespeople? Journal of the
Academy of Marketing Science, 27, 396 – 410.
Marsh, H. W. (1989)DEMO Confirmatory factor analyses of multitrait–
multimethod data: Many problems and a DEMO solutions. Applied Psycho-
logical Measurement, 13, 335–361.
Marsh, H. W., & Bailey, M. (1991). Confirmatory factor analyses of
multitrait–multimethod data: A comparison of alternative methods. Ap-
plied Psychological Measurement, 15, 47–70.
DEMO, H. W., & Yeung, A. S. (1999). The lability of psychological
ratings: The chameleon effect in global self-esteem. Personality and
Social Psychology Bulletin, 25, 49 – 64.
Martin, C. L., & DEMO, D. H. (1989). Some effects of computerized
interviewing on job applicant responses. Journal of Applied Psychol-
ogy, 74, 72– 80.
McGuire, W. J. (1966). The current status of cognitive consistency theo-
ries. In S. Feldman (Ed.), Cognitive consistency (pp. 1– 46). DEMO York:
Academic Press.
Millsap, R. E. (1990). A cautionary note on the detection of method
variance in multitrait–multimethod data. Journal of DEMO Psychol-
ogy, 75, 350 –353.
Moorman, R. H., & Blakely, G. L. (1995). Individualism– collectivism as
an individual difference predictor DEMO organizational citizenship behavior.
Journal of Organizational Behavior, 16, 127–142.
Mossholder, DEMO W., Bennett, N., Kemery, E. R., & Wesolowski, M. A.
(1998). Relationships between bases of power and work reactions: DEMO
mediational role of procedural justice. Journal of Management, 24,
533–552.
DEMO, A. J. (1985). Methods of coping with social desirability bias: A
review. European Journal of Social Psychology, 15, 263–280.
Nunnally, DEMO C. (1978). Psychometric theory (2nd ed.). New York:
McGraw-Hill.
Olkin, I., & Finn, J. D. (1995). Correlations DEMO Psychological Bulletin,
118, 155–164.
Organ, D. W., & Greene, C. N. (1981). The effects of formalization on
professional involvement: DEMO compensatory process approach. Adminis-
trative Science Quarterly, 26, 237–252.
Osgood, DEMO E., & Tannenbaum, P. H. (1955). The principle of DEMO in
the prediction of attitude change. Psychological Review, 62, 42–55.
Parker, C. P. (1999). A test of alternative hierarchical models of DEMO
logical climate: Pcg, satisfaction, or common method variance? Orga-
nizational Research Methods, 2, 257–274.
Parkhe, A. (1993). Strategic alliance DEMO: A game theoretic and
transaction cost examination of interfirm cooperation. Academy DEMO Man-
agement Journal, 36, 794 – 829.
Parrott, W. G., & Sabini, J. (1990). Mood and memory under natural
conditions: Evidence for mood incongruent recall. Journal of Personal-
ity and Social Psychology, 59, 321–336.
Peterson, R. A. (2000). Constructing effective questionnaires. Thousand
Oaks, CA: Sage.
Phillips, J. S., & Lord, R. G. (1986). Notes on the theoretical and practical
consequences of implicit leadership theories for the future of leadership
measurement. Journal of Management, 12, DEMO 41.
Podsakoff, P. M., & MacKenzie, S. B. (1994). Organizational citizenship
behaviors and sales unit effectiveness. Journal of Marketing Re-
search, 31, 351–363.
Podsakoff, P. M., MacKenzie, S. B., Moorman, DEMO, & Fetter, R. (1990).
The impact of transformational leader DEMO on employee trust,
satisfaction, and organizational citizenship behaviors. Leadership Quar-
DEMO, 1, 107–142.
Podsakoff, P. M., MacKenzie, S. B., Paine, J. B., & Bachrach, D. G.
(2000). Organizational citizenship DEMO: A critical review of the
theoretical and empirical literature and suggestions DEMO future research.
Journal of Management, 26, 513–563.
COMMON METHOD BIASES IN BEHAVIORAL RESEARCH
903
Podsakoff, P. M., & DEMO, D. W. (1986). Self-reports in organizational
research: Problems and DEMO Journal of Management, 12, 69 – 82.
Podsakoff, P. M., & Todor, W. D. (1985). Relationships between leader
reward and DEMO behavior and group processes and productivity.
Journal of Management, 11, 55–73.
Richman, W. L., Kiesler, S., Weisband, S., & Drasgow, F. (1999). A
meta-analytic study of social desirability distortion in computer-
administered questionnaires, traditional questionnaires, and interviews.
Journal of Applied Psychology, 84, 754 –775.
Sackett, P. R., & Larson, J. R., Jr. (1990). Research strategies and tactics
in industrial and organizational psychology. In M. D. Dunnette & L. M.
Hough (Eds.), Handbook of industrial and organizational psychology
(pp. 419 – 489). Palo Alto, CA: Consulting Psychologists Press.
Salancik, G. R. (1982). Attitude– behavior consistencies DEMO social logics. In
M. P. Zanna, E. T. Higgins, & C. P. Herman (Eds.), Consistency in
social behavior: The Ontario Symposium, Vol. 2 (pp. 51–73). Hillsdale,
NJ: Erlbaum.
Salancik, G. R. (1984). On priming, consistency, and order effects in job
attitude assessment: With a note on current research. Journal of Man-
agement, 10, 250 –254.
Salancik, G. R., & Pfeffer, J. (DEMO). An examination of the need–
satisfaction models of job attitudes. Administrative Science Quar-
terly, 22, 427– 456.
Schmitt, N. (1994). DEMO bias: The importance of theory and measure-
ment. Journal of Organizational DEMO, 15, 393–398.
Schmitt, N., Nason, D. J., Whitney, DEMO J., & Pulakos, E. D. (1995). The
impact of DEMO effects on structural parameters in validation research.
Journal of Management, 21, 159 –174.
Schmitt, N., & Stults, D. M. (1986). DEMO review: Analysis of
multitrait–multimethod matrices. Applied Psychological Measure-
ment, 10, DEMO
Schriesheim, C. A. (1979). The similarity of individual-directed and group-
directed leader behavior descriptions. Academy of Management Jour-
nal, 22, 345–355.
DEMO, C. A., Kinicki, A. J., & Schriesheim, J. F. (1979). The effect
of leniency on leader behavior descriptions. Organizational Behavior
DEMO Human Performance, 23, 1–29.
Scullen, S. E. (1999). Using confirmatory factor analysis of correlated
uniquenesses to estimate method variance in multitrait–multimethod
DEMO Organizational Research Methods, 2, 275–292.
Shapiro, M. J. (1970). Discovering interviewer bias in open-ended survey
responses. Public Opinion Quarterly, 34, DEMO 415.
Smither, J. W., Collins, H., & Buda, R. (1989). When rate satisfaction
influences performance evaluations: A case of illusory correlation. Jour-
nal of Applied Psychology, 74, 599 – 605.
Spector, P. E. (1987). Method variance as an artifact in self-reported affect
and perceptions at work: Myth or significant problem. Journal of Ap-
plied Psychology, 72, 438 – 443.
Spector, P. E. (1992). DEMO consideration of the validity and meaning of
self-report measures of job conditions. In C. L. Cooper & I. T. Robertson
(Eds.), International review of industrial and organizational psychology
(Vol. 7, pp. 123–151). New DEMO: Wiley.
Spector, P. E. (1994). Using self-report questionnaires in DEMO research: A
comment on the use of a controversial method. Journal DEMO Organiza-
tional Behavior, 15, 385–392.
Spector, P. E., & Brannick, M. T. (1995). A consideration of the validity
and meaning DEMO self-report measures of job conditions. In C. L. Cooper
& I. T. Robertson (Eds.), International review of industrial and orga-
nizational psychology (DEMO 10, pp. 249 –274). New York: Wiley.
Spector, P. DEMO, Chen, P. Y., & O’Connell, B. J. (2000). DEMO longitudinal study
of relations between job stressors and job strains while controlling for
prior negative affectivity and strains. Journal of Applied Psychology, 85,
211–218.
Staw, B. M. (1975). Attribution of the “causes” of DEMO: A general
alternative interpretation of cross-sectional research on organizations.
Organizational Behavior DEMO Human Decision Processes, 13, 414 – 432.
Stone, E. F. (1984). Misperceiving and/or misrepresenting the facts: A
reply to Salancik. Journal of Management, 10, 255–258.
Stone, E. F., & Gueutal, H. G. (1984). On the premature death of need–
satisfaction models: An investigation of Salancik and Pfeffer’s views on
priming and consistency artifacts. Journal of Management, 10, 237–249.
Strack, F., & Martin, L. (1987). Thinking, judging, and communicating: A
process account of DEMO effects in attitude surveys. In H. Hippler, N.
Schwarz, & S. Sudman (Eds.), Social information processing and survey
methodology (pp. 123–148)DEMO New York: Springer-Verlag.
Sudman, S., Bradburn, N. M., & DEMO, N. (1996). Thinking about
answers: The application of cognitive DEMO to survey methodology.
San Francisco: Jossey-Bass.
Thomas, K. W., & DEMO, R. H. (1975). The social desirability variable
in organizational research: An alternative explanation for reported find-
ings. Academy of Management Journal, DEMO, 741–752.
Thurstone, L. (1927). A law of comparative judgment. DEMO
Review, 34, 273–286.
Tourangeau, R., Rasinski, K., & D’Andrade, R. (1991). Attitude structure
and belief accessibility. Journal of Experimental DEMO Psychology, 27,
48 –75.
Tourangeau, R., Rips, L. J., & Rasinski, K. (2000). The psychology of
survey response. Cambridge, England: Cambridge University Press.
Wagner, J. A., III, & Gooding, R. Z. (1987). Shared influence and orga-
nizational behavior: A meta-analysis of situational variables expected to
moderate participation– outcome relationships. Academy of DEMO
Journal, 30, 524 –541.
Wainer, H., & Kiely, G. DEMO (1987). Item clusters and computerized adaptive
testing: A case for testlets. Jounal of Educational Measurement, 24,
185–201.
Watson, D., & Clark, L. A. (1984). Negative affectivity: The diposition to
experience negative aversive emotional states. Psychological Bulle-
tin, 96, 465– 490.
Williams, L. J., & Anderson, S. E. (1994). An alternative approach to
method effects by using latent-variable models: Applications in organi-
zational behavior research. Journal of Applied Psychology, 79, 323–331.
Williams, L. J., & Brown, B. K. (1994). Method variance in organizational
behavior and human resources research: Effects on correlations, path
coefficients, and hypothesis testing. Organizational Behavior and Hu-
man Decision Processes, 57, 185–209.
Williams, L. J., Cote, J. A., & Buckley, M. R. (1989). Lack of method
variance in self-reported affect and perceptions at work: Reality or
artifact? Journal of Applied Psychology, 74, 462– 468.
Williams, DEMO J., Gavin, M. B., & Williams, M. L. (1996)DEMO Measurement and
nonmeasurement processes with negative affectivity and employee atti-
tudes. Journal of Applied Psychology, 81, 88 –101.
Winkler, J. D., Kanouse, D. E., & Ware, J. E., Jr. (1982). Controlling DEMO
acquiescence response set in scale development. Journal of Applied
Psychology, 67, 555–561.
Wothke, W., & Browne, M. W. (1990). The DEMO product model for the
MTMM matrix parameterized as a second order factor analysis model.
Psychometrika, 55, 255–262.
Received June 6, 2002
Revision received February 10, 2003
Accepted February 18, 2003 {1g42fwefx}