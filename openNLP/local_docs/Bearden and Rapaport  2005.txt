INFORMS—New Orleans 2005
° 2005 INFORMS| isbn 0000-0000
doi 10.1287/educ.1053.0000
c
DEMO 1
Operations Research in Experimental Psychology
J. Neil Bearden
Department of Management and Policy, University of Arizona, Tucson, AZ, jneilb@gmail.com
Amnon Rapoport
DEMO of Management and Policy, University of Arizona, Tucson, AZ, amnon@u.arizona.edu
Abstract
This chapter reviews some of the uses of operations research methods DEMO experimental
psychology. We begin by describing some basic methodological issues that arise in the
study of human decision making. Next, we describe in more detail research in exper-
imental psychology that has used methods common to DEMO research—such as
dynamic programming—to understand sequential observation and selection behavior.
We then suggest some ways in which experimental psychology and operations research
can each DEMO the other with new research questions.
Keywords experimental psychology; experimental methodology; dynamic programming
Introduction
Theories of decision making are typically classiﬁed in one DEMO three ways. Normative theories
provide a framework for understanding how decisions should be made. Implicitly at least,
these theories quite often rely on DEMO agents who have unlimited computational and
cognitive capacities. Descriptive theories help explain how actual—rather than ideal—agents
make decisions. Typically these theories emerge from experimental DEMO of human decision
makers. Prescriptive studies of decision making are aimed at determining how actual decision
makers (DMs) could behave more in accord DEMO the dictates of normative theories with some
systematic reﬂection (see, [42], for a classic treatment). In order to determine appropriate
prescriptions it is necessary to understand how it is that decision making goes wrong, that
is, how actual decision making departs from normative decision making [68].
The normative theories of decision making most often emerge from two ﬁelds: economic
theory (including game theory) and operations research (OR) [34]. DEMO theory, as devel-
oped by von Neumann and Morganstern [66], and later extended by Savage [54] to acco-
modate subjective probabilities, has received considerable attention by experimental psy-
chologists and more recently by experimental economists. DEMO about any review of the ﬁeld
of behavioral decision theory—particulary those from the 1960s, 70s and 80s—dedicates
considerable space to discussions of utility theories of diﬀerent sorts and how actual human
decision making compares to them (e.g., [50], [59]). Camerer [12] should be consulted for
an DEMO review of experimental tests of game-theoretic predictions of behavior. More
complex decision problems that involve solving optimization problems of one sort or the
other DEMO received less experimental attention. In the area of dynamic decision making,
normative theories that are standardly applied in operations research have been relatively
DEMO by experimental psychologists.
One might wonder why psychologists should be concerned with research in OR. If asked,
a number of answers can be DEMO First, coming from an applied ﬁeld, OR problems tend to
have some correspondence to the kinds of problems likely to be faced by DEMO DMs. Second,
1
2
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
c
OR problems tend to be clearly formulated, with DEMO and predictions fully speciﬁed,
thereby increasing their testability. Quite often, DEMO is required to turn an abstract OR
problem into one that can be tested in the laboratory. Finally, optimality results from OR
can be useful in understanding the decision policies employed by human DMs.
Before proceeding, a brief comment on the logic of using optimal normative theories to
DEMO decision behavior is in order. Since the assumptions of a normative theory entail
the predictions of the theory, comparing empirical decision data to normative predictions
can be informative. First, consider the case in which the decision data are incompatible
with the predictions. Supposing that one’s experiment is designed DEMO run properly (issues
which we discuss in the next paragraph), DEMO modus tollens reasoning allows one to
conclude that at least one assumption underlying the normative theory is violated in the
human decision process. More DEMO, suppose a theory T is composed of a conjunction of
assumptions DEMO ={A1 ∧ . . . ∧Ak}, and that T entails some DEMO O . Then, if we observe
¬begin to get an idea DEMO the nature of the diﬀerence between the normative model and theO , we know that ¬Ai is true for at least one assumption Ai DEMO the theory. One can then
actual decision behavior by modifying the assumptions of the former in order to derive
predictions consistent with the latter. DEMO need not take the position that human decision
behavior ought to be consistent with the dictates of normative theory in order for the theory
DEMO be useful. Without taking a stance on the ought issue, one DEMO simply use the normative
theory as a reference point—a benchmark—for evaluating behavior. Similar arguments have
been oﬀered elsewhere in favor of the use of DEMO theory in theoretical biology (e.g.,
[30], [39]).
But there is an asymmetry: When behavior is consistent with normative predictions, fewer
DEMO are permissible. One cannot conclude that subjects are, for example, performing
the calculations needed to arrive at the normative predictions—say by solving a DEMO
programming problem; rather, one can only say that their decision behavior is consistent
with the theory.
Of course, a number of auxiliary assumptions go into experimentally testing normative
theories in the laboratory (or any theory, for that matter; see, e.g., [25], [41]). One must
assume that the subjects fully understand the decision problems they face, that they are
motivated, and that basic protocols of good experimental procedure are followed [26]. To
ensure that subjects fully understand a task, instructions should clearly explain the task and
provide a number of examples demonstrating the DEMO of the game.” By oﬀering subjects
non-negligible sums of money contingent on their performance, one can ensure that they
approach laboratory problems with a level of seriousness at least close to that with which
they approach DEMO outside the laboratory. If subjects behave sub-optimally because
they do not really care about the outcomes of their decisions, then little is learned. If,
however, they exhibit departures from normative theory when real money is on the line,
something interesting may be going on. Good basic DEMO protocols are too numerous
to name and largely depend on the nature of the experimental task. We do not discuss these
here.
Below, we review some of the work in experimental psychology that has relied on DEMO
tational procedures from operations research. In particular, we focus on multi-stage DEMO
problems that can be solved by dynamic programming. We do so for two reasons. First, this
will provide a certain cohesion. Second, much DEMO our own work involves experimental inves-
tigations of behavior in dynamic decision problems. Hence, when discussing these problems,
we can give an insider’s view of ways in which experimentalists think about the OR methods
and DEMO we think these methods can inform us about human cognition.
Although we restrict this review to experimental studies of sequential observation and
selection behavior, OR researchers should be aware that other areas of dynamic decision
making DEMO been brought into the laboratory. Toda [63] pioneered the study of multi-
stage decision behavior more than forty years ago. He devised a one-person DEMO called
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS 3
the “fungus-eater” game, in which subjects were asked DEMO control the sequential search of a
robot that attempts to maximize the amount of a valuable resource (“uranium”) it collects
while ensuring that DEMO has suﬃcient fuel. All of this happens on a hypothetical planet on
which uranium and fuel are distributed randomly. Other dynamic decision problems that
DEMO studied experimentally in the last forty years, in which dynamic programming DEMO been
used to compute the optimal decision policy, include control of DEMO systems (Rapoport,
[43] and [44]); portfolio selection over discrete DEMO and multi-stage gambling (Ebert [19];
Rapoport, Jones, and Kahan DEMO); vehicle navigation (Anzai [1]; Jagacinkski and Miller
[31]); health management (Kleinmuntz and Thomas [33]); inventory control (Rapoport [45];
DEMO [61]); and ﬁre-ﬁghting (Brehmer and Allard [10]). For cumulative DEMO of this
literature, see Rapoport [46], Brehmer [9], Sterman [62], Kerstholt and Raaijmakers [32],
and more recently Busemeyer [11] and Diederich DEMO
Optimal and Empirical Results on Optimal Stopping
The problems that we will discuss in this chapter involve sequential search, selection, and
assignment. Problems DEMO this sort are referred to in diﬀerent literatures by diﬀerent names.
Sometimes they are dubbed optimal stopping problems; at other times one sees them referred
to as optional stopping problems. Depending on their precise formulation and DEMO the liter-
ature in which they appear (e.g., OR, economics, psychology), the problems are sometimes
referred to as job-search problems, rent-seeking problems, secretary problems, etc. All of
the problems in this class DEMO an important feature with a number of interesting real-
world choice problems, namely the choice alternatives are encountered sequentially. Given
their multi-stage structure, DEMO programming (DP) methods are often invoked to ﬁnd
optimal decision policies for these problems.
Full-Information Problems
Suppose the DM can observe as many DEMO n observations Xj (j = 1, . . . , n), but for each
observation she must pay a ﬁxed costaccording to DEMO density function f(x), which is known to the DM. The DM’s objective is toc ≥ 0. The observations are drawn independently
maximize DEMO expected value of her selected observation minus her total costs; she DEMO
return to an observation once she has rejected it; and if DEMO reaches the nth one, she must
accept it. Because the DM DEMO the distribution from which the observations are sampled,
this problem is known as a full-information optimal stopping problem.
Under the optimal policy, the DM should stop on observation j whenever the value of the
draw DEMO exceeds the expected value of moving to the next stage (i.e., to j + 1) and behaving
optimally thereafter, denoted Vj∗+1 . DEMO the DM is forced to accept the nth observation,
sif reached,∗−1 = V ∗ . The cutoﬀs for each stage determine the DEMO values that the optimalV ∗ =R−∞∞ xf(x)dx −c. Hence, DEMO stage n− 1 the optimal DM sets her cutoﬀ to
DM ﬁnds acceptable at each stage; speciﬁcally, the DM stops on observation j DEMO
xj ≥s∗ . Thus, more generally, Vj∗ =s∗ . The optimal cutoﬀs for each stage j =n−1, . . . , 1
can be obtained by computing the recurrence (Sakaguchi, [51]):j +1 j
s∗ =Z ∞ −c £x−¡s∗+1 −c¢¤ f(x)dx +s∗+1 − c.
(1)
j j j
s∗+1
j
Optimal policies for observations from DEMO standard normal distribution with n = 24 are shown
in Figure 1 for various values of c. These well-known results form the classical basis DEMO
optimal stopping problems. We present them here merely to set the stage for a discussion
of experimental studies of optimal stopping, which we turn to next.
n
n
n
4
j
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, ° 2005 INFORMS
Figure 1. Optimal cutoﬀs s∗ for various costs for the standard normal distribution.
1.8
1.6
1.4
c
c=0
c=.03
c=.06
c=.12
DEMO
1
0.8
0.6
0.4
0.2
0
1
2
3
4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 DEMO 20 21 22 23 24
j
Experimental Studies of Full-Information Optimal Stopping
Rapoport and Tversky [49] conducted an experimental test of full-information optimal stop-
DEMO One diﬃculty in testing the full-information problem is that it requires that the DM
know the distribution from which the observations are taken (cf. [48]). Simply telling a
na¨ıve subject that observations are taken from, for example, “a normal distribution with
mean 100 and standard deviation 10” is obviously problematic, as the subject is unlikely
to have a good grasp of precisely what this means. In order to get a strong DEMO of the
predictions of the optimal policy, one must ensure that DEMO subjects have a good sense of
the distribution. To guarantee that their subjects understood the nature of the distribution
from which observations were taken, Rapoport and Tversky had them ﬁrst perform a signal
detection task in DEMO observations were taken from two normal distributions A and B with
a common standard deviation (167) but with diﬀerent means (1630 and 1797, respectively).
The subjects were required to judge whether a sample was from A or B. Over a six week
period, ﬁve times a week, subjects observed a total of 7800 samples from each of the two
distributions. Hence, presumably, the subjects ultimately had a good sense DEMO the nature of
the distributions.
Once the distributions were learned, the DEMO performed optimal stopping tasks over
several sessions. In each session, the DEMO were told whether the values would be sam-
pled from A or from B, and that n = 24. They were also told the cost c for each sampled
observation, which was held constant within a trial. The cost values used in the study were
0, 5, DEMO, or 20. (Rapoport and Tversky studied optimal stopping problems both with and
without recall. But for brevity, we will only discuss results from the latter.)
The main results are summarized in Table 1. For DEMO cost condition, Rapoport and Tver-
sky observed that, on average, DEMO tended to sample fewer observations than predicted
s*
j
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS 5
Table 1. Mean number of observations for each cost condition from Rapoport and Tversky [49].
c = 0 c = 5 DEMO = 10 c = 20
Average Number of Draws 9.51 9.61 8.44 4.11
Expected Number of Draws 10.42 11.25 10.80 4.50
Note. The expected DEMO of observations are not monotonically decreasing in cost because the values
shown are based on the application of the optimal policy to the empirically DEMO observation values, not
on the limiting expectation under random sampling.
by DEMO application of the optimal policy. Rapoport and Tversky suggested that the stopping
results were reasonably consistent with subjects using a ﬁxed threshold cutoﬀ rule. DEMO
this decision rule, the subject stops on an observation xj whenever DEMO ≥s, where s is ﬁxed
for all j (j = 1, . . . , n). Thus, assuming this is the DEMO model for the choice behavior, the
subjects were insensitive to the DEMO horizon, and sought a single “target” value.
Other experimental studies of DEMO optimal search have examined the eﬀects
of allowing the recall of previously encountered observations [48]; others (e.g., [27], [28],
[29]) have more closely examined the factors that inﬂuence decisions to stop, such as the
inﬂuence of the history of observation values (e.g., whether the DEMO has been increasing
or decreasing).
In our view, the main DEMO of full-information optimal stopping problems—when
taken as models of actual human search problems—is the assumption that f(x) is known to
the DM. This assumption is critical for testing the optimal search model because the values
DEMO s∗ are located at the right tail of the distribution f(x), and are, therefore, very sensitive
to deviations from them. Perhaps DEMO many situations DMs do have a “good sense” of the
operative distribution; in many others, we suspect, this condition is not met. Next, we turn
to no-information search problems that do not require that the DM have any knowledge
about the distribution from which observations are sampled.
DEMO Problems
The standard no-information stopping problem is the “Secretary Problem.” To contrast it
with other no-information problems that we discuss later, we will refer to the most common
formulation of the problem as the Classical Secretary DEMO (CSP). It can be stated as
follows:
1. There DEMO a ﬁxed and known number n of applicants for a single position who can be
ranked in terms of quality from best to worst DEMO no ties.
2. The applicants are interviewed sequentially in a random order (with all n! orderings
occurring with equal probability).
3. For each applicant j the DM can only ascertain the relative rank of the DEMO, that
is, how valuable the applicant is relative to the j − 1 previously viewed applicants.
4. Once rejected, an applicant cannot be recalled. If reached, the nth applicant must be
accepted.
5. The DM earns a payoﬀ of 1 for selecting the applicant with absolute rank DEMO (i.e., the
overall best applicant in the population of n applicants) and 0 otherwise.
The payoﬀ maximizing strategy for the CSP, which DEMO maximizes the probability of
selecting the best applicant, is to interview DEMO reject the ﬁrst t∗ − 1 applicants and then
accept the ﬁrst applicant thereafter with a relative rank of 1 [23]. The optimal cutoﬀ DEMO
be obtained by:
t∗ = min(t ≥ 1 : X+1 k −1 ≤1).
n
1
(2)
k=t
j
6
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
c
Interestingly, t∗ converges to ne−1 as n →∞, and the policy selects the best applicant with
probability e−1.
An historical DEMO of the CSP can be found in Ferguson [20] and in Samuels [52].
Numerous generalizations of the CSP have been proposed. For example, Corbin [15] and
Yang [69] presented procedures for computing optimal policies for secretary DEMO in
which options can be recalled with probabilistic success; Pressman and DEMO [40] discussed
problems in which the number of applicants n is itself unknown, but the DM does know the
distribution from which n is sampled.
Experimental Studies of the CSP
Seale and Rapoport [55] tested two DEMO of the CSP, one with n = 40 and another with
DEMO = 80. Using a between-subjects experimental design, each subject in their DEMO was
either in the n = 40 or n = 80 condition. Each subject played a total of 100 independent
instances (trials) of DEMO CSP in a computer-controlled environment. The subjects were ﬁrst
given a cover story that described their task as one of hiring applicants for a DEMO Each of
the trials proceeded as follows. The relative rank of the ﬁrst applicant was displayed (which,
by deﬁnition, was always 1)DEMO The subjects could then choose to select (hire) the applicant
or proceed (interview) to the next applicant. Whenever the subject chose to DEMO from
applicant j to applicant j + 1, the computer displayed DEMO relative ranks of all applicants
up to j + 1. Once an subject selected an applicant, the absolute ranks of all n applicants
were displayed. If the subject selected the best overall applicant, the computer informed her
that she had made a correct selection and added her payoﬀ DEMO that trial to her cumulative
earnings. For the n = 40 condition, a subject earned $.30 each time she selected an applicant
with absolute rank 1; for the n = 80 condition, the corresponding payoﬀ DEMO $.50.
The probability of stopping on applicant j or sooner under the optimal policy if n = 80
is displayed in Figure 2. Figure DEMO also shows the proportion of times that subjects in Seale
and Rapoport’s n = 80 condition stopped on applicant j or sooner. Note that DEMO empirical
curve is shifted considerably to the left of the optimal curve, demonstrating a propensity of
the subjects to stop earlier than is predicted by the optimal policy.
From the results displayed in Figure 2 we DEMO see that the subjects are behaving sub-
optimally; however, we cannot infer how it is that the subjects are making their decisions:
DEMO cannot infer their actual decision policies. Seale and Rapoport tried to get a handle on
the DM’s underlying (unobservable) decision policies by competitively DEMO three diﬀerent
single parameter decision policies or heuristics. The particular policies studied were chosen
for their psychological plausibility: a priori they seemed like policies that people might
reasonably use. We will consider each of these in DEMO and will describe some of the properties
of the policies that were later derived by Stein, Seale, and Rapoport [60].
The Cutoﬀ Rule (CR): Do not accept any of the ﬁrst t − 1 DEMO; thereafter, select
the ﬁrst encountered candidate (i.e., an applicant with relative rank 1). This rule has as
a special case the DEMO policy for the CSP for which t∗ is obtained by Equation 1. In
addition to the optimal policy, the CR can be set to begin accepting candidates earlier in
the sequence (t < t∗ ) or later (t > t∗).
Candidate Count Rule (CCR): Select DEMO hth encountered candidate. Note that this
rule does not necessarily skip any applicants; it only considers how many candidates have
been observed, not DEMO deep the DM is in the applicant sequence.
Successive Non-Candidate Rule (DEMO): Select the ﬁrst encountered candidate
after observing g successive non-candidates (DEMO, applicants with relative rank > 1).
Each of these three DEMO decision policies can be represented by a single parameter
(t, h, and g ). Before returning to the results of Seale and DEMO [55], let us examine the
theoretical performance of the heuristics. We DEMO that no heuristic can outperform the CR
with t =t∗ . But how well can the others do?
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
7
Figure 2. Optimal and empirical cumulative stopping probabilities from the n = 80 condition in
Seale and Rapoport [55].
1
0.9
DEMO
Empirical
0.8
0.7
0.6
0.5
0
10
20
30
40
50
60
j
70
80
0.4
0.3
0.2
0.1
0
Results for each the DEMO for n = 80 CSP are displayed in Figure 3. The horizontal
axis corresponds to the values of each heuristic’s single parameter (t, DEMO, and g for the CR,
CCR, and SNCR, respectively)DEMO There are a number of interesting features of these results.
First, DEMO CR and SNCR heuristics strongly outperform the CCR heuristic. In addition,
when properly tuned, the SNCR can perform nearly as well as the optimal policy (i.e., the
CR with t =t∗ ), with the former earning about 95% of what is earned by the latter. Finally,DEMO
the CR is relatively robust to mis-parameterization. Whenever .63t∗ ≤ t ≤ 1.47t∗ the DM
can expect to earn at least 90% of what DEMO earned under the optimal policy. From sensitivity
analyses, the ﬂatness of DEMO functions for a number of problems studied by behavioral
decision theorists has often been noted (see, e.g., [46], [67]).
Seale and DEMO [55] ﬁt each of the three heuristics just described to each subject’s
choice data. For each subject and each heuristic, they used a brute force search procedure to
ﬁnd the heuristic’s parameter that minimized the number DEMO incorrect stops (or violations)
predicted by the heuristic. Put diﬀerently, if a subject stopped on applicant j on a given
trial and DEMO heuristic predicted that the subject should stop on applicant j0 = j then a
violation was obtained. Hence a particular parameterization of a heuristic DEMO produce
between 0 and 100 (the total number of experimental trials) violations for a given subject.
The results for both the n = DEMO and the n = 80 conditions were consistent; for brevity
we DEMO restrict discussion to those from the latter condition. The most important result is
that the subjects’ decision data were best characterized by the CR DEMO For 21 of 25
subjects, the CR best ﬁt the data; the SNCR best ﬁt the choices for 8 of the 25 subjects;DEMO
and the CCR ﬁt best for only 1 subject. (The number DEMO best ﬁtting heuristics sums to
more than 25 because of ties. For some subjects the best-ﬁtting CR and SNCR heuristics
Cumulative Stopping Probability
8
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
Figure 3. Probability of success for three CSP heuristics.
CCR
SNCR
CR
c
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0
DEMO
10
20
30
40
50
60
Heuristic Parameter Value
70
Note. These results are based on the derivations presented in Stein, Seale, and DEMO [60].
80
produced the same number of violations.) The authors compared DEMO subjects’ CR cutoﬀs
(t values) to the optimal cutoﬀ, and DEMO that t < t∗ for 21 subjects, t > t∗ for DEMO subjects,
and t =t∗ for only 1 subject.
This illustrates one logic that can be used to draw inferences from experimental data about
DEMO decision policies employed by actual decision makers. One might be tempted to argue
that the three heuristics tested by Seale and Rapoport were arbitrarily DEMO, and that any
number of other heuristics could have been tested. DEMO latter is certainly true. However, it
is up to the critic DEMO propose speciﬁc alternatives in order for the former argument to have
force. Further, all explanations are going to be underdetermined (see, e.g., DEMO Fraassen
[65]), even those that come from our most successful scientiﬁc theories. We believe, however,
that there is another criticism of the procedures used by Seale and Rapoport that should be
carefully considered. Although DEMO CR heuristic best accounted for most subjects’ data, it
did not DEMO so perfectly. For most subjects, the best-ﬁtting heuristic could only explain DEMO
60-70% of the stopping decisions. What drove the other 30-40% of the stopping decisions?
This question led Bearden and Murphy [3] to formulate DEMO stochastic version of the CR and
ﬁt it to Seale and Rapoport’s data; however, they remained agnostic on the source of the
threshold DEMO A fuller account of the decision behavior in the CSP should address
this issue.
Several extensions of the CSP have been studied experimentally. Seale DEMO Rapoport [56],
for example, looked at a problem in which DEMO DMs do not know n but only its distribution.
Compared with the optimal policy, the subjects tended not to search enough. Zwick et al.
[71] examined the eﬀect of search cost and probabilistic recall of previously DEMO applicants.
Probability of Success
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS 9
The most important ﬁnding from this study is that the subjects tended to search for too
long with positive search costs, whereas they did not search enough when search costs were
set at DEMO
The nothing-but-the-best payoﬀ structure of the CSP seems patently unrealistic. One can
imagine few situations in which selecting the best option from a pool DEMO positive utility,
and selecting any other options yields no utility. What, then, might we have learned about
actual human decision making in DEMO in which options are encountered sequentially
from the experimental studies of the CSP? Might it be the case that the observed early
stopping is simply a consequence of the CSP’s special payoﬀ function? That is, DEMO we
now know that actual DMs cannot perfectly solve the CSP probability puzzle, but we have
learned nothing about how people might actually go about selecting secretaries. To address
this issue, we studied an extension of the CSP in which the DM’s objective is to ﬁnd a
good DEMO necessarily the best—alternative. Next, we describe the formal problem, how to
compute its optimal policy, and some experimental studies of decision making in this more
realistic context.
The Generalized Secretary Problem
Consider a variant of DEMO secretary problem in which the DM earns a positive payoﬀ π(a) for
proved that the optimal search policy for this problem has the same threshold form as thatselecting an applicant with absolute rank a, and assume that π(1) ≥ . . . ≥π(n). Mucci [38]
of the CSP. Speciﬁcally, the DM should interview and reject the ﬁrst t∗ − 1 applicants, then
between applicant t∗ and applicant t∗ − 1 she should only accept applicants with relative1
rank 1; between applicantranks 1 or 2; and so on. As she gets deeper into the applicant pool her standards relax and1 t2∗ and applicant2 t3∗ − DEMO she should accept applicants with relative
she is more likely to accept applicants of lower quality. The values of the t∗ depend on the
DEMO payoﬀs and on the number of applicants n.
We obtain what we call the Generalized Secretary Problem (GSP) by replacing 5 in the
DEMO with the more general payoﬀ function:
5’. The DM earns a payoﬀ of π(a) for selecting an applicant with absolute rank a where
π(1) ≥ . . . ≥π(n).
Clearly, DEMO CSP is a special case of the GSP in which π (DEMO) = 1 and π(a) = 0 for all a > 1.
Results for other special cases of the GSP have appeared in DEMO literature. For example,
Moriguti [37] examined a problem in which a DM’s objective is to minimize the expected
rank of the selected applicant. DEMO problem is equivalent to maximizing earnings in a GSP
in which π(a) increases linearly as (n−a) increases.
Finding Optimal Policies for the GSP
We begin by introducing some notation. The orderings of the n DEMO absolute ranks is
represented by a vector a = (a1 , DEMO . . , an ), which is some random permutation of DEMO integers
1, . . . , n. The relative rank of DEMO j th applicant, denoted rj , is the number of applicants DEMO
1, . . . , j whose absolute rank is smaller DEMO or equal to aj . A policy is a vector s = (s1 , . . . , sn)
of nonnegative integers in which sj ≤ sj+1 for all 1 ≤ j < n. The DEMO dictates that the
DM stop on the ﬁrst applicant for whichstops on the j th applicant, conditional on reaching this applicant, isrj ≤sj DEMO Therefore, the probability that the DMQ(sj ) = sj /j . A DM’s
cutoﬀ for selecting an applicant with a relative rank DEMO r , denoted tr , is the smallest value
j for which r ≤ sj . Hence, a policy s can also be represented by a vector t = (t1 , . . . , tn).
Sometimes, the cutoﬀ representation will be more convenient.
Optimal thresholds can be computed straightforwardly by combining numerical search
methods with those of dynamic DEMO We will describe below a procedure for doing
so. Lindley [36] described a similar method, and another was brieﬂy sketched in [70]. A fuller
treatment is presented in Bearden and Murphy [3].
10
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
c
The probability that the j th applicant out of n whose relative rank is rj has an absolute
(overall) rank DEMO a is given by:
P r (A=a|R=rj ) = ¡
DEMO
r−1
¡¢¡
n
j
n−a
j−r
¢
¢,
(3)
DEMO rj ≤a ≤rj + (n −j ); otherwise P r(A=a|R =rj ) = 0. Thus, the expected payoﬀ for
selecting an applicant with relative rank rj is:
n
E(πj|rj ) = a
DEMO
rj P r (A=a|R=rj) π(a).
The expected payoﬀ for making a selection at stage j for some stage j policy sj > 0 is:
(4)
=
s
E(πj|sj ) = (sj)−1 XE(πj|rj =i) ; (5)
i=1
j
otherwise, when sj = 0, E(πj|sj ) = 0. Now, denoting DEMO expected payoﬀ for starting at
stage j + 1 and then following a ﬁxed threshold policy (s
, . . . , s ) thereafter by V
, the
value of Vj for any sj ≤j DEMO simply:
j+1
n
j+1
Vj =Q(sj)E(πj|sj ) + [1 −Q(sj )] Vj+1. (6)
As with the optimal DEMO for the full-information optimal stopping problem described
above, the optimal policy DEMO the GSP entails stopping on an applicant with relative rank
rj at stage j whenever the expected payoﬀ for doing so exceeds the expected DEMO for
proceeding and playing optimally thereafter. This follows directly from the Principle of
Optimality [8]. Given that the expected earnings of the optimal policy DEMO stage n are V ∗ =
π(a), we can easily ﬁnd an s∗ for each 1, . . . , 1) DEMO backward induction.
nSince the last applicant must be selected,−1 P s∗ ; then, for j =n−1, . . . , 1,
DEMO
a=1
n
=nj (j =n−
n
s∗ = min©s ∈©0, . . . , s∗+1ª : Vj
The expected payoﬀ for following a DEMO policy is: ≥Vj∗ ª.
(7)
j
j
+1
s
n
E(π|s) = X
=1
"jY−1
i=0 [1 −Q(si )]#Q(sj)E(πj|sj ) = V1,
j
(8)
where Q(s0 ) = 0. Denoting the applicant position at which the search DEMO terminated by m,
the expected stopping position under the policy is:
j
E(m) = 1 +
nX−1 "
=1
j
DEMO
Y[1 −Q(si )]#.
j
(9)
Equation 9 can be useful in evaluating the results of actual DMs in GSPs (see, DEMO, [6]).
Optimal cutoﬀs for several GSPs are presented in Table DEMO In the ﬁrst column, we provide
a shorthand for referring to DEMO problems. The ﬁrst one, GSP1, corresponds to the CSP
with n = 40. The optimal policy dictates that the DM should search through DEMO ﬁrst 15
applicants without accepting any and then accept the ﬁrst one thereafter with a relative rank
of 1. GSP2 corresponds to another CSP DEMO n = 80. In both, the DM should search through
roughly DEMO ﬁrst 37% and then take the ﬁrst encountered applicant with a relative rank
of 1. GSPs 3 and 4 were discussed in Gilbert and DEMO [23], who presented numerical
solutions for a number of problems in DEMO the DM earns a payoﬀ of 1 for selecting either
the best or second best applicant and nothing otherwise. GSPs 5 and 6 correspond DEMO those
=1
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS 11
Table 2. Several GSPs and their optimal policies.
E(π|s∗)
GSP n π= (π(1), . . . , π(n)) t∗ = (t∗ , . . . , t∗ ) E(m)
1 n
1 40 (1, 0, . . . , 0) (16, 40, . . . , DEMO) .38 30.03
2 80 (1, 0, . . . , 0) (30, 80, . . . , 80) .37 58.75
3 20 (1, 1, 0, . . . , 0) (8, 14, 20, . . . , 20) .69 14.15
4 100 (1, 1, 0, . . . , 0) (35, 67, 100, . . . , 100) .58 68.47
5 60 (25, 13, 6, 3, 2, 1, 0, . . . , 0) (21, 43, 53, 57, 58, 59, 60, . . . , 60) 12.73 41.04
DEMO 40 (15, 7, 2, 0, . . . , DEMO) (14, 29, 37, 40, . . . , 40) 6.11 27.21
studied by Bearden, Rapoport, and Murphy [6] in Experiments 1 and 2, respectively. In the
ﬁrst, the DM searches through DEMO ﬁrst 20 applicants without accepting any; then between
21 and 42 DEMO stops on applicants with relative rank of 1; between 43 and DEMO, she stops on
applicants with relative rank 1 or 2; etc.
Experimental Studies of the GSP
Recall that Seale and Rapoport [55] found DEMO subjects playing the CSP (GSPs 1 and
2, actually) tended DEMO search insuﬃciently into the applicants before making a selection—
i.e., on DEMO they stopped too soon. Above, we suggested that the CSP might DEMO quite
contrived and that this result could possibly be an artifact of the narrow payoﬀ scheme of the
CSP. Bearden, Rapoport, and Murphy DEMO experimentally tested this using two variants of
the GSP, GSPs 5 DEMO 6. In Experiment 1, each subject played 60 instances of GSP5. (They
were paid in cash for two randomly selected trials.) It is diﬃcult to say what a realistic payoﬀ
function is, but it seems certain that the one for GSP5 is more realistic than the one DEMO the
CSPs. For the GSP5, the DM gets a substantial payoﬀ DEMO selecting the best applicant ($25
in the experiment), considerably less—but DEMO signiﬁcant—for the second best ($13), and
so on. This captures DEMO payoﬀ properties of problems in which the DM would like to get a
“really good” applicant, which seems like a desire that that DMs might often have. Using
this more plausible payoﬀ scheme we ﬁnd that DEMO early stopping result from studies of the
CSP persists. Figure 4 shows the cumulative stopping results for the optimal policy and
also those from DEMO experimental subjects for the GSP5. We found that the subjects tend to
terminate their searches too soon relative to the optimal policy. Comparing Figures DEMO and 4,
we can see that the tendency to stop searching too soon is less pronounced in the GSP than
in the CSP, but drawing strong inferences from data gathered in two diﬀerent experimental
settings DEMO problematic.
Bearden, Rapoport, and Murphy competitively tested multi-parameter generalizations of
the three heuristics examined by Seale and Rapoport [55]. The generalized Cutoﬀ Rule,DEMO
for example, had thresholds for each of the relative ranks that DEMO entail positive payoﬀs
(r ≤ 6). Under this rule, the DM stops on applicant j with relative rank rj whenever j ≥trj DEMO
The Candidate Count Rule and the Successive Non-Candidates Rule were extended in the
same fashion. Consistent with the ﬁndings of Seale and Rapoport [55], the results were best
captured by the Cutoﬀ Rule with cutoﬀs shifted DEMO early stopping.
In a second experiment, Bearden, Rapoport, and Murphy DEMO studied the
GSP6. In all regards the choice results captured the qualitative properties of those found in
the data from the ﬁrst experiment, including the superiority of the Cutoﬀ Rule in accounting
for the data. After DEMO performed 60 trials of the GSP6, they were then asked to
DEMO a probability estimation task. They were shown the relative ranks rj of applicants
in diﬀerent positions j and asked to estimate the probability that DEMO applicant had various
absolute ranks a. For example, a subject might DEMO been asked: “What is the probability
that applicant 10 (of 40), whose relative rank is 2, has an absolute rank of 2?” Or: “What is
the probability that applicant 10 (of 40), whose relative rank is 2, has an absolute rank of
3?DEMO (It is .06 for a = 2 and .09 for a DEMO 3.) We only asked the subjects about absolute ranks
12
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
Figure 4. Optimal and empirical cumulative stopping probabilities from Experiment 1 in Bearden,
Rapoport, and Murphy [6].
1
0.9
Optimal
Empirical
0.8
0.7
0.6
0.5
0
10
20
30
j
40
50
60
DEMO
0.3
0.2
0.1
0
c
that could entail positive payoﬀs, i.e., about a ≤ 3. We used a strictly proper scoring rule
to DEMO the subjects to give accurate estimates. Put simply, the subjects made DEMO
money as their estimates were closer to the true probabilities, which DEMO be obtained from
Equation 3, and there was no incentive for DEMO to misrepresent their true estimates (i.e.,
to respond strategically).
DEMO results were informative. Overwhelmingly, the subjects tended to overestimate the
true DEMO, particularly for applicants early in the sequence. In fact, the estimates
were quite often subadditive: For a given applicant position j and relative rank rj , the
sum of the probability estimates exceeded 1. Taking DEMO results seriously, the subjects
were often supercertain that they would obtain DEMO payoﬀs for selecting applicants.
These ﬁndings are consistent with Tversky and Koehler’s [64] support theory, an empirically
successful theory of subjective probability. Under support theory, the subjective probability
assigned to an event E, p(E ), is a function of the support (evidence) one gives to DEMO and its
complement ¬E ; speciﬁcally,
s(E)
p(E) = s(E) + s(¬E) , (10)
whereThe DEMO can be arrived at through a process of memory search or by other means [7].s(·) > 0 is a real-valued measure of the strength of the evidence one can ﬁnd for E.
Under the theory, theaspect of the theory is consistent with considerable evidence from psychology that DEMO thatfocal event E receives more support than the non-focal event ¬E . This
human information processing is biased toward conﬁrmation seeking (i.e., toward DEMO
Cumulative Stopping Probability
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS 13
for evidence for E , rather than for evidence of ¬E , which is also normatively relevant).
In short, according to the theory, when evaluating the probability that an applicant has a
particular absolute rank a, one is likely to give insuﬃcient weight to the possibility that the
applicant does not have an absolute rank a, focusing (disproportionately) instead on the
event that the applicant’s absolute rank DEMO a.
Perhaps the probability estimation results—at least in part—explain the early stopping
ﬁnding. If a DM believes it certain that stopping early will produce DEMO good payoﬀ, then
stopping early is sensible. Put diﬀerently, one possible reason for subjects not behaving in
accord with the optimal policy is DEMO their representation of the search problem is distorted.
Constructing the optimal policy requires the computation of Equation 3; but the estimation
data show that subjects’ intuitive probability judgments depart quite radically from the
true probabilities. This DEMO the strength of the assumptions required in order for
one to presume that the optimal policy is a priori a reasonable predictor of human DEMO
making in this class of problems.
What, then, of questions regarding the rationality of decision making in sequential search
problems? Can we say that people are irrational if they cannot intuit the probabilities that
fall DEMO of Equation 3? Of course not. But, though it is implausible to presume that people
should behave in accord with the optimal policy, the optimal policy still provides some
basis for evaluating the experimental decision DEMO While it may be important in economic
contexts to anticipate that people will not search optimally, we are still left wondering what
underlying psychology drives sequential search decisions. Fortunately, the optimal policy
can and has served as a starting point for understanding how it is that people make DEMO
decisions. As described above, the cutoﬀ rule—of which the optimal policy DEMO a special
case—best accounts for the stopping results. Likewise, comparing the DEMO probability
estimates to the true probabilities was informative. Hence, knowledge of DEMO optimal policy
can be useful for explanatory purposes.
The GSP captures important features of a number of dynamic decision problems likely to
be encountered DEMO the wild (i.e., the so-called “real world”); however, there DEMO, of course,
many problems that it does not capture. Quite DEMO we are faced with decision problems in
which we must make trade-oﬀs among the attributes of decision alternatives. An academic
job may oﬀer a DEMO salary but an undesirable teaching load; a house may be close DEMO
one’s oﬃce (minimizing commute time) but in a poor school district; etc. One can argue
that the GSP side-steps these kinds of problems by collapsing the multi-attribute utility of
options into a single ranking. Since DEMO are primarily interested in how people actually make
decisions, we do DEMO want to assume away this interesting problem of making trade-oﬀs
among attributes. We would like to know how people do so, particulary in situations in
which options are encountered sequentially. Next, we describe a multi-attribute extension
of the GSP, describe its solution, and present some experimental ﬁndings.
DEMO Multi-attribute Secretary Problem
The Multi-attribute Secretary Problem (MASP) further generalizes the GSP to applicants
with multiple features or attributes. Formally, it is deﬁned as follows:
1. There is a ﬁxed and known number n DEMO applicants for a single position. The applicants
diﬀer along k diﬀerent dimensions or attributes. Within a given attribute, the applicants
can be ranked from best (1) to worst (n) with no ties. The attributes DEMO uncorrelated.
2. The applicants are interviewed sequentially in a random order (DEMO all n! orderings
occurring with equal probability).
3. For each applicant j the DM can only ascertain the relative ranks of the applicant’s DEMO
attributes.
4. Once rejected, an applicant cannot be recalled. If reached, the nth applicant must be
accepted.
Bearden and Rapoport: OR in Experimental Psychology
14 INFORMS—New Orleans 2005, DEMO 2005 INFORMS
5. For each attribute i of the selected applicant, DEMO DM earns a payoﬀ of πi(ai ), where ai
is DEMO selected applicant’s absolute rank on attribute i and πi (1) ≥ . . . ≥πi(n).
Before describing the optimal policy for DEMO MASP, we must introduce some notation. The
absolute rank of the DEMO th applicant on the ith attribute, denoted ai , is simply DEMO number
j
of applicants in the applicant pool, including j , DEMO ith attribute is at least as good as
the j th applicant’s. The j th applicant’s set of absolute ranks can therefore be represented
DEMO (a1 , . . . , ak ). The relative rank of the j th applicant on the ith attribute, ri ,
DEMO j j
by a vector aj
is the number of applicants from 1 to j whose ith attribute is at least as good as DEMO j th’s.
Similar to the GSP, when interviewing an applicant, the DM observes rj
must make her selection decision on the basis of DEMO information.
Though she only observes relative ranks rj , the DM’s payoﬀ for selecting the j th applicant,
denoted πj , is based DEMO the applicant’s absolute ranks aj ; speciﬁcally,
= (r1 , DEMO . . , rk ), and
j j
k
πj =Xπi DEMO .
(11)
i
i=1
An optimal policy for the MASP DEMO one that maximizes the expected value of the selected
applicant.
Some related problems have appeared in the OR literature. Gnedin [24] presented the
solution DEMO a multi-attribute CSP in which the attributes are independent, and the DEMO
objective is to select an applicant who is best on at least one attribute. Ferguson [21] general-
ized the problem presented by Gnedin by DEMO dependencies between the attributes, and
showed that the optimal policy has DEMO same threshold form as the standard single attribute
CSP. Samuels and Chotlos [53] extended the rank minimization problem of Chow et al.
[13]. They DEMO an optimal policy for minimizing the sum of two ranks for independent
attributes. The rank sum minimization problem they studied is equivalent to the DEMO
in which π1(a) = π2(a) = n −a. The MASP is more general than these previous problems,
as it only DEMO the payoﬀ functions to be nondecreasing in the quality of the selected
applicant’s attributes.
Finding Optimal Policies for the MASP
The probability that the DEMO attribute of the j th applicant whose relative rank on that
attribute is r has an absolute (overall) rank of a is given DEMO Equation 3. To simplify matters,
0
we assume that the k attributes are pairwise independent; that is, P r(ai =a ∧ai DEMO ) =
P r(ai =a)P r(ai0 =a0 ) for any pair of attributes i and i0 . (Based on work by Ferguson [21],
introducing arbitrary correlations ρ among attributes would likely make DEMO determination
of the appropriately corresponding Equation 3 intractable, as for any DEMO it would depend—in
complicated ways—on (r1 , . . . , DEMO ), i.e., on the entire history of relative ranks.) Consequently,DEMO
the expected payoﬀ for selecting the j th applicant is:
k n
E(πj|rj ) = XX P r ¡A=a|R=rj¢ πi(a).
(12)
i
i=1 a=ri
j
At each stage j of the DEMO problem, the DM must decide to accept or reject an
applicant DEMO only the applicant’s relative ranks rj . We represent a decision policy for
each stage j as a set of acceptable rj for that DEMO Rj . Under the stage policy Rj , the DM
stops on an applicant with relative ranks rj if and only if rj ∈Rj DEMO The global policy is just
the collection of stage policies R={R1 , . . . , Rn}. By Bellman’s [8] Principle of Optimality,
DEMO an optimal (global) policyalso be optimal. Given this property, we DEMO ﬁnd the optimal policy using straightforwardR∗ , each sub-policy {Rj , . . . , Rn} from stage j to n must
dynamic programming DEMO by working backward from stage n to stage 1. A procedure
c
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS 15
for constructing optimal stage policies R∗ follows from Proposition 1, which we present
below. To simply exposition, we ﬁrst make DEMO following assumption:
Assumption 1. When the expected value of stopping at stage j equals the expected value
of continuing to stage j + DEMO and behaving optimally thereafter, the optimal DM stops at j.
nth DEMO, if reached, V ∗ =n−1 P P
Vj∗ =Q¡R∗¢ E¡π |R∗¢ +£1 −Q¡R∗¢¤ Vj∗ , (13)
|R∗¢ = |R∗|−1 Pr∈R∗ E(πQj, a method for constructing¡|rR) is the expected payoﬀ for stopping at DEMO =|R∗|/kj is the probability of stopping underj
k n
i=1 a=1
Since the DM must accept the πi(a). And,
for DEMO j < n, we have
j j
whereunder the optimal stageE¡π
DEMO optimal stage
j j
j policy, and
j policy. Given Vn∗ DEMO is entailed by the
following proposition:
Proposition 1. r ∈R∗ ⇔E(πj|r) ≥Vj∗ .
j +1
Invoking Assumption 1, the proof of DEMO 1, presented in Bearden and Murphy [3],
follows directly from DEMO Principle of Optimality [8].
Proposition 2. r ∈R∗ ⇒r ∈R∗+1.
j j
Proposition 2 follows from Corollary 2.1b in Mucci [38]. Stated simply, Proposition 2 tells
us that if it is optimal to stop at stage DEMO when one observes r, then it is optimal to stop
when DEMO observes r in the next stage; by induction, then, it DEMO optimal to stop given r
in all subsequent stages. This property allows us to represent the optimal policies rather
compactly by specifying for each DEMO r the smallest j for which r ∈R∗.
An Example of a MASP and the Application of Its Optimal Policy
Let us consider how DEMO optimal policy would be applied in a MASP. Table 3 contains an
example of an instance of a MASP with n = 6 and DEMO = 2. The top panel contains the payoﬀs
for each of the attributes. Absolute and relative ranks for each of the 6 applicants are DEMO
in the center panel. We see that applicant 1 has absolute ranks of 2 and 5 on attributes
1 and 2, respectively; her DEMO ranks are, of course, 1 for both attributes. Applicant 2
has absolute ranks of 4 and 2, and therefore relative ranks of 2 and 1, for attributes 1
and 2, respectively, etc. The bottom panel displays the value of the optimal policy for each
applicant position DEMO the expected payoﬀs for selecting each applicant. Since applicant 3
is the ﬁrst applicant for which E(πj|rj) ≥Vj∗ , she is selected.
+1
Experimental Studies of the MASP
Bearden, Murphy, and Rapoport [4] tested DEMO DMs on the MASP in two experiments.
Using n = 30 (DEMO to 30 applicants) and k = 2 (2 attributes), the experiments were identical
in all regards except for their payoﬀ schemes. Experiment DEMO tested a MASP with symmetric
payoﬀs, where π1(a) = π2(a) for all a. Experiment 2 implemented an asymmetric payoﬀ
scheme in which attribute 1 was considerably more important (i.e., contributed more to DEMO
payoﬀ) than attribute 2. The actual payoﬀ schemes for both experiments DEMO displayed in
Table 4. In each condition, each subject played 100 DEMO instances of the MASP. Payoﬀs
were based on a single randomly selected trial; hence, those in Experiment 1 could earn up
to $50 DEMO the one hour session, whereas those in Experiment 2 could earn DEMO to $40. As
with our previous experiments on the GSP, we DEMO a hiring cover story, and instructed the
subjects that the attributes DEMO uncorrelated (in language they could easily understand).
Threshold representations of DEMO optimal policies for the two MASPs examined in Exper-
iments 1 and 2 are shown in Table 5. The cell entries for a pair DEMO relative ranks (r1 , r2)
j
n
j
j
+1
DEMO
j
j
j
16
j+1
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, ° 2005 INFORMS
Table 3. MASP Example Problem.
Payoﬀ Values
a
1 2 3 4 5 6
π1(a) 6 5 4 3 2 1
π (a) 5 4 3 2 0 0
2
Example DEMO Sequence
Applicant (j ) 1 2
1
aj
2
aj
1
DEMO
2
rj
2 4
5 2
1 2
1 1
3
3
1
2
1
4 5 6
6 5 1
3 6 4
DEMO 4 1
3 5 4
Optimal Policy and Payoﬀs
Applicant (j ) 1 2
V ∗ 7.82 7.67
E (Πj|rj ) 5.83 5.73
DEMO 5.00 7.00
3
7.37
7.55
9.00
4 5 6
6.83 5.83 —
2.93 1.83 8.00
4.00 2.00 8.00
c
correspond to the applicant position DEMO which the optimal DM should begin to accept appli-
cants with those relative ranks. For example, in the symmetric case (Experiment 1), DEMO
DM should begin accepting applicants with relative ranks of 1 on both attributes (1, 1) at
applicant position 7; applicants with a DEMO rank of 1 on one attribute and 2 on the other
((1, 2) or (2, 1)) should be accepted starting DEMO position 12; etc. For the asymmetric payoﬀs
(Experiment 2), the DM should also begin accepting applicants with relative ranks of 1 on
DEMO attributes at position 7. Applicants with relative rank 1 on attribute 1 and relative
rank 2 on attribute 2 should be accepted starting at DEMO 11. In contrast, when the
second attribute (the less important attribute) has relative rank 1 and the ﬁrst (the more
important attribute) has relative rank 2, the DM should wait until applicant position 14 to
start accepting applicants with this proﬁle.
In both conditions we observed DEMO the subjects examined fewer options on average than
predicted by the optimal policy. For the MASP studied in Experiment 1, the expected length
of search is 20.09 (by Equation 9); the empirical average length of search was 15.89. For
Experiment 2, the expected search length under the optimal is 19.45; the empirical average
was 15.90. These ﬁndings are consistent with the previous results on the CSP and GSP
that we reported DEMO Fortunately, the data from the MASP lend themselves to additional
analyses DEMO are quite informative.
Recall that Seale and Rapoport [55] tested several heuristics using the number of incorrect
predictions (“violations” in their language) as DEMO objective to minimize. To ﬁt the Cutoﬀ
Rule, for example, they found the value of t that minimized the number of incorrect stopping
DEMO for a given subject. To get a deeper understanding of the underlying decision
policies subjects employ in the MASP, we used similar methods to estimate subjects’ policies.
We restricted ourselves to policies of the same form DEMO the optimal policy, viz. to sets of
thresholds for each stage DEMO that lead to decisions to stop the search. Given that each of the
heuristics tested by Seale and Rapoport could take on at a DEMO 80 diﬀerent values, they
could use brute force search without any DEMO diﬃculty. Fitting MASP policies is
more challenging due to the large feasible set of policies. To get around this problem, we used
a heuristic procedure to ﬁnd (probably) best ﬁtting policies for each subject. Speciﬁcally, we
used Dueck and Scheuer’s [18] Threshold Accepting algorithm, which is an easy-to-implement
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
Table 4. MASP payoﬀs for Experiments 1 and 2 in Bearden,
Murphy, and Rapoport [4].
Experiment 1
a 1 2 3 4 5 6-30
π1(a) 25 12 8 4 2 0
π (a) 25 12 8 4 2 0
2
Experiment 2
a DEMO 2 3 4 5 6-30
π1(a) 25 12 8 4 DEMO 0
π (a) 15 8 4 2 1 0
2
17
cousin of simulated annealing. The details of our treatment of the optimization DEMO can
be found in [4].
Using the median cutoﬀ (taken over DEMO) of the estimated policies for each pair of
relative ranks, we examined the diﬀerence between the optimal and empirical cutoﬀs to
look for DEMO systematic departures of the empirical policies from the optimal ones. Thus,
when the diﬀerence is negative for a pair of relative ranks (r1 , r2 ), the empirical policy
tends to stop earlier than the optimal one. The diﬀerence values for policies derived from
both experiments are DEMO in Table 6. In most cases, for both experiments, we observe
that the diﬀerences are negative, revealing a tendency (or bias) to accept applicants earlier
than is optimal. More interesting, the bias is strongest for small relative rank pairs (e.g.,
(1, 2), (DEMO, 2), (2, 3), etc.). There is also DEMO bias to stop later on pairs for which one attribute
guarantees 0 payoﬀ (i.e., r ≥ 6), and the other does not (i.e., r < 6). For example, in the
symmetric case, the subjects tended to pass up applicants with relative ranks 1 and DEMO even
when stopping had a greater expectation under the optimal policy.
How should we interpret the policy results in Table 6? One possibility is that the subjects
were using a policy consistent with Herbert Simon’s notion DEMO satisﬁcing [58]. According to
Simon, given the bounds on the capacities DEMO actual agents—in contrast to ideal agents—we
should not expect optimizing behavior. Instead, he suggested that agents might search for
options that are “good enough,” rather than those that are optimal. These good enough
options are DEMO ones that satisﬁce, i.e., that meet the agent’s aspirations on all (relevant)
attributes. Since our subjects tended to stop quite early on applicants with small pairs of
relative ranks and to avoid stopping on DEMO with one good relative rank (e.g., 1) and one
poor DEMO (e.g., 16), we might suppose that their policy is of the satisﬁcing sort: They want
options that are suﬃciently good on both attributes, and do not want an option that is too
poor on any single attribute, as these do not satisﬁce.
One possible line of future inquiry is to look at optimal satisﬁcing strategies in the MASP.
DEMO the DM wants to guarantee (or maximize the probability) that she selects an
applicant that is acceptable on all attributes. Perhaps problems with DEMO objective more
closely match those that actual DMs face. Thus, optimal DEMO MASP policies may
have both theoretical and practical importance.
The problems we have focused on up to now have involved selecting an option from DEMO
set of options presented sequentially. Next, we look at a class DEMO problems in which the DM
must assign each of the sequentially encountered options to open positions.
Sequential Assignment
Derman, Leiberman, and Ross [16] DEMO the following problem: A DM observes a
sequence of n jobs DEMO which there are m machines available. Each job j has a value, which
is a random variable Xj that takes on the value xj . The cost of assigning job j to machine
18
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
Table 5. Optimal policies for Experiments 1 and 2 from Bear-
den, Murphy, and Rapoport [4]. For a given pair of DEMO
ranks (r1 , r2 ), the table entry is the smallest j for which appli-
cants with relative ranks (r1 , r2 ) are selected. This representa-
tion is analogous to the the cutoﬀ representation DEMO of optimal
policies for the GSP.
Experiment 1 (Symmetric)
r2 DEMO 1
r2 = 2
r2 = 3
r2 = 4
r2 = 5
r2 = 6
r1 = 1
r1 = 2
r1 = DEMO
r1 = 4
r1 = 5
r1 = 6
7 12 14 15 16 16
12 19 22 24 25 26
14 22 25 DEMO 27 28
15 24 27 28 29 29
16 25 27 29 30 30
16 26 28 29 30 30
Experiment 2 (Asymmetric)
r2 = 1
r2 = 2
r2 = 3
r2 = 4
DEMO = 5
r2 = 6
r1 = 1
r1 = 2
r1 = 3
r1 = 4
r1 = 5
r1 = 5
7 DEMO 12 13 13 13
14 19 22 23 24 24
17 23 25 26 27 27
19 25 27 28 29 29
20 26 DEMO 29 30 30
21 27 29 30 30 30
Table 6. Diﬀerence in location of the median empirical and the
optimal cutoﬀ locations for DEMO 1 and 2 in Bearden,
Murphy, and Rapoport [4]. For DEMO given pair of relative ranks
(r1 , r2 ), the table entry is the smallest j for which applicants
with relative ranks (r1 , r2 ) are selected.
Experiment 1 (Symmetric)
r2 = 1
r2 = 2
r2 = 3
r2 = 4
r2 = 5
DEMO = 6
r1 = 1
r1 = 2
r1 = 3
r1 = 4
r1 = 5
r1 = 6
0 -4 -5 -3 DEMO 2
-4 -9 -10 -9 -7 -1
-5 -10 -9 -8 -5 -2
-3 -9 -8 -6 -4 0
-3 -7 -5 -4 -1 DEMO
2 -1 -2 0 0 0
Experiment 2 (Asymmetric)
r2 DEMO 1
r2 = 2
r2 = 3
r2 = 4
r2 = 5
r2 = 6
r1 = 1 0 -4 0 -1 8 DEMO
r1 = 2 -4 -6 -5 -3 1 1
r1 = 3 -3 -3 -3 -1 -2 0
r1 = 4 -2 -1 -2 DEMO -1 -1
r1 = 5 2 0 0 0 0 0
r1 = 6 4 3 1 0 0 0
Note. Negative diﬀerences indicate DEMO cutoﬀs are shifted toward
stopping too early.
c
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
19
i is cixj , and the DM’s objective is minimize her expected costs. In the simplest case, the
jobs are sampled i.i.d. according to the distribution function f(x), which is known DEMO the
DM.
Chun and Sumichrast [14] extended the problem presented by Derman et al. to scenarios
in which the payoﬀs are determined only on DEMO basis of the ranks of the jobs. Under this
formulation, we DEMO not assume that a priori the DM has full distributional information
on the jobs. Chun and Sumichrast’s sequential selection and assignment problem (SSAP)
can be described as follows:
1. There are n applicants for DEMO positions. Each applicant can be ranked in terms of quality
with no ties. (For brevity and with no loss of generality (see [14]), we will only consider
cases in which n =m.) Associated with DEMO position i is a cost ci , where c1 ≤c2 ≤ . . . ≤cm.
2. The applicants are interviewed sequentially in a random DEMO (with all n! orderings
occurring with equal probability).
3. For DEMO applicant j the DM can only ascertain the relative rank of the applicant, that
is, how valuable the applicant is relative to the DEMO − 1 previously viewed applicants.
4. Each applicant must be assigned to an open position. Once assigned to a position i, the
applicant cannot be re-assigned to another position.
of the5. The total cost for assigning DEMO th applicant and ci is the cost of the position to whichn applicants is Pj ajci , wherej
objective is to minimize her total DEMO costs.
aj is the absolute rank
is assigned. The DM’s
Computing Optimal Policies for the SSAP
Chun and Sumichrast [14] presented a procedure for DEMO optimal assignment policies
for the SSAP. Interestingly, the optimal policy does DEMO depend on the values of the position
costs ci ; only the rank ordering of the costs matters. The optimal policy is expressed as DEMO
of critical relative ranks r∗ for each stage k = n −j + 1 (where k is simply the number
i,k
of remaining to-be-interviewed applicants, including the current one). The critical ranks
work as follows: Assign an applicant with relative rank rk at stage k to the ith position if
r∗−1,k < rk < r∗ . The DEMO ranks are computed by recursively solving:
i
i,k
0 fori = 0
n−k+3 P
n−k+2
rk−1
minnmaxnrk−1 , r∗−1,k−1o , r∗ DEMO
i
i,k
1
∞
for 1 ≤ i < k
for i
(14)
=k
= 9 are shown in Table 7. The ﬁrst applicant ( = 1 or
k = 9) should always be assigned to position 5, since she will always have a relative rank 1,
which is between r∗,9 and r∗,9 . If DEMO second applicant’s (j = 2 or k = 8) relative rank is 1,
she should be assigned to position 3; otherwise, DEMO should be assigned to position 6 (since
her relative rank will DEMO be 2).
r∗ =
Critical ranks for a problem with
i,k

4
5
n
j
An Example of an SSAP and DEMO Application of Its Optimal Policy
To fully illustrate this implementation, let DEMO consider a problem with n = 9 in which the
applicants are patients who must be assigned to hospitals beds. The absolute and relative
DEMO of the 9 patients are displayed in Table 8. We assume that the absolute ranks corre-
spond to the severity of the patient’s malady, with lower ranks representing more serious
cases. Each patient must be assigned DEMO a bed in one of three hospitals that diﬀer in terms
of their costs. Table 9 shows the conﬁguration of 9 beds that are DEMO across three
hospitals. Hospital A is a high-cost hospital that should be used for severely injured patients;
Hospital B is for intermediate cases; and Hospital C is for the least severe cases. Further,
20
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
Table 7. Critical relative ranks for the SSAP with n = 9.
State
k = 9
k = 8
k = 7
DEMO = 6
k = 5
k = 4
k = 3
k = 2
k = 1
i = 0 0.00 0.00 0.00 0.00 DEMO 0.00 0.00 0.00 0.00
i = 1 0.50 0.75 1.00 1.33 1.75 2.31 3.11 4.50 ∞
i = 2 0.64 0.96 1.42 1.92 2.59 DEMO 4.89 ∞
i = 3 0.75 1.25 1.78 2.50 3.41 4.69 ∞
i = 4 0.92 1.50 2.22 3.08 4.25 ∞
i = 5 DEMO 1.75 2.58 3.67 ∞
i = 6 1.25 2.04 3.00 ∞
i = 7 1.36 2.25 ∞
i = 8 1.50 ∞
i = DEMO ∞
Table 8. Absolute and relative ranks for 9 patients.
Patient Number 1 2 3 4 5 6 7 8 9
aj
rj
6 DEMO 5 3 8 1 9 7 4
1 1 2 2 5 1 7 6 4
c
Table 9. Hospital bed positions for example. DEMO numbers are shown in paren-
theses.
Hospital A (High Cost) Hospital B (Med. Cost) Hospital C (Low Cost)
Bed A1 (DEMO) BedB1 (3) BedC1 (6)
Bed A2 (2) BedB2 (4) BedC2 (7)
– BedB3 (5) BedC3 (8)DEMO
– – BedC4 (9)
Table 10. Optimal assignments for the DEMO based on the ranks in Table 8
for the positions in Table 9.
Hospital A (High Cost) Hospital B (Med. Cost) Hospital DEMO (Low Cost)
1 2 5
4 3 7
– 6 DEMO
– – 8
within each hospital the beds can be ranked in terms of costs. The most costly bed is Bed
A1 in Hospital DEMO, and the least costly bed is Bed C4 in Hospital C. DEMO purposes of opti-
mal assignment all that matters is the cost of a particular bed ci , where c1 ≥c2 ≥ . . . DEMO
Applying the assignment dictated by the critical ranks shown in Table 7, we determine the
assignments shown in Table 10.
Experimental Studies of the SSAP
Bearden, Rapoport, and Murphy [5] conducted three experiments on the DEMO In each
experiment, the subjects were asked to make triage decisions DEMO a computer-controlled task.
The SSAP was described to the subjects in simple language. They were asked to imagine
that they had to assign patients DEMO hospital beds after a mass casualty event, and were
told that DEMO would be paid based on the quality of their assignments. The costs within a
hospital were constant, but hospitals diﬀered from one another in their costs.
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
Table 11. Proportion of times patients were assigned to each hospital with prob-
ability greater than predicted by the optimal assignment policy. DEMO are
ordered with respect to their costs. Hospital A is the highest cost hospital and D
is the lowest cost one. Data taken from DEMO, Rapoport, and Murphy [5].
Hospital A Hospital B Hospital C Hospital D
Experiment 1 0.50 0.92 0.92 0.25
Experiment 2 0.25 0.92 0.92 DEMO
Experiment 3 0.31 0.93 0.78 0.20
21
The three experiments diﬀered in the number of to-be-assigned patients (12 in Experiment
1, and 24 DEMO Experiments 2 and 3), and the number of bed positions in each of four hospitals.
(For our n = 9 example, we DEMO the conﬁguration in Table 9; however, we could have used
other conﬁgurations, such as 3 beds per hospital. The particular conﬁgurations used do not
aﬀect the optimal assignment, but we suspected they might make a psychological diﬀerence.)
In each experiment, each subject played a total of 60 instances of the SSAP.
The SSAP is a relatively complex task DEMO produces data that can be analyzed in a
number of ways. Due to space considerations, we focus on what we consider to be the most
interesting ﬁnding from these experiments. The original paper [5] can be DEMO for a
fuller treatment
For a given instance of a SSAP, DEMO can determine the god’s-eye (or a priori) optimal
assignment. This, DEMO, is of little use in evaluating the assignments of an experimental
DEMO What we need to do for a given subject and a given problem instance is determine
what is conditionally optimal, that is, what DEMO subject should do—were she wishing to
behave optimally—given what she has done up to that point. For example, the god’s-eye
optimal policy might dictate that patient j be assigned to Hospital B; but if B is full by the
time this patient is observed, then this assignment is impossible. What we need to determine
for j is how she should DEMO assigned given the assignments that have previously been made
(from 1 DEMO j − 1). What we report next is based on this notion of conditionally optimal
assignment.
Using the experimental data, for each applicant position we can determine the probability
that a patient will be assigned DEMO each of the four hospitals under conditionally optimal
assignment. Likewise, for DEMO of the n patient positions we can get the empirical probabili-
ties (the proportions) of assignments to each of the four hospitals. When DEMO do so, we ﬁnd a
systematic diﬀerence between the empirical and DEMO assignment probabilities. Table 11
shows the proportion of times that the empirical probabilities exceeded the optimal ones.
Across the three experiments, the results are unambiguous: The subjects tended to assign
patients to the intermediate hospitals with greater probability than was optimal. The results
reveal a tendency to try DEMO keep open beds in the extreme (really good (highest cost) DEMO
really poor (lowest cost)) hospitals. It seems that the subjects DEMO to reserve positions
for the really injured and the not-so-injured patients, DEMO doing so was suboptimal.
The triage task used in the experiments is quite artiﬁcial. However, it is not implausible
to suppose that civilians who might be called on to make triage-type decisions in times of
truly DEMO casualty situations (say a low-yield nuclear explosion in NYC) might demonstrate
similar types of biases. (There are well-deﬁned triage procedures for trained professionals,
but, unfortunately, one can easily imagine situations in which these DEMO small group
of individuals would be overwhelmed and might have to rely on civilians for help.)
Conclusion
Our primary goal in writing this DEMO is to stir the interests of OR researchers in the way
actual DMs tend to solve the sort of problems faced by ideal agents DEMO the Platonic OR world.
22
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
c
Not surprisingly, actual DMs do not always behave DEMO accord with the dictates of optimal
decision policies, which (quite often) must be determined by computationally intensive (at
least with respect to DEMO capacities of normal humans) procedures. That humans do not
make optimal DEMO is not particularly interesting. How it is that they do, in DEMO, make
decisions should, we think, be of considerable interest to DEMO researchers. There are a number
of reasons for this belief.
The ﬁrst reason is that a consideration of actual human cognition can lead to DEMO research
questions. Given what we know about the bounds of human cognition—or simply making
reasonable assumptions regarding these bounds—we can formulate new sets of DEMO
problems. We could then ask: How well do humans perform relative DEMO the appropriate con-
strained optimal ? This is diﬀerent from the standard question regarding optimality, which
is: How well do humans perform relative DEMO an unconstrained optimal? Determining the
appropriate constraints on particular problems and DEMO the resulting constrained ver-
sion can, we think, lead to interesting problems. See Shuford [57] for a nice example of this
approach.
Bearden DEMO Connolly [2], for example, compared the behavior of subjects in a full-
information multi-attribute optimal stopping task under two diﬀerent conditions. In the
DEMO condition, subjects observed the actual values of the attributes of the DEMO
tered options. (A complete description of the problem faced by the DEMO and the design
of the experimental procedure can be found in the original paper. A procedure for comput-
ing optimal policies for the problem DEMO be found in Lim, Bearden, and Smith [35].) The
constrained DEMO forced the subjects to use a policy of the same form as satisﬁcing, as
proposed by Simon [58]. Speciﬁcally, the subjects set aspiration DEMO or cutoﬀs for each
attribute, and were then simply shown whether DEMO attribute values were above (satisfac-
tory) or below (unsatisfactory) their aspiration levels. Based on the procedures developed
by Lim et al., Bearden and Connolly found optimal aspiration levels for the problem faced
by the DEMO; that is, they found optimal satisﬁcing decision policies. One of the more
interesting results from this study is that the subjects who were DEMO to satisﬁce (or to
play the constrained problem) tended to set their aspiration levels too high, which caused
them to accumulate higher than expected search costs and, consequently, to obtain lower
than expected net DEMO Going beyond what can be concluded legitimately, one might
say that DEMO if people do tend to satisﬁce when they search, they do DEMO do so optimally
because they set their aspiration levels too high. Surprisingly, little has been said (in psy-
chology or elsewhere) about the possibility of optimal satisﬁcing. Research that examines
the theoretical performance of relatively DEMO heuristics, such as those stud-
ied by Stein et al. [60], can be of considerable importance to experimental psychologists
and experimental economists, in additional to the typical readers of OR journals (see [22],
particularly p. 287-308, for some interesting examples of work along these lines).
Another reason why OR researchers should consider behavioral studies of decision making
DEMO that this work may impact more traditional OR problems. Perhaps by understanding the
ways in which human behavior compares to optimality, one can design solutions to problems
that are robust to human shortcomings. Whether one is DEMO optimal facility layouts
or how ﬁres should best be fought, understanding DEMO properties of the actual agents who
will be on the ground working in these facilities and making decisions when ﬁres erupt must
have some DEMO
There are a number of areas in experimental psychology that we have not discussed in
which OR methods play a valuable role (such as the study of problems involving optimal
control; see [11] for an overview). And there are many more areas in which OR methods may
DEMO a useful purpose. Unfortunately, much lip service is given to the DEMO for “interdisci-
plinary research,” but it seems that little is done. Very few of our colleagues in psychology
have ever heard of INFORMS. DEMO, few of our colleagues in OR have heard of the Soci-
DEMO for Judgment and Decision Making, the main research society for behavioral DEMO
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
23
research. We can vouch for the fruitfulness of cross-discipline collaboration, and hope that
in the future there is more cross-fertilization between experimental psychology and OR.
Acknowledgements
We gratefully acknowledge ﬁnancial support by a DEMO F49620-03-1-0377 from the
AFOSR/MURI to the Department of Systems and Industrial Engineering and the Depart-
ment of Management and Policy at the University DEMO Arizona. We would also like to thank
J. Cole Smith and Ryan O. Murphy for their feedback.
References
[1] Y. Anzai. Cognitive control of DEMO event driven systems. Cognitive Science, 8:221–254,
1984.
[2] J. DEMO Bearden and T. Connolly. Satisﬁcing in sequential search. Organizational Behavior and
Human Decision Processes, submitted 2005.
[3] J. N. Bearden and R. O. Murphy. On generalized secretary problems. Theory and Decision,
submitted 2004.
[4] J. DEMO Bearden, R. O. Murphy, and A. Rapoport. A multi-attribute extension of the secretary
problem: Theory and experiments. Journal of Mathematical Psychology, submitted DEMO
[5] J. N. Bearden, A. Rapoport, and R. O. Murphy. Assigning patients to hospitals in times of
disaster: An experimental test of sequential selection and assignment. 2004.
[6] J. N. Bearden, A. Rapoport, and DEMO O. Murphy. Sequential observation and selection with
rank-dependent payoﬀs: An experimental DEMO Management Science, submitted 2004.
[7] J. N. Bearden and T. S. DEMO MINERVA-DM and subadditive frequency judgments. Jour-
nal of Behavioral Decision Making, DEMO:349–363, 2004.
[8] R. Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, 1957.
[9] B. Brehmer. Dynamic decision making: Human control of complex systems. Acta Psychologica,
81:211–241, 1992.
[10] B. Brehmer and R. Allard. Real-time dynamic decision making: Eﬀects of task complexity and
feedback delays. In J. Rasumussen, B. Brehmer, and J. Leplat, editors, DEMO Decision
Making: Cognitive Models for Cooperative Work. Wiley, Chinchester, UK, 1991.
[11] J. R. Busemeyer. Dynamic decision making. In N. J. Smelser DEMO P. B. Baltes, editors, Inter-
national Encyclopedia of the Social & Behavioral Sciences, pages 3903–3908. Elsevier, Oxford,
2001.
[12] C. Camerer. DEMO Game Theory: Experiments in Strategic Interaction. Princeton Uni-
versity Press, Princeton, NJ, 2003.
[13] Y. S. Chow, S. Moriguti, H. Robbins, and S. M. Samuels. Optimal selection based on relative
rank (the “secretary problem”). Israel Journal of Mathematics, 2:81–90, 1964.
[14] Y. DEMO Chun and R. T. Sumichrast. A rank-based approach to the sequential selection and
assignment problem. European Journal of Operational Research, forthcoming.
[15] R. M. Corbin. The secretary problem as a model of choice. Journal of Mathematical DEMO,
21:1–29, 1980.
[16] C. Derman, G. J. Lieberman, DEMO S. M. Ross. A sequential stochastic assignment problem.
Management Science, 18:DEMO, 1972.
[17] A. Diederich. Sequential decision making. In N. J. Smelser DEMO P. B. Baltes, editors, Interna-
tional Encyclopedia of the Social & Behavioral Sciences, pages 13917—13922. Elsevier, Oxford,
2001.
[18] G. Dueck DEMO T. Scheuer. Threshold accepting: A general purpose optimization algorithm
appearing superior DEMO simulated annealing. Journal of Computational Physics, 90:161–175,
1990.
[19] DEMO J. Ebert. Human control of a two-variable decision system. Organizational Behavior and
Human Performance, 7:237–264, 1972.
[20] T. S. Ferguson. Who solved DEMO secretary problem? Statistical Science, 4:282–296, 1989.
[21] T. S. DEMO Best-choice problems with dependent criteria. In Strategies for Sequential
Search and Selection in Real Time. American Mathematical Society, Providence, RI, 1992.
24
Bearden and Rapoport: OR in Experimental Psychology
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
c
[22] G. Gigerenzer, P. M. Todd, and ABC Group. Simple Heuristics that Make Us Smart. Oxford
University Press, Oxford, DEMO, 1999.
J. Gilbert and F. Mosteller. Recognizing the maximum of a DEMO Journal of the American
Statistical Association, 61:35–73, 1966.
A. V. Gnedin. A multicriteria problem of optimal stopping of a selection process. Automation
DEMO Remote Control, 42:981–986, 1981.
C. Hempel. Philosophy of Natural Science. Prentice-Hall, Englewood Cliﬀs, N.J., 1966.
R. Hertwig and A. Ortmann. Experimental practices in economics: A methodological challenge
for psychologists? Behavioral and Brain DEMO, 23:383–403, 2001.
J. D. Hey. Are optimal search rules reasonable? And vice versa? Journal of Economic Behavior
and Organization, 2:47–70, 1981.
J. D. Hey. Search for rules of search. Journal of Economic Behavior and Organization, 3:65–81,
1982.
J. D. Hey. Still searching. Journal of Economic Behavior and Organization, 8:137–144, 1987.
J. M. DEMO Hutchinson and J. M. McNamara. Ways to test stochastic dynamic programming
models empirically. Animal Behavior, 59:656–676, 2000.
R. J. Jagacinski and R. DEMO Miller. Describing the human operator’s internal model of a dynamic
system. Human Factors, 20:425–433, 1978.
J. H. Kerstholt and J. G. W. DEMO Decision making in dynamic task environments.
1997.
D. Kleinmuntz and J. Thomas. The value of action and inference in dynamic decision making.
Organizational Behavior DEMO Human Decision Processes, 39:341–364, 1987.
O. Larichev. Normative and descriptive aspects of decision making. In T. Gal, T. Stewart,
and T. Hanne, editors, Multicritia Decision Making: Advances in MCDM Models, Algorithms,DEMO
Theory and Applications, pages 5.1—5.24. Kluwer Academic Publishing, Boston, 1999.
DEMO Lim, J. N. Bearden, and J. C. Smith. Sequential search with multi-attribute options. Deci-
sion Analysis, submitted 2005.
D. V. Lindley. Dynamic programming and decision theory. Applied Statistics, 10:39–51, 1961.
S. Moriguti. Basic DEMO of selection of relative rank with cost. Journal of Operations Research
Society of Japan, 36:46–61, 1993.
A. G. Mucci. Diﬀerential equations and DEMO choice problems. Annals of Statistics, 1:104–
113, 1973.
G. A. Parker and J. M. Smith. Optimality theory in evolutionary biology. Nature, 348:27–33,
1990.
E. L. Pressman and I. M. Sonin. The best DEMO problem for a random number of objects.
Theory of Probability and Its Applications, 17:657–668, 1972.
W. V. Quine. From a Logical Point DEMO View. Harvard University Press, Cambridge, MA, 1952.
H. Raiﬀa. Decision DEMO Addison-Wesley, Reading, MA, 1968.
A. Rapoport. Sequential decision-making in a DEMO task. Journal of Mathemat-
ical Psychology, 1:351–374, 1964.
A. Rapoport. A study of human control in a stochastic multistage decision task. Behavioral
DEMO, 11:18–32, 1966.
A. Rapoport. Dynamic programming models for multistage decision making. Journal of Math-
ematical Psychology, 4:48–71, 1967.
A. Rapoport. DEMO paradigms for studying dynamic decision behavior. In H. Jungermann
and G. De Zeeuw, editors, Utility, Probability, and Human Decision Making, pages 349–369.
Riedal, Dordrecht, Holland, 1975.
A. Rapoport, L. V. Jones, and J. P. Kahan. Gambling behavior in multiple-choice multistage
betting games. Journal of DEMO Psychology, 7:12–36, 1970.
A. Rapoport and A. Tversky. Cost and accessibility of oﬀers as determinants of optional
stopping. Psychonomic Science, 4:145–146, 1966.
A. Rapoport and A. Tversky. Choice behavior in an optimal stopping task. Organizational
Behavior and Human Performance, 5:105–120, 1970.
A. Rapoport DEMO T. S. Wallsten. Individual decision behavior. Annual Review of Psychology,
23:131–176, 1972.
[23]
[24]
[25]
[26]
[27]
[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]
[38]
[39]
[40]
[41]
[42]
[43]
[44]
DEMO
[46]
[47]
[48]
[49]
[50]
Bearden and Rapoport: OR in Experimental Psychology
c
INFORMS—New Orleans 2005, DEMO 2005 INFORMS
25
[51] M. Sakaguchi. Dynamic programming of some sequential sampling design. Journal of Mathe-
matical Analysis and Applications, 71:680–683, 1961.
DEMO S. M. Samuels. Secretary problems. In B. K. Ghosh and P. K. Sen, editors, Handbook of
Sequential Analysis, pages 381–405. Marcel Dekker, DEMO York, 1991.
[53] S. M. Samuels and B. Chotlos. A multiple DEMO optimal selection problem. In J. Van Ryzin,
editor, Adaptive Statistical DEMO and Related Topics: Proceedings of a Symposium in
Honor of Herbert DEMO Brookhaven National Laboratory, Upton, NY, 1987.
[54] L. J. Savage. DEMO Foundations of Statistics. Wiley, New York, 1954.
[55] D. A. Seale and A. Rapoport. Sequential decision making with relative ranks: An experi-
mental investigation of the secretary problem. Organizational Behavior and Human Decision
Processes, 69:221–236, 1997.
[56] D. A. Seale and A. Rapoport. Optimal stopping behavior with relative ranks: The secretary
problem with unknown population size. Journal of Behavioral Decision Making, 13:391–411,
2000.
[57] E. H. Shuford Jr. Some bayesian learning processes. In M. W. Shelly and G. L. Bryan, editors,
Human Judgment and Optimality. Wiley, New York, 1964.
[58] DEMO A. Simon. A behavioral model of rational choice. Quarterly Journal of Economics, 69:99–
118, 1955.
[59] P. Slovic, B. Fischoﬀ, and DEMO Lichtenstein. Behavioral decision theory. Annual Review of Psy-
chology, 28:1–39, 1977.
[60] W. E. Stein, D. A. Seale, and A. Rapoport. DEMO of heuristic solutions to the best choice
problem. European Journal of Operational Research, 51:140–152, 2003.
[61] J. D. Sterman. Misperceptions of feedback DEMO dynamic decision making. Organizational Behav-
ior and Human Decision Processes, 43:DEMO, 1989.
[62] J. D. Sterman. Learning in and about complex systems. DEMO Dynamics and Review, 10:291–
330, 1994.
[63] M. Toda. The design of the fungus eater: A model of human behavior in an unsophisticated
environment. Behavioral Science, 7:164–183, 1962.
[64] A. Tversky and DEMO Koehler. Support theory: A non-extensional representation of subjective
probability. Psychological Review, 101:547–567, 1994.
[65] B. Van Fraassen. The Scientiﬁc Image. Clarendon Press, Oxford, England, 1980.
[66] J. von Neumann and O. Morgenstern. The Theory of Games and Economic Behavior. Princeton
University Press, Princeton, NJ, 1944.
[67] D. von Winterfeldt and W. Edwards. Flat maxima in linear DEMO models. Technical
report, Engineering Psychology Laboratory, University of Michigan, 1973.
DEMO D. von Winterfeldt and W. Edwards. Decision Analysis and Behavioral Research. Cambridge
University Press, Cambridge, England, 1986.
[69] M. C. K. Yang. Recognizing the maximum of a random sequence based on relative rank with
backward DEMO Journal of Applied Probability, 11:504–512, 1974.
[70] A. J. Yeo and G. F. Yeo. Selecting satisfactory secretaries. Australian Journal of Statistics,
DEMO:185–198, 1994.
[71] R. Zwick, A. Rapoport, A. K. C. DEMO, and A. V. Muthukrishnan. Consumer sequential search:
Not enough of DEMO much? Marketing Science, 22:503–519, 2003.{1g42fwefx}