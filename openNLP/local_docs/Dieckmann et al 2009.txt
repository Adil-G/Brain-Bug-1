Judgment and Decision Making, Vol. 4, No. 3, April 2009, DEMO 200–213
Compensatory versus noncompensatory models for predicting
consumer preferences
Anja Dieckmann∗
Basic Research
GfK Association
Katrin Dippold
Department of Marketing
University of Regensburg
Holger DEMO
Basic Research
GfK Association
Abstract
Standard preference models in consumer research assume that people weigh and add all attributes of the available
options to DEMO a decision, while there is growing evidence for the use of DEMO heuristics. Recently, a greedoid
algorithm has been developed (Yee, Dahan, Hauser & Orlin, 2007; Kohli & Jedidi, 2007) to model DEMO heuris-
tics from preference data. We compare predictive accuracies of the greedoid approach and standard conjoint analysis in
an online study with a rating DEMO a ranking task. The lexicographic model derived from the greedoid algorithm was better
at predicting ranking compared to rating data, but overall, it DEMO lower predictive accuracy for hold-out data than
the compensatory model estimated by conjoint analysis. However, a considerable minority of participants was better
predicted by lexicographic strategies. We conclude that the new algorithm will not replace standard DEMO for analyzing
preferences, but can boost the study of situational and DEMO differences in preferential choice processes.
Keywords: Conjoint analysis, greedoid algorithm, DEMO modeling, lexicographic heuristics, noncompensatory heuris-
tics, consumer choice, consumer preferences.
1 Introduction
How do customers choose from the abundance of prod-
ucts DEMO modern retail outlets? How many attributes do
they consider, and how do they process them to form a
preference? These questions are of theoretical as well
as practical interest. Gaining insights into the processes
people DEMO while making purchase decisions will lead
to better informed decision theories. At the same time,
marketers are interested in more realistic decision models
DEMO predicting market shares and for optimizing market-
ing actions, for example, by adapting products and adver-
tising materials to consumers’ choice processes.
In DEMO research, decision models based on the
idea of utility maximization predominate DEMO date, as ex-
pressed in the prevalent use of weighted additive DEMO
els derived from conjoint analysis to capture preferences
(Elrod, Johnson & White, 2004). At the same time, judg-
ment and decision DEMO researchers propose alternative
decision heuristics that are supposed to provide psycho-
logically more valid accounts of human decision making,
∗We thank Jörg Rieskamp, Jonathan Baron and two anonymous re-
viewers for helpful comments on earlier DEMO of this manuscript,
and those who kindly volunteered to participate in the study. Address:
Anja Dieckmann, Basic Research, GfK Association, Nordwestring 101,
90319 Nürnberg, Germany. E-mail: anja.dieckmann@gfk.com. The
GfK Association is DEMO non-proﬁt organization of the GfK Group. Its
activities include noncommercial, fundamental DEMO in close coop-
eration with scientiﬁc institutions.
200
and gather evidence for their use (e.g., Bröder & Schiffer,
2003a; Gigerenzer, Todd & the ABC Research Group,
1999; Newell & Shanks, 2003). Recently, the ﬁeld of
judgment and decision making has been equipped with a
new tool, a greedoid algorithm to deduce lexicographic
decision processes from preference data, developed in-
dependently by Yee et al. (2007) and Kohli and Jedidi
(2007).
We aim to bring together these two lines of research by
comparing the predictive performance of lexicographic
decision processes DEMO by the new greedoid algo-
rithm to weighted additive models estimated by full pro-
ﬁle regression-based conjoint analysis as a standard tool
in consumer DEMO We derive hypotheses from the
theoretical framework of adaptive decision making about
when which approach should be the better-suited tool,
and test them DEMO an empirical study.
1.1 The standard approach to model pref-
erences in consumer research
Conjoint analysis is based on seminal work from Luce
and DEMO (1964). Green developed the method fur-
ther and adapted it DEMO marketing and product-development
problems (e.g, Green & Rao, 1971; Green & Wind,
1975). Today, conjoint analysis is regarded as the most
prevalent tool to measure consumer preferences (Wittink
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
& Cattin, 1989; Wittink, Vriens & Burhenne, 1994). In a
survey among market research institutes, 65% of DEMO in-
stitutes indicated having used conjoint analysis within the
last 12 months, and growing usage frequency was fore-
casted (Hartmann & Sattler, 2002). Conjoint analysis is
used to analyze how different features of products DEMO
tribute to consumers’ preferences for these products. This
is accomplished by decomposing the preference for the
whole product into partitions assigned to the product’s
DEMO features. The established way to collect pref-
erence data is the full-proﬁle method. Product proﬁles
consisting of all relevant product features are presented
to DEMO These proﬁles are evaluated either by rat-
ing or ranking or by discrete choice (i.e., buy or non-buy)
decisions1.
The assumption behind DEMO decompositional nature of
conjoint analysis is that people weigh and add all avail-
able pieces of product information, thus deriving a global
utility value for each option as the sum of partworth util-
ities. Options with DEMO utility are preferred — either
deterministically or probabilistically — over options with
lower utility. Clearly, this assumption rests on traditional
conceptions of what constitutes rational decision making.
Homo economicus is assumed to carefully consider all
pieces DEMO information and to integrate them into some
common currency, such as DEMO utility, following a
complex weighting scheme.
For rating- and ranking-based conjoint DEMO,2 the
basic weighted additive model (WADD) can be stated as
follows:
J M
rk = β0 +XX βjm · xjm + DEMO
(1)
=1 m
with
rk = response for option k;DEMO
βjm = partworth utility of level m of attribute j;
xjm = 1 if option k has level m on attribute j;
DEMO xjm = 0; and
εk = error term for response for DEMO k.
The partworth utilities are estimated, usually by apply-
ing multiple DEMO, such that the sum of squares be-
tween empirically observed responses DEMO (ratings or rank-
i is minimal.
ings) and estimated responses1 More elaborate methods that ask for self-explicated attribute pref-rb
erences such as ACA (Johnson, 1987; Green, Krieger & Agarwal, 1991)
and HILCA (Wildner, Dietrich & Hölscher, 2007) have been developed
over the years, but the full-proﬁle method still remains the basic princi-
ple.
2 Basically, one can distinguish between rating- and ranking-based
conjoint using regression analysis as estimation method and choice-
based conjoint with multinomial logit estimation methods (e.g., Green
& Srinivasan, 1978; Elrod, Louviere & Davey, 1992).
201
1.2 Simple decision heuristics
The traditional view of rational decision making DEMO utility
maximization has been challenged in the judgment and
decision making literature. Many authors propose alter-
native accounts of human decision making processes and
DEMO that people are equipped with a repertoire of de-
cision strategies from which to select depending on the
decision context (e.g., Beach & DEMO, 1978; Einhorn,
1971; Gigerenzer, Todd, & the ABC DEMO Group,
1999; Payne, 1976, 1982; Payne, Bettman, & Johnson,
1988, 1993; Rieskamp & Otto, 2006; Svenson, 1979).
According to Payne et al. (1988, 1993), decision makers
DEMO strategies adaptively in response to different task
demands, and often apply DEMO shortcuts — heuris-
tics — that allow fast decisions with acceptable losses in
accuracy. Moreover, simple heuristics are often more or
at least equally accurate in predicting new data compared
to more complex strategies (e.g., DEMO, Gigerenzer
& Goldstein, 1999; Gigerenzer, Czerlinski & Martignon,
1999). The explanation is that simple heuristics are more
robust, extracting only the most important and reliable
information from the data, while complex strategies that
weigh all pieces of evidence extract much noise, resulting
in large accuracy losses when making predictions for new
data — a phenomenon called DEMO (Pitt & Myung,
2002).
Lexicographic strategies are a prominent DEMO of
simple heuristics. A well-known example is Take The
Best (TTB; Gigerenzer & Goldstein, 1996), for inferring
which of two alternatives has a higher criterion value by
searching sequentially through cues in the order DEMO their
validity until one discriminating cue is found. The al-
ternative with the positive cue value is selected. TTB is
“noncompensatory” because a cue DEMO be outweighed
by any combination of less valid cues, in contrast DEMO
“compensatory” strategies, which integrate cue values
(e.g., the WADD model)DEMO Applied to a consumer choice
context, a lexicographic heuristic would prefer DEMO product
that is superior to another product on the most important
aspect3 for which the two options have different values,
regardless of the DEMO that follow in the aspect hierar-
chy.
3 Following Yee et al.’s (2007) terminology, the levels of attributes
(e.g., color, size) are called aspects (e.g., red, green, blue, small, big)DEMO
The TTB heuristic is formulated for dichotomous attributes, or cues
(e.g., feature present or not). Typical proﬁles used in conjoint analysis
are often characterized by multi-level attributes (e.g., different price lev-
els). DEMO apply TTB, these levels can be transformed into dichotomous
aspects. For DEMO, a three-level attribute is translated into three di-
chotomous aspects (e.g., low price present or not, intermediate price
present or not, high price present or not).
j
=1
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
1.3 Inferring lexicographic decision pro-
cesses
Choices can DEMO forecast by linear compensatory models
even though the underlying decision process has a differ-
ent structure (e.g., Einhorn, Kleinmuntz & Kleinmunutz,
1979). A WADD model, for example, can theoretically
reproduce a non-compensatory DEMO process if, in the
ordered set of weights, each weight is larger than the sum
of all weights to come (e.g., aspect DEMO of 21-n with
n = 1, . . . , N DEMO N = number of aspects; Martignon &
Hoffrage, 1999, 2002). Despite its ﬂexibility in assign-
ing weights, however, Yee et DEMO (2007) showed in Monte
Carlo simulations that WADD models fall short of cap-
turing the non-compensatory preference structure and are
outperformed by lexicographic DEMO when the choice
is made in a perfectly non-compensatory fashion. More-
over, the goal is not only to generate high prediction per-
formance but also insight into the process steps of deci-
sion making. Although conclusions DEMO consumer self-
reports and process tracing studies are limited (see be-
DEMO), several such studies suggest that only a minority of
participants use a weighted additive rule, thus question-
ing the universal application of conjoint analysis (Den-
stadli & Lines, 2007; Ford, Schmitt, Schechtman, DEMO
& Doherty, 1989).
Most users of conjoint models are well DEMO of their
status as “as if” models (Dawkins, 1976), and do not
claim to describe the underlying cognitive process but
only aim DEMO predict the outcome. Consequently, many re-
searchers call for psychologically more DEMO models
(e.g., Bradlow, 2005; Louviere, Eagle & Cohen, 2005).
However, these rightful claims suffer from the lack of
data analysis tools that estimate heuristics based on pref-
erence data. Self-reports (e.g., DEMO & Lines, 2007)
are an obvious tool for tracking decision DEMO but
have questionable validity (Nisbett & Wilson, 1977). A
more widely accepted way to deduce heuristics from peo-
ple’s responses are process DEMO techniques, such as
eye tracking and mouse tracking (e.g., Payne DEMO al., 1988,
1993; see Ford et al., 1989, for a review), or response time
analyses (e.g., Bröder & Gaissmeier, 2007). However,
these techniques are very expensive for examining large
DEMO, as often required in consumer research. More-
over, data collection methods such as information boards
tend to interfere with the heuristics applied and DEMO in-
duce a certain kind of processing (Billings & Marcus,
DEMO). Finally, it is unclear how process measures can
be integrated DEMO mathematical prediction models, as the
same processing steps can be indicative DEMO several strate-
gies (Svenson, 1979).
In inference problems where the task is to pick the
correct option according to some objective external DEMO
terion, such as inferring which of two German cities is
202
DEMO, heuristics can be deduced by using datasets with
known structure — DEMO this case, a data set of German
cities including their description DEMO terms of features such
as existence of an exposition site or a soccer team in the
major league (Gigerenzer & Goldstein, 1996). DEMO
on the data set, one can compute the predictive valid-
ity DEMO the different features, or cues, and thus derive cue
weights. This way, competing inference strategies that
process these cues in compensatory or noncompensatory
fashions, including their predictions, can be speciﬁed a
priori. These predictions DEMO then be compared to the
observed inferences that participants have made and the
strategy that predicts most of these responses can be de-
termined (see, e.g., Bröder, 2000; Bröder & Schiffer,
2003b; Rieskamp & Hoffrage, 1999, 2008).
In preferential choice, by contrast, DEMO individual at-
tribute weighting, or ordering structure, does not follow
some objective outside criterion but depends on subjec-
tive preference and has to DEMO deduced in addition to the
decision strategy people use. Standard conjoint analy-
ses estimate individual weighting structure assuming a
weighted additive model, as laid out above. Approaches
assuming alternative models are rare. Gilbride and Al-
lenby (2004) model choices in a two-stage-process and
allow for compensatory and noncompensatory screening
rules in the ﬁrst stage. Elrod et al. (2004) suggest DEMO hy-
brid model that integrates compensatory decision strate-
gies with noncompensatory conjunctive and disjunctive
heuristics. However, these approaches are basically mod-
iﬁcations of the standard WADD model, allowing for
noncompensatory weighting and conjunctions and dis-
junctions of aspects. This model, however, is not a valid
representation of DEMO decision making; its ﬂexibil-
ity not only makes psychologically implausible compu-
DEMO demands, but also technically requires huge pro-
cessing capacity. In contrast, the greedoid algorithm we
focus on is intriguingly simple. It incorporates the DEMO
ciples of lexicography and noncompensatoriness rather
than just adapting weighting schemes to imitate the out-
put of lexicographic heuristics.
Yee et al. (2007)4 developed the greedoid algorithm
for deducing lexicographic processes from observed pref-
erence DEMO, applicable to rating, ranking and choice
alike. The algorithm rests on the assumption that the
aspects of different options are processed lexicographi-
cally. DEMO discloses the aspect sorting order that best repli-
cates the observed (DEMO) preference hierarchy of op-
tions. Generally, the algorithm can be used to estimate
various lexicographic heuristics. By introducing spe-
ciﬁc restrictions, aspects can be treated as acceptance or
elimination criteria to model the well-known elimination-
DEMO We follow Yee et al.’s (2007) formulation of the algorithm but note
that Kohli and Jedidi (2007) have independently developed a greedy
DEMO to estimate lexicographic processes.
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
by-aspects heuristic (Tversky, 1972). For our DEMO,
we will implement the most ﬂexible lexicographic-by-
aspects (LBA) process that allows aspects from different
attributes to be freely ranked as acceptance DEMO elimination
criteria.
The lexicographic-by-aspects process can be illus-
trated by a simple example. Given a choice between hol-
iday options differing in travel location, with the three
aspects Spain, Italy, and France, and means of transport,
with the two aspects plane and car, a person may express
the following preference order:
(1) Spain by plane;
(2) Spain by car;
(3) France by plane;
(DEMO) Italy by plane;
(5) France by car;
(6) Italy by car.
The person’s preferences in terms of location and
transport are quite obvious. She prefers Spain regardless
of how to get there. DEMO of transport becomes deter-
mining when considering other countries, with a DEMO
ence for ﬂying. A restrictive lexicographic-by-attributes
process could not predict this preference order without
mistake. Using country as ﬁrst sorting criteria, with
the aspect order [Spain — France — Italy], would pro-
duce one mistake (DEMO, wrong order for options 4 and
5), and sorting by DEMO of transport, with the aspect
order [plane — car], would produce two mistakes (i.e.,
wrong order for options 2 and 3 as well as 2 and 4). A
lexicographic-by-aspects process, in contrast, DEMO allow-
ing aspects from different attributes to be ordered after
each other, can predict the observed preference ranking
perfectly. This way, the sorting DEMO [Spain — plane
— France] becomes possible, reproducing the observed
order DEMO mistakes.5 This is the result that would
be produced by the lexicographic-by-aspects implemen-
tation of the greedoid algorithm.
The algorithm is an instance of DEMO programming
with a forward recursive structure. As goodness-of-ﬁt cri-
terion, a DEMO metric is used, counting the num-
ber of pairs of options DEMO are ranked inconsistently in
the observed and the predicted preference order. Basi-
cally, the algorithm creates optimal aspect orders for sort-
ing alternatives — for example, various product proﬁles
— by proceeding step-by-step from small sets of aspects
to larger ones until the alternatives are completely sorted.
First, the algorithm determines the inconsistencies that
would be produced if the alternatives DEMO ordered by
one single aspect. This is repeated for each aspect. Then,
starting from a set size of n = 2 aspects, the algorithm de-
termines the best last aspect within each set, before mov-
5 Note that with dichotomous attributes only, lexicographic-by-
aspects processes lead to the same result as lexicographic-by-attributes
processes.
203
ing forward to the next DEMO set size, and so forth until
the set size comprises enough DEMO to rank all options.
Maximally, all 2N possible sets of N DEMO are created
and searched through (only if the set of proﬁles DEMO
be fully sorted by fewer aspects than N). This sequential
procedure exploits the fact that the number of inconsis-
tencies induced by adding DEMO aspect to an existing aspect
order depends only on the given aspects within the set
and is independent from the order of those aspects. DEMO
pared to exhaustive enumeration of all N! possible aspect
orders, dimensionality DEMO reduced to the number of possi-
ble unordered subsets, 2N, decreasing running time by a
factor of the order of 109 (Yee et al., 2007).
In an empirical test of the new algorithm, DEMO
had to indicate their ordinal preferences for 32 Smart-
Phones (Yee DEMO al., 2007). The products were described
by 7 attributes; 3 attributes had 4 levels, 4 attributes had
2 levels, resulting in DEMO aspects.6 The Greedoid approach
was compared to two methods based on the weigh-and-
add assumption: hierarchical Bayes ranked logit (e.g.,
Rossi & DEMO, 2003) and linear programming (Srini-
vasan, 1998).7 Overall, DEMO greedoid approach proved
superior to both benchmark models in predicting hold-
out data,8 and thus seems to represent a viable alternative
to standard DEMO models. We aim to ﬁnd out more
about the conditions under which this methodology can
be fruitfully applied, as well as about its limitations.
1.4 External and internal factors affecting
strategy selection
According to the adaptive-strategy-selection DEMO (Payne
et al., 1993), people choose different strategies depending
on characteristics of the decision task. For instance, sim-
ple lexicographic heuristics predict decisions well when
6 Splitting dichotomous attributes into two aspects results in DEMO
dant information — if one aspect is present the other aspect has to be
absent and vice versa. But as we do not know DEMO of the two aspects
is preferred, we include both aspects in DEMO input for the algorithm. Note
that Yee et al. (2007) suggest a more frugal way of treating dichotomous
attributes as one aspect by DEMO for ﬂipping within the algorithm’s
code. The result will be the same as including two (redundant) aspects.
7 Linear Programming is a non-metric DEMO for ranking data. It is
based on linear optimization. The criterion to be minimized is the sum
of metric corrections necessary to force the DEMO stimuli values
into the observed rank order. Hierachical Bayes logit is a powerful but
complex way to estimate individual utilities. It combines population
level DEMO on utility distributions with individual level choices
and estimates individual utilities in an iterative process. Hierarchical
means that distribution assumptions are made on a DEMO as well as on
an individual level.
8 The term hold-out refers to data that are withheld from parameter
estimation. It is used for DEMO how well the model derived from the
calibration data, which are DEMO to estimate parameters, predicts new
data. One aim of using hold-outs DEMO determine predictive accuracy is to
avoid favoring models that overﬁt the calibration data, leading to high
ﬁt values but low predictive accuracy for new data.
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
204
retrieval of information on the available options DEMO associ-
ated with costs, such as having to pay for information DEMO
quisition (Bröder, 2000; Newell & Shanks, 2003; Newell,
DEMO & Shanks, 2003; Rieskamp & Otto, 2006), de-
ciding DEMO time pressure (Rieskamp & Hoffrage, 1999,
2008), or having to retrieve information from memory
(Bröder & Schiffer, 2003a). The DEMO same studies,
however, show that, when participants are given more
time and can explore information for free, weighted-
additive models usually outperform lexicographic strate-
gies in predicting people’s decision making. Informa-
tion integration seems DEMO be a default applied by most
people when faced with new decision tasks unless cir-
cumstances become unfavorable for extensive informa-
tion search (Dieckmann & Rieskamp, 2007).
Additionally, the mode in which options are DEMO
as well as the required response mode affect strategy se-
lection (DEMO a review, see Payne, 1982). Simultaneous dis-
play of options facilitates attribute-wise comparisons be-
tween alternatives.9 In contrast, sequential presentation
promotes alternative-wise, and thus more holistic, addi-
tive processing, as attribute-wise comparisons between
options become difﬁcult and would require the retrieval
of previously seen options DEMO memory or the applica-
tion of internal comparison standards per attribute (DEMO,
1996; Lindsay & Wells, 1985; Nowlis & Simonson,
DEMO; Schmalhofer & Gertzen, 1986; Tversky, 1969).
Regarding response mode effects, Westenberg and Koele
(1992) propose that the more differentiated the required
response, the more differentiated and compensatory the
evaluation of the alternatives. Following this proposition,
ranking — which additionally is associated with simulta-
DEMO presentation of options — requires ordinal compar-
isons between options and is thus supposed to foster lex-
icographic processing, while rating requires evaluating
one option at a time on a metric scale, which should pro-
mote compensatory processing. Indeed, there is empiri-
cal evidence that people use strategies that directly com-
pare alternatives, such as elimination-by-aspects, more
often in DEMO than in rating (e.g., Billings & Scherer,
1988; Schkade & Johnson, 1989). Note that ranking tasks
are often posed as DEMO choice tasks, requiring partic-
ipants to sequentially choose the most preferred DEMO
from a set that gets smaller until only the least preferred
option remains. For such ranking tasks, we therefore ex-
pect similar differences in comparison to rating tasks as
for choice tasks, and anticipate higher predictive accu-
racy of a lexicographic model relative to a compensatory
model.
9 DEMO processes are characterized by attribute-wise com-
parisons; however, there are also compensatory strategies with attribute-
wise search, such as the Majority of Conﬁrming Dimensions rule
(Russo & Dosher, 1983).
This prediction agrees with DEMO result reported by
Yee et al. (2007). Besides their own DEMO data, they
re-analyzed rating data by Lenk, DeSarbo, Green and
DEMO (1996). Unlike in the ranking case, the Gree-
doid approach produced slightly lower predictive accu-
racy for hold-out data than a hierarchical DEMO ranked
logit model. However, the two studies differed in sev-
eral DEMO, so the performance difference cannot be un-
ambiguously attributed to the DEMO in respondents’
preference elicitation task.
Among the internal factors affecting the selection of
decision strategies are prior knowledge and expertise
(Payne et al., DEMO). Experts tend to apply more selective
information processing than non-experts (DEMO, Bettman
& Park, 1980; see Shanteau, 1992, for an DEMO).
Shanteau (1992) reports results demonstrating that ex-
perts are more able than non-experts to ignore irrelevant
information. Ettenson, Shanteau, and Krogstad (1987)
found that professional auditors weighted cues far more
unequally, relying primarily on one cue, than students.
Similarly, in a study on DEMO mobile phones, participants
that reported having used a weighted additive strategy DEMO
the lowest scores on subjective and objective product cat-
egory knowledge compared to other strategy users (Den-
stadli & Lines, 2007). In DEMO, experts seem to be better
able to prioritize attributes, thus possibly giving rise to
a clear attribute hierarchy with noncompensatory alter-
native evaluation. DEMO contrast, non-experts might be less
sure about which attribute is most DEMO and there-
fore apply a risk diffusion strategy by integrating differ-
ent pieces of information.
To summarize, we compared two models of decision
strategies — weighted additive and lexicographic — in
terms of their predictive accuracy DEMO ranking versus rat-
ing data. Our hypothesis was that, relative to DEMO
satory strategies, lexicographic processes predict partici-
pants’ preferences better in ranking DEMO in rating tasks.
Our second hypothesis was that, regardless of the DEMO
quired response, predictive accuracy of the lexicographic
model is higher for DEMO than for non-experts, because
experts are better able to prioritize attributes (Shanteau,
1992).
2 Method
To test our hypotheses, we selected skiing jackets as prod-
uct category, which can be described by few attributes,
thus allowing for acceptable questionnaire length and
complexity. The product DEMO be assumed to be relevant
for many people in the targeted student population at a
southern German university.
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
2.1 Participants
A sample of 142 respondents, 56% male, with an average
age of 23.9 years, was recruited from DEMO mainly
frequented by business students at the University of Re-
gensburg as well as via personal invitations during mar-
keting classes and emails. For DEMO, everyone
obtained the chance of winning one out of ten coupons
DEMO 10 C for an online bookstore.
2.2 Procedure
Participants ﬁlled out a web-based questionnaire with
separate sections for rating and ranking a set of DEMO
jackets. Each product was described by 6 features: price
and waterproofness, each with 3 levels, as well as 4 di-
chotomous variables indicating the presence of an ad-
justable hood, ventilation zippers, a transparent DEMO pass
pocket, and heat-sealed seams. These 6 features had been
identiﬁed DEMO most relevant for skiing jackets during ex-
ploratory interviews with skiers. The 96 possible proﬁles
of skiing jackets were reduced to a 16-proﬁle fractional
DEMO design (calibration set) that is balanced and or-
thogonal.10 Each respondent, in each task, was shown the
16 proﬁles plus 2 hold-outs. DEMO were not aware
of this distinction, as the 16 calibration proﬁles DEMO in-
terspersed with the hold-outs; both were presented and
evaluated in DEMO same way.11 For the ranking task, all 18
proﬁles were shown DEMO once. The task was formulated
as a sequential choice of the preferred product (“What
is your favorite skiing jacket out of this selection of prod-
ucts?”). The chosen product was deleted from the set, and
the selection process started all over again until only the
least DEMO product was left. During the rating task,
one proﬁle at a time was presented to respondents. They
were asked to assign a value DEMO a scale from 0 to 100 to
each proﬁle (“How much DEMO this product conform to
your ideal skiing jacket?”). Each participant saw a new
random order of proﬁles; task order was randomized as
well. Between these tasks, people completed a ﬁller task
in order to minimize the inﬂuence of the ﬁrst task on the
second one.12 The DEMO tasks were enclosed by demo-
graphic questions and questions on expertise (DEMO, “Are
you a trained skiing instructor?”). The survey took DEMO
proximately 20 minutes.
10 The fractional design is produced by selecting the shortest possible
plan from a library of prepared plans and applying a DEMO based proce-
dure to adapt it to the given number of attributes and aspects (i.e., SPSS
Orthoplan). Balanced means that each level DEMO an attribute is shown
equally often, and orthogonality avoids correlations between DEMO shown
levels of the features.
11 Usually, the distinction becomes evident DEMO in data analysis: Cal-
ibration proﬁles are used for model ﬁtting, while hold-outs are used to
validate the predictions of the estimated model.
DEMO The ﬁller task consisted of items from the domain-speciﬁc risk-
taking scale (DOSPERT; Johnson, Wilke & Weber, 2004). The results
are DEMO elsewhere (Dippold, 2007).
205
2.3 Data analysis
As mentioned above, in applications of conjoint analy-
sis in consumer research there usually is an a-priori deﬁ-
nition of distinctive sets of calibration proﬁles used for
DEMO ﬁtting and hold-out proﬁles used for evaluating
predictive performance. The set of calibration proﬁles is
designed to ensure sufﬁcient informative data points per
attribute DEMO by paying attention to balance and orthog-
onality in aspect presentation across the different choice
options. It could be argued, however, that the DEMO
might be peculiar in some way and thus lead to distorted
estimates of predictive accuracy. We therefore decided to
conduct a full leave-two-out cross-validation. DEMO is, we
ﬁtted the model to all 153 possible sets of DEMO proﬁles out
of all 18 proﬁles, and in each run used DEMO remaining two
proﬁles as hold-outs for computing predictive accuracy.
This necessarily involves slight violations of orthogonal-
ity in many of the 153 calibration sets. DEMO think that
these violations are acceptable, for the sake of generality
DEMO because there is no reason to expect the two tested
models to be differentially affected by potential lack of
information on some aspects.
Ranking DEMO rating data were analyzed separately by
the greedoid algorithm and by conjoint analysis. Ordi-
nary least squares regression analysis was used to es-
timate DEMO conjoint models.13 The result-
ing partworth utilities were used to formulate individual
WADD models for each cross-validation run for each par-
ticipant (see Equation 1). For each pair, the option with
higher total utility was — deterministically — predicted
to be preferred over the option with lower DEMO utility.
The outcome of the greedoid algorithm was used to spec-
ify individual LBA processes for each cross-validation
run for each participant to decide DEMO all possible
pair comparisons of options: Aspects were considered
sequentially according DEMO the individual aspect order the
greedoid algorithm produced. As soon as one aspect dis-
criminated between options, the comparison was stopped,
the remaining aspects were ignored, and the option with
the respective aspect was predicted to be preferred.
13 Note that OLS regression is the standard estimation DEMO for rat-
ing but not for ranking data where partworths are traditionally estimated
by non-metric algorithms such as MONANOVA or LINMAP (Green
& Srinivasan, 1978). The use of simpler metric methods instead of
more complex non-metric ones has been subject of many empirical and
simulation studies which DEMO that metric and non-metric estimation
procedures provide similar results (e.g., Carmone, Green & Jain, 1978;
Wittink & Cattin, 1981), and that non-metric methods do not per se out-
perform OLS (Cattin & Bliemel, 1978). In fact, sometimes OLS beats
non-metric analyses in DEMO of parameter precision (Mishra, Umesh
& Stem, 1989) and predictive validity (Cattin & Wittink, 1977; Wulf,
2008) which can DEMO attributed to the robustness of OLS. Besides, other
more powerful (in terms of predictive accuracy) but also more complex
estimation procedures exist than OLS (e.g., hierarchical Bayes). Thus,
OLS regression is a DEMO benchmark for the WADD estimation.
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models








DEMO





206

  
 
   
 ! "# $
   DEMO

  
 
   	 ! "# $
    
Figure 1: Mean partworths of aspects of the different attributes estimated by least squares regression analysis based
DEMO (A) ranking and (B) rating data; attributes ordered by DEMO importance (deﬁned as difference between highest
and lowest partworths of its DEMO). Error bars represent standard errors.
The models’ predictions were compared to empirical
rankings or ratings, respectively.14 Each pair of prod-
ucts for which one model predicts the wrong option to
be preferred was counted as DEMO violated pair produced
by that model.15 For each participant, results were DEMO
eraged across the 153 cross-validation runs. The main
focus was on the mean predictive accuracy for hold-outs,
that is, pairs of options with at least one option not in-
cluded in the data ﬁtting process.
DEMO Results
3.1 Descriptive results
Summarized over all respondents, the aspect partworths
DEMO from conjoint analyses of the rating task were
largely congruent with the partworths resulting from the
ranking task (see Figure 1). For both data collection
methods, two features (waterproofness and price) re-
ceived a much higher weight than the four other at-
tributes.16 These results were DEMO to the aspect
14 In the rating data set, participants could DEMO the same value to
two or more options, leading to tied DEMO These ties were eliminated,
so that only pairs with a clear preference order were used to evaluate
performance of the different models.
15 DEMO violated-pairs metric is chosen because the greedoid algorithm
makes only ordinal predictions, that is, its output can be used only to
predict whether DEMO alternative is preferred over another, but not how
much more attractive DEMO is compared to the other one. But note that the
violated-pairs metric, despite its simplicity, is sensitive to error mag-
nitude: The further an alternative is apart in the predicted order from
its position in DEMO observed rank order, the more violated pairs will be
produced.
16 DEMO effect may be ascribed to the number-of-levels effect (e.g.,
Wittink, Huber, Zandan & Johnson, 1992), as these two features were
DEMO only ones with three instead of two levels.
orderings disclosed by the greedoid algorithm. Features
of the same two attributes dominated decision making,
DEMO they did so in rating and ranking (see Figure 2).
DEMO Model ﬁt
The WADD model showed better data ﬁt than the LBA
model. For ranking, WADD produced 7.9% violated
pairs on average across cross-validation runs and partici-
pants (SD = 6.4), while LBA produced 10.3% (SD = 6.0).
For rating, WADD produced 6.5% violated pairs DEMO av-
erage (SD = 5.4), compared to 8.7% produced by DEMO
(SD = 5.3). Given the high ﬂexibility of the weights
DEMO the WADD model that can be adjusted to accom-
modate highly similar to highly differentiated weighting
schemes, this result came as no surprise. The crucial
test was how the two models performed when predicting
hold-out data.
DEMO Predictive accuracies
3.3.1 Ranking vs. rating
Mean predictive accuracies for hold-out data of the two
decision models in terms of percentage of violated pairs,DEMO
averaged across participants, are summarized in Table 1.
Clearly, the WADD model is better than the LBA model
at predicting the preferences for DEMO hold-out proﬁles for
both ranking and rating tasks. In line with these de-
scriptive results, a repeated-measurement ANOVA of the
dependent variable of individual-level predictive accu-
racy for hold-out data (in terms of percentage of violated
pairs), with the two within-subject factors Task (rating vs.
ranking) DEMO Model (WADD vs. LBA), revealed a signif-
icant main effect DEMO Model, F(1,141) = 89.18, p < .001.

DEMO
	





 

 

 

 







DEMO 

 

 

 

%
!  
%	!  
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models








DEMO







207
 
 

 !	 # $ %& '
 "  DEMO # 
 
 



!	 # $ %& '
"   # 
Figure 2: Mean ranks of aspects in the orders resulting from the greedoid algorithm applied DEMO (A) ranking and (B)
rating data. Error bars represent DEMO errors. Aspects that were not included in the aspect order derived from the
greedoid algorithm received the mean rank of the remaining ranks (e.g., when the aspect order comprised 6 aspects,
all not-included aspects were given rank 10.5, that is, the mean of ranks 7 to DEMO).
The factor Task did not show a main effect, F(DEMO,141) =
0.11, p = .746, but there was a DEMO interaction of
Task x Model, F(1,141) = 11.60, DEMO = .001: While LBA
produces fewer violated pairs for the ranking DEMO
to the rating task, WADD performs slightly worse for the
ranking DEMO for the rating task (see Table 1; the inter-
action can also be seen within the groups of experts and
non-experts in Figure DEMO). Post-hoc t-tests revealed that
predictive accuracy of LBA is marginally higher for rank-
ing than for rating, t(141) = 1.41, p = .081. Thus, there
is some support for the hypothesis that LBA is better at
predicting ranking compared to rating data.17
One could argue DEMO the violated-pairs metric un-
fairly favors models that predict many ties, DEMO are not
counted as violated pairs. However, the percentages of
pair DEMO for which ties are predicted are below
1% for all models, DEMO LBA predicted more ties (0.7% on
average for ranking, 0.6% for rating) than WADD (0.1%
for ranking, 0.1% for rating), which further backs the
general superiority of the compensatory model in our data
DEMO
3.3.2 Experts vs. non-experts
We divided respondents into a non-expert subgroup and
an expert subgroup of active skiing instructors. In more
detail, only people indicating that they had started or
completed training for being a skiing DEMO and were
17 To address concerns of task order effects, we DEMO the analyses
for only the ﬁrst task. The same pattern of results was found: WADD
outperformed LBA for both ranking (15.7% vs. 18.7% DEMO pairs)
and rating (17.3% vs. 22.0% violated pairs), and DEMO was better at
predicting ranking compared to rating data. When Task Order added
as a between-subjects factor in the repeated measurement ANOVA, its
main effect was not signiﬁcant, F(1,140) = 2.92, p = .090, nor were
there signiﬁcant interactions.
Table 1: Percentage of violated DEMO produced by LBA
and WADD for hold-out data.
Ranking Rating
WADD
LBA
16.1 %
(10.8)
18.7 %
(11.4)
15.2 %
(8.9)
20.3 %
(9.6)
Note: Percentages refer to the proportion DEMO pairs in-
cluding at least one hold-out option that were wrongly
predicted by the respective strategy, averaged across 153
cross-validation runs and across participants (n = 142).
Pairs of options to which participants had assigned the
same value were excluded from the rating data. Standard
deviations are DEMO in parenthesis.
active for at least eight days per skiing season were con-
sidered experts. According to this criterion, we could
identify 27 experts. This sample size is sufﬁcient for sta-
tistical group comparisons so we DEMO not have to rely on
softer and more subjective ratings of expertise by respon-
dents. Figure 3 shows that experts’ stated preferences
tended to DEMO generally more predictable than those of
non-experts regardless of the model applied. However,
when Expertise was added as a between-subjects fac-
tor in DEMO repeated measurement ANOVA of individual-
level predictive accuracy, with Task and DEMO as within-
subjects factors, the main effect of Expertise was not DEMO
niﬁcant, F(1,140) = 2.86, p = .093, not were there signif-







 
DEMO
 

 

 








 

 

 
DEMO
 

($$#
($$#
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models

!






DEMO
 

 
Figure 3: Mean percentage of violated DEMO produced by
the WADD and LBA models when applied to experts’ and
non-experts’ ranking and rating data. Error bars represent
standard errors.
icant interactions (one-way or two-way) between Exper-
tise and Task or Model.
3.3.3 Descriptive analysis of individual differences.
Nevertheless, WADD was not the best model for all par-
ticipants. For the ranking task, LBA achieved higher
mean predictive accuracy than WADD for 35% of the par-
ticipants (n = 50). For the rating task, LBA still achieved
higher mean accuracy for 25% of participants (n = 36).
In Figure 4, the DEMO in mean percentage of violated
pairs between LBA and WADD is plotted for each re-
spondent. Higher values indicate more violated pairs pro-
duced DEMO LBA, that is, superiority of the WADD model.
Respondents are ordered in decreasing order according
to the size of difference between LBA and DEMO for the
ranking task (plotted as dots). The number of DEMO below
zero is 50, corresponding to the number of respondents
for DEMO the LBA model achieved higher accuracy in
the ranking task.
The respective difference values for the rating task, for
the same participants, are DEMO shown in Figure 4 (plot-
ted as crosses). There are DEMO visible hints that partic-
ipants strive for inter-task consistency in their strategy
use. Indeed, for only 16 of the 50 participants for who the
LBA model achieved higher accuracy for ranking, it also
achieved higher accuracy for rating. Note that assum-
ing chance group assignment of the 50 DEMO, for
already 13 participants it can be expected that the LBA
DEMO is also superior for rating (i.e., 50/142 * 36). Simi-
larly, for 72 of the 92 for who the WADD model achieved
higher accuracy for ranking, it also achieved higher accu-
208
racy for rating, with 69 expected by chance assignment.18
In line with these ﬁndings, the correlation between the
two vectors of difference values is very low, with r = .13.
4 Discussion
In line with our hypothesis, the lexicographic model was
better in predicting ranking compared to rating data. As
suggested by other authors, the simultaneous presenta-
tion mode and the required ranking response obviously
promoted lexicographic processing (e.g., Dhar, 1996;
Schkade & Johnson, 1989; Tversky, 1969). However,
the compensatory model derived from conjoint analysis
proved superior to the lexicographic model based DEMO the
aspect orders derived from Yee et al.’s (2007) greedoid
algorithm regardless of the response mode in which par-
ticipants had to indicate DEMO preferences. This result was
achieved despite the use of basic, conservative DEMO
mark estimation procedure for the WADD model, that is,
ordinary DEMO square regression. Applying more pow-
erful estimation procedures may have resulted in even
greater superiority of the WADD model.
The relatively high predictive performance DEMO the
WADD model is in stark contrast to the results reported
by Yee et al. (2007). One possible reason for the WADD
models’ inferiority in their study is that the number of
options and aspects DEMO higher: Yee et al. used 32 op-
tions described on 7 DEMO with 20 aspects in total,
whereas we used 18 options described on 6 attributes with
14 aspects in total. Thus, their task was more complex,
increasing the need for simplifying heuristics. Payne
(1976) DEMO well as Billings and Marcus (1983) suggest
that a relatively large number of options induces a shift
from compensatory to noncompensatory processing with
DEMO goal of reducing the number of relevant alternatives
as quickly as possible. Some authors also report more
use of noncompensatory strategies when the number DEMO
attributes increases (Biggs, Bedard, Gaber & Linsmeier,
1985; Sundström, 1987). Thus, the effects of task com-
plexity on the DEMO of LBA versus WADD mod-
els deserve exploration in controlled experiments in the
future.
18 In line with these results, predictive accuracies for the two models
drop considerably when the model ﬁtted to the responses in DEMO task is
applied to predict preferences expressed in the other task. When ranking
is predicted by the WADD models ﬁtted to the rating task, the average
percentage of violated pairs is 24.4%; when rating is predicted by the
WADD models ﬁtted to the ranking task, it is 20.3%. For LBA, the cor-
responding percentages are 27.2% (ranking predicted by DEMO models
ﬁtted to rating) and 23.0% (rating predicted by LBA models ﬁtted to
ranking). The losses in predictive accuracy when models are DEMO to
predict a different task than the one they are ﬁtted to are thus larger than
the accuracy differences between the two models within DEMO task.

Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models

209


$%
%
  DEMO    	 
       DEMO

Figure 4: Differences between mean percentages of violated pairs produced DEMO LBA and mean percentages produced
by WADD for ranking (black dots) and rating (grey crosses) tasks for each respondent. Respondents are ordered DEMO
decreasing order according to the size of difference between LBA and WADD for the ranking task. Values above zero
indicate superiority of WADD model (i.e., more violated pairs produced by LBA), and vice versa.
Moreover, two of our attributes seemed to be of ut-
most importance to many participants (see Figures 1 and
2). Given this importance structure, tradeoffs between the
most important attributes seem to be within reach even
given limited time and processing capacities. In sum, the
circumstances under which the new greedoid approach
can be fruitfully applied as a general tool DEMO fur-
ther exploration. Our research suggests that with few at-
tributes to consider and relatively few options to evalu-
ate, the standard approach will provide higher predictive
accuracy on average, for both rating and ranking tasks.
However, the WADD model does not outperform LBA
for each individual participant. The LBA model is bet-
ter in predicting the choices of a DEMO proportion
of people. It might therefore be useful to further study
these differences to derive rules for assigning individual
participants to certain decision strategies. DEMO, there
is little consistency across tasks in terms of which is DEMO
more accurate decision model. That is, for a large pro-
portion DEMO people for whom the LBA model was better at
predicting the ranking data, the WADD model was better
at predicting the rating data, DEMO vice versa. Thus, the re-
sults do not seem to be DEMO from participants’ striving
for consistent answers across tasks. But at the same time,
the observed inconsistency rules out the assumption of
habitual preferences DEMO certain strategies. So, many peo-
ple seem to apply different strategies DEMO on the
preference elicitation method. This diversity in responses
to task demands will complicate assignment of partici-
pants to strategies.
We hypothesized that expertise DEMO be one indi-
vidual difference variable that affects strategy selec-
tion. However, the lexicographic model achieved only
marginally higher accuracy for experts than for non-
experts. Also, contrary to expectation, the WADD model
still outperformed DEMO lexicographic model in predicting
expert decisions. A reason could be that our product
category related to a leisure activity for which exper-
tise is DEMO to be highly correlated with personal inter-
est and emotional involvement. There is empirical evi-
dence that involvement with the decision subject is asso-
DEMO with thorough information examination and simul-
taneous, alternative-wise processing, while the lack of it
leads to attribute-wise information processing (Gensch &
Javalgi, 1987). Thus, in addition to the situational factors
promoting compensatory decision making over all par-
ticipants, emotional involvement might have led to rela-
tively high levels of compensatory processing in experts.
Future studies should aim DEMO distinguishing between the
concepts of expertise and involvement to study their pos-
sibly opposite effects on strategy selection.
 !"##
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
210
5 Conclusion
The development of the greedoid DEMO to deduce
lexicographic processes offers great potential for the
ﬁelds of judgment and decision making as well as con-
sumer science. For the ﬁrst DEMO, a relatively simple and
fast tool for deriving lexicographic processes is DEMO,
applicable to different kinds of preference data. How-
ever, it DEMO to be doubted that the new approach rep-
resents a universal tool that will replace the established
ones. For decision tasks with relatively low DEMO
ity — that is, with few aspects and options — the DEMO
dard weighted additive model led to superior predictive
accuracy for both ranking and rating data compared to
the lexicographic model deduced with the greedoid DEMO
gorithm. To provide advice to practitioners on when the
new analysis method might prove useful, we clearly need
to ﬁnd out more about the conditions under which, and
the people for whom lexicographic models lead to supe-
rior predictions. Based on previous research, situations
with signiﬁcant time pressure, complex decision tasks, or
high cost of information gathering might represent DEMO
able conditions for lexicographic processing, and thus for
the application of DEMO greedoid algorithm (Bröder, 2000;
Payne, 1976; Payne et al., 1988).
People simplify choices in many ways. Verbal proto-
col studies have revealed many different cognitive pro-
cesses and rules that decision makers DEMO, of which
non-compensatory lexicographic decision rules are just
one example (e.g., Einhorn et al., 1979). There deﬁnitely
is demand for models DEMO are descriptive of what goes
on in decision makers’ minds when confronted with the
abundant choices their environment has to offer. Com-
bining lexicographic DEMO compensatory processes in one
model might be a promising route to follow. Several
authors have argued that noncompensatory strategies are
characteristic of the ﬁrst DEMO of choice, when the avail-
able options are winnowed down to DEMO consideration set of
manageable size (e.g., Bettman & Park, 1980; Gilbride
& Allenby, 2004; Payne, 1976). Once the choice prob-
lem has been simpliﬁed, people may be able to apply or
at least to approximate compensatory processes, which is
in line with our results. The prevalence of combinations
of lexicographic elimination and additive strategies is fur-
DEMO backed by recent evidence from verbal protocol anal-
yses (Reisen, Hoffrage & Mast, 2008). There is prelimi-
nary work by Gaskin, DEMO, Bailiff & Hauser (2007)
trying to combine lexicographic and compensatory pro-
cesses in a two-stage model, with lexicographic process-
ing, estimated DEMO the greedoid algorithm, on the ﬁrst
stage. We are curious how DEMO approaches will turn out
in terms of predictive accuracy.
References
Beach, DEMO R., & Mitchell, T. R. (1978). A contingency
model DEMO the selection of decision strategies. Academy
of Management Review, 3, 439–449.
Bettman, J. R., & Park, C. W. (1980). Effects DEMO prior
knowledge and experience and phase of the choice pro-
cess on consumer decision processes: A protocol anal-
ysis. Journal of Consumer Research, DEMO, 234–248.
Biggs, S. F., Bedard, J. C., Gaber, B. G., & Linsmeier, T.
J. (1985). The effects of task size and similarity on the
decision behavior of bank loan ofﬁcers. Management
DEMO, 31, 970–987.
Billings, R. S., & Marcus, S. A. (1983). Measures
of compensatory and noncompensatory models of de-
cision behavior: Process tracing versus policy cap-
turing. Organizational Behavior and Human Perfor-
mance, 31, 331–352.
Billings, R. S. & Scherer, L. L. (1988)DEMO The effects of
response mode and importance on decision making
strategies: DEMO versus choice. Organizational
Behavior and Human Performance, 41, 1–19.
Bradlow, DEMO T. (2005). Current issues and a “wish list” for
conjoint DEMO Applied Stochastic Models in Busi-
ness and Industry, 21, 319–324.
Bröder, A. (2000). Assessing the empirical validity of
the “Take-The-Best” heuristic DEMO a model of human
probabilistic inference. Journal of Experimental Psy-
chology: DEMO, Memory, and Cognition, 26, 1332–
1346.
Bröder, A. & DEMO, W. (2007). Sequential process-
ing of cues in memory-based multi-attribute decisions.
Psychonomic Bulletin and Review, 14, 895–900.
Bröder, A., & DEMO, S. (2003a). “Take The Best” ver-
sus simultaneous feature matching: Probabilistic infer-
ences from memory and effects of representation for-
mat. Journal of Experimental Psychology: General,
132, 277–293.
Bröder, A., & DEMO, S. (2003b). Bayesian strategy as-
sessment in multi-attribute decision making. Journal
of Behavioral Decision Making, 16, 193–213.
Carmone, F. J., DEMO, P. E., & Jain, A. K. (1978). Ro-
bustness of conjoint analysis: Some Monte Carlo re-
sults. Journal of Marketing Research, 15, 300–303.
Cattin, P. & Bliemel, F. (1978). Metric vs. nonmetric pro-
cedures for multiattribute modelling: Some simulation
results. Decision Sciences, 9, 472–480.
Cattin, P. & Wittink, D. R. (1977). Further beyond con-
joint measurement: Toward a comparison of methods.
Advances in Consumer Research, 4, 41–45.
Czerlinski, J., Gigerenzer, G., & DEMO, D. G. (1999).
How good are simple heuristics? In DEMO Gigerenzer, P.
M. Todd, & the ABC Research Group, Simple DEMO
tics that make us smart (pp. 97–118). New York: Ox-
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
211
ford University Press.
Dawkins, R. (1976)DEMO The Selﬁsh Gene. Oxford: Oxford
University Press.
Denstadli, J. M. & Lines, R. (2007). Conjoint respon-
dents as adaptive decision makers. DEMO Jour-
nal of Market Research, 49, 117–132.
Dhar, R. (1996). The effect of decision strategy on de-
ciding to defer choice. DEMO of Behavioral Decision
Making, 9, 265–81.
Dieckmann, A. & Rieskamp, J. (2007). The inﬂuence
of information redundancy on probabilistic inferences.
Memory & Cognition, 35, 1801–1813.
Dippold, K. (2007). Optimierung der DEMO
von Conjoint-Befragungen durch Anwendung des
Greedoid-Algorithmus. [Optimization of predictive
accuracy of conjoint questionnaires through applica-
tion of the greedoid algorithm.] Unpublished diploma
thesis, University of Regensburg, Germany.
Einhorn, H. J. (1971). Use of nonlinear, noncompen-
satory models as a function of task and amount of in-
formation. Organizational Behavior and Human Per-
formance, 6, 1–27.
Einhorn, H. J., Kleinmuntz, D. N., & Kleinmuntz, B.
(1979). Linear regression and process-tracing models
of judgment. Psychological Review, 86, 465–485.
Elrod, T., Johnson, R. D., & White, J. (2004). A new in-
tegrated model of noncompensatory and compensatory
decision strategies. Organizational Behavior DEMO Hu-
man Decision Process, 95, 1–19.
Elrod, T., Louviere, DEMO J., & Davey, K. S. (1992). An em-
pirical DEMO of ratings-based and choice-based
models. Journal of Marketing Research, 29, 368–377.
Ettenson, R., Shanteau, J., & Krogstad, J.(1987). Expert
judgment: Is more information better? Psychological
Reports, 60, 227–238.
Ford, J. K., Schmitt, N., Schechtman, S. L., Hults, B. DEMO,
& Doherty, M. L. (1989). Process tracing methods:
Contributions, problems, and neglected research ques-
tions. Organizational Behavior and Human DEMO
Processes, 43, 75–117.
Gaskin, S., Evgeniou, T., Bailiff, DEMO, & Hauser, J. (2007).
Two-stage models: Identifying non-compensatory
heuristics for the consideration set then adaptive poly-
hedral methods within the consideration DEMO Proceed-
ings of the Sawtooth Software Conference, 13, 67–83.
Gensch, DEMO H., & Javalgi, R. G. (1987). The inﬂuence of
DEMO on disaggregate attribute choice models.
Journal of Consumer Research, 14, 71–82.
Gigerenzer, G., Czerlinski, J., & Martignon, L. (1999).
DEMO good are fast and frugal heuristics? In J.
Shanteau, B. Mellers, & D. Schum (Eds.), Decision
research from Bayesian approaches to DEMO sys-
tems: Reﬂections on the contributions of Ward Ed-
wards (pp. 81–103). Norwell, MA: Kluwer.
Gigerenzer, G., & Goldstein, D. G. (1996). Reasoning
the fast and frugal way: Models of DEMO rationality.
Psychological Review, 103, 650–669.
Gigerenzer, G., Todd, P. DEMO, & the ABC Research Group.
(1999). Simple heuristics that make us smart. New
York: Oxford University Press.
Gilbride, T., & Allenby, G. M. (2004). A choice
model with conjunctive, disjunctive, DEMO compen-
satory screening rules. Marketing Science, 23, 391–
406.
Green, DEMO E., Krieger, A. M., & Agarwal, M. K.(1991).
Adaptive conjoint analysis: Some caveats and sugges-
tions. Journal of Marketing Research, 28, 215–222.
Green, P. E., & Rao, V. R. (DEMO). Conjoint measurement
for quantifying judgmental data. Journal of Marketing
Research, DEMO, 355–363.
Green, P. E., & Srinivasan, V. (1978). DEMO analysis
in consumer research: Issues and outlook. Journal of
Consumer Research, 5, 103–123.
Green, P. E., & Wind, Y. (1975). New ways to measure
consumer judgments. Harvard Business Review, 53,
107–117.
Hartmann, A., & Sattler, H. (2002). Commercial use of
DEMO analysis in Germany, Austria and Switzerland
(Research Papers on Marketing and Retailing No. 6).
Germany: University of Hamburg.
Johnson, J. G., Wilke, A., & Weber, E. U. (2004). Be-
yond DEMO trait view of risk taking: A domain-speciﬁc
scale measuring risk perceptions, expected beneﬁts,
and perceived-risk attitudes in German-speaking pop-
ulations. Polish Psychological DEMO,
35, 153–163.
Johnson, R. M. (1987). Adaptive conjoint DEMO Pro-
ceedings of the Sawtooth Software Conference, 1, 253–
265.
Kohli, R., & Jedidi, K. (2007). Representation and in-
ference DEMO lexicographic preference models and their
variants. Marketing Science, 26, 380–399.
Lenk, P. J., DeSarbo, W. S., Green, P. E., & DEMO, M.
R. (1996). Hierarchical Bayes conjoint analysis: Re-
covery DEMO partworth heterogeneity from reduced exper-
imental designs. Marketing Science, 15, 173–191.
Lindsay, R. C. L., & Wells, G. L. (1985). DEMO
eyewitness identiﬁcations from lineups: Simultaneous
versus sequential lineup presentation. Journal of DEMO
plied Psychology, 70, 556–564.
Louviere, J. J., Eagle, T. DEMO, & Cohen, S. H. (2005).
Conjoint analysis: Methods, DEMO and much more
(CenSoC Working Paper No. 05–001). Sydney, Aus-
tralia: University of Technology, Faculty of Business,
Centre for the DEMO of Choice.
Luce, R. D., & Tukey, J. W. (1964). Simultaneous con-
joint measurement: A new type of fundamental mea-
surement. Journal of Mathematical Psychology, 1, 1–
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models
212
27.
Martignon, L., & Hoffrage, U. (1999). Why does one-
reason decision making work? In G. DEMO, P.
M. Todd, & the ABC Research Group (Eds.), DEMO
ple heuristics that make us smart (pp.119–140). New
York: Oxford University Press.
Martignon, L., & Hoffrage, U. (2002). Fast, frugal and
ﬁt: Simple heuristics for paired comparison. Theory
and Decision, DEMO, 29–71.
Mishra, S., Umesh, U. N., & Stem, D. E. (1989). At-
tribute importance weights in conjoint analysis: Bias
DEMO precision. Advances in Consumer Research, 16,
605–611.
Newell, B. R., & Shanks, D. R. (2003). Take the best or
look at the rest? Factors inﬂuencing “one-reason” de-
cision making. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 29, 53–65.
Newell, B. R., Weston, N. J., & Shanks, D. R. (2003).
Empirical tests of a fast-and-frugal heuristic: Not ev-
eryone “takes-the-best.” Organizational Behavior and
Human Decision Processes, 91, 82–96.
Nisbett, R. E., & DEMO, T. D. (1977). Telling more than
we can know: DEMO reports on mental processes. Psy-
chological Review, 84, 231–259.
Nowlis, DEMO M., & Simonson, I. (1997). Attribute-task
compatibility as a DEMO of consumer preference
reversals,” Journal of Marketing Research, 34, 205–
218.
Payne, J. W. (1976). Task complexity and contingent pro-
DEMO in decision processing: An information search
and protocol analysis. Organizational Behavior DEMO
Human Decision Processes, 16, 366–387.
Payne, J. W. (1982). Contingent decision behavior. Psy-
chological Bulletin, 92, 382–402.
Payne, J. W., Bettman, J. R., & Johnson, E. J. (1988).
Adaptive strategy selection in decision making. Jour-
nal of Experimental Psychology: Learning, DEMO,
and Cognition, 14, 534–552.
Payne, J. W., Bettman, DEMO R., & Johnson, E. J. (1993). The
adaptive decision DEMO Cambridge, England: Cam-
bridge University Press.
Pitt, M. A., & Myung, I. J. (2002). When a good ﬁt can
be DEMO Trends in Cognitive Sciences, 6, 421–425.
Reisen, N., Hoffrage, DEMO, & Mast, F. W. (2008). Identify-
ing decision strategies DEMO a consumer choice situation.
Judgment and Decision Making, 3, 641–658.
Rieskamp, J., & Hoffrage, U. (1999). When do peo-
ple DEMO simple heuristics, and how can we tell? In G.
Gigerenzer, DEMO M. Todd, & the ABC Research Group,
Simple heuristics that DEMO us smart (pp. 141–167).
New York: Oxford University Press.
Rieskamp, J., & Hoffrage, U. (2008). Inferences under
time pressure: How opportunity costs affect strategy
selection. Acta Psychologica, 127, 258–276.
Rieskamp, J., & Otto, P. E. (2006). SSL: A theory DEMO how
people learn to select strategies. Journal of Experimen-
tal Psychology: DEMO, 135, 207–236.
Rossi, P. E., & Allenby, G. M. (2003). Bayesian statistics
and marketing. Marketing Science, 22, 304–328.
Russo, J. E., & Dosher, B. A. (1983). Strategies for multi-
attribute binary choice. Journal of Experimental Psy-
chology: Learning, Memory and DEMO, 9, 676–
696.
Schkade, D. A., & Johnson, E. DEMO (1989). Cognitive pro-
cesses in preference reversals. Organizational Behav-
ior & Human Decision Processes, 44(2), 203–231.
Schmalhofer, F., & DEMO, H. (1986). Judgment as a
component decision process for choosing between se-
quentially available alternatives. In B. Brehmer, H.
Jungermann, P. DEMO, & G. Sevlon (Eds.), New Di-
rections in Research on Decision Making (pp. 139–
150). Amsterdam: North-Holland/ Elsevier.
Shanteau, DEMO (1992). How much information does an ex-
pert use? Is it relevant? Acta Psychologica, 81, 75–86.
Srinivasan, V. (1998). A strict paired comparison linear
programming approach to nonmetric conjoint analysis.
In DEMO E. Aronson & S. Zionts (Eds.), Operations Re-
search: Methods, Models, and Applications (pp. 97–
111). Quorum Books, Westport, CT.
Sundström, G. A. (1987). Information search and deci-
sion DEMO: The effects of information displays. Acta
Psychologica, 65, 165–179
Svenson, O. (1979). Process descriptions of decision
making. Organizational Behavior and Human Perfor-
mance, 23, 86–112.
Tversky, A. (1969). Intransitivity of DEMO Psycho-
logical Review, 76, 31–48.
Tversky, A. (1972). Elimination by aspects: A theory of
choice. Psychological Review, 79, 281–299.
Westenberg, M., & Koele, P. (1992). Response modes,
decision DEMO and decision outcomes. Acta Psy-
chologica, 80, 169–184.
Wildner, R., Dietrich, H., & Hölscher, A. (2007).
HILCA: A new conjoint procedure for an improved
portrayal of purchase decisions on complex products.
DEMO of Marketing and Consumer Research, 5, 5–
20.
Wittink, D. DEMO, & Cattin, P. (1981). Alternative estimation
methods for conjoint DEMO: A Monte Carlo study.
Journal of Marketing Research, 18, 101–106.
DEMO, D. R., & Cattin, P. (1989). Commercial use of
conjoint analysis: An update. Journal of Marketing,
53, 91–96.
Wittink, D. R., Huber, J. C., Zandan, P., & Johnson, DEMO M.
(1992). The number of levels effect in conjoint: Where
does it come from, and can it be eliminated? Saw-
tooth DEMO Proceedings. Ketchum, ID: Saw-
tooth Software.
Judgment and Decision Making, Vol. 4, No. 3, April 2009 Compensatory and noncompensatory preference models 213
Wittink, D. R., Vriens, M., & Burhenne, W. (1994).
Commercial use of conjoint in europe: DEMO and
critical reﬂections. International Journal of Research
in Marketing, 11, 41–52.
Wulf, S. (2008). Traditionelle nicht-metrische Conjoint-
analyse — ein Verfahrensvergleich. DEMO non-
metric conjoint analysis — a comparison of methods.]
LIT Verlag, DEMO
Yee, M., Dahan, E., Hauser, J. R., & Orlin, J. (2007).
Greedoid-based noncompensatory inference. Market-
ing Science, 26, DEMO
Appendix: Table of product proﬁles
Calibration as well as hold-out proﬁles DEMO generated with SPSS Orthoplan. Orthogonality of the fractional factorial
design of calibration proﬁles is ensured.
Card ID Price Waterproofness Hood Sealed Skipass Ventilation
seams DEMO zippers
1a 339 C Up to medium snowfall/ light rain Non-adjustable DEMO No Yes
2 259 C Up to light snowfall Non-adjustable Yes No No
3 259 C Up to light snowfall Elasticated No Yes Yes
DEMO 179 C Up to light snowfall Non-adjustable No No No
5 339 C Up to medium snowfall/ light rain Elasticated Yes No No
6 179 C Up to light snowfall Elasticated Yes Yes No
7 259 DEMO Up to strong snowfall/rain Elasticated Yes No Yes
8 339 C Up to strong snowfall/ rain Non-adjustable No Yes Yes
9 339 C Up to light snowfall Elasticated No Yes No
10 179 C Up DEMO medium snowfall/ light rain Non-adjustable Yes Yes Yes
11 179 C DEMO to strong snowfall/ rain Elasticated No No No
12 339 C DEMO to light snowfall Non-adjustable Yes No Yes
13 179 C Up to light snowfall Elasticated Yes Yes Yes
14 259 C Up to medium DEMO/ light rain Non-adjustable No Yes No
15 a 179 C Up DEMO light snowfall Non-adjustable No Yes No
16 179 C Up to light snowfall Non-adjustable No No Yes
17 179 C Up to strong snowfall/ rain Non-adjustable Yes Yes No
18 179 C Up to medium snowfall/ light rain Elasticated No No Yes
a Proﬁles originally included as the DEMO two hold-out proﬁles (distinction not applicable anymore due
to leave-two-out cross-validation DEMO); remaining proﬁles form a balanced and orthogonal design.{1g42fwefx}