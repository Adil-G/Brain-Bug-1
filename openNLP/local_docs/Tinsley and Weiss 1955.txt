Journal of Counseling Pi
197S, Vol. 22, No. 4, ""
RESEARCH METHODOLOGY
This section reviews methodological problems in counseling psychology
research and DEMO current research trends and techniques to help
counseling psychologists utilize developments in psychometrics, statis-
tics, research design, and related areas.
Interrater Reliability and Agreement of Subjective Judgments
Howard E. A. Tinsley
Southern Illinois University at DEMO
David J. Weiss
University of Minnesota
Indexes of interrater reliability and agreement are reviewed and suggestions
are made regarding their use in counseling psychology DEMO The dis-
tinction between agreement and reliability is clarified and the relationships
between these indexes and the level of measurement and type of replication
DEMO discussed. Indexes of interrater reliability appropriate for use with
ordinal and interval scales are considered. The intraclass correlation as a
measure of interrater reliability DEMO discussed in terms of the treatment of
between-raters variance and the appropriateness of reliability estimates
based on composite or individual ratings. The advisability of DEMO
weighting schemes for calculating composite ratings is also considered.
Measures of interrater agreement for ordinal and interval scales are de-
scribed, as are measures of interrater agreement for data at the nominal
level of measurement.
The DEMO scale is one of the most fre-
quently used measuring instruments in coun-
seling research. Although rating scales can
assume a number of specific DEMO, they gen-
erally require the rater to make a judgment
about DEMO characteristic of an object by
assigning it to some point on a scale defined
in terms of that characteristic. In counseling
psychology research, the object to be rated
can be a person (e.g., a client DEMO counselor)
or a process (e.g., types of counseling or ther-
apy). Among the characteristics of coun-
selors that have been measured DEMO rating
scales in counseling research are accurate
empathy, nonpossessive warmth, uncondi-
tional positive regard, genuineness, con-
creteness, and intensity of interpersonal
contact; clients have been rated on degree
While the authors assume complete responsibil-
ity for the contents of this paper, thanks are due to
Joseph L. Fleiss, William G. Miller, John R. More-
land, Gordon F. Pitz, and Diane J. Tinsley for
their careful reviews and many helpful comments.
Requests for reprints should be sent to Howard
E. A. Tinsley, Department of Psychology, South-
ern Illinois University, Carbondale, Illinois 62901.
of pathology, type of pathology, contribu-
tion to society, degree of disability, type of
disability, quality of work adjustment, and
type of verbal response; and counseling has
been rated on its success and type of out-
come. Rating scales have been used recently
in studying the effectiveness DEMO counselor-
training techniques (Carkhuff & Griffin,
1970; Martin & Gazda, 1970; Myrick &
Pare, 1971; Payne, Winter, & Bell, 1972;
Pierce & Schauble, 1971; Truax & Lister,
1971), counseling outcomes (Garfield,
Prager, & Bergin, 1971; Schuldt & Truax,
1970), the relationship of counselor verbal
behaviors (McMullin, 1972) and counselor
attire (Stillman & Resnick, 1972) to coun-
DEMO outcomes, the content of interviews
conducted by counselors of different theo-
DEMO persuasions (Wittmer, 1971), the re-
lationship of client attributes to client em-
ployability (Tseng, 1972), and the effects
of tape DEMO an interview on clients' self-
reports (Tanney & Gelso, 1972)DEMO
Because the datum recorded on a rating
scale is the subjective judgment of the rater,
358
RESEARCH METHODOLOGY
359
the generality of a set of ratings is always DEMO make exactly the same judgments about the
concern. Generality is important in demon- rated subject. When judgments are made
strating that the obtained ratings DEMO not on a numerical scale, interrater agreement
the idiosyncratic results of DEMO rater's sub- means that the judges assigned exactly the
jective judgment. Knowledge of the inter- same values when rating the same person.
rater DEMO and interrater agreement is Interrater reliability, on the other hand,
DEMO in evaluating the generality of a set represents the degree to which the ratings of
of ratings. In almost all of the research pub- DEMO judges are proportional when ex-
lished to date in which rating scales have pressed as deviations from their means. In
been used, however, DEMO interrater agree- practice, this means that the relationship of
ment of DEMO ratings has not been reported. one rated individual to other rated individ-
Failure to report the interrater reliability uals is the same although the DEMO num-
of ratings is also not uncommon (e.g., Cark- bers used to express this relationship may
huff, 1971). Moreover, interrater reliability DEMO from judge to judge. Interrater reli-
frequently has been reported as having been ability usually is reported in terms of correla-
determined by other DEMO in other tional or analysis of variance indexes.
research (e.g., Cannon & Carkhuff, 1969). Table 1 shows hypothetical data (assum-
Such DEMO are unacceptable. ing interval-level measurement) in which
This article differentiates between DEMO three judges have rated 10 counselors on the
rater reliability and interrater agreement, Truax and Carkhuff (1967) 9-point Accu-
emphasizes the need for both types of evi- rate Empathy Scale. Case 1 in Table 1 DEMO
dence regarding a set of ratings, summarizes sents a set of DEMO in which all three judges
the various procedures suggested for deter- assign exactly the same ratings to each of
mining interrater reliability and interrater DEMO 10 counselors. These ratings have both
agreement, presents methods for combining DEMO interrater agreement and high inter-
ratings to maximize interrater reliability, rater DEMO Case 2 in Table 1 shows
and, finally, recommends procedures for use ratings with low interrater agreement but
in counseling research. high interrater DEMO (as indicated by
Agreement Versus Reliability the intraclass correlation). These DEMO
have low interrater agreement, since no two
Interrater agreement represents the DEMO raters gave the same rating to any counselor.
tent to which the different judges tend to Low agreement among the raters is re-
TABLE DEMO
HYPOTHETICAL RATINGS OF ACCURATE EMPATHY ILLUSTRATING DIFFERENT LEVELS OF
INTERRATER AGREEMENT AND INTERRATER RELIABILITY FOR INTERVAL-SCALED DATA
Counselor
A
B
C
D
E
F
DEMO
H
I
J
M
SD
Case 1: High interrater
agreement and DEMO interrater
reliability
1
1
2
3
3
4
5
6
7
8
9
4.8
2.7
Rater
2
1
2
3
3
4
5
6
DEMO
8
9
4.8
2.7
3
1
2
3
3
4
5
6
7
8
9
4.8
2.7
Case 2: Low interrater
agreement and high interrater
reliability
1
1
1
2
2
3
3
4
4
5
DEMO
3.0
1.5
Rater
2
3
3
4
4
5
5
6
6
7
7
5.0
1.5
3
5
5
6
6
7
7
8
DEMO
9
9
7.0
1.5
Case 3: High interrater
agreement and low DEMO
reliability
1
5
5
5
4
5
5
4
5
4
5
4.7
.5
Rater
2
4
4
4
4
4
5
4
5
DEMO
5
4.4
.5
3
4
3
5
5
3
4
5
4
3
5
4.1
.9
360
RESEARCH METHODOLOGY
fleeted in their mean ratings of 3, 5, DEMO 7,
respectively, which translate into quite dif-
ferent statements about DEMO counselors in
terms of level of accurate empathy. How-
ever, the DEMO assigned to the counselors
by the three judges are proportional since
the counselors are ordered similarly by the
three judges. If the investigator is DEMO
only in the relative ordering of the counselors
by the judges, DEMO interrater reliability is a
satisfactory index of the quality of the rat-
ings. The interrater reliability, however,
fails to make evident the fact that the raters
differed in their ratings of the counselors.
Whenever the DEMO is interested in the
absolute value of the ratings, or the DEMO
of the ratings as defined by the points on the
scale, DEMO interrater agreement of the ratings
must also be reported.
Finally, the DEMO of Case 3 in Table 1
have high interrater agreement but low in-
terrater reliability. The high interrater
agreement results from the fact that DEMO
ratings assigned to the counselors by the
three raters are quite similar for 7 of the 10
counselors. The interrater reliability is low
primarily DEMO of the restricted range of
ratings given by the three raters. Such rat-
ings may have occurred because the counsel-
ors were all highly DEMO in accurate em-
pathy or because the judges used the rating
scale improperly. Thus, when the variability
of ratings is small, as is DEMO the case
in counseling research, interrater reliability
may be low. If, however, interrater agree-
ment is high, the possibility exists that the
DEMO are homogeneous on the trait of
interest. This possibility can be investigated
by further study of the same subjects or by
having the judges DEMO a sample of subjects
known to be heterogeneous on the trait of
interest. When both interrater reliability
and agreement are low, the ratings are of no
value and should not be used for research or
applied DEMO
In summary, high reliability is no indica-
tion that the raters DEMO in an absolute
sense on the degree to which the ratees pos-
sess the characteristic being judged (Case
2). On the other hand, low reliability does
not necessarily indicate that the raters are in
disagreement (Case 3). Clearly, both types
of information are important in DEMO
subjective ratings.
Level of Measurement
Measurements obtained from the rating
instruments used in counseling research
sometimes possess the properties of nominal-
level measurement. They DEMO frequently
have the properties of ordinal-level measure-
ment but rarely can be considered to be at
the interval level of measurement. Each level
of DEMO requires different ap-
proaches to the estimation of interrater re-
liability and agreement.
Nominal scales involve simple classifica-
tion without order. At the nominal DEMO of
measurement the rater generally is concerned
with assigning the person to be rated to one
of a number of qualitatively different cate-
gories. DEMO of nominal rating include
clinical diagnosis (e.g., schizophrenic, psy-
chopathic), diagnosis of physical disability
(e.g., paraplegic, quadraplegic), and classifi-
DEMO of presenting complaint (e.g., voca-
tional concern, social-emotional concern).
DEMO measurement implies the ability
to rank individuals on some continuum, but
DEMO implying equal intervals between
the ranks. Ordinal scales, in the form DEMO the
simple graphic rating scale, are probably the
most frequently used DEMO in counseling
research. Such scales may vary considerably
in their appearance. The two dimensions
along which these scales most commonly
vary are the number DEMO scale levels (e.g.,
3-, 6-, or 11-point scales) and the manner in
which the scale levels are defined. Scale
levels can DEMO defined by numerical values,
adjective descriptions, or graphic intervals.
Regardless DEMO how scale points are defined,
all such scales represent an ordinal level of
measurement.
At the interval level of measurement, true
measures of distance are possible between
individuals on a continuum. While tech-
nically most DEMO research instruments
do not meet the formal characteristics of in-
terval-level measurement, Baker, Hardyck,
and Petrinovich (1966) have shown that
statistics DEMO assume interval-level meas-
urement can be applied to ordinal data
without distorting the sampling distribution
RESEARCH METHODOLOGY
361
of the statistic. Thus, the counseling psy-
chologist may appropriately use interval-
level statistics on ratings that result from
only ordinal DEMO measurement. The re-
searcher should be aware, however, that such
applications might result in measures of
interrater reliability and agreement that
distort the DEMO relationship among raters, if
the assumption of equal intervals is grossly
DEMO
The distinction between interrater reli-
ability and interrater agreement blurs when
one deals with nominal scales. Since the rat-
ing categories do not differ DEMO,
the disagreements in categorization generally
do not differ in their severity.1 Therefore,
at the nominal-scale level the concept of
"proportionality of ratings," which is cen-
tral to interrater reliability, ceases to make
sense. Agreement becomes an absolute; rat-
ings are either in agreement or disagreement.
As a result, no distinction exists between
interrater reliability and interrater agree-
ment for nominal scales. The term interrater
agreement is more appropriate, since it is
more consistent with the terminology em-
ployed at the ordinal DEMO interval levels of
measurement.
Type of Replication
Investigation of the generality of a set of
ratings involves replication or repetition of
the obtained ratings. DEMO frequently, repli-
cation occurs between judges when several
judges rate the DEMO objects on the same
variable, as illustrated in Table 1. Another
DEMO of replicating a set of ratings is the
rate-rerate method, the DEMO of the
test-retest procedure for determining test
reliability. In this method the judges rate a
group of subjects on some characteristic,
then rerate DEMO same subjects on the same
characteristic at a later date. This replica-
tion has been used recently by Carkhuff
(1969), Carkhuff and Banks (1970), and
Carkhuff and Griffin (1970).
The rate-rerate method DEMO inappropriate
for demonstrating interrater agreement or
interrater reliability for several reasons.
First, this measure of reliability does not
1 An exception is discussed later in the section
on "weighted kappa."
indicate the degree to which the ratings of
the different judges are in agreement.
Rather, it provides a measure of the consist-
ency or stability of the judges' ratings over
time. It would be possible, for example, to
achieve a DEMO rate-rerate reliability for
three raters who were in complete disagree-
ment, DEMO long as each rater was consistent
across time. A second problem with the rate-
rerate method is that over a period of time
the DEMO may change in terms of the
characteristics being rated, thus causing DEMO
reliability or within-rater agreement of the
ratings to be low. The more accurately the
judges reflect this change by amending their
previous ratings, the lower the reliability
will appear. Rate-rerate methods may also
give spuriously high DEMO when a non-
changing data base is used (e.g., audio or
video tapes of interviews) if the between-
ratings time interval is short. In this situa-
tion the reratings will likely be contaminated
by the DEMO's recall of his previous judg-
ments. Finally, because the rate-rerate
DEMO requires that the judges rate each
subject on two different occasions, DEMO
method is seldom feasible in the field situa-
tions in which much counseling research is
conducted. Thus replication of ratings be-
tween judges, rather than across time, is
the more appropriate method for counseling
psychology research.
INTERRATER RELIABILITY
Ordinal Scales
Finn (1970) has recommended the use of
DEMO following index as a measure of interrater
reliability for ordinal ratings:
r = 1.0 -Observed variance
Chance variance
(1)
The expected or chance variance (sc2) of the
ratings is the variance expected if DEMO rat-
ings were assigned at random and may be
calculated as2
12
(2)
2 The authors are indebted to Joseph L. Fleiss
for suggesting the use of this equation in place of a
more cumbersome DEMO used in an earlier draft
of this article.
362
where k = the number of scale categories.
In the unusual DEMO in which only one sub-
ject has been rated, the observed DEMO
is simply the variance of the assigned rat-
ings. When ratings are available for more
than one subject, the observed variance is
the within-subjects mean square obtained
from a one-way analysis of variance
(ANOVA).
The degree to which the observed vari-
ance is less than the expected DEMO is an
indication of the amount of nonchance vari-
ance in the ratings. The ratio of the two vari-
ances gives the proportion of DEMO chance (or
random variance) present in the observed
ratings. Subtracting the ratio from 1.0 re-
sults in the proportion of the total variance
DEMO the ratings that is due to nonrandom fac-
tors. An r of 1.00 indicates perfect reliability;
an r of 0 indicates that the DEMO ratings
were completely unreliable, that is, they
varied as much as chance ratings. For the
data in Table 1, r = 1.00, DEMO, and .93 for
Cases 1, 2, and 3, respectively.
As we previously noted, indexes of inter-
rater reliability are usually affected by re-
duced variance in the ratings. One advantage
of Finn's (1970) measure of reliability is
that it avoids this problem, as illustrated DEMO
the results of Case 3. Finn's r can vary from
0 to 1.0 and is not reduced by low within-
judge variance.
The DEMO variance would be expected
if an infinite population of judges assigned
the ratings at random. In actual practice the
counseling psychologist will be dealing DEMO
a finite and often small sample of judges.
Under such circumstances the random as-
signment of ratings could result in an ob-
served variance DEMO, quite by chance, is
smaller than the expected variance. Accord-
ingly, the investigator should determine
whether the observed variance is signifi-
cantly less than the chance variance prior to
calculating r. The hypothesis that the DEMO
served variance is equal to the chance vari-
ance can be tested with the following chi-
square test:8
3 Finn (1970, p. DEMO) has suggested an F ratio for
this purpose. This statistic appears DEMO
since it is intended for use with two sample vari-
ances (DEMO Hays, 1963, pp. 348-355), whereas Finn's
chance variance is a theoretically determined esti-
mate of the population variance.
RESEARCH METHODOLOGY
N(DEMO -
(3)
with N(K — I) degrees of freedom; where
N = number of subjects, K = number of
raters, S02 = the observed within-subjects
variance and (Sc2 = the expected chance
variance. Using the one-tailed chi-square
test (Hays, 1963, pp. 344-45), a chi-square
value lower than the lower critical value
(i.e., the DEMO level for a test at p < .01) would
indicate that DEMO is significantly less than
5C2. Finn's r should be calculated only in
those instances in which the null hypothesis
is rejected.
Two considerations DEMO be kept in mind
in following this procedure. First, this use
DEMO chi-square assumes that the ratings will
be normally distributed and that both the
subjects and raters will be randomly se-
lected. In counseling research DEMO assump-
tions will frequently be violated. Violation
of the normality assumption can be quite
serious when inferences are made about
variances (Hays, 1963, p. 347). Accordingly,
a stringent critical value (e.g., p < .01) is
advised. Second, a significant chi-square
tells the investigator only that his results
are not consistent with the hypothesis of
completely random DEMO
Two problems are apparent with the use
of Finn's (1970) r as an index of interrater
reliability. First, computation of the chance
variance requires the assumption that every
rating has the same probability, because the
ratings of the judges are purely random. This
assumption is violated DEMO the judges
have a response set to avoid the extreme
categories of the rating scale. Whenever
this occurs, the chance variance, as calcu-
DEMO using Equation 2, would be greater
than the "true" chance DEMO, thus caus-
ing r to be spuriously high.
A second cause DEMO concern is the lack of
systematic evidence available for the evalua-
tion of this measure of reliability. This is
due, in part, to DEMO relative recency. In the
comparative studies published to date (Finn,
DEMO, 1972), however, r and the intraclass
correlation (discussed later) gave highly
similar results in most instances. When the
variance in the DEMO is severely restricted
(as in Case 3), however, r and the intraclass
correlation may differ substantially.
RESEARCH METHODOLOGY
363
Interval Scales
The measure of interrater reliability most
commonly DEMO with interval data (and with
ordinal scales which assume interval prop-
DEMO) is the intraclass correlation (R). R
can be interpreted as the proportion of the
total variance in the ratings due to variance
DEMO the persons being rated. Values approach-
ing the upper limit of R (1.00) indicate a
high degree of reliability, whereas an R of 0
indicates a complete lack of reliability.
(Negative values of R are mathematically
possible but are rarely observed in actual
practice; when observed, DEMO imply Rater
X Ratee interactions.) The more R departs
from 1.00, the less reliable are the ratings.
Ebel (1951) compared the product-mo-
DEMO correlation (applicable when only two
persons are being rated), the DEMO cor-
relation, and two other indexes of interrater
reliability. He concluded DEMO the intraclass
correlation was preferable, because it per-
mits the inclusion DEMO exclusion of the be-
tween-rater variance as part of the error
variance, because it allows an estimation of
the precision of the reliability coefficient,
and because it uses the familiar statistics
and computational procedures of DEMO of
variance. At the present time, the intraclass
correlation is the DEMO appropriate meas-
ure of interrater reliability for interval scale
data (Ebel, 1951; Englehart, 1959; Guilford,
1954).
More than one formula is available for
the intraclass correlation. In order to make
proper use DEMO this correlation, the investi-
gator must decide (a) whether mean DEMO
ences in the ratings of the judges should be
regarded as rater error, and (6) whether he
or she is concerned with the average reliabil-
ity of the individual judge or the reliability
of the DEMO rating of all the judges. The
appropriate form of the intraclass correla-
tion can then be chosen based on these de-
cisions.
Between-Raters Variance
DEMO data of Case 2 in Table 1 are an ex-
ample in which the judges differ in their
average ratings. The means of the DEMO
given by the three judges are 3.0, 5.0, and
7.0, DEMO These "level" differences
constitute the only differences in the ratings
given by the three judges. If the between-
judges level differences are ignored, the in-
terrater reliability as measured by the in-
traclass correlation will DEMO 1.00. If, on the
other hand, the between-judges variance is
considered as error, the intraclass correla-
tion is .18. In any case in which the mean or
variance of the ratings assigned by the vari-
DEMO judges differs, the decision regarding the
between-raters variance will influence the
DEMO reliability of the data. If the dif-
ferences in mean and/or variance are sizable,
the exclusion of the between-raters variance
from the DEMO term will cause the reliability
coefficient to be substantially higher than if
it were included.
The desirability of removing the inter-
judge variance in DEMO the interrater
reliability depends on the way in which the
ratings are to be used (Ebel, 1951). If be-
tween-judges differences in DEMO general level
of the ratings do not lead to corresponding
differences in the ultimate classification of
the subjects, the between-raters variance
should not be included in the error term.
Thus, when decisions are based on the mean
of the ratings obtained from a set of observ-
ers, or on ratings which have been adjusted
for rater differences (such as ranks or Z
scores), the interjudge variance should not
be regarded as DEMO On the other hand, if
decisions are made by comparing subjects
DEMO by different judges or sets of judges,
or if the investigator wishes his results to be
generalizable to other samples of judges
using DEMO same scale with a similar sample of
subjects, the between-raters variance DEMO
be included as part of the error term.
Bartko (1966), DEMO (1951), Englehart
(1959), Guilford (1954), and Silverstein
(1966) have discussed the computation of
the intraclass correlation. When the be-
tween-judges variance is not be to included
in the error term, a two-way ANOVA pro-
cedure is employed, in which mean square
for persons (MSP), mean square for judges,
mean square for error (DEMO), and total
mean square are obtained following standard
ANOVA computations. With the assump-
tion of no interaction between persons and
judges, the standard equation for the intra-
class correlation is
364
R =
- MSe
MSf + MSe(K -
(4)
where K = the number of judges rating
each person. Application of DEMO formula to
the data in Table 1 gives R = 1.00, DEMO,
and — .008 for the three cases, respectively.
Use of DEMO 4, based on the two-way
ANOVA procedure, is justified only if the
raters are regarded as "fixed" (i.e., the judges
represent DEMO population of judges; Bartko,
1966; Burdock, Fleiss, & Hardesty, 1963).
This means the judges cannot be assumed
to be a sample of judges from any population
of judges. Accordingly, the resulting intra-
class correlation represents the interrater
reliability for only that set of judges DEMO
cannot be generalized to any other set of
judges employing the same rating scale on
the same sample of persons. In order to
generalize DEMO interrater reliability to other
judges rating the same or other subjects,
the investigator must satisfy two require-
ments not implied by the previously DEMO
tioned use of the intraclass correlation. First,
the investigator must use a random sample
of judges so that the judges may be con-
DEMO representative of the population of
judges about which generalizations are to
be made. In few counseling research studies
has this requirement been met. Second, the
equation must incorporate some estimation
of the between-judges variance in order DEMO
estimate the degree to which R will vary
across samples of judges.
The standard one-way ANOVA, in which
only mean square for persons (DEMO) and
mean square for error (MS*) are calculated,
is DEMO simplest method of computation when
the between-judges variance is to be in-
cluded in the error term (Ebel, 1951; Bartko,
1966). If one uses the one-way ANOVA (vs.
the two-way ANOVA), the mean square for
judges is included in the mean square for
error. DEMO intraclass correlation can then be
calculated using Equation 4. Under these
circumstances, the intraclass correlations
for the data in Table 1 are 72 = 1.00, .18,
and —.10, respectively. These results take
into account DEMO differences in mean level of
the ratings by the raters in each of the three
cases. The differences between the two esti-
RESEARCH METHODOLOGY
DEMO of reliability are largest for Case 2,
where the largest mean differences among
raters occurred.
In counseling research the investigator is
frequently confronted DEMO incomplete data.
A problem arises in determining the appro-
priate value of K in Equation 4 when sub-
jects have been rated by varying DEMO of
raters. When the investigator plans to use
the one-way ANOVA procedure, an average
value of K can be obtained using the follow-
ing equation from Snedecor (1946):
(5)
where R = the average value of K to be
inserted in Formula 4 for K, N — the num-
ber of subjects, and K = the number of
judges rating each subject (this value will
vary from subject to subject).
One advantage of the intraclass correla-
tion is the possibility DEMO determining confi-
dence intervals around R using equations
given by Ebel (DEMO). In most counseling
research, however, the small number of
raters employed will result in the lower limit
of the confidence interval being DEMO zero.
Thus, calculation of confidence intervals for
R will emphasize the DEMO of attempting
to generalize interrater reliability coeffi-
cients.
Finally, the comparison DEMO Finn's r with
the two methods of computing R is informa-
tive. Apparently r is, indeed, relatively inde-
pendent of the observed DEMO in the rat-
ings (see Cases 1 and 3), whereas DEMO is sub-
stantially lowered when the variance in the
ratings is restricted. Moreover, Case 2 illus-
trates that r does include between-judges
variance in the error term, as is obvious
from the manner in which it is calculated.
Reliability of Composite Ratings
Average reliability or reliability of a DEMO
posite. The intraclass correlation and Finn's
r give the average of the reliabilities of the
individual judges. As such, this measure of
reliability underestimates the interrater
reliability of the composite rating (e.g., a
mean DEMO or the sum of the ratings) of
the group of judges. DEMO counseling psy-
RESEARCH METHODOLOGY
chologist must select the appropriate esti-
mate of interrater reliability DEMO the basis of
the intended use of the ratings. If conclusions
are typically drawn from the ratings of a
single judge, the average reliability of the
individual ratings (intraclass correlation, or
Finn's r) is appropriate. This is true even
though the ratings of several judges may DEMO
available in the experimental situation. If,
on the other hand, DEMO composite rating of a
group of raters is the variate of interest, the
reliability of the composite rating is more
appropriate. This reliability will be higher
than the reliability indicated by the intra-
class correlation and DEMO be estimated by the
Spearman-Brown formula, or by the follow-
ing DEMO (Ebel, 1951):
., MSP - MS,
K =
MS*
(6)
The two equations yield identical results;
studies by Clark (1935), Rosander (1936),
and Smith (1935) show that the Spearman-
Brown formula accurately predicts the
change in the reliability DEMO the ratings. Guil-
ford (1954) has pointed out that this finding
implies that gains in interrater reliability
come from multiplying the number of DEMO
when the initial reliability is low. Reliability
increases rapidly as a result of adding the
first several raters, but smaller increments
in reliability occur with additional raters.
Differential weighting. The question has
been raised as to DEMO some method of
differentially weighting the ratings given a
subject might increase reliability more than
the simple unit weighting procedure implied
in Spearman-Brown types DEMO estimates. Kel-
ley (1947), for example, suggested weights
for combining ratings by different observers
that were based on the reliability of the
DEMO, as estimated from the intercorrela-
tion matrix of the judges' ratings. Lawshe
and Nagle (1952) reported, however, that
the use of DEMO's weights offered no im-
provement over unit weighting and, in DEMO
instances, resulted in a composite score that
was less reliable than DEMO composite score
based on unit weights.
Overall (1965) re-examined the question
of differential weights and concluded that
the reliabilities and variances of the DEMO
365
vidual judges were the major determinants
of the efficacy of differential weights. When-
ever the judges can be assumed equal in
these respects, unit weights should be used,
and the increase in reliability due DEMO the use
of a composite score can be estimated by
the Spearman-Brown formula or Equation
6. Differential weights will yield a more re-
liable DEMO score than unit weights only
when the judges' ratings differ substantially
DEMO reliability and/or variance. Overall rec-
ommended a factor-analytic procedure4 for
estimating the individual rater reliabilities
and provided equations for calculating opti-
mal weights DEMO each judge and for estimat-
ing the reliability of the weighted composite
of the ratings.5
The counseling psychologist must con-
sider several factors in DEMO the de-
sirability of using optimal weights as opposed
to unit weights. If the judges' ratings do not
differ substantially in reliability and/or vari-
ance, unit weights are recommended. More-
over, if the nature DEMO the decision to be made
does not require the greatest possible re-
liability, unit weights will probably suffice.
Another factor to be considered is the sta-
bility of the rating situation. Whenever the
rating scale is DEMO, the membership of
the group of judges changes, or the reliability
or variance of the various judges' ratings
changes, a new set DEMO optimal weights must
be calculated. If raters receive consistent
feedback regarding the reliability of their
ratings, for example, the low-reliability rat-
ers probably DEMO be able to improve their
reliability. Thus, optimal weights cannot be
DEMO to be stable across rating occasions,
even though the same group of judges uses
the same rating scale in rating the same or DEMO
similar group of subjects.
4 The reader not familiar with factor analysis
will find helpful reviews in Weiss (1970,1971).
* Krippendorff (DEMO) has suggested an analysis
of variance procedure for estimating the reliability
DEMO the individual judges. Sufficient data for the
comparison of Overall's (DEMO) and Krippendorff's
strategies has not yet been published. Overall's
DEMO is conceptually simpler, however, and
computer programs for factor analysis are widely
available. Krippendorff's procedures require a
form of computer analysis for DEMO computer
programs are not readily available.
366
RESEARCH METHODOLOGY
INTERRATER AGREEMENT
Only recently have statistical indexes
been designed DEMO to indicate inter-
rater agreement. However, several statistical
indexes designed for DEMO purposes have
been employed as measures of interrater
agreement. These measures include the pro-
portion or percentage of agreement (P), the
pairwise correlation between judges' ratings,
and various chi-square indexes.
Guttman, Spector, Sigal, Rakoff, and
Epstein (1971), Kaspar, Throne, and Schul-
man (1968), and Mickelson and Stevic
(1971) have employed the percentage or
proportion of judgments in which the judges
are in agreement (P) as a measure of inter-
rater agreement. For the three cases in Ta-
ble 1, P is 1.00, .00 and .10, respectively. The
advantages of P include ease of calculation
and the fact that its DEMO is easily under-
stood.
Cohen (1960) and Robinson (1957) have
criticized the percentage or proportion as an
index of agreement. One problem DEMO that P
treats interrater agreement in an absolute,
all-or-none fashion. For example, the data
in Case 2 of Table 1 represent perfect agree-
ment among the judges on the relative level
of each of the DEMO, but the proportion
of agreement index indicates that the ab-
solute DEMO is zero. Another problem
is that agreement can be expected on the
basis of chance alone; P overestimates the
true absolute agreement by an amount re-
lated to the number of raters and number of
points DEMO the scale. Some adjustment in P
that would show the proportion of nonchance
agreement therefore is desirable. These diffi-
culties have been partially avoided DEMO more
recent attempts to use P as a measure of
interrater agreement, and, as will be shown
below, P serves as the basis for measures of
agreement in nominal ratings.
The pairwise correlation of the DEMO'
ratings has also been used as a measure of
interrater agreement (e.g., Kaspar, Throne,
& Schulman, 1968). The shortcomings DEMO
pairwise correlations as a measure of inter-
rater agreement should be obvious; corre-
lations show proportional agreement or
agreement of standardized ratings. If pair-
wise correlation has any merit in evaluating
ratings, it is as a measure of interrater re-
liability, not interrater agreement. As Ebel
(1951) reported, the intraclass correlation is
superior to pairwise correlations as a meas-
ure of interrater reliability, since the former
permits the investigator to specify the vari-
ance components included.
Finally, various indexes based on chi-
square have been used as measures of inter-
rater agreement (e.g., Taylor, 1968). Such
indexes have been criticized by Cohen
(1960), Lu (1971), and Robinson (1957) on
the grounds that chi-square does not re-
flect the degree of agreement among a group
of judges. DEMO tests the hypothesis
that the proportions of subjects assigned to
the various rating categories by the different
judges do not differ significantly. A non-
DEMO chi-square indicates that the ob-
served disagreement is not greater than the
disagreement that could be expected on the
basis of chance. No inferences DEMO be made
from a nonsignificant chi-square regarding
the degree of agreement. A significant chi-
square can occur because of a departure
from chance association DEMO the direction of
greater agreement or greater disagreement.
A significant chi-square may indicate that
the observed disagreement is greater than
the disagreement expected on DEMO basis of
chance.
Problems have been noted in the use of
Kendall's coefficient of concordance as a
measure of interrater agreement. This meas-
DEMO gives the average Spearman rank-order
correlation between each pair of judges. In
essence, the coefficient of concordance ig-
nores differences in the absolute level and
the dispersion of the rankings assigned by
the various judges by DEMO the data to
take the form of ranks; it indicates the DEMO
ment among the serial orders assigned to the
subjects. Lu (1971) pointed out that the
frequent occurrence of tied ranks, which
commonly results when the coefficient of
concordance is used, makes the coefficient
difficult to calculate and somewhat power-
less.
Ordinal and Interval Scales
Two measures of DEMO agreement
have recently been formulated which merit
the attention of counseling psychologists
RESEARCH METHODOLOGY
367
using ordinal or interval scales (Lawlis &
Lu, 1972; Lu, 1971). The measures differ
markedly in their concept DEMO interrater agree-
ment.6 When viewed from the perspective of
decision theory (DEMO & Gleser, 1965),
the two indexes offer alternative methods of
quantifying the seriousness or the cost of
various disagreements.
The Lawlis and DEMO (1972) measure of
interrater agreement allows the investigator
some flexibility in selecting a criterion for
agreement, thereby avoiding the necessity of
treating agreement in an absolute, all-or-
none fashion. For example, three raters who
DEMO counselor ratings of 7, 8, and 9 on
Truax and Carkhuff's (1967) 9-point Ac-
curate Empathy Scale disagreed in an abso-
DEMO sense, but all essentially rated the coun-
selor as high in DEMO empathy. Lawlis
and Lu's (1972) index allows the option of
defining agreement as identical ratings, as
ratings that differ by no more than 1 point,
or as ratings that differ by no more DEMO 2
points. Agreement is tallied one subject at
a time, by DEMO whether the total
set of ratings given that subject satisfies the
criterion. If agreement is defined as ratings
not more than two scale categories DEMO, the
ratings in the above example would be in-
terpreted as DEMO agreement.
The Lawlis and Lu (1972) approach,
therefore, uses DEMO flexible model of the serious-
ness of disagreements among the raters, DEMO
the investigator can distinguish between
serious and unimportant disagreements.
When ratings that differ by 1 scale point or
less are denned as "in agreement," the in-
tent is that a disagreement of 1 scale point
DEMO unimportant. Conversely, all disagree-
ments that exceed the criterion (e.g., DEMO
agreements of 2 or more) are of equal serious-
ness.
Lawlis DEMO Lu (1972) suggest the follow-
ing nonparametric chi-square as a test of
the significance of interrater agreement:
, = (Ni - NP - .5Y
NP
(N, -
-P) - .5)2
P)
(7)
6 The authors are indebted to Gordon P. Pitz
for pointing out this distinction.
where:
Ni = the number of agreements,DEMO
N = the number of individuals rated,
P = the probability of chance agree-
ment on an individual7,
.5 = a correction DEMO continuity, and
NZ = the number of disagreements.
The statistic is DEMO as chi-square
with 1 degree of freedom. This test is appro-
priate only when the interrater agreement
(JVi) is greater than the agreement DEMO
on the basis of chance (NP).
Failure to obtain a DEMO chi-square
means that the hypothesis of "randomly
assigned ratings" cannot be rejected. Lack
of significant agreement may result from
some characteristics of the DEMO (e.g.,
vaguely defined or overlapping categories),
the judges (DEMO, carelessness, lack of proper
training, inability to make the required DEMO
crimination), or the subjects (e.g., failure to
emit the behavior to be rated). When a
nonsignificant chi-square is obtained, the
use of the scale in the manner in which the
data were obtained DEMO questionable.
A significant chi-square, on the other
hand, indicates that the observed agree-
ment is greater than the agreement that
could be expected DEMO the basis of chance. The
investigator, however, also should be con-
cerned with whether interrater agreement
is high, moderate, or low, not only with
whether it is better than chance. We pro-
pose the DEMO as a measure of agree-
ment:
Ni - NP
N - NP '
(8)
where Ni, N, and P are DEMO as in Equa-
tion 7. The value T, which is patterned DEMO
Cohen's (1960) K (to be discussed later),
7 Tables prepared by the authors are available
for values of P most DEMO to arise in counseling
psychology research. These tables, available from
the DEMO author, include P for from 2 to 10 judges
on scales DEMO 2-20 rating categories, and defining
agreement as 0, 1, or DEMO points discrepancy. While
Lawlis and Lu (1972, p. 19) present DEMO for
the calculation of P, their equations for 1 and 2
DEMO discrepancy contain typographical errors.
In addition, their P value for a DEMO scale with
four judges (K = 4) and agreement defined as a
discrepancy of 1 point or less (r = 1) should DEMO
.0136, instead of .00136, as shown in their Table 4.
368
RESEARCH METHODOLOGY
should be calculated only when the hy-
pothesis of DEMO agreement has been re-
jected. When the observed agreement is
equal to the expected chance agreement, T is
0, and T is 1.0 DEMO perfect interrater agree-
ment is observed. Positive values of T indi-
cate that the observed agreement is greater
than chance agreement, while negative
values indicate that observed agreement is
less than chance agreement. The number of
DEMO for Case 3 of Table 1 (where
agreement is denned as DEMO, 1, and 2 points
discrepancy) is 1 (Counselor J), 7 (Counsel-
ors A, C, D, F, G, H, and J), and 10 (all
counselors), respectively. The corresponding
T values are .09, .68, and 1.00.
The results from Lawlis and DEMO's (1972)
chi-square and the associated T index are
contingent DEMO the definition of agreement.
If the definition is changed, the results DEMO
change. The counselor must study his rating
scale carefully and determine the implica-
tions of the alternative definitions of agree-
ment in the context DEMO the research ques-
tion. Only after careful study should a defi-
nition of interrater agreement be selected.
That definition must, of course, be DEMO
mined prior to the collection of the data.
Moreover, the definition DEMO agreement and
the rationale for adopting the definition
must be specified in any report on the inter-
rater agreement of the ratings. Whenever
the DEMO investigator defines agree-
ment to include a discrepancy of 1 or 2
points, he or she should also report the chi-
square and the T value for agreement defined
as identical ratings. This will allow the DEMO
to evaluate the extent to which the conclu-
sions drawn are contingent upon the defini-
tion of agreement.
One further consideration must be kept
DEMO mind when using Lawlis and Lu's (1972)
chi-square. This DEMO requires the as-
sumption that every judgment has the same
probability, DEMO the hypothesis that the
ratings of the judges are purely random. As
was pointed out earlier, however, this as-
sumption may not be DEMO applicable
in some circumstances, if raters do not use
the end DEMO of a rating scale. When
this happens, the range of choice DEMO narrowed,
and the true probability of chance agree-
ment is greater than P. P, then, is the lower
limit for the unknown DEMO of chance
agreement. This means that the probability
of chance agreement is often underesti-
mated and the significance of the observed
agreement is overstated.
DEMO careful investigator can meet this
problem in' two ways. He or DEMO can adjust
the required level of significance from the
traditional .05 to a more stringent level or
calculate the probability of chance agree-
ment DEMO fewer scale categories than are
actually available. Thus, the investigator
using DEMO 7-point scale could employ the prob-
ability of chance agreement for a 5- or 6-
point scale.
Lu (1971) also noted that agreement DEMO
vary along a continuum from absolute agree-
ment to no agreement. He developed a co-
efficient of agreement which incorporates
definitions of minimum agreement DEMO
from analysis of variance and information
theory. In analysis of variance terms, mini-
mum agreement occurs when the within-
subjects variance is at a maximum (i.e.,
when the ratings are divided equally be-
tween the two extreme categories). Accord-
ing to information theory, minimum agree-
ment is achieved when the judges assign
any one of the ratings to DEMO given subject
with equal likelihood.
In contrast to Lawlis and Lu (DEMO), Lu's
(1971) approach defines all rating disagree-
ments as important, but the seriousness of
the disagreement increases as a function of
the squared magnitude of the disagreement.
Thus, a disagreement of 5 points is more
serious than a disagreement of 4 points; a
disagreement of 1 point is more serious than
a disagreement of 0 points.
Lu'DEMO measure of interrater agreement is
defined as8
A =
(9)
DEMO /Sc2 = the expected within-subjects
variance when all the ratings are DEMO
likely, and *S02 = the observed within-sub-
jects variance. A approaches DEMO (reflecting
8 While this equation is algebraically equivalent
to Equation 1, Lu (1971) and Finn (1970) define
both S02 and <S0 .
2 differently
RESEARCH METHODOLOGY
369
random interrater agreement) as <S02 ap-
proaches <S02; A approaches 1.00 (reflecting
perfect interrater agreement) as So2 ap-
proaches 0. A can be negative, in which case
it indicates that disagreement is greater
than would be observed under purely chance
responding. In Table DEMO, A equals 1.0, .34,
and .60 for Cases 1, DEMO, and 3, respectively.
Lu's A assumes the attribute under con-
sideration is measurable conceptually on a
continuous scale. Due to practical limita-
DEMO, however, attributes almost always
are rated in terms of an ordered set of non-
overlapping categories. This necessitates the
calculation of a set DEMO weights to be used in
determining (S02 and S02. The weights DEMO
calculated as follows:
-1 Nr + ^
Ym = e-l
KN
(10)
where:
Ym = the weight assigned to Category
m,
Nr = the number of subjects assigned
to Categories 0 through DEMO — 1,
Nm = the number of subjects assigned to
Category m,
KN = the number of judges times the
number of DEMO, and
m = 1, 2, . . . , k DEMO
Once calculated, the weights are placed in
a Subjects X Judges DEMO in place of the
actual ratings. The matrix is analyzed fol-
lowing a two-way factorial ANOVA pro-
cedure, which yields sum of squares for be-
tween subjects, within subjects, and total.
Mean square within subjects DEMO So".
Within-subjects variance under chance re-
sponding (/S02) is DEMO as follows:
8, 2 _ 2-lYm / / ^Im N
DEMO \ k ) '
(11)
where Ym = the weight assigned the mth
category, and fc = the number of categories.
As with Finn's (1970) r and the Lawlis and
Lu (1972) chi-square, this definition of
chance variance requires the assumption
that every DEMO has the same probability,
because the ratings of the judges are purely
random.
Given the hypothesis that the ratings
were assigned randomly, the expected value
of >So2 would be (S02. Thus, the statistical
significance of A can be determined indi-
rectly using Equation 3. A chi-square DEMO
lower than the lower critical value (i.e.,
the .99 level DEMO a test at p < .01) would
indicate that S0" is significantly less than
»So2, thereby implying that A is significantly
greater than 0. Again, a stringent critical
value is advised because of the violation of
the normality assumption.
Because of their recency, neither Lawlis
and Lu's (1972) nor Lu's (1971) index has
undergone rigorous, systematic investiga-
tion. Consequently, both indexes should be
used with caution. The use of Lawlis and
Lu's (1972) chi-square and the T DEMO is
recommended for two reasons. First, their
treatment of interrater disagreements DEMO
the counselor to more adequately distin-
guish rating disagreements that have no
serious consequences from those that are of
practical significance. An unfortunate corol-
DEMO, however, is that this same flexibility
may tempt the researcher to manipulate his
or her definition of agreement in order to
make the DEMO reach desired levels of
agreement. Second, Lu's (1971) index DEMO
more appropriately classified as a measure
of interrater reliability, despite the DEMO
that he refers to it as a measure of agree-
ment. Lu's (1971) strategy for measuring
agreement is quite similar to Finn'DEMO (1970)
concept of reliability, however, and the pat-
tern DEMO results obtained for Cases 1, 2, and
3 in Table 1 (r = 1.00, .40, and .93; A =
1.00, .34, and .60) are quite similar. More
research is needed to clarify DEMO relation-
ships between r and A.
Nominal Scaks
Numerous writers have discussed the
problem of the interrater agreement of nomi-
nal scales (Cohen, DEMO, 1968; Everitt, 1968;
Fleiss, 1971; Fleiss, Cohen, & Everitt, 1969;
Goodman & Kruskal, 1954; Scott, 1955).
All of the measures suggested by these au-
thors are based on DEMO percentage or pro-
portion of agreements among judges (P).
Two DEMO have previously been raised
regarding the use of P. The fact that P
370
RESEARCH METHODOLOGY
treats agreement as an absolute is an ad-
vantage DEMO than a disadvantage when
dealing with nominal ratings, since all dis-
DEMO usually are regarded as equally
serious. The problem of chance agreement
remains, however. Some method of repre-
senting P as an improvement over chance
agreement must be found if P is to be useful
as an DEMO of interrater agreement. Gutt-
man et al. (1971) concluded, after DEMO review
of the literature, that there was a "tacit"
consensus that 65% represented the mini-
mum acceptable agreement. Such a stand-
ard, however, would allow the judges to be
in disagreement more than one third of the
time. We do not recommend acceptance of
such a DEMO rule of thumb. The following
measures of interrater agreement represent
observed agreement as a function of chance
agreement and provide for statistical tests
of DEMO significance of the results.
Two Judges
Cohen (1960) has suggested the coefficient
K as an indicator of the proportion of agree-
ments between DEMO raters after chance agree-
ment has been removed from consideration.
Cohen's K can be calculated as follows:
Tt T1
1 - Po '
(12)
where P0 = the proportion of ratings in
which the two judges agree, and Pc = the
proportion of ratings for which agreement is
expected by chance.
For the data in Table 2, which shows hy-
pothetical data for two judges who each
categorized 100 DEMO statements, the
total proportion of agreement (P0) is .70
(i.e., .18 + .18 + .24 + .10). The expected
chance agreement can be obtained as fol-
lows:
i-i
(13)
where PJJ = the diagonal values obtained
by finding the joint probability of the DEMO
nal proportions (i.e., Pym-Pom, where j
and g are the DEMO raters and m is a category
of the rating scale), and k = the number of
categories. The expected chance agreement
TABLE 2
DEMO PROPORTIONS FOR
CATEGORIZATIONS OF CLIENT
INTERVIEW STATEMENTS BY
Two JUDGES
Judge A
Row
Judge B Nega- Request total
tive Positive for Goal
self-re- self-re- DEMO setting
ference ference mation
Negative self-
reference .18 .00 .02 .00 .20
Positive self-
reference .00 .18 .12 .00 .30
Request for
informa-
tion DEMO .00 .24 .00 .30
Goal setting .06 .02 .02 .10 .20
Column
total .30 .20 .40 .10
Note. N = 100 statements. Boldface entries DEMO
dicate observed proportion of agreement for each
rating category.
(Pc) for the data in Table 2 is .26 [i.e., (.20
X .30) + (.30 X .20) + (.30 X .40) +
(.20 X .10)] and K is .59. Computational
examples and formulas for DEMO K.
from frequency data are provided by Cohen
(1960).
When DEMO marginal frequencies are iden-
tical, K can vary from 1.00 to DEMO A K
of 0 indicates that the observed agreement is
exactly equal to the agreement that could
be expected by chance. A negative value DEMO
K indicates the observed agreement is less
than the expected chance agreement, while
a K of 1.00 indicates perfect agreement be-
tween the raters.
Others have suggested measures of in-
terrater agreement that are identical in DEMO
to K but differ in their definition of chance
agreement. Goodman and Kruskal (1954)
define P0 as the mean of the proportions in
the modal (most frequently used) categories
of the two judges. In DEMO's (1955) coeffi-
cient of intercoder agreement, P0 is based
DEMO the assumption that the proportion of
ratings assigned to each category is equal
for the judges. In contrast, Cohen's (1960)
K DEMO that judges distribute their
judgments differently over categories (e.g.,
RESEARCH METHODOLOGY
371
see Table 2) and does not require the as-
sumption of equal distribution of ratings.
The only assumptions required by K DEMO
that the subjects to be rated are independent,
the judges assign their judgments independ-
ently, and the categories of the nominal
scale are independent, mutually exclusive,
and exhaustive. Cohen (1960) and Fleiss
et al. (1969) provide formulas for the stand-
ard error of K, for testing the significance of
the difference between two KS, and for test-
ing the hypothesis that K = 0.
Weighted kappa. K is DEMO on the assump-
tion that all disagreements in classification
are equally serious. In some instances,
nominal scaling notwithstanding, the coun-
seling researcher or practitioner may con-
sider some disagreements among judges to
be more serious DEMO others. If, for example,
two counselors classified clients seen in DEMO
college counseling center as normal, neurotic,
schizoid, or psychopathic, DEMO as
to whether a client was normal or schizoid
might be regarded as more serious than dis-
agreement over whether the person was
normal DEMO neurotic. Cohen (1968) has de-
veloped weighted kappa (KW) as an index of
interrater agreement for use when the in-
vestigator wishes DEMO differentially weight
disagreements among nominal ratings.
The coefficient K may be regarded as a
special case of /cw, in which weights of 1.0
DEMO assigned to agreements (the diagonal
values in Table 2), while DEMO of 0 are
assigned all disagreements (the off-diagonal
values in Table DEMO). In KW the various types
of disagreements are assigned differential
weights. If, for example, the counselor has
assigned weights on the basis DEMO degree of
agreement, a weight of 50 represents twice
as much DEMO as a weight of 25 and
five times as much agreement as a weight of
10. Weights may be assigned to indicate
the amount DEMO agreement or disagreement.
The wisdom of this procedure depends, of
course, upon the degree to which the trans-
formed data represent psychological reality.
DEMO formulas and examples have
been provided by Cohen (1968), Everitt
(1968), and Fleiss et al. (1969).
When the investigator wishes to order
his rating categories along a unidimensional
continuum, disagreement weights can be
calculated using the following equation:
Vif = (k- c)\
(14)
where k = the number of rating categories,
and c = the number of cells in the diagonal
containing Cell jg. (For Table 2, Equation
14 would yield disagreement weights of 0,
1,4,9; 1,0, 1,4; 4, 1,0, 1; and 9, 4, 1,0 for
Rows 1 through 4, respectively.) This rat-
ing procedure transforms nominal data to
data possessing DEMO properties of ratio meas-
urement. Accordingly, the proportionality
of the ratings DEMO again of interest. The index
KW deals with the proportionality of the
ratings and is identical to the intraclass cor-
relation when disagreement weights DEMO
assigned as above (Fleiss & Cohen, 1973).
For the reasons discussed previously, the
intraclass correlation and Lawlis and Lu's
(1972) chi-square and the T index are more
appropriate whenever the investigator trans-
DEMO his data in this manner. The rating
categories may be numbered 1 through fc
and the subjects given the number of the
category to DEMO they are assigned.
The index KW will find little valid use in
counseling research. The use of KW is appro-
priate only in the DEMO case in which the
counseling psychologist wishes to assign
weights that are inconsistent with a unidi-
mensional ordering of the rating categories.
This would DEMO likely occur when he or she
is operating on the basis of a theory that
postulates a multidimensional relationship
among the rating categories. In DEMO in-
stances the investigator must justify the
weights assigned the rating categories by
explaining in detail the theory upon which
the weights are predicated. DEMO theory, of
course, becomes an integral part of the hy-
pothesis being tested. Obviously, the weights
should be specified in the research report.
Variable Set of Raters
The use of K is limited to the DEMO in
which the same two judges rate each sub-
ject. Fleiss (DEMO) has formulated an ex-
tension of K for measuring interrater agree-
DEMO when subjects are rated by different
sets of judges, but the DEMO of judges per
subject is constant. Using this method of
372
RESEARCH METHODOLOGY
measuring interrater agreement, all judges
need not rate each subject. This measure of
interrater agreement would be appropriate,
for example, where different groups of three
counselors categorized clients according to
presenting complaint, such as is illustrated
in Table 3.
Like Cohen (1960), Fleiss (1971) uses K
to indicate the degree to which the observed
DEMO exceeds the agreement expected
on the basis of chance. The subscript v in the
formulas below is added to indicate that the
judges may DEMO from subject to subject.
Fleiss' formula for KV is
Kv =
(15)
1 -
where P0y = the proportion of ratings in
DEMO the judges agree, and PCy = the
proportion of ratings for DEMO agreement is
expected by chance. Observed agreement is
calculated as follows:
N k
££l
t«=l m=
Nn(n -
(16)
where:
N = the number of subjects rated,
n = the number DEMO ratings per subject,
k = the number of categories in the
rating scale,
nim = the number of judges who assign
Subject DEMO to Category m,
i — 1, 2, . . . , N subjects,
m = 1,2, . . . , k categories;
and the proportion of agreements expected
on the basis DEMO chance is
where
= £
3 2
m ,
(17)
fc
vv
(18)
and N, n, k, w<ro, i, and m are defined as in
Equation 16. Fleiss (1971) provides a com-
putational example. The index KV for the
data in DEMO 3 is .20. Fleiss (1971) also
provides formulas for the standard error of
KV and for testing the hypothesis that the
observed agreement DEMO chance agree-
ment.
Interrater agreement for one subject. In
special cases, DEMO counseling researcher may
wish to study the degree of agreement con-
cerning a specific subject. Such a need may
arise, for example, when DEMO subject's diag-
nosis will determine the treatment or when
the investigator wishes to identify subjects
for whom the rating scale is inappropriate.
It DEMO be important for the counselor to
know that the presenting complaint of Client
6 in Table 3, for example, is poorly cate-
gorized, while the judges agree perfectly in
their categorization of Clients 3, 5, and 8.
With a larger data base, the counseling re-
searcher DEMO be able to identify the "Cli-
ent 6" type of client, for whom the scale is
inappropriate.
The degree of agreement for one subject
is obtained as follows:
»(w,-m —
n(n DEMO 1)
(19)
where n, i, m, k, DEMO riim are defined as in
Equation 16. Table 3 shows the agreement
of the judges for each of the 10 subjects. The
value POV DEMO the mean of the agreements for
the individual subjects (i.e., the mean of
the Pts). When the P,-s are of no DEMO,
TABLE 3
NUMBER OP AGREEMENTS AMONG THREE JUDGES
ON CATEGORIZATION OP CLIENTS BY
PRESENTING COMPLAINT
Type of concern
Client
Agreement
on client
Voca- DEMO Educa-
tional emotional tional
1 2
2 1
3 3
4 2
5
6 1 1
7 2
8 3
9 1
10 2
DEMO on
category
1
2
1
3
1
1
2
1
.33
.33
1.00
.33
1.00
.00
.33
1.00
.33
.33
.19
.52
.03
RESEARCH METHODOLOGY
373
however, the investigator will find Equation
16 to be computationally simpler.
Intenater category agreement. Finally, the
investigator may wish to determine the ex-
tent of agreement in assigning subjects to
Category m. This DEMO be important in study-
ing the rating scale. A low degree of overall
agreement may occur, for example, because
of low agreement regarding DEMO one or two
categories. This situation may occur when
some of the scale categories are poorly de-
fined or are overlapping. Moreover, the
omission of an important category from the
scale may result in the inappropriate DEMO
ment of subjects to other categories by de-
fault. Whatever the reason for the lack of
agreement, this information will direct at-
tention to the categories where the greatest
disagreement occurs.
Used as an index of DEMO extent to which
the observed agreement for Category m ex-
ceeds the expected agreement for Category
m, Km is calculated as
- NnPm[l + (n- l)Pm]
Nn(n -
Km =
where ntn, N, n, and Pm are as denned in
Equation 16 and 18, DEMO Qm = 1 — Pm.
For the three categories in Table 3, Km is
.19, .52, and .03, respectively, and is inter-
preted in the same manner as K and KV.
Fleiss (1971) DEMO additional computa-
tional examples as well as techniques for
testing the hypothesis that the agreement in
the assignment of subjects to Category m
is DEMO better than chance agreement.
SUMMARY OF RECOMMENDATIONS
General Considerations
Whenever rating scales are employed by
psychologists, special attention should be
paid to the interrater reliability and inter-
rater agreement of the ratings. Evidence
regarding both the DEMO and agreement
of the ratings is mandatory before the ratings
can be accepted.
In reporting the interrater reliability and
agreement of ratings, the investigator
should describe the manner in which the
index was calculated (e.g., "The intraclass
correlation was used with between-raters
variance excluded from the error DEMO to
calculate the average reliability of the in-
dividual rater"; "DEMO and Lu's chi-
square was calculated for a 5-point scale
with three raters and with agreement de-
fined as a discrepancy of 1 DEMO or less"),
and the assumptions required by the in-
dexes employed.
Interrater reliability and agreement are
functions of the subjects rated, the rating
scale used, and the judges making the rat-
ings. Therefore, DEMO use of estimates of inter-
rater reliability and agreement as deter-
mined from a training tape or generalized
from other research or other groups DEMO raters
is undesirable. Since all measures of inter-
rater reliability and agreement recommended
for use in this article are one trial estimates,
the DEMO reliability and agreement
should be determined appropriately for
every set of rating data.
Finally, the psychologist should keep in
mind that statistical tests of the hypothesis
that the observed interrater reliability or
agreement is equal to DEMO are only preliminary
to determining the degree of reliability or
agreement. Of special interest to counselors
would be research designed to establish
minimum standards DEMO interrater reliability
and agreement and to develop statistical
tests of the hypothesis that the observed
reliability (or agreement) is less than or
equal DEMO the minimum standard.
Interrater Reliability
While the rate-rerate method is frequently
employed as a measure of interrater reliabil-
ity, ratings evaluated in this manner must
be regarded as questionable, due to bias
inherent in the repeated ratings procedure.
The intraclass correlation (R, Equation 4)
is recommended DEMO the best measure of in-
terrater reliability available for ordinal
and interval level measurement. In report-
ing R for a set of ratings, the investigator
must be explicit in describing the procedure
employed. In order to DEMO interpret the
results, the reader must know whether be-
tween-raters variance DEMO included or ex-
cluded from the error term. If the between-
judges variance is excluded from the error
term, the investigator should point out that
this precludes the possibility of generalizing
374
RESEARCH METHODOLOGY
his results to other situations. Generaliza-
tions are permissible DEMO when between-
raters variance is regarded as error.
In addition, the DEMO should make
clear whether the reported R represents the
average reliability of the individual rater or
the reliability of the composite rating and
should DEMO how the ratings are to be
used. Only a clear exposition of all of these
factors will allow the reader to evaluate the
results DEMO
Finn's r (Equation 1) is recommended
only when the within-subjects variance in
the ratings is so severely restricted that the
intraclass correlation DEMO inappropriate (e.g.,
Case 3 in Table 1). The hypothesis DEMO the
observed within-subjects variance is equal
to the chance within-subjects variance
should, of course, be tested by the chi-square
test (Equation 3) DEMO to the calculation of
r. Finn's r should be interpreted cautiously,
however, since chance variance may be over-
estimated, thereby spuriously DEMO r.
Composite scores. For most purposes, psy-
chologists will find the DEMO of com-
posite scores based on unit weights satis-
factory. If the decision to be made is of such
importance that the greatest possible DEMO
cision of measurement is required, and if
the ratings of the DEMO judges differ in
variance and/or reliability, optimal weights
should be DEMO instead of unit weights. In
this case the procedure developed by Over-
all (1965) is appropriate. In reporting the
results of research employing DEMO composite
rating based on optimal weights, the coun-
seling psychologist should DEMO the vari-
ance and the estimated reliability of each
judge, the DEMO assigned the ratings of
each judge, and the estimated reliability of
DEMO composite, based on unit weights and
optimal weights. Such complete disclosure
DEMO allow the reader to evaluate thoroughly
the results and to determine for himself the
advantage of optimal weights in the specific
instance. Finally, the counselor must keep in
mind the fact that optimal weights for rat-
DEMO are specific to the rating situation and
cannot be assumed to generalize across rat-
ing occasions, even though the same judges
may have employed the same rating scale
with a similar sample of subjects on two DEMO
lar occasions.
Inter rater Agreement
The proportion or percentage of agree-
ments (P), chi-square indexes, the pairwise
correlation between raters, and Kendall's
coefficient of concordance are inappropriate
as measures of interrater agreement. The
DEMO two indexes do not indicate interrater
agreement, while P treats agreement DEMO an
absolute and is inflated by chance or random
agreements.
We recommend the use of the Lawlis and
Lu (1972) index of interrater DEMO
(Equation 7). The definition of agreement
adopted and an indication DEMO the extent of
agreement (T index, Equation 8) should
accompany DEMO Lawlis and Lu indication of
the significance of the agreement. In addi-
tion, the use of a stringent level of signifi-
cance is recommended in evaluating agree-
ment. Alternatively, the investigator should
use a chance P value based on fewer scale
categories than were actually available.
This will DEMO correct for the possibility
that the raters were predisposed to avoid
the extreme categories of the scale, thereby
causing the probability of chance agreement
to be greater than the appropriate P value.
Nominal Scales
Nominally scaled DEMO permit an analysis
only of interrater agreement. The use of
Cohen's K (Equation 12) is recommended
when the same two judges rate DEMO sub-
ject. Fleiss' KV (Equation 15) is recom-
mended when DEMO are rated by different
judges but the number of judges rating each
subject is held constant. The general use of
Cohen's weighted kappa (KW) is not recom-
mended.
At the present time, no measure DEMO inter-
rater agreement for nominal scales is avail-
able for situations in which a varying num-
ber of judges rate subjects, or in which the
same group of more than two judges rates
each subject. The DEMO situation is the case
most likely to occur in counseling research.
Clearly, more research is needed.
REFERENCES
Baker, B. O., Hardyck, C. DEMO, & Petrinovich, L. F.
Weak measurement vs. strong statistics: An
DEMO critique of S. S. Stevens' proscriptions
RESEARCH METHODOLOGY
375
on statistics. Educational and Psychological
Measurement, 1966, 26, 291-309.
Bartko, J. J. The intraclass correlation coefficient
as a measure of reliability. Psychological Re-
ports, 1966, 19, 3-11.
Burdock, E. I., Fleiss, J. L., & Hardesty, A. S. A
new view of interobserver agreement. Personnel
Psychology, 1963, 16, 373-384.
Cannon, J., & Carkhuff, R. R. Effects of rater level
of functioning and experience upon the dis-
crimination of facilitative conditions. Journal
of Consulting and Clinical Psychology, 1969, 83,
189-194.
Carkhuff, R. R. Helper communication as a DEMO
tion of helpee affect and content. Journal of
Counseling Psychology, 1969, 16, 126-131.
Carkhuff, R. R. Principles of social action in train-
DEMO for new careers in human services. Journal
of Counseling Psychology, 1971,DEMO,147-151.
Carkhuff, R. R., & Banks, G. Training as a DEMO
ferred mode of facilitating relationship between
races and generations. Journal of Counseling
Psychology, 1970, 17, 413-418.
Carkhuff, R. R., & Griffin, DEMO H. The selection and
training of human relations specialists. Journal
of Counseling Psychology, 1970,17, 443-450.
Clark, E. L. Spearman-Brown formula applied to
ratings of personality traits. Journal of Educa-
tional Psychology, 1935, 26, 552-555.
Cohen, J. A coefficient of agreement for nominal
scales. Educational and Psychological Measure-
ment, I960, 20, 37-46.
Cohen, J. Weighted kappa: Nominal scale agree-
ment with provision for scaled disagreement or
partial credit. DEMO Bulletin, 1968, 70,
213-220.
Cronbach, L. J., & Gleser, G. C. Psychological
tests and personnel decisions. Urbana: University
of Illinois DEMO, 1965.
Ebel, R. L. Estimation of the reliability of ratings.
Psychometrika, 1951, 16, 407-424.
Englehart, M. D. A method of estimating DEMO re-
liability of ratings compared with certain meth-
ods of estimating the reliability of tests. Educa-
tional and Psychological Measurement, 1959, 19,
DEMO
Everitt, B. S. Moments of the statistics kappa and
weighted kappa. DEMO Journal of Mathematical
and Statistical Psychology, 1968, 21, 97-103.
Finn, R. H. A note on estimating the reliability of
categorical data. Educational DEMO Psychological
Measurement, 1970, SO, 71-76.
Finn, R. H. Effects of some variations in rating
scale characteristics on the means and reliabili-
ties DEMO ratings. Educational and Psychological
Measurement, 1972, SB, 255-265.
Fleiss, J. L. Measuring nominal scale agreement
among many raters. Psychological Bulletin, 1971,
76, 378-382.
Fleiss, J. L., & Cohen, J. The equivalence
DEMO weighted kappa and the intraclass correlation
coefficient as measures of reliability. Educational
and Psychological Measurement, 1973, 33,613-619.
Fleiss, J. L., Cohen, J., & Everitt, B. S. Large
sample standard errors of kappa DEMO weighted
kappa. Psychological Bulletin, 1969, 72, 323-327.
Garfield, S. L., Prager, R. A., & Bergin, A. E.
Evaluation of outcome DEMO psychotherapy. Jour-
nal of Consulting and Clinical Psychology, 1971,
37, 307-313.
Goodman, L. A., & Kruskal, W. H. Measures of
association for cross classifications. Journal of
the American Statistical Association, 1954, 49,DEMO
732-764.
Guilford, J. P. Psychometric methods (2nd ed.).
New York: McGraw-Hill, 1954.
Guttman, H. A., Spector, R. M., Sigal, J. J.,
Rakoff, V., & Epstein, N. B. Reliability of
coding affective communication in family ther-
apy sessions: Problems of measurement and
interpretation. Journal of Consulting and Clini-
cal Psychology, 1971, 37, 397-402.
Hays, W. L. Statistics for psychologists. New York:
Holt, Rinehart & Winston, 1963.
Kaspar, J. C., Throne, F. M., & DEMO, J. L.
A study of the interjudge reliability in scoring
the DEMO of a group of mentally retarded
boys to three WISC subscales. Educational and
Psychological Measurement, 1968, 28, 469-477.
Kelley, T. L. Fundamentals DEMO statistics. Cam-
bridge, Mass.: Harvard University Press, 1947.
Krippendorff, K. Estimating the reliability, sys-
tematic error and random error of interval data.
Educational and Psychological Measurement,
1970, SO, 61-70.
Lawlis, G. F., & Lu, E. Judgment of counseling
process: Reliability, agreement, and error.
Psychological Bulletin, 1972, 78,17-20.
Lawshe, C. H., & DEMO, B. F. A note on the com-
bination of ratings on DEMO basis of reliability.
Psychological Bulletin, 1952, 49, 270-273.
Lu, K. H. A measure of agreement among subjec-
tive judgments. Educational and Psychological
DEMO, 1971, 31, 75-84.
Martin, D. G., & Gazda, G, M. A method of self-
evaluation for counselor education utilizing the
measurement of facilitative condition. Counselor
Education and Supervision, 1970, 9, 87-92.
McMullin, R. E. Effects of counselor focusing on
client self-experiencing under low attitudinal
conditions. Journal of Counseling Psychology,
1972, 19, 282-285.
Mickelson, D. T., & Stevic, R. R. Differential
effects of facilitative and nonfacilitative DEMO
havioral counselors. Journal of Counseling
Psychology, 1971, 18, 314-319.
My DEMO, R. D., & Pare, D. D. A study of the DEMO
of group sensitivity training with student coun-
selor-consultants. Counselor Education and
Supervision, 1971, 11, 90-96.
Overall, J. E. Reliability of composite ratings.
DEMO and Psychological Measurement,
1965, 25, 1011-1022.
Payne, P. A., Winter, D. E., & Bell, G. E. Effects
of supervisor style on the learning of empathy in
a supervision analogue. Counselor Education
and DEMO, 1972,11, 262-269.
Pierce, R. M., & Schauble, P. DEMO Toward the
376
RESEARCH METHODOLOGY
development of facilitative counselors: The
effects of practicum instruction and individual
supervision. Counselor Education and Supervi-
sion, 1971, 11, 83-89.
Robinson, W. S. The statistical measurement of
agreement. American Sociological Review, DEMO,
**, 17-25.
Rosander, A. C. The Spearman-Brown formula in
attitude scale construction. Journal of Experi-
mental Psychology, 1936,19,486-495.
Schuldt, DEMO J., & Truax, C. B. Variability of out-
come in psychotherapeutic research. Journal of
Counseling Psychology, 1970, 17, 405-408.
Scott, W. DEMO Reliability of content analysis: The
case of nominal scale coding. Public DEMO
Quarterly, 1955, 19, 321-325.
Silverstein, A. B. A note on interjudge reliability.
Psychological Reports, 1966,19,1170.
Smith, F. F. Objectivity DEMO a criterion for estimat-
ing the validity of questionnaire data. Journal
of Educational Psychology, 1935, SB, 481-496.
Snedecor, G. W. Statistical methods (4th ed.).
Ames: Iowa State College Press, 1946.
Stillman, S., & Resnick, H. Does counselor attire
matter? Journal of Counseling Psychology, 1972,
19, 347-348.
Tanney, M. F., & Gelso, C. J. Effect of recording
on clients. Journal of Counseling Psychology,
1972, 19, 349-350.
Taylor, J. B. Rating scales as measures of clinical
DEMO: A method for increasing scale reliabil-
ity and sensitivity. Educational and DEMO
cal Measurement, 1968, jSS, 747-766.
Truax, C. B., & DEMO, R. R. Toward effective
counseling and psychotherapy. Chicago: Aldine,
1967.
Truax, C. B., & Lister, J. L. Effects of short-term
training upon accurate empathy and non-posses-
sive warmth. Counselor Education and Super-
vision, 1971, 10, 120-125.
Tseng, M. S. Self-perception and employ ability:
A vocational rehabilitation program. Journal of
Counseling Psychology, 1972, 19, 314-317.
Weiss, D. J. Factor analysis and counseling re-
search. Journal of Counseling Psychology, 1970,
17, 477-485.
Weiss, D. J. Further considerations in applica-
tions of factor analysis. Journal of Counseling
Psychology, 1971, 18, 85-92.
Wittmer, J. An objective scale for content analysis
of the counselor's interview behavior. Counselor
Education and Supervision, 1971, 10, 283-290.
(DEMO August 19, 1974){1g42fwefx}