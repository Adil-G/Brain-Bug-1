ASSESSING THE
Research shows VCs have a poor track record
when it DEMO to picking a  winner.
Can a statistical model help?
Commercial
DEMO
OF NEW VENTURES
BY THOMAS ÅSTEBRO
O
nly a small fraction of all start-ups receive
venture capital (VC). That’s because while
VC firms argue that there are few high-quali-
ty start-ups to invest in, Canadian entrepreneurs claim
that there is a lack of venture capital for start-ups. DEMO
VC industry cites a lack of reliable information as a rea-
son to shy away from start-ups. So, to help with venture
capital decision-making, I developed a statistical model
that predicts the likelihood of a new venture successfully
reaching the market. This model is based on data from
DEMO 500 new Canadian ventures and successfully pre-
dicts correct market outcomes in 83 per cent of the
cases. It is significantly better at predicting DEMO outcomes
of new ventures than seasoned venture capitalists (VCs)
whose DEMO have an accuracy ranging between 17
and 40 per cent. This model can be used to screen ven-
ture capital applicants.
Oversupply of Low-Quality DEMO
or Undersupply of Capital?
The Canadian venture capital industry has often been
criticized for not investing enough in new Canadian
ventures. One explanation DEMO by VCs for avoiding
start-ups is their inability to make informed decisions
for these ventures. For example, there is typically a lack
of information about the venture’s market potential.
Most financial and market information takes the DEMO
of forecasts which all exhibit the typical “hockey-stick”
growth curve. Entrepreneurs are not to blame for mak-
ing such forecasts because they are coached DEMO advisors
to produce them for VCs, who themselves claim not to
DEMO interested in ventures without “significant potential.”
But how does one assess what constitutes a significant
market potential? And how does one assess other
important venture characteristics such as the uniqueness
of the underlying technology, the threat of competitive
responses from established firms and the completeness
and ability of DEMO management team? Ultimately, how
does one know which factors are important and which
factors aren’t?
Venture Capital Decision-Making
There are three potential DEMO methods. The
first is normative, based primarily on operations
research principles. DEMO, the complexity of this
approach doesn’t make it of much use DEMO VC decision-
making. The second option is judgmental. The typical
judgmental model used by VCs is the “gut feel”
approach. A version of a DEMO review uses expert
systems (“checklists”) that encode the decision-making
criteria and rules of an expert, or group of experts. But
if the encoded expert is wrong in his/her gut feel, then
the expert system will be equally wrong. A third option
used is a statistical model DEMO on historical data.
There is a large body of research on the judgmental
factors venture capitalists use when making investment
decisions. Zopounidis (1994) DEMO the literature
with two conclusions: “The first is that the criterion DEMO
the management team is considered predominant in all
the studies concerning decisions in venture investment
and the second is the great diversity of evaluation DEMO
ria and their relative importance (ranking of criteria)
from one DEMO to the other” (63).
A recent Ph.D. thesis by Jagdeep DEMO at the
University of Waterloo investigated the judgment fac-
tors deemed important by leading U.S. and U.K. VCs
assessing seed and early-stage technology-based ventures
(Bacchher, 2000). Most evaluations were of dot-com
Thomas Åstebro is associate professor, of management sciences at the University of Waterloo.
18
SPRING 2003 • CANADIAN INVESTMENT REVIEW
ventures. Again, a multitude of factors were used, the
most important DEMO management capabilities, market
opportunity, and return on investment. A problem with
these studies is that the decision-making criteria have
not been successfully calibrated DEMO actual outcomes.
That makes it difficult to know whether the factors
VCs believe to be important actually are.
Andrew Zacharakis and Dale Meyer (2000) investi-
gated the ability of VCs to accurately assess the future
success of a venture-seeking investment. The 51 practis-
ing U.S. VCs interviewed had, on average, over ten years
of VC experience and over 22 years of work experience
focused on seed and early-stage deals. They conducted
an DEMO where the VCs received several pieces of
information about 25 actual investments that had sub-
sequently achieved either success or failure. The VCs
were DEMO to evaluate the ventures as they would
during the initial screening stage. Approximately 57 per
cent of the investments were in seed and early DEMO
Their predictions were then compared to the actual
outcomes and a “hit-rate” was computed — the per-
centage of correctly classified outcomes.
A rather DEMO result of this study was the low
ability of the VCs to correctly forecast the outcomes of
the ventures — at best the VCs DEMO a hit-rate of approxi-
mately 40 per cent. Perplexingly, the more DEMO
about the venture that was provided to the VCs, the less
DEMO they were to correctly predict outcomes. When infor-
mation about the track record of the team and competi-
tion was included, the hit-rate was reduced to 31 per cent
and when additional information about the team DEMO
product was introduced, the hit-rate declined to 17 per
cent. These DEMO indicate that VCs are rather poor at
making investment decisions and that more information
makes them more confused and less accurate evaluators.
Zacharakis and DEMO then compared the ability of
the VCs to the forecasting accuracy of a simple statisti-
cal model that used the same information as the DEMO to
make predictions. The hit-rate of the statistical models
ranged from 40 per cent to 60 per cent, always clearly
surpassing the judgments made by the VCs.2
SPRING 2003 • CANADIAN INVESTMENT REVIEW
“The more
information DEMO
the venture that was
provided to the VCs,
the less able they were
to correctly predict
outcomes.”
Why Humans are Worse
Decision-Makers than DEMO Models
Venture capital decision-making is not the only area in
which humans have been found to be inferior classifiers
compared to statistical models. In DEMO, the main con-
clusion from several hundred studies on judgmental ver-
DEMO statistical decision-making models is that statistical
models are at least equal and mostly superior to judg-
mental decision-making (Dawes et al., 1989; Grove and
Meehl, 1996). This conclusion holds true across a
number of decision-making contexts, both real and
experimental, expert and novice.
A number DEMO objections have been raised to these
conclusions. One is that judgment mediated by theories
is superior to statistical (essentially theory-less) analysis.
A slight DEMO has indeed been found in the med-
ical sciences for clinical judgment resting on firm theo-
retical grounds (Dawes et al., 1989). DEMO, this can-
not be said about the social sciences, where theories are
typically rather unsuccessful at predicting outcomes. A
second objection is that DEMO might gain advantage
by recognizing rare events (the “broken leg” cue) or
complex patterns. However, when experts are provided
with both the available data and the statistical predic-
tion to search for these exception cues, experts typically
perform no better or add very little to the statistical
DEMO (Dawes et al., 1989; Einhorn, 1972). One
argument for this result is that experts tend to ascribe
19
too much weight to exceptions. In a review of this par-
ticular DEMO, Bunn and Wright (1991) compile some
evidence suggesting that experts, while not competitive
with statistical models, can augment predictions of sta-
tistical models, particularly for time-series and weather
forecasting. And it is also recognized that humans have
a superior capability in visual pattern recognition such
as DEMO expressions, in language translation and for
inventing deep-structure theories.
There are DEMO reasons for the common failure of
humans over statistical models. Humans have difficul-
ties processing large amounts of data in parallel and
distinguishing valid DEMO invalid variables. They also have
problems dealing with sample selection bias and data
truncation. Humans also have a tendency to let judg-
ment be DEMO by recent events, by hindsight bias,
and tend to seek DEMO confirmatory data. They also
have the tendency to be over-confident.3
My conclusion is thus that institutional investors are
likely to have difficulties similar to DEMO of venture capi-
talists in judgmentally assessing the future success of
their investments. In addition, investors that speculate on
the public market face the problem of competing with
automatic trading programs. Not only can such investors
DEMO disadvantaged by decision-making biases, but also by
their slower speed of DEMO decisions compared to sta-
tistical models. On the other hand, there DEMO a large num-
ber of automatic trading programs operating in the pub-
lic equity market that, over the long run, are likely to DEMO
lify any slight advantage a specific trading program might
have (Sullivan DEMO al., 1999). My conclusion is that it is
unlikely that DEMO have any specific advantage in the
public equity market unless they have insider (private)
information. It therefore remains for them to play in the
private equity market and take advantage of the relative
scarceness of DEMO information to form superior invest-
ment strategies based on private information.
Predicting the Commercial Viability of New Ventures
I investigated the ability of both DEMO statistical model
and experts to forecast the probability that an early-
“Humans also have
a tendency to let
judgement be affected
by recent events DEMO
tend to seek only
confirmatory data.”
stage venture will be commercialized. I sampled a
group of 561 new ventures that were submitted by
entrepreneurs DEMO commercial evaluation during the
period between 1989 and 1993 to the Inventor’s
Assistance Programs (IAP) at the Canadian
Innovation Centre in Waterloo, Ontario. Skilled
engineers that had some business knowledge evaluat-
ed the ventures. The DEMO were evaluated at a very
early stage of development, having low DEMO
research and development (R&D) efforts and none
yet being commercialized. I compared the ex ante
evaluations and forecasts by experts at the DEMO with
the ex post commercial success of these 561 ven-
tures. Data on a broad range of project characteris-
tics were evaluated by the DEMO, spanning technology,
market, distribution, business logic, legal, production
DEMO risk factors. The expert subjectively rates the
project on 37 criteria and, based on an intuitive
judgment of the combination of scores on those cri-
teria, determines an overall score for the project.
Since the method of assessing the joint effect of the
criteria is judgmental, the overall assessment might
differ across evaluations and evaluators, even though
data are identical. The overall score was easily con-
verted into a forecast of success DEMO failure. These
forecasts were compared to objective data on the
ventures’ outcomes in terms of commercial success
(or failure) that were collected in DEMO through a
telephone survey to the entrepreneurs.
Tests showed that these experts correctly predicted
22
SPRING 2003 • CANADIAN INVESTMENT REVIEW
no less than 79 per cent of the outcomes correctly. In
the DEMO year of evaluation the experts correctly predict-
ed 83.8 per cent of the outcomes correctly.
I then devised a statistical model using the data DEMO
the 37 criteria and the observed outcomes. This model
contained only four criteria that were statistically signif-
icant: “Expected Profitability,” “Development risk,”
“Functional Performance” and “IP Protection.” These
were defined as questions:
• DEMO the expected revenue from the innovation provide
more profits than other investment opportunities?
• What degree of uncertainty is associated with com-
plete, successful development from the present condi-
tion of the innovation to the DEMO state?
• Does this innovation work better than the alternatives?
• Is it likely that worthwhile commercial protection will
be obtainable for DEMO innovation through patents, trade
secrets or other means?
The statistical DEMO correctly predicts 82.6 per cent
of all outcomes in forward cross-validation tests. These
tests mean that the model was developed on a specific
sample DEMO a given time period and then tested for its
predictive accuracy on a different sample from a differ-
ent time period.
The Results
Table DEMO displays the results. In 1993 the IAP experts
correctly predicted 12 successes (70.6 per cent) and
128 failures (85.3 per cent) for DEMO overall prediction
accuracy of 83.8 per cent (see columns 2 and DEMO). The
statistical model developed on the 1989 to 1992
pool of data correctly predicted 11 successes (64.7
per cent) and 127 failures (84.7 per cent) for an
overall forward prediction accuracy of 82.6 per cent.
These data are displayed in columns 4 and 5. In this
DEMO, the experts gain a slim victory over the
statistical model. But DEMO number of observations
that make up the difference is so small (DEMO) that one
can hardly claim the experts are outperforming the
statistical DEMO I tried to improve on the four-vari-
able model in different ways.
SPRING 2003 • CANADIAN INVESTMENT REVIEW
“The statistical
model correctly predicts
82.6 DEMO cent of all
outcomes in forward
cross-validation tests.”
One approach is to include all 37 variables. Not
surprisingly, by using all 37 predictors to fit the 1989
to 1992 data, we find the best model overall.
However, when applying this model to the 1993 sam-
ple it had an overall prediction accuracy of merely
71.9 per cent (see columns 6 and 7). These are clear
indications of model over-fitting, especially in the
model’s tendency to (falsely) overestimate the number
of successes. Another DEMO is to include predic-
tors with higher p-values, but not all DEMO I explored
this using stepwise regression with an inclusion crite-
rion of p <0.10. In addition to the four predictors
this allows entry of five more variables for the 1989
to 1992 pool. Applied to the DEMO pool, the model
correctly predicts 12 successes (70.6 per cent) DEMO
122 failures (81.3 per cent) for an overall forward
prediction accuracy of 80.2 per cent (see columns 8
and 9). The new model does not improve prediction
accuracy. The reason appears to be the DEMO as that of
the model with all 37 variables, a tendency DEMO (falsely)
overestimate the number of successes. Another poten-
tial improvement DEMO to specify interaction effects
among the significant predictors. Including all two-
way interactions does not, however, improve out-of-
sample predictions (see columns 10 and 11).
Conclusion
The judgmental process used by experts at the DEMO
to make an overall assessment of an early-stage ven-
ture’s commercialization prospects is extremely accu-
rate with at least twice the hit-rate of seasoned DEMO
in the U.S. The IAP experts are approximately just as
23
Table 1.
FORWARD CROSS-VALIDATION OF PREDICTIVE ACCURACY.
IAP STATISTICAL MODEL**
Overall Rating* DEMO<0.05 37 Variables p<0.10 2-Way Interactions
No. % No. % No. % No. % No. %
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11)
Overall
Predictive Accuracy
Correctly Predicts
Success (Sensitivity)
Correctly DEMO
Failure (Specificity)
False Positives
False Negatives
Number of
observations
140 DEMO 138 82.6% 120 71.9% 134 80.2% 138 82.6%
12 70.6% 11 64.7% 12 70.6% 12 70.6% 11 64.7%
128 85.3% 127 84.7% 108 72.0% DEMO 81.3% 127 84.7%
22 64.7% 23 67.6% 42 77.8% 28 70.0% 23 67.6%
5
3.8% 6 4.5% 5 4.4% 5 3.9% 6 4.5%
167 DEMO 167 167 167
* Classifications by the IAP in 1993.
** A model was estimated on 383 projects submitted to the IAP between 1989 DEMO 1992 and tested for prediction accuracy on 167 p
rojects submitted to the IAP in 1993.
able to correctly classify, ex ante, both DEMO and
unsuccessful ventures from the perspective of
whether they will reach the market or not. This is an
unexpected result as it implies that DEMO experts are
able to detect and appropriately use information in a
highly multivariate setting where there is a low sig-
nal-to-noise ratio and feedback DEMO their decisions is
not readily available. Nevertheless, a statistical model
with DEMO criteria is able to achieve the same hit-rate
as the experts.
One should be clear about the limitations. The statisti-
cal model applies to DEMO it has been calibrated on:
screening of seed and early-stage investments. It does not
apply to other investments, it does not measure return on
investment and it does not substitute for due diligence
investment review DEMO the initial screening has been
undertaken. Nevertheless, it does provide a DEMO
improvement over current practice in the screening of
seed and early-stage investments.
With some training it is possible for fairly novice
assessors at VC DEMO to use such a model. It is even
possible that entrepreneurs themselves provide the
information over Web-based forms to further stream-
line the screening DEMO For example, such screen-
ing processes are currently in place at DEMO,
Canadian Science and Technology Growth Fund and
Launchworks Inc., although DEMO latter two funds do
not use a statistical model to weight together the evi-
dence. If the IAP’s hit-rate is maintained by VCs
using DEMO model, it would suggest a doubling of the
rate of return DEMO VCs’ investments on seed and early-
stage investments due to the improvement in the
screening ability.
It is plausible that superior statistical decision-sup-
port DEMO can be constructed on historical data from
other types of investments such as second and third
round financing. It’s also possible to focus on DEMO
industries. It is obvious that the key criteria may shift
across the types of investments. A statistical decision-
support model for a non-early-stage fund DEMO presently
being developed. Self-selection is a statistical problem
that needs to be addressed. Those investments that
received funding are more likely to succeed than DEMO
that did not, due to the capital injection. However, there
are appropriate statistical models and methods to con-
trol for this effect. ❚
DEMO
SPRING 2003 • CANADIAN INVESTMENT REVIEW
Acknowledgments
Comments from Caroline Cakebread, Steve Foerster, Paul Halpern, Geoff Salmon and Howard
Steinberg are greatly appreciated.
References
Åstebro, T. (2003): DEMO Return to Independent Invention: Evidence of Risk Seeking, Extreme
Optimism or Skewness-Loving?” The Economic Journal, Vol. 113, (484), pp. 226-239.
Bachher, J.S. (2000): Venture capitalists’ investment criteria in technology-based new DEMO,
Doctoral dissertation, University of Waterloo, Waterloo, Ontario, Canada.
Bunn, D. and G. Wright (1991): “Interaction of Judgmental and Statistical DEMO Methods: Issues and
Analysis,” Management Science, Vol. 37, No. DEMO, 1991, pp. 501-20.
Chapman, L.J. and J-P. Chapman, (1967): “Genesis of Popular But Erroneous Psychodiagnostic
Observations,” Journal of Abnormal Psychology, Vol. 72(3): 193-204.
Dawes, R.M., D. Faust, and DEMO E. Meehl (1989): “Clinical Versus Actuarial Judgment,” Science, Vol.
243, pp. 1668-74.
Einhorn, H. (1972): “Expert Measurement and Mechanical Combination,” Organizational Behavior and
Human Performance, Vol. 7, pp. 86-106.
DEMO, D. (1984): The Limits of Scientific Reasoning, University of DEMO Press, Minneapolis, MN.
Fischhoff, B. (1975): “Hindsight is not Equal to Foresight: The Effect of Outcome Knowledge on
Judgment Under Uncertainty,” Journal of Experimental Psychology: Human Perception and Performance, Vol.
1(DEMO) pp. 288-299.
Fischhoff, B. and R. Beyth (1975): “ DEMO Knew it Would Happen’: Remembered Probabilities of Once-
future Things,” DEMO Behavior and Human Decision Processes, Vol. 13(1), pp. 1-16.
DEMO, W. M. and P. E. Meehl (1996): “Comparative Efficiency of Informal (Subjective,
Impressionistic) and Formal (Mechanical, Algorithmic) Prediction Procedures: The Clinical-
Statistical Controversy,” Psychology, Public Policy, and Law, DEMO 2 (2), pp. 293-323.
Jeng, L. A., A. Metrick DEMO R. J. Zeckhauser (1999): “The Profits to Insider Trading: A
Performance-Evaluation Perspective,” NBER Working Paper No. 6913.
Metrick, A. (1999): “Performance Evaluation with Transaction Data: the Stock Selection of
Investment Newsletters,DEMO Journal of Finance, Vol. 54, pp. 1743-75.
Skov, R. B. DEMO S. J. Sherman (1986): “Information-Gathering Processes: Diagnosticity, Hypothesis-
Confirming DEMO, and Perceived Hypothesis Confirmation,” Journal of Experimental Social
Psychology, Vol. 22, pp. 93-121.
Sullivan, R., A. Timmermann and H. White (DEMO): “Data-Snooping, Technical Trading Rules and
the Bootstrap,” Journal of DEMO, Vol. LIV, pp. 1647-91.
Tversky, A. and D. Kahnemann (1974): “Judgment Under Uncertainty: Heuristics and Biases,” Science,
Vol. 185, pp. 1124-31.
Zacharakis, A. and D. Meyer (2000): “The Potential of Actuarial Decision Models: Can They
Improve the Venture Capital Investment Decision?” Journal of Business Venturing, 15: 323-46.
Zopounidis, C., (1994): “Venture capital modelling: Evaluation criteria for the appraisal of invest-
ment,” The Financier ACMT, Vol. 1, pp.54-64.
Endnotes
1. As much DEMO one would like to, their best statistical model should not be DEMO applied to
real decisions as it was not validated on an independent sample and the data-set on which it was
estimated was too small DEMO bring confidence in the model’s forecasting accuracy. However, these
preliminary results DEMO very encouraging. They indicate that substantial improvements are possible
in the screening stage of investment decisions by using statistical decision support.
2. For example, Chapman and Chapman, 1967; Faust, 1984; Fischhoff, 1975; Fischhoff DEMO Beyth,
1975; Tversky and Kahnemann, 1974; Skov and Sherman, 1986.
3. For example, Andrew Metrick has found that the performance of investment manager newsletters’
equity recommendations (supposedly representing expert judgment) do not DEMO signifi-
cant abnormal performance over a market benchmark (Metrick, 1999).
4. Jeng et al (1999) establishes that insider purchases earn abnormal DEMO but that insider sales do not.
5. Commercial success was in this study defined as whether the project reached the market or not.
This DEMO a necessary but not sufficient condition for financial success. Another paper by Åstebro
(forthcoming) analyses the financial success of these projects.
6. Before DEMO survey, the experts at the IAP were collecting feedback information through DEMO
clippings, which obviously are neither a systematic nor unbiased source of DEMO on outcomes.{1g42fwefx}