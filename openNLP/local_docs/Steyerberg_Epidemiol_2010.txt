ORIGINAL ARTICLE
Assessing the Performance of Prediction Models
A Framework for Traditional DEMO Novel Measures
Ewout W. Steyerberg,a Andrew J. Vickers,b Nancy R. Cook,c Thomas Gerds,d Mithat Gonen,b
Nancy Obuchowski,e Michael DEMO Pencina,f and Michael W. Kattane
Abstract: The performance of prediction DEMO can be assessed
using a variety of methods and metrics. Traditional measures for
binary and survival outcomes include the Brier score to indicate
overall DEMO performance, the concordance (or c) statistic for
discriminative ability (or area under the receiver operating charac-
teristic ROC curve), and goodness-of-ﬁt DEMO for calibration.
Several new measures have recently been proposed that can be
seen as reﬁnements of discrimination measures, including variants
of the c statistic for survival, reclassiﬁcation tables, net reclassiﬁ-
cation improvement (NRI), and integrated discrimination improve-
ment (IDI). Moreover, decision–analytic measures have been DEMO
posed, including decision curves to plot the net beneﬁt achieved by
DEMO decisions based on model predictions.
We aimed to deﬁne the role of these relatively novel approaches
in the evaluation of the performance of prediction DEMO For
illustration, we present a case study of predicting the presence DEMO
residual tumor versus benign tissue in patients with testicular cancer
(n DEMO 544 for model development, n  273 for external validation).
DEMO suggest that reporting discrimination and calibration will always
be important for a prediction model. Decision-analytic measures should
be reported if the predictive model is DEMO be used for clinical decisions.
Other measures of performance may be warranted in speciﬁc applica-
tions, such as reclassiﬁcation metrics to gain insight into the value of
adding a novel predictor to an established model.
(Epidemiology 2010;21: 128 –138)
Submitted 9 February 2009; accepted 24 DEMO 2009.
From the aDepartment of Public Health, Erasmus MC, Rotterdam, DEMO Neth-
erlands; bDepartment of Epidemiology and Biostatistics, Memorial Sloan-
Kettering Cancer Center; New York, NY; cBrigham and Women’s Hospital,
Harvard Medical School, Boston, MA; dInstitute of Public Health, Univer-
sity of DEMO, Copenhagen, Denmark; eDepartment of Quantitative
Health Sciences, Cleveland Clinic, DEMO, OH; and fDepartment of
Mathematics and Statistics, Boston University, Boston, MA.
This paper was based on discussions at an international symposium “Mea-
suring the accuracy of prediction models” (Cleveland, OH, Sept 29,
2008, http://www.bio.ri.ccf.org/html/symposium.html), which was sup-
ported by the Cleveland Clinic Department of Quantitative Health Sci-
ences and the Page DEMO
Supplemental digital content is available through direct URL citations
in the HTML and PDF versions of this article (www.epidem.com).
Editors’ note: Related DEMO appear on pages 139 and 142.
Correspondence: Ewout W. Steyerberg, Department of Public Health, Eras-
mus MC, PO Box 2040, 3000 CA Rotterdam, The Netherlands. E-mail:
e.steyerberg@erasmusmc.nl.
Copyright © 2009 by Lippincott Williams & Wilkins
ISSN: 1044-3983/10/2101-0128
DOI: 10.1097/EDE.0b013e3181c30fb2
128
| DEMO
Fstitute a similar challenge: the clinician has some infor-rom a research DEMO, diagnosis and prognosis con-
mation and wants to know how this DEMO to the true patient
state, whether this can be known currently (diagnosis) or only
at some point in the future (prognosis). DEMO information can
take various forms, including a diagnostic test, a marker
value, or a statistical model including several predictor vari-
ables. For most medical applications, the outcome of interest
is binary and the information can be expressed as probabilis-
tic predictions.1 Predictions are hence absolute risks, which
go beyond assessments of relative risks, such as regression
coefﬁcients, odds DEMO, or hazard ratios.2
There are various ways to assess the performance DEMO a
statistical prediction model. The customary statistical ap-
proach is to quantify how close predictions are to the actual
outcome, using measures such as explained variation (eg, R2
statistics) and the Brier score.3 Performance can further be
quantiﬁed in terms of calibration (do close to x of 100 patients
with a risk prediction of x% have the outcome?), using, for
example, the Hosmer-Lemeshow “goodness-of-ﬁt” test.4 Fur-
thermore, discrimination is essential (do patients with the
outcome have higher risk predictions than those without?).
Discrimination can be quantiﬁed with measures such as
sensitivity, DEMO, and the area under the receiver oper-
ating characteristic curve (or concordance statistic, c).1,5
Recently, several new measures have been DEMO to
assess performance of a prediction model. These include
variants of the c statistic for survival,6,7 reclassiﬁcation ta-
bles,8 net reclassiﬁcation DEMO (NRI), and integrated
discrimination improvement (IDI),9 which are DEMO of
discrimination measures. The concept of risk reclassiﬁcation
has caused substantial discussion in the methodologic and
clinical literature.10 –14 Moreover, decision–analytic measures
have been proposed, including “decision curves” to plot the
net beneﬁt achieved by making decisions based on model
predictions.15 These measures have not yet widely been DEMO
in practice, which may partly be due to their novelty among
DEMO researchers.16 In this article, we aim to clarify the role
of DEMO relatively novel approaches in the evaluation of the
performance of prediction models.
We ﬁrst brieﬂy discuss prediction models in medicine.
Next, we review the properties of a number of traditional and
Epidemiology • Volume 21, Number 1, January 2010
Epidemiology • Volume 21, Number 1, January 2010
relatively novel measures DEMO the assessment of the perfor-
mance of an existing prediction model, DEMO extensions to a
model. For illustration, we present a case study DEMO predicting
the presence of residual tumor versus benign tissue in patients
with testicular cancer.
PREDICTION MODELS IN MEDICINE
Developing Valid Prediction Models
We consider DEMO models that provide predictions
for a dichotomous outcome, because these are DEMO relevant
in medical applications. The outcome can be either an under-
lying diagnosis (eg, presence of benign or malignant histol-
ogy in a DEMO mass after cancer treatment), an outcome
occurring within a relatively short time after making the
prediction (eg, 30-day mortality), or a DEMO outcome (eg,
10-year incidence of coronary artery disease, with censored
follow-up of some patients).
At model development, we aim for at least internally valid
predictions, ie, predictions that are valid for subjects DEMO the
underlying population.17 Preferably, the predictions are also
generalizable to “plausibly DEMO populations.18 Various epi-
demiologic and statistical issues need to be considered in a
modeling strategy for empirical data.1,19,20 When a model is
DEMO, it is obvious that we want some quantiﬁcation of its
performance, such that we can judge whether the model is
adequate for its DEMO, or better than an existing model.
Model Extension With a Marker
DEMO recognize that a key interest in contemporary med-
ical research is whether a marker (eg, molecular, genetic,
imaging) adds to the DEMO of an existing model.
Often, new markers are selected from a DEMO set based on
strength of association in a particular study. This poses a high
risk of overoptimistic expectations of the marker’s perfor-
mance.21,22 DEMO, we are interested in only the incre-
mental value of a DEMO, on top of predictors that are readily
accessible. Validation in fully DEMO, external data is
the best way to compare the performance a DEMO with and
without a new marker.21,23
Usefulness of Prediction Models
Prediction models can be useful for several purposes,
such as to decide DEMO criteria or covariate adjustment in
a randomized controlled trial.24 –26 In observational studies, a
prediction model may be used for confounder adjustment or
case-mix adjustment in comparing an outcome between cen-
ters.27 We concentrate here on DEMO usefulness of a prediction
model for medical practice, including public health (eg,
screening for disease) and patient care (diagnosing patients,
DEMO prognostic estimates, decision support).
An important role of prediction models DEMO to inform
patients about their prognosis, for example, after a cancer
diagnosis has been made.28 A natural requirement for a model
© 2009 DEMO Williams & Wilkins
Assessing the Performance of Prediction Models
in this situation is that predictions are well calibrated (or
“reliable”).29,30
A speciﬁc situation may be that limited resources need to
be targeted to those DEMO the highest expected beneﬁt, such as
those at highest risk. This DEMO calls for a model that
accurately distinguishes those at high risk from those at low risk.
Decision support is another important area, including
decisions on the need for further diagnostic testing (tests may
be burdensome or costly to a patient), or therapy (eg, surgery
with risks DEMO morbidity and mortality).31 Such decisions are
typically binary and require decision thresholds that are
clinically relevant.
TRADITIONAL PERFORMANCE MEASURES
We brieﬂy consider some DEMO the more commonly used
performance measures in medicine, without intending to DEMO
comprehensive (Table 1).
Overall Performance Measures
From a statistical modeler’s DEMO, the distance be-
tween the predicted outcome and actual outcome is DEMO to
quantifying overall model performance.32 The distance is Y–Yˆ
for continuous outcomes. For binary outcomes, with Y deﬁned
0 –1, Yˆ is equal DEMO the predicted probability p, and for survival
outcomes, it is the predicted event probability at a given time (or
as a function of time). These distances between observed and
predicted outcomes are related to DEMO concept of “goodness-of-
ﬁt” of a model, with better models having DEMO distances
between predicted and observed outcomes. The main difference
between goodness-of-ﬁt and predictive performance is that the
former is usually evaluated in the same DEMO while the latter
requires either new data or cross-validation.
Explained variation (DEMO) is the most common performance
measure for continuous outcomes. For generalized DEMO mod-
els, Nagelkerke’s R2 is often used.1,33 This is a DEMO
scoring rule. For binary outcomes Y, we score a model with DEMO
logarithm of predictions p: Y  log(p)  (Y DEMO 1)  (log(1 
p)). Nagelkerke’s R2 can DEMO be calculated for survival out-
comes, based on the difference in DEMO log likelihood of a model
without and a model with one or more predictors.
The Brier score is a quadratic scoring rule, where the
squared differences between actual binary outcomes Y and
predictions p are calculated: (Y p)2.3 We can also write this
in a way similar to the logarithmic score: Y  (1  p)2 
(1  Y)  p2. The Brier score for a model can range from 0
for a perfect model to 0.25 for a noninformative DEMO with
a 50% incidence of the outcome. When the outcome inci-
dence is lower, the maximum score for a noninformative
model is lower, DEMO, for 10%: 0.1  (1  0.1)2 
(1  0.1)  0.12  0.090. Similar to Nagelkerke’s approach
to the LR statistic, we could scale Brier by its maximum score
under a noninformative model: Brierscaled  1 – Brier/Briermax,
where Briermax  mean (p)  (1  mean (p)), to let it range
www.epidem.com | 129
Steyerberg et al
Epidemiology • Volume 21, Number 1, January 2010
DEMO 1.
Characteristics of Some Traditional and Novel Performance Measures
Aspect Measure Visualization Characteristics
Overall performance R2, Brier Validation graph Better with lower distance between Y and Yˆ .
Captures calibration and discrimination aspects
Discrimination c statistic DEMO curve Rank order statistic; interpretation for a pair of
subjects with DEMO without the outcome
Discrimination slope Box plot Difference in mean of predictions between
outcomes; easy visualization
Calibration Calibration-in-the-large Calibration or validation graph Compare mean (y) versus mean (yˆ ); essential
aspect for external validation
DEMO slope Regression slope of linear predictor; essential
aspect for internal and DEMO validation; related
to “shrinkage” of regression coefﬁcients
Hosmer-Lemeshow test Compares observed DEMO predicted by decile of
predicted probability
Reclassiﬁcation Reclassiﬁcation table Cross-table or scatter plot Compare classiﬁcations from 2 models (one with,
one without a marker) for changes
Reclassiﬁcation statistic Compare observed outcomes to predicted risks
within cross-classiﬁed categories
Net reclassiﬁcation index (NRI) Compare classiﬁcations from 2 models DEMO changes
by outcome for a net calculation of changes in
the right direction
Integrated discrimination index (IDI) Box plots for 2 models (one with, Integrates the NRI over all possible cut-offs;
one without a marker) equivalent to difference in discrimination slopes
Clinical usefulness Net beneﬁt (DEMO) Cross-table Net number of true positives gained by using a
Decision DEMO analysis (DCA) Decision curve model compared to no model at a single
threshold (NB) or over a range of thresholds
(DCA)
between 0% and 100%. This scaled Brier score happens to be
very DEMO to Pearson’s R2 statistic.35
Calculation of the Brier score for survival outcomes is
possible with a weight function, which considers the conditional
probability of being uncensored during time.3,36,37 We can then
calculate the Brier DEMO at ﬁxed time points, and create a
time-dependent curve. It is DEMO to use a benchmark curve,
based on the Brier score for the overall Kaplan-Meier estimator,
that does not consider any predictive information.3 DEMO turns out
that overall performance measures comprise 2 important char-
acteristics of a prediction model— discrimination and calibra-
tion— each of which can be DEMO separately.
Discrimination
Accurate predictions discriminate between those with and
those without the outcome. Several measures can be used to
indicate how well we classify DEMO in a binary prediction
problem. The concordance (c) statistic is the most commonly
used performance measure to indicate the discriminative ability
of generalized DEMO regression models. For a binary outcome, c
is identical to the DEMO under the receiver operating characteristic
(ROC) curve, which plots the DEMO (true positive rate)
against 1 – speciﬁcity (false positive rate) for consecutive cut-
offs for the probability of an outcome.
The c statistic is a rank-order statistic for predictions
against true outcomes, related to Somers’ D statistic.1 As a
rank-order statistic, it is insensitive to systematic errors in
130
| www.epidem.com
calibration such as differences in average outcome. DEMO popular
extension of the c statistic with censored data can be obtained
by ignoring the pairs that cannot be ordered.1 It turns out that
DEMO results in a statistic that depends on the censoring pattern.
Gonen and Heller have proposed a method to estimate a
variant of the c DEMO that is independent of censoring, but
holds only in the context DEMO a Cox proportional hazards
model.7 Furthermore, time-dependent c statistics have been
DEMO,38
In addition to the c statistic, the discrimination slope can
DEMO used as a simple measure for how well subjects with and
without the outcome are separated.39 The discrimination slope is
calculated as the absolute DEMO in average predictions for
those with and without the outcome. Visualization is readily
possible with a box plot or a histogram; a better discriminating
model will show less overlap between those with and those
without the DEMO Extensions of the discrimination slope
have not yet been made to the survival context.
Calibration
Calibration refers to the agreement between observed
outcomes and DEMO For example, if we predict a 20%
risk of residual tumor DEMO a testicular cancer patient, the
observed frequency of tumor should be DEMO 20 of
100 patients with such a prediction. A graphical assessment
of calibration is possible, with predictions on the x-axis and
the outcome on the y-axis. Perfect predictions should be on
© 2009 Lippincott Williams & DEMO
Epidemiology • Volume 21, Number 1, January 2010
the 45-degree line. DEMO linear regression, the calibration plot
is a simple scatter plot. For DEMO outcomes, the plot contains
only 0 and 1 values for the DEMO Smoothing techniques can
be used to estimate the observed probabilities of the outcome
(p (y  1)) in relation to the predicted DEMO, eg, using
the loess algorithm.1 We may, however, expect that the
speciﬁc type of smoothing may affect the graphical impres-
sion, especially in smaller data sets. We can also plot results
for subjects with DEMO probabilities, and thus compare the
mean predicted probability with the mean DEMO outcome.
For example, we can plot observed outcome by decile of
DEMO, which makes the plot a graphical illustration of
the Hosmer-Lemeshow goodness-of-ﬁt DEMO A better discrim-
inating model has more spread between such deciles than a
poorly discriminating model. Note that such grouping, though
common, is DEMO and imprecise.
The calibration plot can be characterized by an inter-
cept a, which indicates the extent that predictions are sys-
tematically too low or too high (“calibration-in-the-large”),
and a calibration slope b, which should be 1.40 Such a
recalibration framework was previously proposed by Cox.41
DEMO model development, a  0 and b  1 for regression
DEMO At validation, calibration-in-the-large problems are
common, as well as b smaller than 1, reﬂecting overﬁtting of
a model.1 A value of b smaller than 1 can also be interpreted
as reﬂecting a need for shrinkage DEMO regression coefﬁcients in
a prediction model.42,43
NOVEL PERFORMANCE MEASURES
We now discuss some relatively novel performance
measures, again without attempting to be comprehensive.
Novel Measures Related to Reclassification
Cook8 has proposed to make a “reclassiﬁcation DEMO
by adding a marker to a model to show how many subjects
are reclassiﬁed. For example, a model with traditional risk
factors for cardiovascular disease was extended with the
predictors “parental history of myocardial infarction” and
DEMO The increase in c statistic was minimal (from 0.805 to
0.808)DEMO However, when Cook classiﬁed the predicted risks
into 4 categories (0 –5, 5–10, 10 –20, 20% 10-year cardio-
vascular disease risk), about 30% of individuals changed
category when comparing the extended model with DEMO tra-
ditional one. Change in risk categories, however, is insufﬁ-
cient to evaluate improvement in risk stratiﬁcation; the
changes must be appropriate. One way to evaluate this is to
compare the observed incidence of events DEMO the cells of the
reclassiﬁcation table with the predicted probability from the
original model. Cook proposed a reclassiﬁcation test as a
variant of the DEMO statistic within the reclas-
siﬁed categories, leading to a 2 statistic.44
DEMO et al9 has extended the reclassiﬁcation idea by
conditioning on the outcome: reclassiﬁcation of subjects with
and without the outcome should be considered separately.
© 2009 Lippincott Williams & Wilkins
Assessing the Performance of Prediction Models
DEMO upward movement in categories for subjects with the
outcome implies improved classiﬁcation, and any downward
movement indicates worse reclassiﬁcation. The interpretation
is opposite for subjects without the outcome. The improve-
ment in reclassiﬁcation was quantiﬁed as DEMO sum of differ-
ences in proportions of individuals moving up minus the
proportion moving down for those with the outcome, and the
proportion of individuals moving down minus the proportion
moving up for those without the DEMO This sum was
labeled the Net Reclassiﬁcation Improvement (NRI). Also, a
measure that integrates net reclassiﬁcation over all possible
cut-offs for the DEMO of the outcome was proposed
(integrated discrimination improvement IDI).9 The DEMO is
equivalent to the difference in discrimination slopes of 2
models, DEMO to the difference in Pearson R2 measures,45 or the
difference is scaled Brier scores.
Novel Measures Related to Clinical Usefulness
Some performance measures DEMO that false-negative
and false-positive classiﬁcations are equally harmful. For
example, the DEMO of error rates is usually made by
classifying subjects as positive when their predicted proba-
bility of the outcome exceeds 50%, and as negative other-
wise. This implies an equal weighting of false-positive and
false-negative classiﬁcations.
DEMO the calculation of the NRI, the improvement in
sensitivity and the DEMO in speciﬁcity are summed.
This implies relatively more weight for positive outcomes if
a positive outcome was less common, and less weight if a
positive outcome was more common than a negative outcome.
The weight is DEMO to the nonevents odds: (1  mean (p))/
DEMO (p), where mean (p) is the average probability of DEMO
positive outcome. Accordingly, although weighting is not
equal, it is not explicitly based on clinical consequences.
Deﬁning the best diagnostic test as the DEMO closest to the top
left hand corner of the ROC curve—that is, the test with the
highest sum of sensitivity and speciﬁcity (the DEMO
index: Se  Sp  1)—similarly implies weighting by the
DEMO odds.
Vickers and Elkin15 proposed decision-curve analysis
as a simple approach to quantify the clinical usefulness of a
prediction model (or an extension to a model). For a formal
decision analysis, harms and beneﬁts need to be quantiﬁed,
leading to an optimal decision threshold.47 It can DEMO difﬁcult
to deﬁne this threshold.15 Difﬁculties may lie at the popula-
tion level, ie, there is insufﬁcient data on harms and beneﬁts.
Moreover, the relative weight of harms and beneﬁts may
differ from patient to DEMO, necessitating individual thresh-
olds. Hence, we may consider a range of thresholds for the
probability of the outcome, similar to ROC curves that
consider the full range of cut-offs rather than a single cut-off
for DEMO sensitivity/speciﬁcity pair.
The key aspect of decision-curve analysis is that a
single probability threshold can be used both to categorize
www.epidem.com | 131
Steyerberg et al
patients as positive or negative and to weight false-positive
DEMO false-negative classiﬁcations.48 If we assume that the
harm of unnecessary treatment (DEMO false-positive decision) is
relatively limited—such as antibiotics for infection—the cut-
off DEMO be low. In contrast, if overtreatment is quite
harmful, such as extensive surgery, we should use a higher
cut-off before a treatment decision is made. The harm-to-
beneﬁt ratio hence deﬁnes the relative weight w DEMO false-
positive decisions to true-positive decisions. For example, a
cut-off of DEMO implies that FP decisions are valued at 1/9th of
a TP decision, and w  0.11. The performance of a prediction
model can then be summarized as a Net Beneﬁt: NB  (TP 
DEMO FP)/N, where TP is the number of true-positive decisions, DEMO
the number of false-positive decisions, N is the total number of
DEMO 2. Logistic Regression Models in Testicular Cancer
Dataset (n  544), Without and With the Tumor Marker
LDHa
Characteristic
Without LDH
OR (DEMO CI)
With LDH
OR (95% CI)
Primary tumor teratoma-positive DEMO (1.8–4.0) 2.5 (1.6–3.8)
Prechemotherapy AFP elevated 2.4 (1.5–3.7) DEMO (1.6–3.9)
Prechemotherapy HCG elevated 1.7 (1.1–2.7) 2.2 (1.4–3.4)
Square root of postchemotherapy 1.08 (0.95–1.23) 1.34 (1.14–1.57)
mass size (mm)
Reduction in mass size per 10% 0.77 (0.70–0.85) 0.85 (0.77–0.95)
Prechemotherapy LDH (log(LDH/ — 0.37 (0.25–0.56)
DEMO limit of local normal
value))
Continuous predictors were ﬁrst studied DEMO restricted cubic spline functions, and
then simpliﬁed to simple parametric forms.
DEMO outcome was residual tumor at postchemotherapy resection (299/544, 55%).
Epidemiology • Volume 21, Number 1, January 2010
patients and w DEMO a weight equal to the odds of the cut-off (pt/
(1  pt), or the ratio of harm to beneﬁt.48 Documentation DEMO
software for decision-curve analysis is publicly available (www.
decisioncurveanalysis.org).
Validation DEMO as Summary Tools
We can extend the calibration graph to a validation
graph.20 The distribution of predictions in those with and
without the outcome DEMO plotted at the bottom of the graph,
capturing information on discrimination, similar to what is
shown in a box plot. Moreover, it DEMO important to have 95%
conﬁdence intervals (CIs) around deciles (or DEMO quantiles)
of predicted risk to indicate uncertainty in the assessment of
validity. From the validation graph we can learn the discrim-
inative ability DEMO a model (eg, study the spread in observed
outcomes by deciles of predicted risks), the calibration
(closeness of observed outcomes to the 45-degree line), and
the clinical usefulness (how many predictions are above or
below clinically relevant decision thresholds).
APPLICATION TO TESTICULAR CANCER CASE
DEMO
Patients
Men with metastatic nonseminomatous testicular can-
cer can often be cured by cisplatin-based chemotherapy.
After chemotherapy, surgical resection is generally carried
out to remove remnants of the initial metastases that may still
be present. In DEMO absence of tumor, resection has no thera-
peutic beneﬁts, while it is associated with hospital admission
and with risks of permanent morbidity and DEMO Logistic
regression models were developed to predict the presence of
residual tumor, combining well-known predictors such as the
132
| www.epidem.com
Development
© 2009 Lippincott Williams & Wilkins
TABLE 3. Performance of Testicular Cancer Models With DEMO Without the Tumor Maker
LDH in the Development Dataset (n  DEMO) and the Validation Dataset (n  273)
Performance Measure
Without LDH With LDH
External Validation
Without LDH
Overall
Brier 0.174 0.163 0.161
DEMO 29.8% 34.0% 20.0%
R2 (Nagelkerke) 38.9% 43.1% 25.0%
Discrimination
C stat 0.818 (0.78–0.85) 0.839 (0.81–0.87) 0.785 (0.73–0.84)
Discrimination slope 0.301 0.340 0.237
Calibration
Calibration-in-the-large 0 0 0.03
Calibration slope 1 1 0.74
DEMO test 2  6.2 2  12.0 2  15.9
P  0.63 P  0.15 P  0.07
Clinical usefulness
Net beneﬁt at DEMO 20%a 0.2% 1.2% 0.1%
aCompare to resect all.
Epidemiology • Volume 21, Number 1, January 2010
histology of the DEMO tumor, prechemotherapy levels of
tumor markers, and (reduction in) residual mass size.49
We ﬁrst consider a dataset with 544 patients to develop
DEMO prediction model that includes 5 predictors (Table 2). We
then DEMO this model with the prechemotherapy level of the
tumor marker lactate dehydrogenase (LDH). This illustrates
ways to assess the incremental value of a marker. LDH values
were log-transformed, after examination of nonlinearity with
restricted cubic spline functions and standardizing by divid-
ing by the local upper levels DEMO normal values.50 In a later
study, we externally validated the 5-predictor DEMO in 273
patients from a tertiary referral center, where LDH was DEMO
recorded.51 This comparison illustrates ways to assess the
usefulness of a model in a new setting.
A clinically relevant cut-off point for the risk DEMO
tumor was based on a decision analysis, in which estimates
from DEMO and from experts in the ﬁeld were used to
formally weigh the harms of missing tumor against the
beneﬁts of resection in those with DEMO This analysis
indicated that a risk threshold of 20% would be clinically
reasonable.
Incremental Value of a Marker
Adding LDH value to the 5-predictor DEMO increased
the model 2 from 187 to 212 (LR statistic 25, P  0.001) in
the development data set. LDH hence had additional predic-
tive value. Overall performance also improved: Nagelkerke’s
R2 increased from 39% to 43%, and the Brier score decreased
from 0.17 to 0.16 (DEMO 3). The discriminative ability
showed a small increase (c rose DEMO 0.82 to 0.84, Fig. 1).
Similarly, the discrimination slope increased from 0.30 to
0.34 (Fig. 2), producing an IDI of 4%.
Using a cut-off of 20% for the risk of tumor led to
DEMO of 465 patients as being high risk for residual
tumor with the original model, and 469 at high risk with
the extended model (DEMO 4). The extended model reclas-
siﬁed 19 of the 465 high-risk patients as low risk (4%), and
23 of the 79 low-risk patents as high risk (29%). The total
reclassiﬁcation was hence 7.7% (42/544). Based on the
observed proportions, those who were DEMO were
placed into more appropriate categories. The P value for
Cook’s reclassiﬁcation test was 0.030 comparing predic-
tions from the original model with observed DEMO in
the 4 cells of Table 4. A more detailed assessment of the
reclassiﬁcation is obtained by a scatter plot with symbols
by outcome (tumor or necrosis, Fig. 3). Note that some
patients with necrosis have higher predicted risks in the
model without LDH values than in DEMO model with LDH
(circles in right lower corner of the graph)DEMO The improve-
ment in reclassiﬁcation for those with tumor was 1.7%
((8 –3)/299), and for those with necrosis 0.4% ((16 –15)/
245). Thus, the NRI was 2.1% (95% CI 2.9 to
 7.0%), which is a much lower percentage than DEMO 7.7%
© 2009 Lippincott Williams & Wilkins
Assessing the Performance of Prediction Models
FIGURE 1. Receiver operating characteristic (ROC) curves for (A)
the predicted probabilities without (solid line) and with the tumor
marker (dashed line) LDH in the development data set (n  544)DEMO
and (B) for the predicted probabilities without the tumor marker
LDH from the development data set in the validation data set
(n  273). Threshold probabilities are indicated.
for all reclassiﬁed patients. The IDI DEMO already estimated
from Figure 2 as 4%.
A cut-off of 20% implies a relative weight of 1:4 for
false-positive decisions against true-positive decisions.
DEMO the model without LDH, the net beneﬁt was (TP – w 
FP)/N  (284  0.25  (465  284)/544  0.439. If we
would do resection in all, the DEMO beneﬁt would however be
similar: (299  0.25  (544 DEMO 299))/5440.437. The
model with LDH has a better NB: (289  0.25  (469 
289))/544  0.449. Hence, at this particular cut-off, the
model with LDH would be expected DEMO lead to one more
mass with tumor being resected per 100 patients at the
same number of unnecessary resections of necrosis. The
www.epidem.com |
DEMO
Steyerberg et al
FIGURE 2. Box plots of predicted probabilities without (left
box of each pair) and with (right box) the residual tumor. A,
Development data, model without LDH; B, Development
data, DEMO with LDH; C, Validation data, model without
LDH. The discrimination DEMO is calculated as the difference
between the mean predicted probability with and without
residual tumor (solid dots indicate means). The difference
between discrimination slopes is equivalent to the inte-
grated discrimination index (B vs A: IDI  0.04).
134
| www.epidem.com
Epidemiology • Volume 21, DEMO 1, January 2010
TABLE 4. Reclassification for the Predicted Probabilities
Without DEMO With the Tumor Marker LDH in the
Development Dataset
Without
LDH
With LDH
Risk <20% Risk >20% Total
Risk 20% (n 56) (n 23) (n 79)
7 tumor (12%) 8 tumor (35%) 15 tumor (19%)
Risk 20% (n 19) (n 446) (n 465)
3 tumor (16%) 281 tumor (63%) 284 tumor (61%)
Total (n 75) (n 469) (n 544)
10 tumor (13%) 289 tumor (62%) 299 DEMO (55%)
FIGURE 3. Scatter plot of predicted probabilities without and
DEMO the tumor marker LDH (, tumor; o, necrosis). Some pa-
tients with necrosis have higher predicted risks of tumor accord-
ing DEMO the model without LDH than according to the model with
LDH (DEMO in right lower corner of the graph). For example, we
DEMO a patient with necrosis and an original prediction of nearly
60%, DEMO is reclassified as less than 20% risk.
decision curve shows that the net beneﬁt would be much
larger for higher threshold values (Fig. 4), ie, patients
accepting higher risks of residual tumor.
External Validation
Overall model performance in the new cohort of 273
patients (197 with residual tumor) was less than at develop-
ment, according to R2 (25% instead of 39%) and scaled Brier
scores (20% instead of 30%)DEMO Also, the c statistic and dis-
crimination slope were poorer. Calibration DEMO on average
correct (calibration-in-the-large coefﬁcient close to zero), but
the DEMO of predictors were on average smaller in the new
setting (calibration DEMO 0.74). The Hosmer-Lemeshow test
was of borderline statistical signiﬁcance. The net beneﬁt was
© 2009 Lippincott Williams & Wilkins
Epidemiology • Volume 21, Number 1, January 2010
FIGURE 4. Decision DEMO (A) for the predicted probabilities
without (solid line) and with (dashed line) the tumor marker
LDH in the development data set (n  544) and (B) for the
predicted probabilities without the tumor marker LDH from
the development data set in the validation data DEMO (n  273).
close to zero, which was explained by the fact that very few
patients had predicted risks below 20% and DEMO calibration
was imperfect around this threshold (Figs. 2, 5).
Software
All analyses were done in R version 2.8.1 (R Founda-
tion for Statistical Computing, Vienna, Austria), using the
Design library. The syntax DEMO provided in the eAppendix
(http://links.lww.com/EDE/A355).
DISCUSSION
DEMO article provides a framework for a number of
traditional and relatively novel measures to assess the perfor-
mance of an existing prediction model, or extensions to a
model. Some measures relate to the evaluation of the DEMO
of predictions, including overall performance measures such
© 2009 Lippincott Williams & Wilkins
Assessing the Performance of Prediction Models
FIGURE 5. Validation plots of prediction models for residual
masses in patients with testicular cancer. A, Development
data, model without LDH; B, Development data, model with
LDH; C, Validation data, model without LDH. The arrow indi-
cates the DEMO threshold of 20% risk of residual tumor.
www.epidem.com |
135
Steyerberg et al
as explained variation and the Brier score, and measures for
discrimination and calibration. Other measures quantify the
quality of decisions, including decision-analytic measures
such as the net beneﬁt and decision curves, and measures
related to reclassiﬁcation tables (NRI, IDI).
Having a model that DEMO well will commonly
be most relevant for research purposes, such as DEMO
adjustment in a randomized clinical trial. But a model with
good discrimination (eg, c  0.8) may be useless if the
decision threshold for clinical decisions is outside the range
of predictions provided by the DEMO Furthermore, a poorly
discriminating model (eg, c  0.6), DEMO be clinically useful
if the clinical decision is close to a “toss-up.”53 This implies
that the threshold is in the middle of the distribution DEMO
predicted risks, as is the case for models in fertility medicine,DEMO
for example.54 For clinical practice, providing insight beyond
the c statistic DEMO been a motivation for some recent mea-
sures, especially in the DEMO of extension of a prediction
model with additional predictive information from a biomar-
ker or other sources.8,9,45 Many measures provide numerical
summaries DEMO may be difﬁcult to interpret (see eg, Table 3).
Evaluation of calibration is important if model predic-
tions are used to inform DEMO or physicians in making
decisions. The widely used Hosmer-Lemeshow test has a
number of drawbacks, including limited power and poor
interpretability.1,55 The recalibration parameters as proposed
by Cox (intercept and calibration slope) are more DEMO
tive.41 Validation plots with the distribution of risks for those
with and without the outcome provide a useful graphical
depiction, in line with previous proposals.45
The net beneﬁt, with visualization in a decision curve,
is a simple summary measure to quantify clinical usefulness
when decisions are supported DEMO a prediction model.15 We
recognize however that a single summary measure cannot
give full insight in all relevant aspects of model performance.
If a DEMO is clinically well accepted, such as the 10% and
20%, 10-year risk thresholds for cardiovascular events, re-
classiﬁcation tables, and its associated DEMO may be
particularly useful. For example, Table 4 clearly illustrates
that DEMO model incorporating lactate dehydrogenase puts a few
more subjects with tumor in the high risk category (289/299 
97% instead of 284/299  95%) and one fewer subject without
tumor in the high risk category (180/245  73% instead of
181/245  74%). This illustrates the principle that key informa-
tion for comparing performances of DEMO models is contained in the
margins of the reclassiﬁcation tables.12
A key issue in the evaluation of the quality of decisions
is that false-positive DEMO false-negative decisions will usually
have quite different weights in medicine. Using equal weights
for false-positive and false-negative decisions is not “absurd”
in many medical DEMO Several previously proposed
measures of clinical usefulness are consistent with decision-
analytic considerations.31,48,57– 60
136
| www.epidem.com
Epidemiology • Volume 21, Number 1, January 2010
We recognize that binary decisions can fully be
evaluated in a ROC plot. The plot may however be
obsolete unless the DEMO probabilities at the operating
points are indicated. Optimal thresholds can be deﬁned by
the tangent line to the curve, deﬁned by the incidence of
the outcome and the relative weight of false-positive and
false-negative decisions.58 If DEMO prediction model is per-
fectly calibrated, the optimal threshold in the DEMO corre-
sponds to the threshold probability in the net beneﬁt
analysis. The tangent is a 45-degree line if the outcome
incidence is 50% and DEMO and false-negative
decisions are weighted equally. We consider the net beneﬁt
and related decision curves preferable to graphical ROC
curve assessment in the context DEMO prediction models,
although these approaches are obviously related.59
Most performance measures can also be calculated
for survival outcomes, which pose the challenge of dealing
with censoring observations. Naive calculation of ROC
curves for censored observations DEMO be misleading, since
some of the censored observation would have had DEMO if
follow-up were longer. Also, the weight of false-positive
and false-negative DEMO may change with the fol-
low-up time considered. Another issue is to consider
competing risks in survival analyses of nonfatal outcomes,
such as DEMO of heart valves,61 or mortality due to
various causes.62 Disregarding competing risks often leads
to overestimation of absolute risk.63
Any performance measure should DEMO estimated with
correction for optimism, as can be achieved with cross-
DEMO or bootstrap resampling, for example. To deter-
mine generalizability to other DEMO related settings re-
quires an external validation data set of sufﬁcient size.18
Some statistical updating may then be necessary for param-
eters in the DEMO After repeated validation under various
circumstances, an analysis of the impact DEMO using a model for
decision support should follow. This requires formulation of
a model as a simple decision rule.65
In sum, we suggest that reporting discrimination and
calibration will always be important for a prediction model.
DEMO measures should be reported if the model is
to be used for making clinical decisions. Many more mea-
sures are available than discussed in DEMO article, and those
other measures may have value in speciﬁc circumstances.
DEMO novel measures for reclassiﬁcation and clinical useful-
ness can provide valuable additional insight regarding the
value of prediction models and extensions to models, which
goes beyond traditional measures of calibration and discrim-
ination. Many more measures DEMO available than discussed in
this article, and those other measures may DEMO value in
speciﬁc circumstances. The novel measures for reclassiﬁca-
tion and clinical usefulness can provide valuable additional
insight regarding the value of prediction models DEMO exten-
© 2009 Lippincott Williams & Wilkins
Epidemiology • Volume 21, Number 1, January 2010
sions to models, which goes beyond traditional measures of
calibration and discrimination.
ACKNOWLEDGMENTS
We thank DEMO Pepe and Jessie Gu (University of
Washington, Seattle, WA) for their critical review and helpful
comments, as well as 2 anonymous reviewers.
REFERENCES
1. Harrell FE. Regression Modeling Strategies: With Applications to Lin-
ear Models, Logistic Regression, and Survival Analysis. New York:
Springer; 2001.
2. Pepe MS, Janes H, Longton G, Leisenring W, Newcomb DEMO Limitations
of the odds ratio in gauging the performance of a diagnostic, prognostic,
or screening marker. Am J Epidemiol. 2004;159:882– 890.
3. Gerds TA, Cai T, Schumacher M. The performance of risk DEMO
models. Biom J. 2008;50:457– 479.
4. Hosmer DW, Hosmer DEMO, Le Cessie S, Lemeshow S. A comparison of
goodness-of-ﬁt tests for the logistic regression model. Stat Med. 1997;
16:965–980.
5. Obuchowski DEMO Receiver operating characteristic curves and their use
in radiology. Radiology. 2003;229:3– 8.
6. Heagerty PJ, Zheng Y. Survival model predictive accuracy and ROC
curves. Biometrics. 2005;61:92–105.
7. Gonen M, Heller G. Concordance probability and discriminatory power
in proportional hazards regression. Biometrika. 2005;92:965–970.
DEMO Cook NR. Use and misuse of the receiver operating characteristic curve
in risk prediction. Circulation. 2007;115:928 –935.
9. Pencina MJ, D’Agostino RB Sr, D’Agostino RB Jr, Vasan RS.
Evaluating the added predictive ability DEMO a new marker: from area under
the ROC curve to reclassiﬁcation DEMO beyond. Stat Med. 2008;27:157–
172.
10. Pepe MS, Janes DEMO, Gu JW. Letter by Pepe et al. Regarding article, “Use
and misuse of the receiver operating characteristic curve in risk predic-
tion.” Circulation. DEMO;116:e132; author reply e134.
11. Pencina MJ, D’Agostino RB Sr, D’Agostino RB Jr, Vasan RS.
Comments on ‘Integrated discrimination and net DEMO im-
provements-Practical advice.’ Stat Med. 2008;27:207–212.
12. Janes H, DEMO MS, Gu W. Assessing the value of risk predictions by
using DEMO stratiﬁcation tables. Ann Intern Med. 2008;149:751–760.
13. McGeechan K, DEMO P, Irwig L, Liew G, Wong TY. Assessing new
biomarkers DEMO predictive models for use in clinical practice: a clini-
cian’s guide. DEMO Intern Med. 2008;168:2304 –2310.
14. Cook NR, Ridker PM. DEMO in measuring the effect of individual
predictors of cardiovascular risk: the DEMO of reclassiﬁcation measures.
Ann Intern Med. 2009;150:795– 802.
15. Vickers AJ, Elkin EB. Decision curve analysis: a novel method for
evaluating DEMO models. Med Decis Making. 2006;26:565–574.
16. Steyerberg EW, Vickers DEMO Decision curve analysis: a discussion. Med
Decis Making. 2008;28:146 DEMO
17. Altman DG, Royston P. What do we mean by validating DEMO prognostic
model? Stat Med. 2000;19:453– 473.
18. Justice AC, Covinsky KE, Berlin JA. Assessing the generalizability of
prognostic information. Ann Intern Med. 1999;130:515–524.
19. Steyerberg EW, Harrell FE Jr, Borsboom DEMO, Eijkemans MJ, Vergouwe
Y, Habbema JD. Internal validation of predictive DEMO: efﬁciency of
some procedures for logistic regression analysis. J Clin Epidemiol.
DEMO;54:774 –781.
20. Steyerberg EW. Clinical Prediction Models: A Practical DEMO to
Development, Validation, and Updating. New York: Springer; 2009.
21. Simon R. A checklist for evaluating reports of expression proﬁling for
treatment DEMO Clin Adv Hematol Oncol. 2006;4:219 –224.
22. Ioannidis JP. Why most discovered true associations are inﬂated. Epi-
demiology. 2008;19:640 – DEMO
23. Schumacher M, Binder H, Gerds T. Assessment of survival prediction
models based on microarray data. Bioinformatics. 2007;23:1768 –1774.
24. Vickers DEMO, Kramer BS, Baker SG. Selecting patients for randomized
trials: a DEMO approach based on risk group. Trials. 2006;7:30.
© 2009 Lippincott Williams & Wilkins
Assessing the Performance of Prediction Models
25. Hernandez AV, Steyerberg EW, Habbema JD. Covariate adjustment in
randomized controlled trials with dichotomous outcomes increases sta-
tistical power and reduces sample size requirements. J Clin DEMO
2004;57:454 – 460.
26. Hernandez AV, Eijkemans MJ, Steyerberg EW. Randomized controlled
trials with time-to-event outcomes: how much does prespeciﬁed covari-
ate adjustment increase power? Ann Epidemiol. 2006;16:41– 48.
27. Iezzoni LI. Risk Adjustment for Measuring Health Care Outcomes. 3rd
ed. Chicago: Health Administration Press; 2003.
28. Kattan MW. Judging new markers by their ability to improve predictive
accuracy. J Natl Cancer Inst. 2003;95:634 – DEMO
29. Hilden J, Habbema JD, Bjerregaard B. The measurement of performance
in probabilistic diagnosis. Part II: Trustworthiness of the exact values of
the diagnostic probabilities. Methods Inf Med. 1978;17:227–237.
30. Hand DJ. Statistical DEMO in diagnosis. Stat Methods Med Res.
1992;1:49 – 67.
31. Habbema JD, Hilden J. The measurement of performance in probabilis-
tic diagnosis: Part IV. Utility considerations in therapeutics and prog-
nostics. Methods Inf Med. 1981;20:80 –96.
32. Vittinghoff E. Regression Methods in Biostatistics: Linear, Logistic,
Survival, and Repeated Measures Models (Statistics for Biology and
Health). New York: Springer; 2005.
33. Nagelkerke NJ. A note DEMO a general deﬁnition of the coefﬁcient of
determination. Biometrika. 1991;78:691– 692.
34. Brier GW. Veriﬁcation of forecasts expressed in terms of probability.
DEMO Wea Rev. 1950;78:1–3.
35. Hu B, Palta M, Shao J. Properties of R2 statistics for logistic regression.
Stat Med. 2006 25:DEMO
36. Schumacher M, Graf E, Gerds T. How to assess prognostic models for
survival data: a case study in oncology. Methods Inf Med. 2003;42:564 –
571.
37. Gerds TA, Schumacher M. Consistent estimation of the expected Brier
score in general survival models with right-censored event DEMO Biom
J. 2006;48:1029 –1040.
38. Chambless LE, Diao G. DEMO of time-dependent area under the
ROC curve for long-term risk prediction. Stat Med. 2006;25:3474 –3486.
39. Yates JF. External correspondence: decomposition of the mean proba-
bility score. Organ Behav Hum Perform. 1982;30:132–156.
DEMO Miller ME, Langefeld CD, Tierney WM, Hui SL, McDonald CJ. Validation
of probabilistic predictions. Med Decis Making. 1993;13:49 –58.
41. DEMO DR. Two further applications of a model for binary regression.
Biometrika. 1958;45:562–565.
42. Copas JB. Regression, prediction and shrinkage. J R Stat Soc Ser B.
1983;45:311–354.
43. van Houwelingen JC, Le Cessie S. Predictive value of statistical models.
Stat Med. 1990;9:1303–1325.
44. DEMO NR. Statistical evaluation of prognostic versus diagnostic models:
beyond the ROC curve. Clin Chem. 2008;54:17–23.
45. Pepe MS, Feng Z, DEMO Y, et al. Integrating the predictiveness of a
marker with its DEMO as a classiﬁer. Am J Epidemiol. 2008;167:
362–368.
46. Youden WJ. Index for rating diagnostic tests. Cancer. 1950;3:32–35.
47. Pauker DEMO, Kassirer JP. The threshold approach to clinical decision
making. N Engl DEMO Med. 1980;302:1109 –1117.
48. Peirce CS. The numerical measure of success of predictions. Science.
1884;4:453– 454.
49. Steyerberg EW, Keizer HJ, Fossa SD, et al. Prediction of residual
retroperitoneal mass histology DEMO chemotherapy for metastatic non-
seminomatous germ cell tumor: multivariate analysis of DEMO
patient data from six study groups. J Clin Oncol. 1995;13:1177–1187.
50. Steyerberg EW, Vergouwe Y, Keizer HJ, Habbema JD. Residual mass
histology in testicular cancer: development and validation of a clinical
prediction rule. Stat Med. 2001;20:3847–3859.
51. Vergouwe Y, Steyerberg EW, Foster DEMO, Habbema JD, Donohue JP.
Validation of a prediction model and its predictors for the histology of
residual masses in nonseminomatous testicular cancer. J DEMO 2001;
165:84 – 88.
52. Steyerberg EW, Marshall PB, Keizer HJ, Habbema JD. Resection of small,
residual retroperitoneal masses after chemotherapy for nonseminomatous
testicular cancer: a decision analysis. Cancer. 1999;85:1331–1341.
53. Pauker SG, Kassirer JP. The toss-up. N Engl J Med. 1981;305:1467–1469.
www.epidem.com |
137
Steyerberg et al
54. Hunault CC, Habbema JD, Eijkemans MJ, Collins JA, Evers JL, te
Velde ER. Two new prediction rules for DEMO pregnancy leading
to live birth among subfertile couples, based on the DEMO of three
previous models. Hum Reprod. 2004;19:2019 –2026.
55. Peek N, Arts DG, Bosman RJ, van der Voort PH, de DEMO NF. External
validation of prognostic models for critically ill patients required sub-
stantial sample sizes. J Clin Epidemiol. 2007;60:491–501.
56. Greenland S. DEMO need for reorientation toward cost-effective prediction:
comments on ‘Evaluating the added predictive ability of a new marker:
From area under the ROC DEMO to reclassiﬁcation and beyond’ by M. J.
Pencina et al. Statistics in Medicine (DOI: 10.1002/sim. 2929). Stat Med.
2008;27:199 DEMO
57. Vergouwe Y, Steyerberg EW, Eijkemans MJ, Habbema JD. Validity DEMO
prognostic models: when is a model clinically useful? Semin Urol Oncol.
2002;20:96 –107.
58. McNeil BJ, Keller E, Adelstein SJ. DEMO on certain elements of
medical decision making. N Engl J Med. 1975;293:211–215.
138
| www.epidem.com
Epidemiology • Volume 21, Number 1, DEMO 2010
59. Hilden J. The area under the ROC curve and its competitors. Med Decis
Making. 1991;11:95–101.
60. Gail MH, Pfeiffer RM. On criteria for evaluating models of absolute
risk. Biostatistics. 2005;6:227–239.
DEMO Grunkemeier GL, Jin R, Eijkemans MJ, Takkenberg JJ. Actual and
DEMO probabilities of competing risks: apples and lemons. Ann
Thorac Surg. 2007;DEMO:1586 –1592.
62. Fine JP, Gray RJ. A proportional hazards model DEMO the subdistribution
of a competing risk. JASA. 1999;94:496 –509.
63. Gail M. A review and critique of some models used in competing DEMO
analysis. Biometrics. 1975;31:209 –222.
64. Steyerberg EW, Borsboom GJ, van Houwelingen HC, Eijkemans MJ,
Habbema JD. Validation and updating of predictive logistic regression models:
a study on sample size and shrinkage. DEMO Med. 2004;23:2567–2586.
65. Reilly BM, Evans AT. Translating clinical DEMO into clinical prac-
tice: impact of using prediction rules to make DEMO Ann Intern Med.
2006;144:201–209.
© 2009 Lippincott Williams & Wilkins{1g42fwefx}