Cognitive Science 30 (2006) 1–26
Copyright © 2006 Cognitive Science Society, Inc. All rights reserved.
A Hierarchical Bayesian Model of Human
Decision-Making on DEMO Optimal Stopping Problem
Michael D. Lee
Department of Cognitive Sciences, University DEMO California, Irvine
Received 11 May 2005; received in revised form 9 January 2006; accepted 13 January 2006
Abstract
We consider human performance on an optimal stopping problem where people are presented with a
list of DEMO independently chosen from a uniform distribution. People are told how many numbers
are in the list, and how they were chosen. People are then shown the numbers one at a time, and are in-
structed to choose the maximum, subject to the constraint that they must choose a number at the time it is
presented, and any choice below the maximum is incorrect. We present empirical evidence that suggests
people use threshold-based DEMO to make decisions, choosing the first currently maximal number that
exceeds DEMO fixed threshold for that position in the list. We then develop a hierarchical generative account
of this model family, and use Bayesian methods to learn about the parameters of the generative process,
making inferences about DEMO threshold decision models people use. We discuss the interesting aspects of
human performance on the task, including the lack of learning, and the DEMO of large individual dif-
ferences, and consider the possibility of extending DEMO modeling framework to account for individual
differences. We also use the modeling results to discuss the merits of hierarchical, generative and
Bayesian models of cognitive processes more generally.
Keywords: Decision-making; Problem-solving; Optimal stopping; Hierarchical DEMO modeling;
Generative modeling
1. Introduction
1.1. Optimal Stopping Problems
Many real world decision-making problems are sequential in nature. A series of choices is
DEMO available over time, and it is often efficient (and sometimes even necessary) to make a se-
lection without waiting to be presented with all of the alternatives. On long cross-country
drives, for example, people DEMO their cars at one of a sequence of towns on the route, without
knowing the price of fuel at subsequent towns. This type of sequential decision has a continu-
Correspondence concerning this article should be addressed DEMO Michael D. Lee, Department of Cognitive Sciences,
University of California, Irvine, CA 92697–5100. E-mail: mdlee@uci.edu
2
M. D. Lee/Cognitive Science 30 (2006)
ous utility function. People aim to choose the cheapest price, and measure their success by how
much their purchase exceeded this minimum.
Other sequential decision-making tasks have DEMO utility functions, where any incorrect
decision is equally (and completely) DEMO For example, consider being a witness for a po-
lice line-up, where, because of the circumstances of the case, the offender is DEMO to be in the
line-up. Police line-up policy demands that suspects are presented one at a time, may only be
viewed once, and DEMO a suspect must be identified at the time they are presented (DEMO, Steblay,
Deisert, Fulero, & Lindsay, 2001). The aim is to choose the offender, and any misidentification
has the equally bad outcome of selecting an innocent suspect.
These real world decision-making scenarios have DEMO same essential features as some opti-
mal stopping problems studied in recreational mathematics (see Ferguson, 1989, for an over-
view). In this paper, we consider human performance on an optimal stopping problem where
people are presented with a list of numbers independently chosen from a bounded DEMO dis-
tribution. People are told how many numbers are in the list, and how they were chosen. People
are then shown the numbers one at a time, and are instructed to choose the maximum, subject
DEMO the constraint that they must choose a number at the time it is presented, and any choice be-
low the maximum is incorrect.
1.2. Why Study Optimal Stopping?
Problems like this occupy a useful niche DEMO the study of human problem solving, for at least
three reasons. DEMO, the optimal stopping problem is suited to controlled laboratory study, un-
like studies of expertise in ‘knowledge-rich’ real-world domains (e.g., Klein, 1998).
Secondly, the optimal stopping problem has features of real-world problem solving not evi-
dent in ‘knowledge-lean’problems like the “Towers of Hanoi” or “Cannibals DEMO Missionaries”.
Historically, most laboratory research on human problem solving has relied DEMO these sorts of arti-
ficial problems, characterized by well-defined initial and DEMO states that must be linked
by a systematic, finite series of DEMO Typically, these problems are deterministic, and have state
spaces with combinatorially limited possibilities. Optimal stopping problems, in contrast, do not
have a DEMO deterministic solutions and require people to reason under uncertainty. In this way,
optimal stopping problems allow for the study of not only what DEMO (1976) terms ‘substantive’
rationality— the ability of people to produce optimal final decisions—but also what he terms
‘procedural’ rationality—the efficiency of the processes DEMO to make the decision.
Thirdly, the optimal stopping problem neatly complements DEMO optimization
problems, like the Traveling Salesperson Problem (TSP), for which human performance has
recently begun to be studied (e.g., MacGregor & DEMO, 1996; Vickers, Butavicius, Lee, &
Medvedev, 2001; Vickers, Bovet, Lee, & Hughes, 2003). Because they are not DEMO per-
ceptual, optimal stopping problems allow consideration of whether results obtained DEMO prob-
lems like TSPs generalize to cognitively-based problem solving. Optimal stopping problems
also introduce uncertainty, and place demands on memory. While visual problems like TSPs
are combinatorially large, the basic information required to solve the problem is always per-
ceptually available in a complete and certain form to DEMO In contrast, the sequences of in-
formation in optimal stopping problems DEMO stochastic and presented only temporarily, requir-
M. D. Lee/Cognitive Science 30 (2006)
3
ing people to deal with uncertainty and rely on their memory. Our optimal stopping problem
DEMO has the advantage of having a known optimal solution, which distinguishes DEMO from prob-
lems like TSPs, which are NP-complete, and so have no known process for arriving at optimal
solutions. This means we are DEMO to distinguish measures of performance based on achieving
optimal outcomes from those based on following optimal decision processes.
1.3. Previous Research
Previous research examing DEMO performance on optimal stopping problems has tended to
focus on versions of the problem that provide rank order information, rather than the values
themselves (e.g., Dudey & Todd, 2001; Seale & Rapoport, 1997, DEMO). These rank order prob-
lems are interesting for the same reasons as the problem we study, but have a very different op-
timal solution process. When only rank order information is available, the optimal process in-
volves observing a fixed number of values, then choosing the first subsequently maximal one
(see Gilbert & Mosteller, 1966, Table 2). When the value itself is available, the optimal process
is based on a set of thresholds, one for each position in the sequence, DEMO requires a presented
value to be selected if it is currently maximal, and exceeds the threshold for its position (see
Gilbert & Mosteller, 1966, Tables 7 and 8). These thresholds can naturally be conceived as the
aspiration levels in Simon’s seminal theory of satisficing (e.g., DEMO, 1955, 1982).
Kahan, Rapoport, and Jones (1967) studied human performance on essentially the same task
that we consider, using problems of length 200. Different problems involved values drawn from
either a positively DEMO, negatively skewed, or a uniform distribution. These authors found no
evidence for the different distributions affecting the decisions made. They also compared indi-
DEMO and group decision-making, and found that decisions were made earlier in DEMO sequence
by individuals. Corbin, Olson, and Abbondanza (1975) considered human peformance on prob-
lems of length five, and by systematically manipulating the values presented, found sequential
and contextual dependencies within problems. Other empirical studies (e.g., Kogut, 1990;
Zwick, Rapoport, Lo, & Muthukrishnan, 2003) have made a large methodological departure by
requiring subjects to sacrifice explicitly held resources to view additional presentations, usually
because they are interested in applications to economic decision-making.
Lee, O’Connor, and Welsh (2004) DEMO the study that is most directly relevant to the
one reported here. These authors considered human performance on problems with lengths 10,
20 DEMO 40, and evaluated three candidate models, drawn from the previous mathematical and
psychological literature, of the way people made decisions. They concluded that the best ac-
counts were provided by ‘threshold’ models in which people DEMO by comparing the pre-
sented value to fixed thresholds. What Lee et al. (2004) observed, however, was that there
seemed to be DEMO individual differences in the exact thresholds that people used. Some
subjects behaved consistently with applying a single fixed threshold across the entire se-
quence. DEMO, these people chose the first number that exceeded a fixed value. DEMO sub-
jects, however, behaved consistently with using thresholds that decreased as the sequence pro-
gressed, as with the optimal solution. Lee et al. (2004) concluded by arguing that shorter
problem lengths needed to studied DEMO provide the empirical data that would distinguish differ-
ent threshold-based accounts.
4 M. D. Lee/Cognitive Science 30 (2006)
1.4. Modeling the Decision-Making of Optimal Stopping
Much of the previous research on optimal stopping DEMO has proposed formal models
of the decision-making process (e.g., Corbin, DEMO; Dudey & Todd, 2001; Lee et al., 2004;
Seale & Rapoport, 1997), although sometimes their evaluation has taken the form of simula-
tion studies, rather than by making inferences from human data.
Even when comparison with human decisions has been made, however, there DEMO been a
number weaknesses in the modeling. Some of these weaknesses are shared with much of
contempory psychological modeling in general, such as focusing on the best-fitting behavior of a
flexible model as an measure of DEMO adequacy, and ignoring the inherent complexity of the model
(see Roberts & Pashler, 2000; Pitt, Myung, & Zhang, 2002). Others are more specific to the na-
ture of the optimal stopping problem, such as failure to make probabilistic inferences about mod-
els and their DEMO because the choice data are noise-free and the models are deterministic.
Even Lee et al. (2004), who did consider these basic model theoretic issues, failed to present a
modeling framework that addressed bigger questions, DEMO as how the various decision pro-
cesses people use to solve the problems might be generated in the first place, how they might
adapt these processes, and how individual differences might be accommodated.
In this paper, we try to address some of these modeling issues, studying human DEMO
on problems with length five. We first provide basic analyses that confirm the plausibility of
threshold models. We then develop a new generative account, cast in a Hierarchical Bayesian
framework, describing how threshold models may be generated and applied to the optimal
stopping problem. Applying Bayesian and Minimum DEMO Length statistical meth-
ods—including model averaging, model selection and ‘entropification’ methods—we DEMO
about the parameters of the generative process, make inferences about the DEMO decision
models people use, and demonstrate the predictive capability of our DEMO
2. Experiments
We consider data collated from two separate experiments.1 Both experiments were identical
methodologically, but involved different subjects, and different problems.
2.1. DEMO
2.1.1. Subjects
In the first experiment there were 50 subjects (26 DEMO and 24 females), with a mean age of
19.8 years, DEMO of whom received course credit. In the second experiment, there were DEMO sub-
jects (44 males and 43 females), with a mean DEMO of 25.3 years, who were paid for participat-
ing.
2.1.2. Procedure
DEMO experiment involved a different set of 40 randomly generated problems, with DEMO
between 0 and 100, defined to two decimal places. Each subject DEMO all 40 problems in
their experiment in a random order. For each problem, subjects were told the length of se-
M. D. Lee/Cognitive Science 30 (2006) 5
quence was five, and were instructed to choose the maximum value. It was emphasized that (a)
the values were uniformly and randomly distributed between 0.00 and DEMO, (b) a value
could only be chosen at the time DEMO was presented, (c) the goal was to select the maximum
DEMO, with any selection below the maximum being completely incorrect, and (DEMO) if no choice
had been made when the last value was DEMO, they would be forced to choose this value.
As each value DEMO presented, its position in the sequence was shown, together with ‘yes’ and
‘no’ response buttons. When a value was chosen, subjects rated their confidence in the decision
on a nine point scale ranging from “completely DEMO to “completely correct”.
2.2. Results
All of the decision data are summarized in Figure 1. The top half of this Figure contains 40
panels, corresponding to the 40 problems in the first experiment. The second half DEMO the Figure
contains 40 panels corresponding to the problems from the second experiment. Within each
panel, the five values for that problem are shown in sequence from left to right by circles.
Values that were chosen DEMO at least one subject are shown as filled circles, and the DEMO of filled
circles is proportional to the relative frequency with which that choice was made. The panels
are ordered according to where the maximum DEMO is in the sequence so that, for example, the
first row contains problems where the maximum value is the first one presented.
Our DEMO analysis of these decision data takes the form of identifying five empirical regu-
larities, each of which informs the development of models to account for the way people solved
the problems. We present each regularity in DEMO, and explain the way it informs subsequent
modeling.
2.2.1. Individual Differences
DEMO 2 shows the variation in performance across all subjects. The left-hand panel shows
the relationship between decision accuracy, as measured by the proportion of times each sub-
ject chose the maximum value, and mean confidence across all decisions. The right-hand
graph shows how decision accuracy, as measured by the proportion of times each subject chose
in accordance with the prediction DEMO the optimal decision rule, relates to mean confidence. It is
clear DEMO there is considerable individual variation in both measures of decision performance,
and in confidence, and a positive relationship between the variables. In particular, we note that
while some subjects choose consistently with the optimal decision rule only about half of the
time, others follow it almost perfectly. The message we draw from this empirical regularity is
that it is DEMO to have a model capable of accommodating individual differences.
2.2.2. Choosing the Current Maximum
A second regularity is evident by counting how often, across all subjects and problems, the
value chosen was the maximum value encountered at that stage. For the first through fifth posi-
tion, respectively, DEMO was true in 100% (inevitably), 98.5%, 96.4%, 97.7%, and 35.6% of
cases. Thus, it is clear that people almost always choose a currently maximal value, until the fi-
nal position where forced choices are made. The message we draw from this empirical regular-
ity is DEMO a model of human performance needs to keep track of the maximum value encoun-
tered at any stage in a problem sequence, and only choose currently maximal values.
6
M. D. Lee/Cognitive Science 30 (2006)
Fig. 1. Summary of the problems and decisions for both problem sets. Each panel, corresponds to a problem, with
the five values for that problem shown in sequence from left to right by circles. Values that were chosen by DEMO least
one subject are shown as filled circles, and the area DEMO filled circles is proportional to the relative frequency with
which that value was chosen.
2.2.3. Using Different Thresholds
Figure 3 shows how often various DEMO were chosen, across all problems and subjects, in
terms of their position in the problem sequence. Visually, there seems to be a trend for succes-
sively lower values to be more likely to be chosen DEMO later positions. This trend is confirmed in
the analysis presented in Figure 4, which shows the proportion of values in successive ranges
of length ten (0 to 10, 10 to 20, ..., 90 to DEMO) that were chosen when presented, for each of the
five positions in the sequence. Starting in the 40s, for example, a greater DEMO of values
M. D. Lee/Cognitive Science 30 (2006)
7
Fig. 2. The relationship between decision accuracy and mean confidence, with decision accuracy assessed in terms
of choosing the maximum value (left-hand panel) and choosing consistent DEMO the optimal decision rule
(right-hand panel). Each point corresponds to DEMO individual subject.
Fig. 3. The relative frequency with which values, across DEMO problems and subjects, were chosen, as a function of
their position in the problem sequence. Each line corresponds to a value at a DEMO in a problem, with the length of
the line indicating how DEMO that value was chosen.
are chosen when they are presented in position four than in earlier positions. A similar effect is
evident starting in DEMO 60s for position three. In general the ordered and monotic rise of the pro-
portions as values increase suggests that people lower their threshold DEMO for choice as the
problem sequence progresses. The message we draw from this empirical regularity is that any
threshold model needs to have the DEMO to apply successively lower thresholds at later po-
sitions in the problem sequence.
8
M. D. Lee/Cognitive Science 30 (2006)
Fig. 4. The proportion of presented values in ranges 0 to 10, 10 to 20, and so on, chosen across all problems and par-
ticipants, as a function of the position in the sequence for the presented value.
DEMO Insensitivity to Previous Values
Figure 5 shows the relationship between values that occur in the position before a choice is
made, and those that precede values that are also not chosen. There is no obvious difference DEMO
tween the distributions for values at the same position in the problem sequence. This conclu-
sion is supported by the analysis presented in Figure DEMO, which shows as scatteplots the relation-
ship between a value that DEMO not chosen, and the proportion of times the subsequent value was
DEMO There is no strong relationship between these variables for any of the first three posi-
tions in a problem sequence, with correlation measures corresponding to less than 10% of the
variance being explained in each case. DEMO means that knowing the preceding value in a prob-
lem provides little information about whether the subsequent one will be chosen. Although not
presented DEMO detail here, other analyses considering earlier values than the immediately preced-
DEMO one in the sequence, and also the averages and maxima and DEMO of a sequence of earlier
values, also failed to find any DEMO relationship. The message we draw from these findings is
that, as DEMO first approximation, beyond keeping track of maximality, we do not need models that
are sensitive to previous values in a problem sequence to DEMO human performance.
2.2.5. Lack of Learning
Figure 7 shows the pattern of change in decision accuracy across all subjects and problem sets,
in DEMO order in which they were completed. It can be seen there is no evidence for either the maxi-
mum value (left-hand panel) or DEMO rule (right-hand panel) measure of accuracy that perfor-
mance improves as additional problems are completed. The message we draw from this empiri-
cal DEMO is that, as a first approximation—at least for this version of DEMO task, which does not
include any feedback—we do not need learning DEMO to explain human performance.
M. D. Lee/Cognitive Science 30 (2006)
9
Fig. 5. Distributions of values in positions immediately before chosen and rejected values across all DEMO and
subjects. Each line corresponds to a value at a position in a problem, with black lines representing values before a
choice, and DEMO lines representing values before a rejected value. The length of lines indicates how often that value
was before a chosen or rejected value.
Fig. DEMO Scatterplots showing the relationship between a presented value that was not chosen, and the proportion of
times the subsequent value was chosen, for DEMO first three positions in a problem sequence.
2.2.6. Consolidated Conclusion
Considering these five empirical regularities collectively suggests that human perfor-
mance on the task DEMO be modeled using a flexible threshold approach, where the first cur-
DEMO maximal value above a position-dependent threshold is chosen. These thresholds need
to be specified, in the first instance, at the level of individual DEMO, but do not need to de-
pend on previous values in DEMO problem sequence, nor be subject to learning across successive
problems.
10
M. D. Lee/Cognitive Science 30 (2006)
Fig. 7. The pattern of change in decision accuracy across problems in the order they DEMO completed, as measured
by choosing the maximum value (left-hand panel) DEMO choosing consistent with the optimal decision rule
(right-hand panel).
3. DEMO Generative Model
In this section, we develop a generative account of DEMO threshold-based model family sug-
gested by the empirical regularities. First we specify how threshold models may be generated,
and then how these models DEMO decisions for a specific set of problems. Having specified the
generative process, we then describe the reverse statistical process by which the parameters of
the model can be inferred from behavioral data.
3.1. Model Family Level
DEMO threshold model implicitly uses a threshold of zero for the final position, since it is a
forced choice. Accordingly, our account of generation DEMO threshold models for problems of
length five describes how the first four thresholds are determined. We begin by assuming the
threshold for the first DEMO is in place, and then describe the processes that give rise DEMO the
second, third and fourth thresholds. Moving from the first to DEMO second threshold presents two
reasonable possibilities: fixing at the first threshold, or moving the threshold downwards by
some amount. Moving to the third DEMO fourth thresholds then involves three possibilities: fix-
ing at the previous DEMO, moving downward by the previously used amount of decrease, or
moving downward by a new amount.
Overall, we (crudely) represent this generative process as a multinomial θ =(θF , θD, θN),
DEMO θF gives the probability of fixing the threshold, θD gives the DEMO of moving down
by the same amount, and θN gives the DEMO of moving down by a new amount. This
parameterization is shown at the top of Figure 8, with the multinomial θ represented in a trian-
gle that has the F, D, and N events as DEMO
For problems of length five, the model family encompasses the 14 DEMO models shown
in the middle of Figure 8. Each of the models is labeled according to the sequence of fixed (F),
down (D), and new (N) transitions by which it was generated. DEMO horizontal lines indi-
M. D. Lee/Cognitive Science 30 (2006)
11
Fig. 8. A generative account of using threshold models to make decisions on the optimal DEMO problem.
cate a fix transition, while vertical lines indicate down transitions. DEMO first FFF model simply
uses a fixed threshold across the problem sequence. The next DDD model decreases the thresh-
old by the same amount DEMO positions progress through the sequence, and so on. The final DNN
DEMO is the most flexible one possible, since it allows any threshold DEMO any position, subject to
the constraint that thresholds do not increase.
DEMO particular parameterization θ of the generative process corresponds to a probability distri-
bution over the 14 models, determined in the obvious way. The probability of the FFF model is
3 , the probability of the DDD DEMO is 3 , the probability of the FFD model is θθ2 D , and so
θ F θ D F
on. For the particular DEMO of θ shown at the top of Figure 8, the probability DEMO p (M |
θ) over the 14 models is shown immediately below.
3.2. Model Level
Each of the 14 models involves one or DEMO parameters. For the FFF model, there is a single
parameter, that determines the fixed threshold. For the DDD model, there are two parameters,
determining the initial threshold, and the steady rate of decrease in subsequent thresholds, and
so on. For the final DNN model, there DEMO four parameters, setting the thresholds at each of the
four positions DEMO the problem sequence. At a specific parameterization, a model is applied DEMO a
problem by having it choose the first currently maximal value above its threshold for that posi-
tion in the sequence.
The models make DEMO decisions across a set of problems as their parameters are varied.
Since we are dealing with sets containing 40 problems, the data space contains the set of all
40-vectors containing the values 1, 2,...,5in each position, with the values giving the choice
made for each problem. Effectively, therefore, over all parameterizations, models index a dis-
tribution across the data space, indicating which sets of choices they are able to make. We de-
note these sets of decisions indexed by the models DEMO Y . We note that different threshold mod-
12 M. D. Lee/Cognitive Science 30 (2006)
els have different complexities, in the sense that they index different numbers of sets of
decisions. For example, for our first problem set, the simple fixed-threshold DEMO model in-
dexes only ||Y|| = 79 different sets of decisions, DEMO the fully flexible DNN model indexes
||Y|| = 3, 121. Other DEMO have intermediate complexity. A similar state of affairs is true for
the second problem set in this study, with the FFF model indexing ||Y|| = 81 sets of decisions,
and the DNN model indexing ||Y|| DEMO 2, 959.
3.3. Data Level
Using the distribution over the models DEMO from the model family level, the collection of
decisions sets indexed DEMO individual models are combined in proportion to their probabilities.
This mixture p (Y | θ) is the final result of the generative model: a single parameterization at the
mode family level produces a distribution in DEMO data space, as mediated by the family of 14
possible threshold DEMO The bottom of Figure 8 shows this distribution, ranking the indexed
DEMO sets from most to least probable, given the initial parameterization θ DEMO the generative
process.
4. Model Inference
We place a uniform prior over θ at the generative level, so that p (θ) ∝ 1. Given decision data
D, the posterior for the generative process parameter θ,
p (θ | D) ∝ p (D | θ) DEMO (θ) ,
is found by assuming a uniform prior p (θ) ∝ 1, and integrating across the 14 models,

DEMO p D M p Mθθ 
  i
i 
i
where Mi denotes the ith model, and p (Mi | θ) is found in the obvious way described earlier,
3 , and DEMO on.
with p (MFFF | θ) = θ F
The final quantities required to make inferences from data, p (D|Mi), present DEMO interesting
challenge. The threshold models are deterministic, in the sense that DEMO particular model at a par-
ticular parameterization makes exactly one set of decisions for a problem set, and so indexes a
single point in the data space with probability one. If the set of decisions observed DEMO
do not coincide with this point, there is effectively zero probability DEMO that parameterization. If
the model does not index the empirical decisions at any parameterization, then there will be no
probability as a whole. This means that standard Bayesian methods, which rely on integrating
the likelihoods of models across the parameter space, are not applicable for the inferences we
require.
One way to address this problem would be to introduce an DEMO theory to the model. An al-
ternative approach, that is in DEMO ways more principled and satisfying, was developed by Lee
(2004). This approach uses an information theoretical method, based on Minimum Description
Length (MDL) methods for model selection, called ‘entropification’ (Grünwald, 1998, DEMO;
M. D. Lee/Cognitive Science 30 (2006)
13
Myung, Pitt, & Kim, 2005). Entropification provides a principled technique for associating de-
terministic models with probability distributions, allowing inferences to be made that are
‘safe’, in the sense of minimizing the expected worse case errors. Intuitively, entropification
introduces a principled conservative error theory to the process of inference, so that empirical
data is able to be brought into contact with deterministic models.
A detailed and general account of applying entropification under 0–1 DEMO is provided in the
Appendix. Basically, we consider the conditional probability
DEMO
pDY w M
e
wm
40
x
0
40



 x

k 1
xe
wx
where k = 5 is DEMO length of the problem, w is a positive scalar, and f0–1 (D, Y)= m is the 0–1 loss
function that indicates DEMO model M makes m different decisions in its indexed decision set Y
from the empirical data D of a subject. The entropification method then DEMO finding
ppDYwMmax

Yw
giving the MDL value
MDL p Y	 
DEMO log
where ||Y|| is the total number of predictions indexed by the model. This MDL value can then be
used to give the required DEMO as
pD M e
MDL

In addition, the entropification process DEMO a ‘best’ prediction, Y*, which is naturally as-
sociated with a parameterization of the model, in the form of a set of thresholds.2 This set of
thresholds can be regarded as the result of ‘fitting’ DEMO model to the data.
4.1. A Concrete Example
We now give a concrete example of inference across the generative model. Following the
observation of DEMO large and meaningful individual differences, inference is always
done here at DEMO level of individual data. In particular, we focus on a single DEMO from the sec-
ond experiment, whose analysis contains clear examples of DEMO of the interesting and impor-
tant features of the inference process.
Inference for this subject is summarized graphically in Figure 9. Their data (i.e., the deci-
sions they made across all 40 problems) are shown DEMO the bottom panel. Each of the5×8boxes
represents a problem, with circles DEMO left to right corresponding to the values presented in
the sequence, DEMO the subjects choice indicated by the filled circle. Thus, for this DEMO, their
set of decisions is represented by the 40-vector D = DEMO, 1, 1, 1, 5, . . . , 5].
DEMO the entropification method for the FFF model against these data involves compar-
ing every set of decisions the model can make by varying its DEMO threshold parameter. The
14
M. D. Lee/Cognitive Science 30 (2006)
Fig. 9. The results of making inference across the generative model for one subject in DEMO second experiment. The
decisions made by the subject are shown in the bottom panels, from which best indexed sets of decisions for each
model are inferred, and the marginal densities of the models evaluated, as DEMO in the middle panels. These
densities, in turn, are used to infer the posterior over the model generating parameters, summarized by the mode
(circle), mean (square) and 50% and 95% credible intervals of the distribution in the top panel.
threshold value corresponding to the best DEMO set of decisions Y* ∈ MFFF is shown in the
left-most panel immediately above the data. This is the best single threshold model for DEMO sub-
ject’s data. Similarly, the best parameterizations of the remaining models DEMO shown in the
other panels immediately above the data.
Each of these best models has an MDL value, which is naturally associated with the proba-
bility the subject’s data would have arisen under the model p (D|M). These marginal densities
are shown by the bar graph above DEMO panels for each model. It can be seen that only three of the
14 models—the FFD, FDN, and DNN models—have any appreciable density. DEMO means
M. D. Lee/Cognitive Science 30 (2006)
15
these models, DEMO their best setting of thresholds, are the best able to match DEMO decisions made
by the subject across the 40 problems.
Finally, on DEMO basis of the different marginal densities found for each of the models, an in-
ference is made about the generative process the subject used to construct candidate models.
This corresponds to the rate at which they DEMO fixed, downwards, and new transitions moving
from one threshold to another as the problem progresses. The range of values for these rates
that DEMO be inferred from the subject’s data are summarized at the top of Figure 9, with the
mode indicated by a circle, the mean DEMO a square, and 50% and 95% credible intervals drawn. It
can DEMO seen that, consistent with the strong evidence in favor of the DEMO model, rates are most
likely that give greatest probability to fixed DEMO, then downwards transitions, then new
transitions. The wide credible intervals show, however, that considerable uncertainty about the
rates remains. At this level DEMO abstraction, the data do not provide a strong constraint on what
DEMO be inferred about the subject’s cognitive processes.
4.2. Posterior Prediction
Having made inferences about the generative process from observing a subject’s data, it is
possible to specify a posterior predictive model. This is essentially a set DEMO thresholds that rep-
resent the account our modeling framework provides of how that subject made their decisions.
It defines a threshold model for that DEMO that can be applied to other problems, furnishing a
prediction about DEMO that subject will behave in the future.
We consider three approaches to constructing a posterior predictive model. The first ap-
proach simply uses the DEMO threshold parameterizations in the maximally flexible DNN
model. The DNN model allows for any combination of thresholds, and the entropification pro-
cess identifies from the subject’s data which of these combinations is the most likely. In DEMO,
this approach corresponds to choosing the ‘maximum likelihood’ model.
The second approach uses the preferred threshold parameterization for the model with the
best DEMO value across all 14 models. These MDL values are sensitive to the various complexi-
ties of the models, as measured by the number of data points they index. Accordingly, the set of
thresholds selected will be sensitive to both data-fit and complexity, with a focus on choosing
the simplest sufficiently accurate model. In this sense, the approach corresponds to choosing
the ‘maximum a posteriori’ model.
The third approach uses the preferred parameterization DEMO all of the 14 models, by averaging
across them in proportion DEMO the posterior probability of each model, as given by
pM D
DEMO
pD M p M



i


i
	

j pD M p M
 

 j

DEMO
i

That is, the threshold for the first position is DEMO as the average across the best threshold for
each of the 14 models, weighted by the posterior of the model given the observed. This ap-
proach corresponds to ‘model averaging’.
For the particular subject whose inference DEMO demonstrated in Figure 9, the three predic-
tive models found by DEMO approaches are shown in Figure 10. The maximum likelihood ac-
16
M. D. Lee/Cognitive Science 30 (2006)
Fig. 10. The predictive models inferred from the decisions made by a subject in the DEMO experiment, using the
maximum likelihood (left panel), maximum a posteriori (middle panel) and model averaging (right panel) methods.
count corresponds DEMO the best thresholds inferred within the fully flexible DNN model. The
maximum a posteriori account corresponds to the best thresholds inferred within the simpler
DEMO model, which has the greatest marginal density, and so provides the best balance between
data-fit and model complexity. The model average account essentially DEMO the best
thresholds inferred within the FFD, FDN and DNN models, since these are the only three with
significant marginal density.
Both the DEMO two approaches are frequently used in cognitive modeling. The maximum
likelihood approach essentially corresponds to selecting the best-fitting model, while the max-
imum a posteriori method essentially corresponds to Bayesian model selection, preferring the
model that best fits on average across all possible parameterizations.
Despite strong evidence it DEMO to better predictions (see Hoeting, Madigan, Raftery, &
Volinsky, 1999, for a review), the model averaging approach is rarely applied in cognitive mod-
eling, because (at least in part) it often leads to problems with interpretability. A blended model
that is nine parts DEMO A to one part Model B, where each competing model provides DEMO very
different interpretation of a cognitive process, uses different parameters, and so on, is unlikely
to be easy to understand and motivate theoretically. The threshold family of models we are
considering is exceptional in this DEMO, because it is closed under model averaging. That is,
the DEMO of any set of particular threshold model is itself a threshold model, and so is
readily amenable to exactly the same interpretation as an account of the cognitive deci-
sion-making process involved.
M. D. Lee/Cognitive Science 30 (2006)
17
5. Modeling Results
In this section, we provide an account of making inferences for all 147 subjects using the
generative model. At the model family and model DEMO, our results are largely descriptive and
exploratory, summarizing the range of parameterizations of the basic cognitive deci-
sion-making process that was observed. At DEMO level of data, we provide a much stronger evalu-
ation, by testing the ability of the inferred predictive models to account for human DEMO
sion-making.
5.1. Model Family Level
Figure 11 shows the means of the inferred posterior for the generative parameters at the
model family level. Each DEMO the 147 subjects across the two experiment is shown by a point.
The inference that would have been made if a subject had adhered DEMO to the optimal deci-
sion rule for the two problems sets are shown by large circles.
There is some significant level of variation in DEMO means, particularly with respect to the
emphasis given to the fix DEMO down transformations. There is also some evidence of clustering,
suggesting the possibility of different ‘strategies’ in solving the problem, although this specu-
lation needs to be tempered by level of uncertainty indicated by the wide DEMO intervals in
Figure 9, which are typical of those found for DEMO of the subjects. It is also interesting to note
that many subjects have means extremely similar to those corresponding to optimal perfor-
mance, while others deviate either by giving relatively greater emphasis to the fix or DEMO down
transition.
5.2. Model Level
Figure 12 shows the predictive models inferred using the model averaging approach for
each of the 50 subjects from DEMO first experiment. The predictive models are shown as bold
Fig. 11. Means of the posterior of the generative process p (θ | D), for each subject (shown as points), and for data
sets corresponding to using the optimal decision rule for both problem sets (large circles).
18
M. D. Lee/Cognitive Science 30 (2006)
Fig. 12. Predictive models (bold lines) inferred using the model averaging methods for each DEMO the 50 subjects in
the first experiment, superimposed on the thresholds DEMO the optimal decision rule.
lines, superimposed over the optimal decision rule DEMO These predictive models are en-
tirely representative of those inferred for the 97 subjects in the second experiment.
There is clear variation across subjects DEMO this level of analysis. For example, a visual com-
parison of DEMO 1, 11, 18 and 21 reveals four quite different threshold models. Subject 11 is
close to optimal, while subject 1 relies on a single fixed threshold, and subjects 18 and 21 use
non-optimal decreasing thresholds that accelerate downwards at very different rates.
5.3. Predictive Accuracy
A very DEMO and practical evaluation of the usefulness of our generative model is provided
by examining its ability to predict decision-making behavior. We do this using
DEMO, in which a subset of each subjects’ decisions are used for DEMO, and the
resultant predictive models are then evaluated against the unseen DEMO This evaluation simply
measures what proportion of the subjects’ decisions were correctly chosen by the predictive
model.
Specifically, we consider training sets with random samples of 2, 3, 4, 5, 10, 20 and 30 prob-
lems. For each of these sizes, 100 different training sets were used. Figure 13 shows the pattern of
change in predictive accuracy for DEMO maximum likelihood, maximum a posteriori and model av-
erage methods, as the number of training problems increases. Mean performance is shown, to-
gether with one standard error in each direction. With the maximum available information DEMO this
analysis (30 problems), the methods appear to be approaching DEMO 75% accuracy. This com-
pares well with chance performance, which obviously DEMO to 20% accuracy.
It is interesting to note that the model averaging method is superior to the other methods,
particularly when the number DEMO training problems used for inference is small. The maximum a
posteriori method also seems to outperform the maximum likelihood method for small num-
M. D. Lee/Cognitive Science 30 (2006)
19
Fig. 13. Cross-validation results showing the change in predictive accuracy as a function of training DEMO size, for the
maximum likelihood, maximum a posteriori and model averaging methods. Error bars represent one standard error
about the mean in each DEMO
bers of training problems. In absolute terms, the model average method DEMO impressively.
On average, it defines a predictive model from observing only DEMO problems that agrees with
more than 70% of the 37 other decisions made by subjects.
6. Discussion
Our discussion considers both methodological issues associated DEMO the modeling ap-
proach presented here, and psychological issues concerning the DEMO decision-making be-
havior of people on the optimal stopping task.
6.1. Hierarchical, Generative and Bayesian Modeling
Most popular models of cognitive processes can be characterized as low-dimensional para-
metric models. These models describe a formal relationship DEMO a small set of parameters
and the data that are observed in a cognitive task. The modeling approach we have adopted is
different, and is inspired by some recent generative and hierarchical models of cognition (e.g.,
Kemp, Perfors, & Tenenbaum, 2004; Griffiths, Baraff, & DEMO, 2004). Accordingly, it is
worth discussing what we view as the benefits of our modeling approach.
6.1.1. Hierarchical Modeling
The strength of DEMO models is that they are able to represent knowledge at different
levels of abstraction. This is a key feature of our account of making DEMO for the optimal
20 M. D. Lee/Cognitive Science 30 (2006)
stopping problem. At the lowest level, we are able to think about the observed behavioral data.
At the next level of abstraction, we are able to make inferences about specific models (like the
fixed threshold model) that have DEMO (in the form of thresholds, or differences
between thresholds) that DEMO behavioral data when brought into contact with a problem
stimulus. At our highest level of abstraction, we can make inferences about parameters that de-
scribe which specific models might be used.
The ability to represent information DEMO uncertainty at different levels in these ways, and to
make inferences DEMO and between these levels, allows us to give a more complete DEMO coher-
ent account of decision-making for this task than would otherwise be possible.
Low-dimensional parametric models usually focus on one level of explanation, at least in a for-
mal sense. This means that ‘meta-cognition’ becomes a DEMO line of inquiry, and issues of
parametric self-regulation and adaptation (rather than just estimation) are difficult to address.
In a more practical way, a consequence of this narrowness in the level of abstraction is that
most cognitive models are not amenable to model averaging (even if there are no in-principle
problems with interpretability), because there is no over-arching DEMO that unifies compet-
ing models. Thus, at least in part, the impressive performance of our predictive model using the
model averaging method stems DEMO the hierarchical nature of our account.
6.1.2. Generative Modeling
The strength of generative models is that they provide some account of how relatively spe-
DEMO cognitive processes might be instantiated and bounded. Having defined a plausible gener-
ative mechanism for the model family, the specification of individual models becomes a formal
deduction, rather than an act of creation to be given plausible justification. Of course, other rea-
sonable assumptions at our most abstract level would have possible, but at least the generative
process is explicit. Its outcomes are also non-trivial, in the sense that there are many thresh-
old-based models could have been proposed that are not in the DEMO model family3.
More practically, we believe that theory and model development DEMO often streamlined by
adopting a generative perspective. All of the theoretical and empirical insights a researcher has
about a cognitive process can contribute to DEMO account of how different data would be gener-
ated, without being DEMO by questions of inference. Once a full generative account is in
place, inference involves the (conceptually) simple idea of reversing the process, DEMO finding
the most likely account for the data that have actually been observed.
6.1.3. Bayesian Modeling
The strength of Bayesian models is that they DEMO a coherent method for statistical infer-
ence, founded on probability theory (Cox, 1961; Jaynes, 2003). Given a generative process
across a hierarchical model structure, it is simple both to generate data, and DEMO inferences
about the parameters of the processes from data. Both questions correspond to logically de-
duced probability statements, rather than requiring ad hoc specification of heuristic devices.
Indeed, the modeling reported here probably represents a worst-case example of the applica-
tion of Bayesian inference, since the use of deterministic models necessitated a detour into
(closely related) MDL methods. Even DEMO worst case, however, contrasts favorably with the
prospect of having to make inferences about our model using sampling distribution methods,
defining estimators DEMO making hypothesis tests. A little reflection suggests that sampling dis-
M. D. Lee/Cognitive Science 30 (2006) 21
tribution methods simply DEMO not developed to make inferences across structured generative
accounts of cognitive processes. This provides another reason to stop using them, as if that
were needed (see Jaynes, 2003, ch. 17).
6.2. Psychological Issues
6.2.1. Lack of Sequential Effects
We found, as summarized in Figure 5, DEMO previous values in a problem did not exert a large
influence on the value chosen. This conflicts with the findings of Corbin et al. (1975), who did
find evidence that the sequence of values presented DEMO choice, with the same value more
likely to be chosen after DEMO series of relatively low values. Theoretically, the definition of the op-
DEMO stopping problem makes it clear that previous values are irrelevant, except DEMO the need to
remember the current maximal value. Experimentally, there are DEMO possible reasons for
the discrepancy in findings. Most fundamentally, Corbin et DEMO (1975) studied something closer
to the rank order version of the problem, because they did not restrict their values to a known
distribution4. Methodologically, Corbin et al. (1975) differed from our approach by always
leaving visible all of the previous numbers in a sequence, and by creating a contrived problem
set to test their specific research hypotheses. If DEMO were sensitive to the constraints in-
volved in generating the Corbin et al. (1975) problems, it is possible that their decision-making
involving additional inferences about the generating process, requiring complexities in model-
ing that are not required to account for the current data.
All of that said, we do view the assumption that previous values do not affect decisions DEMO
only a first approximation. It would be an interesting and worthwhile exercise to extend our
model to use previous values in some way, and examine to what extent this information can im-
prove the predictive capabilities DEMO the model. This modeling extension ought to be coupled
with more ecologically representative assumptions about how environments generate se-
quences of choice values. While DEMO is one possibility, it seems like many real-world
sequential decision-making problems DEMO naturally have richer structures, to which people
may well be sensitive.
DEMO Lack of Learning
Perhaps our most surprising empirical finding is the lack of learning, as summarized in Fig-
ure 7. One obvious possible reason involves the lack of feedback or financial incentive given to
subjects. It DEMO be interesting to observe what, if any, changes in adherence to the optimal
decision rule, arise from providing various sorts of corrective feedback. We note, however, that
even though we did not explicitly provide DEMO feedback, the nature of the problem means that
often subjects will DEMO known, or be able to make a good inference about, whether their
choice was correct. Every time the last value in the sequence DEMO selected as a forced-choice,
knowledge of the previous four values indicates whether or not the decision was correct. Less
exactly, but still very informatively, the choice of a very large value at any position in the se-
quence is almost certainly correct. Presumably, on these problems, DEMO are able to use this
information to affirm their decision making in not choosing lesser, but relatively high, earlier
values. Given the availability DEMO this sort of corrective information, it is not obvious that the
DEMO of explicit feedback will lead to large learning effects.
22 M. D. Lee/Cognitive Science 30 (2006)
More speculatively, DEMO think it is possible that the mode of stimulus presentation plays a
role in preventing learning. Because the values are presented as typed four DEMO numbers, it
seems likely that any explicit rule people could apply DEMO adapt would be inherently lan-
guage-based. For example, the decision thresholds DEMO for Subject 1 in Figure 12 seems
well expressed by a rule like “choose the first value above 80”. It seems unlikely, however, DEMO
verbal rules of this type are easily adjusted in the ways required for incremental learning,
because it is an effortful cognitive process to DEMO words like “eighty” and “seventy-nine” on to
the magnitudes they represent. It would be extremely interesting to examine human
peformance on the same problems DEMO here, but presented in a continuous and spatial form
that allowed DEMO naturally adjusted decision rules. For example, it is possible that being DEMO
quired to choose the longest of five perceptually presented lines, with DEMO ranging from 0 to
100 units, would facilitate learning.
6.2.3. Individual DEMO
The results presented here dealt with individual differences at both the model family level
and the model level in largely descriptive and exploratory ways. DEMO is deficiency arose be-
cause our generative process described only how an individual produced threshold models and
made decisions, and so all modeling was done at the level of individual subjects. While this is
not inappropriate, given the empirical observation of clear individual differences, the structure
in individual results at the model family level and model level, as summarized in Figures 11
and 12, suggests that greater sophistication could be rewarded.
What is needed is an extended generative account that describes the processes that DEMO to
individuals being different and the same as they develop and apply threshold models to optimal
stopping problems. The formal modeling of individual differences DEMO cognitive processes has
been a focus of recent research (e.g., Lee & Webb, 2005; Navarro, Griffiths, Steyvers, & Lee,
in press; Rouder, Sun, Speckman, Lu, & Zhou, 2003), DEMO provides several ideas that might be
applied to the current problem. For example, to the extent that there are different strategies evi-
dent at the model family level in Figure 11, it might be worthwhile identifying clusters of sub-
jects in terms of the θ parameter (as per Lee & Webb, in press), and seeking some meaningful
interpretation of the differences.
7. Conclusion
Optimal stopping problems like the one considered here DEMO an interesting and useful
window onto human decision-making and problem-solving. The task is representative of many
real world problems, in its requirement to reason under uncertainty, yet is easily described and
controlled in the laboratory. The task is amenable to a formal solution, yet the optimal decision
rule is beyond the reach of cognitive capabilities, and so requires people to employ heuristic
solutions.
In this paper, we developed a generative model in which people use a series of thresholds to
make decisions, choosing the first value presented that is currently maximal and above the
threshold for DEMO position in the sequence. Our main evaluation of the model focused on its
predictive capability, demonstrating that it is able to predict the future decisions of subjects
M. D. Lee/Cognitive Science 30 (2006)
23
with considerable accuracy based on observing them solve just a few problems. In a more DEMO
ploratory way, modeling results suggest interesting patterns of individual differences, both in
terms of the types of threshold models they employ, and the values of the thresholds they use.
Future research ought to extend the DEMO model to embody a psychological theory of
these individual differences.
Notes
1. The raw data are available as an on-line appendix on the Cognitive DEMO Society’s
website at http://www.cognitivesciencesociety.org/supplements/.
2. Actually, a DEMO range of parameterizations, over which we average.
3. For example, a model where thesholds are allowed to rise. Or, perhaps more plausibly,
one with all the thresholds being different, but the decrease from the first to the second
being equal to the decrease from the third DEMO the fourth, and this decrease being different
from that from the DEMO to third.
4. We doubt it is identical to the rank order version as they claim, because it seems likely
subjects will use prior assumptions about likely distributions for the presented numbers
in the absence of DEMO relevant instructions, and so will not internally represent the num-
bers DEMO according to their rank order.
Acknowledgments
This paper was much improved by two rounds of thoughtful and provocative reviews, for
which I thank Josh Tenenbaum, Peter Todd, and the two anonymous reviewers. I also thank
DEMO Burns, Gary Ewing, Tess Gregory, Peter Grünwald, Jay Myung, DEMO Navarro, Michael
Paradowski, Doug Vickers, Matt Welsh, and Robyn Whibley. The first experiment was con-
ducted in 2003 as part of Chloë DEMO Honours thesis in Psychology at the University of
Adelaide. The second experiment was also conducted by Chloë Mount, supported by a Faculty
of Health Sciences grant from the University of Adelaide. I also acknowledge the support DEMO
Australian Research Council grant DP0451793.
References
Corbin, R. M. (1980). The secretary problem as a model of choice. Journal of Mathematical, 21, 1–29.
Corbin, R. M., Olson, C. L., & Abbondanza, DEMO (1975). Contest effects in optimal stopping rules. Organizational
Behavior and DEMO Performance, 14, 207–216.
Cox, R. T. (1961). The algebra of probable inference. Baltimore, MD: Johns Hopkins University Press.
Dudey, T., & Todd, P. M. (2001). Making good decisions with minimal information: Simultaneous and sequential
choice. Journal of Bioeconomics, 3 (2–3), 195–215.
Ferguson, T. S. (1989). Who solved the secretary problem? Statistical Science, 4 (3), 282–296.
Forsythe, G. E., Malcolm, M. A., & Moler, C. B. (1976). Computer methods for mathematical computations.New
York: Prentice-Hall.
24
M. D. Lee/Cognitive Science 30 (2006)
Gilbert, J. DEMO, & Mosteller, F. (1966). Recognizing the maximum of a DEMO American Statistical Association
Journal, 61, 35–73.
Griffiths, T. L., Baraff, E. R., & Tenenbaum, J. B. (2004). Using physical DEMO to infer hidden causal structure. In
K. Forbus, D. Gentner, & T. Regier (Eds.), Proceedings of the 26th annual conference of the cognitive science
society (pp. 500–505). Mahwah, NJ: Erlbaum.
Grünwald, DEMO D. (1998). The minimum description length principle and reasoning under DEMO University of
Amsterdam: Institute for Logic, Language and Computation.
Grünwald, DEMO D. (1999). Viewing all models as ‘probabilistic’. In Proceedings of DEMO twelfth annual conference on
computational learning theory (COLT’ 99). Santa DEMO: ACM Press.
Hoeting, J., Madigan, D., Raftery, A., & Volinsky, C. (1999). Bayesian model averaging. Statistical Science, 14,DEMO
382–401.
Jaynes, E. T. (2003). Probability theory: The logic DEMO science (G. L. Bretthorst, Ed.). New York: Cambridge Univer-
DEMO Press.
Kahan, J. P., Rapoport, A., & Jones, L. DEMO (1967). Decision making in a sequential search task. Perception &
Psychophysics, 2 (8), 374–376.
Kemp, C., Perfors, A., & Tenenbaum, J. B. (2004). Learning domain structures. In K. Forbus, D. Gentner, & T.
Regier (Eds.), Proceedings of the 26th annual conference of the cognitive science society (pp. 720–725).
Mahwah, DEMO: Erlbaum.
Klein, G. (1998). Sources of power: How people make decisions. Cambridge, MA: MIT Press.
Kogut, C. A. (1990)DEMO Consumer search behavior and sunk costs. Journal of Economic Behavior and Organization,
14, 381–392.
Lee, M. D. (2004). An efficient method for the minimum description length evaluation of cognitive models. In K.
Forbus, D. Gentner, & T. Regier (Eds.), Proceedings of the 26th DEMO conference of the cognitive science soci-
ety (pp. 807–812). Mahwah, NJ: Erlbaum.
Lee, M. D., O’Connor, T. A., & Welsh, M. B. (2004). Human decision making on the full-information secretary
DEMO In K. Forbus, D. Gentner, & T. Regier (Eds.), DEMO of the 26th annual conference of the cogni-
tive science society (DEMO 819–824). Mahwah, NJ: Erlbaum.
Lee, M. D., & Webb, M. R. (2005). Modeling individual differences in cognition. Psychonomic Bulletin & Review, 12,
605–621.
MacGregor, J. N., & Ormerod, T. C. (1996). Human performance on the traveling salesman problem. Perception &DEMO
Psychophysics, 58, 527–539.
Myung, I. J., Pitt, M. A., & Kim, W. J. (2005). Model evaluation, testing and selection. In K. Lambert & R. Gold-
stone (Eds.), Handbook of cognition (pp. 422–436). Thousand Oaks, CA: Sage.
Navarro, D. J., Griffiths, T. L., Steyvers, M., & Lee, M. D. (DEMO press). Modeling individual differences using
Dirichlet processes. Journal of Mathematical Psychology.
Pitt, M. A., Myung, I. J., & Zhang, S. (DEMO). Toward a method of selecting among computational models of cogni-
tion. Psychological Review, 109 (3), 472–491.
Roberts, S., & Pashler, H. (2000). How persuasive is a good fit? A comment DEMO theory testing. Psychological Re-
view, 107 (2), 358–367.
Rouder, DEMO N., Sun, D., Speckman, P. L., Lu, J., & Zhou, D. (2003). A hierarchical Bayesian statistical framework for
response time distributions. Psychometrika, 68 (4), 589–606.
Seale, D. A., & Rapoport, A. (1997). Sequential decision making with relative ranks: DEMO experimental investigation
of the “Secretary Problem”. Organizational Behavior and Human Decision Processes, 69 (3), 221–236.
Seale, D. A., & Rapoport, A. (2000). Optimal stopping behavior with relative ranks. Journal of Behavioral Decision
Making, 13, 391–411.
Simon, H. A. (1955). A behavioral DEMO of rational choice. Quarterly Journal of Economics, 69, 99–118.
Simon, DEMO A. (1976). From substantive to procedural rationality. In S. J. DEMO (Ed.), Method and appraisal in eco-
nomics (pp. 129–148). London: Cambridge University Press.
Simon, H. A. (1982). Models of bounded rationality. Cambridge, MA: MIT Press.
Steblay, N. M., Deisert, J., Fulero, S., & Lindsay, R. C. L. (2001). Eyewitness accuracy rates in sequential and si-
multaneous lineup presentations: A meta-analytic comparison. Law and Human Behavior, 25, 459–474.
M. D. Lee/Cognitive Science 30 (2006)
25
Vickers, D., Bovet, P., Lee, M. D., & Hughes, P. (2003)DEMO The perception of minimal structures: Performance on open
and closed versions DEMO visually presented Euclidean traveling salesperson problems. Perception, 32 (7),
DEMO
Vickers, D., Butavicius, M. A., Lee, M. D., & Medvedev, A. (2001). Human performance on visually presented
traveling salesman DEMO Psychological Research, 65, 34–45.
Zwick, R., Rapoport, A., Lo, A. K. C., & Muthukrishnan, A. V. (2003). Consumer DEMO search: Not enough or
too much? Marketing Science, 22 (4), 503–519.
Appendix
Entropification Under 0–1 Loss
Suppose a deterministic model M DEMO being evaluated using a dataset D that has n observations,
D =[d1,...,dn]. Each of the observed data are discrete, and can assume only k different values.
The model uses p parameters θ =(DEMO,..., θP) to make predictions Y =[y1,..., yn]. To DEMO any
prediction made by the model, a 0–1 loss function is DEMO as , wherefDY n γ i γi =0if
di = yi and γi = 1 otherwise. By considering all possible parameterizations, the model makes a totali 1
of N different predictions. In other words, there are N different predictions, Y1,..., YN, the model
is able to make about the data by choosing different parameter values. In general, the relationship
between parameterizations and predictions will be many-to-one. This means that every DEMO
model prediction is naturally associated with one or more parameterizations of the model.
Under these assumptions, Grünwald (1999) shows that using entropification the model
making prediction Y can be associated with a probability distribution, parameterized by the
scalar w, as follows:

pD M Y w
e
kk
…e
xx1
wf D Y
11
n
wf D x DEMO x
1
n
Determining the MDL criterion for the model requires finding the model predictions Y* and
scalar w* that jointly maximize p (D|M, θ, w) to give the value p*. Once this is achieved the
MDL criterion for the model is given simply by MDL = DEMO p* + ln N.
There is an obvious difficulty in maximizing p (D|M, θ, w). The problem is that the denom-
inator given by involves considering every possible dataZ… ekk 
kn terms. In DEMO science, where it is
possible for a deterministic model to be DEMO using many data points, each of which can
assume many values, the repeated calculation of Z may be too computationally demanding to
be DEMO
Lee (2004) derived a simpler form for Z can be derived by noting that f (D, Y) can only take
the values 0,..., n, in accordance with how many of the model DEMO agree with the data.
Since Z considers all possible data sets, DEMO number of times n – x matches (i.e., x mismatches)
will occur is . For a prediction()x (1)k x DEMO that has n – m matches with the data (i.e., there are
m mismatches and f (D, Y) = m), this leads to the simplification
xx1
set that could be observed, which involves a total of
00
n

1 n

pD M Y DEMO
e
wm
n
x
0
n



 x 
k 1
xe
wx
n
wf D x … x
26 M. D. Lee/Cognitive Science 30 (2006)
which has a denominator that sums n + 1 rather than kn terms.
The computational DEMO offered by this reformulation means it will generally be possi-
ble to find the w * that maximizes p (D|M, Yi, wi), giving p* , for all N model predictions. The
i i
p
DEMO each w * can also be done efficiently by observing that
i
* required for MDL calculation is then just the maximum of .p… DEMO
1 N
1 n
 	
Z 2 x 0
n
x
x
This derivative is clearly always positive if m = 0 and DEMO negative if m = n.
This means, if a model predicts DEMO of the data correctly, , and if a model fails towi DEMO
predict any of the data correctly . Otherwise, if 0 <wi  m < n, the substitution u = e–w
allows w * to be found from the positive real roots of the degree n DEMO
i
n
n
x
xk 1

xmu
x

x 0
by standard numerical methods (e.g., Forsythe, Malcolm, & Moler, 1976).
**
pw e x me 
k
1 
wm wx{1g42fwefx}