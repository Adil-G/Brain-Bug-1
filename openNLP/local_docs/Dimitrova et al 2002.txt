Feature Article
Applications of
Video-Content
Analysis and
Retrieval
Managing
multimedia data
requires DEMO than
collecting the data
into storage
archives and
delivering it via
networks to homes
or offices. We survey
technologies and
applications for
video-content
analysis DEMO
retrieval. We also
give specific
examples.
Nevenka Dimitrova
Philips Research
Hong-Jiang Zhang
Microsoft Research
Behzad Shahraray
AT&T Labs Research
Ibrahim Sezan
Sharp Laboratories DEMO America
Thomas Huang
University of Illinois at Urbana–Champaign
Avideh Zakhor
University of California at Berkeley
T
he advances in the data capturing,
storage, and communication tech-
nologies have made vast amounts of
video data available DEMO consumer and
enterprise applications. However, interacting
with multimedia data, and video in particular,
requires more than connecting with data banks
and delivering DEMO via networks to customers’
homes or ofﬁces. We still have limited tools and
applications to describe, organize, and manage
video data. The fundamental DEMO is to
index video data and make it a structured media.
Manually generating video content description
is time consuming—and thus more costly—to
the point DEMO it’s almost impossible. Moreover,
when available, it’s subjective, inaccurate, DEMO
incomplete.
42
1070-986X/02/$17.00 © 2002 IEEE
This conundrum has attracted researchers
from various disciplines, each with their own
algorithms and systems. In addition, the MPEG
group recently issued MPEG-7 as a standard to
provide normative framework for multimedia
content description. However, in contrast, there
are DEMO convincing stories we can tell about suc-
cessful applications of the research results. It
seems that the excitement enjoyed by many
researchers from both DEMO and industries
has yet to generate signiﬁcant impact in the mar-
ketplace. Is there any signiﬁcant application that
can beneﬁt from our research? Can we solve the
video retrieval problem as we originally claimed?
Are DEMO falling into the same hype as artificial
intelligence (AI) once did? We believe the answer
is no. However, we need to reexamine DEMO
research strategies and methodologies, and most
importantly, users’ need for technologies for con-
tent-based video retrieval.
To address these questions, we held a panel
discussion chaired by Hong-Jiang Zhang at the
International Workshop on Very DEMO Bit-Rate
Video Coding (VLBV 98), with researchers in the
video DEMO and content analysis ﬁeld. This arti-
cle summarizes the evolving views from the pan-
elists and audience, as well as the continued
online discussion regarding state-of-the-art tech-
nologies, directions, and important applications
for research on DEMO video retrieval.
Content-based video retrieval
We perceive a video program as a document.
Video indexing should be analogous to text doc-
ument indexing, where we perform a structural
analysis to decompose a document into para-
graphs, sentences, and words, before building
indices. When someone authors a book, they cre-
ate a table of contents for browsing the content’s
order DEMO a semantic index of keywords and
phrases for searching by content. Similarly, to
facilitate fast and accurate content access to video
data, we DEMO segment a video document into
shots and scenes to compose a table of contents,
and we should extract keyframes or key
sequences as DEMO entries for scenes or stories.
Therefore, the core research in content-based
DEMO retrieval is developing technologies to
automatically parse video, audio, and text to
identify meaningful composition structure and
to extract and represent content attributes DEMO any
video sources.
A typical scheme of video-content analysis
and indexing, DEMO proposed by many researchers,
involves four primary processes: fea-
ture extraction, structure analysis,
abstraction, and indexing. Each
process poses many challenging
research problems. In what follows,DEMO
we brieﬂy review these challenging
research issues and the algorithms
developed so far to address them.
Feature extraction for content
analysis
A critical process DEMO content-
based video indexing is feature extraction, which
we show in DEMO 1. The effectiveness of an
indexing scheme depends on the effectiveness of
attributes in content representation. However,
we can’t map easily extractable video DEMO
(such as color, texture, shape, structure, layout,
and DEMO) easily into semantic concepts (such
as indoor and outdoor, people, or car-racing
scenes). In the audio domain, features (such as
DEMO, energy, and bandwidth) can enable audio
segmentation and classiﬁcation.
Although DEMO content is a major source of
information in a video program, DEMO effective
strategy in video-content analysis is to use attrib-
utes extractable from multimedia sources. Much
valuable information is also carried in other
media components, such as text (superimposed
on the images, or included as closed DEMO),
audio, and speech that accompany the pictorial
component. A combined and cooperative analy-
sis of these components would be far more effec-
DEMO in characterizing video program for both
consumer and professional applications. The
Informedia system,1 AT&T’s Pictorial Transcripts
system,2–5 and Video Scout6 are DEMO of such
approaches.
Structure analysis
Video structure parsing is the next step in over-
all video-content analysis and is the process of
extracting temporal DEMO information of
video sequences or programs. This process lets us
organize video data according to their temporal
structures and relations and thus build table DEMO
contents. It involves detecting temporal bound-
aries and identifying meaningful segments of
video. Many effective and robust algorithms for
video parsing have been developed7–11 DEMO seg-
menting a video program into its temporal com-
position bricks. Ideally, these composition bricks
should be categorized in a hierarchy similar to ﬁlm
storyboards. The top level consists of sequences or
Feature
extraction
Video streams
DEMO
& browsing
Features
Metadata
Abstraction
Structure
analysis
Clustering
& indexing
stories, DEMO are composed of sets of scenes.
Scenes are further partitioned into shots. Each
shot contains a sequence of frames recorded con-
tiguously and representing DEMO continuous action in
time or space. With such structural information,
we can automatically build a video program’s
table of contents.12
An important step DEMO the process of video
structure parsing is that of segmenting the video
into individual scenes. From a narrative point of
view, a scene consists of a series of consecutive
shots grouped together because they’re shot in
DEMO same location or because they share some
thematic content. The process of detecting these
video scenes is analogous to paragraphing in text
document parsing, but it requires a higher level
of content analysis. There are two DEMO for
automatically recognizing program sequences:
one based on film production rules,13 the other
based on a priori program models.10 Both have
had DEMO success because scenes or stories in
video are only logical layers of representation
based on subjective semantics, and no universal
definition and rigid structure exists for scenes
and stories.
In contrast, shots are actual physical basic lay-
ers in video, whose boundaries are determined by
editing points or where the camera switches on
or off. Fortunately, analogous to words or sen-
tences in text documents, shots are a good choice
as the basic unit for video-content indexing, and
they provide the basis for constructing a video
table of contents. Shot boundary detection algo-
rithms that rely DEMO on visual information con-
tained in the video frames can segment the video
into frames with similar visual contents.
Grouping the shots into semantically DEMO
ful segments such as stories, however, usually
isn’t possible without incorporating information
from the video program’s other components.
Multimodal processing algorithms involving the
DEMO of not only the video frames, but also
Summary/
skimmed DEMO
Figure 1. Process
diagram for video-
content analysis and
indexing.
43
July–September 2002
the text, audio, and speech components that
accompany them have proven DEMO in
achieving this goal.12
Video abstraction
Video abstraction is the process of creating a
presentation of visual information about a land-
scape or the DEMO of video, which should be
much shorter than the original video. DEMO abstrac-
tion process is similar to extraction of keywords or
summaries in text document processing. That is,
we need to extract a subset DEMO video data from the
original video such as keyframes or highlights as
entries for shots, scenes, or stories. Abstraction is
especially important given DEMO vast amount of data
for a video program of even a few minutes’ dura-
tion. The result forms the basis not only for video
DEMO representation but also for content-based
video browsing. Combining the structure infor-
mation extracted from video parsing and
keyframes extracted in video abstraction, we can
build a visual table of contents of a video program.
Several terms DEMO corresponding methods
exist for abstracting video content, including
skimming, highlights, DEMO summary. A video
skim is a condensed representation of the video
containing keywords, frames, visual, and audio
sequences. Highlights normally involve detec-
tion of important events in the video. A summa-
ry means that we DEMO important structural
and semantic information in a short version of
the video represented via key audio, video,
frames, and/or segments.
Keyframes DEMO an important role in the video
abstraction process. Keyframes are still images,
extracted from original video data, that best rep-
resent the content of shots in an abstract man-
ner. Often we use keyframes to DEMO the
text of a video log.2 The representational power
of a set of keyframes depends on how they’re
chosen from all frames of a DEMO Not all
image frames within a sequence are equally
descriptive, and DEMO challenge is how to auto-
matically determine which frames are most rep-
resentative. An even more challenging task is to
detect a hierarchical set DEMO keyframes such that a
subset at a given level represents a certain granu-
larity of video content, which is critical for con-
tent-based video browsing. Researchers have
developed many effective algorithms,14 although
robust keyframe extraction DEMO a challeng-
ing research topic.
Automatically extracting video highlights is an
even more challenging research topic, because it
requires more high-level content analysis. Rui
et al.15 have developed a way to reduce the usual
three-hour baseball DEMO normally
includes long and uneventful close-ups—to 10
minutes of the most exciting highlights from
each inning. They use audio features such as
excited speech DEMO baseball hits. The algorithm
to determine the exciting parts weighs all the
probabilities for different features using a support
vector machine (SVM).
Li et al.16,17 have recently reported another
sports-related technology, where they propose
algorithms for automatically detecting all seg-
ments containing interesting events of a DEMO
ular game. Interesting events are specific to a
particular sport. The proposed event detection
algorithms use two types of prior knowledge to
extract semantics DEMO broadcast sports video:
domain and production knowledge. Domain
knowledge in sports means the deﬁnition of key
events that are important for a particular DEMO,
such as a play in American football, a hit in DEMO
ball, and a goal and its set up in soccer.
Production DEMO refers to techniques used
to produce the broadcast video. These techniques
help viewers follow the game in an entertaining,
informative, and captivating manner. They direct
viewers’ attention via well-accustomed produc-
tion patterns that viewers expect, such as scene
transitions after plays, replays, dynamic broad-
caster logos DEMO replay segments, certain
camera angles that are sport or event speciﬁc, and
scoreboard overlays.
The event detection algorithms in Li et al.16,17
DEMO these two types of knowledge in terms
of rule bases where rules are expressed in terms of
low-level visual and aural features that are DEMO
matically computed from the media. They auto-
matically detect all key events in baseball,
American football, and sumo wrestling broadcast
programs. Their algorithms16 detect every play in a
baseball broadcast video and every bout in DEMO sumo
match broadcast. One algorithm17 automatically
detects every play in an American football broad-
cast video. Plays include segments of the game
where the DEMO is put into play and actively played,
including pitches, hits, base steals, and home runs
in baseball, and running or passing DEMO and ﬁeld
goals in football. By detecting an event, we mean
DEMO the start and end points of video seg-
ments containing the event. These algorithms use
the methods Pan et al.18,19 discuss for automatical-
DEMO detecting replay segments.
Li et al.20 presented a prototype system demon-
44
IEEE MultiMedia
strating the results of these technologies (referred
to as High-Impact Sports). The prototype system
used an MPEG-7-compliant XML description for-
mat for the DEMO segments and an MPEG-7 brows-
er that provided novel user interface paradigms
offering summarized viewing or play-by-play non-
linear navigation. A play summary typically DEMO
vides three to six times compaction in the viewing
time for baseball and American football, depend-
ing on the particular game. The compaction ratio
for sumo wrestling can be as high as 20 times.
It should DEMO noted that the High-Impact
Sports technology, unlike the approach in Rui
DEMO al.,15 detects every single play without neces-
sarily attempting to prioritize the events. It there-
fore isn’t limited to generating short highlights.
It DEMO the needs of a sports production studio
or an avid sports fan that may want to see all the
plays (or prefers to be in control of selecting the
exciting plays). It facilitates efﬁcient digital DEMO
management in a production environment. For
sports fans, this technology provides DEMO oppor-
tunity to catch a missed game during its regular
broadcast and to consume even more sports.
A successful skimming approach involves
using information DEMO multiple sources, includ-
ing sound, speech, transcript, and video image
analysis. The Informedia project1 is a good exam-
ple of this approach, which automatically skims
documentary and news videos with textual tran-
scriptions by DEMO abstracting the text using clas-
sical text skimming techniques and then looking
for the corresponding parts in the video. This
method creates a skim DEMO, which represents a
short synopsis of the original. The goal was DEMO
integrate language and image understanding
techniques for video skimming by extracting sig-
nificant information, such as specific objects,
audio keywords, and relevant DEMO structure.
The resulting skim video is much shorter, where
compaction is DEMO high as 20 to 1, and yet retains
the original segment’s DEMO content. Another
example21 combines audio, video, speech, and
text to DEMO TV news programs. This approach
results in the segmentation of the program into
individual stories. The system selects a few repre-
sentative images and DEMO to represent each
story’s contents. The textual information pro-
vided by closed captions or derived from the
audio track using speech recognition plays an
DEMO role in this process. When transcrip-
tions of the program are generated by automatic
speech recognition (ASR), satisfactory results may
not be achievable using a keyword-driven
approach to videos, where soundtrack contains
more than just speech, such as movies. Sundaram
and Chang22 proposed a solution to the skim-
ming problem based on two important ques-
tions:
What’s the DEMO between the visual
complexity of a shot and its comprehension
time?
❚
How does syntactical structure in the video
affect its comprehension?
DEMO
They introduced a framework for determining
visual skims and formulated the problem of skim
generation as a general utility maximization
problem with constraints. This DEMO an important
step because it allows for a principled way to
impose additional constraints and make trade-
offs between them.
Summarization is another challenging DEMO
because of the need to explore the content’s
structure. Liu and Kender23 explore the type of
scene changes to signal transitions between
semantic units DEMO the domain of documentaries.
Their approach was to look for evidence for shot
composition rules by means of Hidden Markov
Models. They found that DEMO best approach is one
that trains the HMM with labeled subsequences
that have approximately equal elapsed time,
rather than subsequences with an equal DEMO
of shots, or subsequences with shots aligned to
some semantic event. DEMO et al.24 proposed
summarization of video programs using the tran-
script. Their process involves cue extraction, cat-
egorization, classification, and a summarizer.
Given a paragraph, the categorization process
finds the underlying topic. Each of the 20 cate-
gories is an aggregation of a set of keywords relat-
DEMO to that particular class. The summarizer
exploits the underlying temporal structure and
domain knowledge as well as textual cues in the
transcript.
Indexing for DEMO and browsing
The structural and content attributes extracted
in feature extraction, DEMO parsing, and abstrac-
tion processes, or the attributes that are entered
manually, are often referred to as metadata. Based
on these attributes, DEMO can build video indices and
the table of contents through, for DEMO, a clus-
tering process that classiﬁes sequences or shots
into different DEMO categories or an indexing
structure. As in many other database systems, DEMO
45
July–September 2002
MPEG-7
Having realized the importance of content management, the Moving
Pictures Expert Group (MPEG) started the standardization activity on
content description. Formally called DEMO Multimedia Content Description
Interface, MPEG-7 provides a standardized description of various DEMO
of multimedia information, as well as descriptions of user preferences
and DEMO history pertaining to multimedia information. The normative
part of the standard focuses on a framework for encoding the descrip-
tors and description schemes. The DEMO doesn’t comprise the extrac-
tion of descriptors (features) or specify search engines that will use the
descriptions. Instead, the standard enables the exchange of content
between different content providers along the media value chain. In
DEMO, it enables the development of applications that will use the
MPEG-7 DEMO without specific ties to a single content provider.
More information about MPEG-7 is available at the MPEG homepage
(http://mpeg.telecomitalialab.com/).
MPEG-7 DEMO an international standard in December 2001. This will
have an impact on availability of additional information along with the
images and video segments for DEMO applications. This fact will help focus
the needs for research on content analysis topics, which aren’t available in
the MPEG-7 description schemes.
need schemes and tools to use the indices and
content metadata to query, search, and browse
large video databases. Researchers have developed
numerous schemes and tools for video indexing
and query. However, robust and effective tools
tested by thorough experimental evaluation with
large data sets are still lacking. Therefore, in the
majority of cases, retrieving or searching video
databases by keywords or phrases will be the mode
of operation. In some cases, we can retrieve with
reasonable performance by content similarity
deﬁned by low-level visual features DEMO, for
instance, keyframes and example-based queries.
Often in queries of video clips, we want to
quantify queries based on particular attributes that
involve objects and subregions within the viewable
image. Some support for automated object DEMO
tion could help us with these queries. The object-
oriented compression scheme standardized by
MPEG-4 provides an ideal data representation for
supporting such indexing DEMO retrieval schemes. It
will also simplify the task of video structure pars-
ing and keyframe extraction, because many of the
necessary content features (DEMO as object motion)
are readily available. Zhang et al.25 proposed a
framework to use such content information in
video content representation, abstraction, DEMO
ing, and browsing. Similarly, Chang et. al.26 have
applied object-based representation in indexing
and retrieving video clips.
46
Although we tend to think DEMO indexing for
supporting fast retrieval of video clips, browsing
is equally DEMO for video data, because the
volume of video data requires techniques DEMO pre-
sent information landscape or structure to give a
quick overview. By browsing, we mean a casual
and quick access of content. The visual table of
contents built based on structure information
and keyframes provides an DEMO representation
for content-based video browsing. Browsing tools
built based on such representation are especially
useful, given that it still isn’t feasible to auto-
matically build semantic content-based indexing
of video programs. However, browsing shouldn’t
be viewed as only a compromised tool for video
indexing. Rather, it’s an effective alternative and
a complementary step for searching video data.
Often users want DEMO access to relevant video
data, although the process may initially lack DEMO
specific goal or focus. Browsing may suitably
address those needs. Furthermore, DEMO is
also intimately related to and essential for video
retrieval. It can help formulate queries, making it
easier for the user to just ask around in the
process of figuring out the most appropriate
query to DEMO Applications of such browsing
tools include video editing and composition,
where we often browse through a large number
of relevant video clips before DEMO the
ﬁnal cut list.
We can also use MPEG-7 for multimedia
indexing. We discuss this further in the sidebar
“MPEG-7.”
Application models: The user’s
perspective and research methodologies
We’re facing a barrier similar to the one DEMO
the AI research community faced for many years:
machine understanding of visual content. This is
one of the major reasons that we haven’t DEMO
many convincing stories about successful appli-
cations of our research results, DEMO in the
marketplace. In reviewing the past success in
developing algorithms for video structure pars-
ing, abstraction and content analysis, and in
examining DEMO research strategies and method-
ologies, we need to address several issues.
DEMO, although a technology’s success will be
ultimately judged by its usefulness DEMO the target-
ed applications, we should distinguish between
the long-term research DEMO and short-term
applications. Long-term research will result in
general solutions to many applications. On the
other hand, short-term applications will educate
IEEE MultiMedia
the users for the potential of this technology
while providing focus for DEMO hard technical
problems for the research community. The ulti-
mate goal for the long-term research is to provide
a link between the extracted low-level DEMO
and the high-level semantic description that
humans perceive without significant effort.
Nevertheless, many working systems exist that
serve as proof to the effectiveness of even partial
and domain-speciﬁc content descriptors for selec-
tive retrieval of visual DEMO based on low-
level feature extraction.
In designing these applications and products
in the area of multimedia content analysis, we
must keep the user in mind at all times, because
users at different levels view a technology’s use-
fulness differently. We can broadly classify users
into two extremes:DEMO
❚ nontechnical consumers and
trained, technical, professional corporate users
who regularly use the products.
❚
The requirements for each of these classes are DEMO
ferent, so we should use different technologies to
address their needs. DEMO and consumer
applications of video indexing technologies can
both absorb functionalities, DEMO stem from sin-
gle modality processing, and extract even low-
level DEMO
For technology-savvy users working with
content analysis, indexing, searching, and
DEMO tools on a daily basis, it makes sense
to design systems DEMO require more user sophis-
tication. For example, major news agencies and
DEMO broadcasters own large video archives. If we
develop automated indexing, analysis, and
search products for these applications, it’s con-
ceivable to have trained individuals to retrieve
and access required multimedia information,
much the same DEMO as today’s trained profes-
sional librarians and information specialists
retrieve information based on textual data.
Under these circumstances, we can expect the
operator or user to search images via textures,
color histograms, or other low-level feature
analysis that we wouldn’t expect a typical con-
sumer to be DEMO or willing to cope with.
At the other extreme, for consumers, the
products and applications need to be extremely
simple for them to DEMO viable in the marketplace.
As an example, increasing consumer access to
DEMO imaging devices such as still digital
cameras and digital camcorders has resulted in
an explosion in the volume of data being gen-
erated. For DEMO to annotate, index, han-
dle, process, and access their data, products
must be designed with simple yet useful func-
tionalities. This would preclude searching tech-
niques that are, for example, based on color
DEMO Instead, consumers might want to
find all the pictures in which DEMO Joe is with
the baby by defining once and for all, DEMO uncle
Joe and baby are pictorially. Clearly, from a
technical point DEMO view, this is harder to solve
than searching color histograms. At DEMO
moment, no technically robust solutions exist
for this problem. Generally speaking, the low-
level features that most indexing and query sys-
tems are DEMO on might prove to be nonviable
in the consumer market in their raw form. We
should conceal algorithms for low-level feature
extraction from the DEMO For example, a video
segmentation and filtering algorithm can only
give DEMO final visual table of contents of home
video with simple interaction for quick
overview and access. The intermediate step—
setting thresholds—is prohibitively beyond the
DEMO horizon.
Professional and educational
applications
Professional activities that involve generating
or using large volumes of video and multimedia
data are prime candidates for taking DEMO of
video-content analysis techniques. Here we dis-
cuss several such applications.
Automated authoring of Web content
Media organizations and TV broadcasting
companies have shown DEMO interest in
presenting their information on the Web. A sur-
vey conducted in 1998 by the Pew Research
Center for the People and the DEMO indicated that
the number of Americans who obtained their
news on the Internet was growing at an aston-
ishing rate. This survey indicated that DEMO million
people got their news online at least once a week.
This number had more than tripled in a two-year
period. (The full survey results are available at
http://people-press.org/reports/).
The process of generating Web-accessible con-
tent usually involves using one of several exist-
ing DEMO tools to manually compose
documents consisting of text, images, and possi-
bly audio and video clips. This process usually
consumes considerable amounts of DEMO When
47
July–September 2002
Figure 2. A snapshot
from the Pictorial
Transcripts system.
48
Figure 3. DEMO snapshot from the DVL system at AT&T.
other representations of the same content are
already composed for presentation in the video
form, we can use such presentations to repurpose
the content for the Web, thereby reducing work.
Analysis of the already composed video content
through image and DEMO understanding, speech
transcription, and linguistic processing can serve
to create alternative presentations of the infor-
mation suitable for the Web. We can automati-
DEMO convert large video archives to digital
libraries. We can also automatically augment
these Web presentations with related and sup-
plementary information and thereby create DEMO
richer source of information than the original
video programs. An example of such an auto-
mated authoring system is the Pictorial
Transcripts system.2–5
Pictorial DEMO uses video and text analy-
sis techniques to convert closed-captioned video
programs to Hypertext Markup Language
(HTML) presentations with still frames contain-
ing DEMO visual information accompanied by text
derived from the closed captions. A content-
based sampling method14 performs the task of
reducing the video frames into DEMO small set of
images that represent the visual contents of each
scene in a compact way. This sampling process is
based on detecting cuts DEMO gradual transitions,
as well as a quantitative analysis of the camera
operations. Linguistic analysis of closed-caption
text refines the text, generates textual indices,
and creates links to supplementary information.
Figure 2 shows a sample DEMO for the Pictorial
Transcripts that uses the output of the content-
based sampling and the processed text from the
closed captions. We can easily DEMO such a
compact presentation over low-bandwidth com-
munications networks. When more bandwidth
is available, the presentation can include audio
and video information. In this case, the still
images and text serve as a pictorial and textual
index into the audio and video media compo-
nents. Users can search DEMO browse a digital
video library (DVL) created in this way using pic-
torial information as well as textual information
extracted from closed captions, or recognized
speech, to retrieve selective pieces of video from
a large archive (see Figure 3).
The AT&T DVL system employs additional
media processing techniques to improve the
organization and presentation of the video DEMO
mation. The system can use speech processing to
correct misalignment between the audio track
and the closed-caption text. When closed-
caption text isn’t available, it can employ a large
vocabulary automatic speech recognizer (LVASR)
to generate a transcript of the program from the
audio track (see Figure 4). The quality of the auto-
matically generated transcripts is determined DEMO
several factors, such as the quality of speech,
background noise, vocabulary size, and language
models. Although we can obtain high-quality
results under favorable conditions, the accuracy
of such automatically generated transcripts is
generally below those generated manually and
therefore isn’t suitable for direct presentation to
the DEMO Nevertheless, these automatically gen-
erated transcripts provide a viable alternative to
DEMO closed-caption text for information retrieval
purposes. When sufficient bandwidth is avail-
able, it can deliver video presentations (such as
IEEE MultiMedia
Image courtesy DEMO NBC news
Image courtesy of NBC news
TV programs) with the same quality as the origi-
nal productions. The real-time transport proto-
col (RTP) and the speciﬁc payload types deﬁned
DEMO the Internet Engineering Task Force (IETF)
have already made it DEMO to deliver high-
quality MPEG-2 encoded video over IP networks.
In the short term, this will only be feasible over
private local IP networks. In the long term, how-
ever, this will let us create DEMO and brows-
able TV.6,12
Searching and browsing large video archives
Another professional application of automat-
ed media content analysis is in organizing and
DEMO large volumes of video data to facilitate
efficient and effective use of these resources for
internal use. Major news agencies and TV broad-
casters DEMO large archives of video that have
been accumulated over many years. Besides the
producers, others outside the organization use
the footage from these archives to meet various
needs. These large archives usually exist on
numerous different DEMO media, ranging from
black-and-white ﬁlm to magnetic-tape formats.
Traditionally, the indexing information used
to organize these large archives has been limited
to titles, dates, and human-generated synopses.
We generally use this information to select video
programs possibly relevant to the application at
hand. We ultimately discern the DEMO rele-
vance by viewing the candidate programs linear-
ly or nonlinearly. Converting these large archives
into digital form is a ﬁrst step in facilitating DEMO
search process. This in itself is a major improve-
ment over the old methods. We must address
several practical issues, however, to make DEMO an
undertaking feasible and economical. These large
video libraries create a unique opportunity for
using intelligent media analysis techniques to
create advanced searching and DEMO tech-
niques to ﬁnd relevant information quickly and
inexpensively. Intelligent video segmentation
and sampling techniques can reduce the visual
contents of the video program DEMO a small number
of static images. We can browse these images to
spot information and use image similarity search-
es to ﬁnd shots with DEMO content and motion
analysis to categorize the video segments.
Higher-level analysis can extract information rel-
evant to the presence of humans or objects in DEMO
video. Audio event detection and speech detec-
tion can extract additional information to help
the user ﬁnd segments of interest.
Despite the limitations in DEMO robust and efﬁ-
Figure 4. Search based on automatic speech recognition.
cient extraction of information from the con-
stituent media streams, existing methods are
effective in reducing manual labor.27
Easy access to educational material
The availability DEMO large multimedia libraries
that we can efﬁciently search has a strong impact
on education. Students and educators can expand
their access to educational material. DEMO
Telecommunications Act of 1996 has acknowl-
edged the signiﬁcance of this. It has special provi-
sions for providing Internet access to schools and
public DEMO This holds the promise of turning
small libraries that contain a small number of
books and multimedia sources into ones with
immediate access to DEMO book, audio program,
video program, and other multimedia education-
al material. It also gives students access to large
data resources without even DEMO the class.
Indexing and archiving multimedia
presentations
Intelligently indexing multimedia presenta-
tions is another area where content-based analy-
sis can play a major role. DEMO video
compression and transmission standards have
made it possible to transmit presentations to
remote sites. We can then store these presenta-
tions for on-demand DEMO Different media
components of the presentation can be processed
to characterize and index it. Such processing
could include analyzing the speaker’s gestures,
slide DEMO detection, extracting textual
49
July–September 2002
Image courtesy of C-SPAN
Figure 5. Sample screen of the system for indexing and archiving
multimedia DEMO
information by performing optical character
recognition (OCR) on the slides, DEMO recogni-
tion, speaker identification and discrimination,
and audio event detection. DEMO information
extracted by this processing generates powerful
indexing capabilities that would enable content-
based retrieval of different segments of a presen-
tation. Users can DEMO an archive of
presentations to ﬁnd information about a topic.
Figure 5 shows a sample screen from a techni-
cal presentation with the speaker DEMO the left win-
dow. The slides synchronize with the talk. This is
done using a specialized scene change detection
algorithm to ﬁnd the slide DEMO An ofﬂine-
generated transcript of the talk synchronizes with
the video using speech processing and searches
and jumps to the correct points in the DEMO (as the
search results window shows at the bottom of
Figure DEMO).
Indexing and archiving multimedia
collaborative sessions
Multimedia collaborative systems can also
beneﬁt from effective multimedia understanding
and indexing techniques. Communication net-
works give DEMO the ability to work together
despite geographic distances. The multimedia
collaborative sessions involve real-time exchange
of visual, textual, and auditory information. The
information DEMO is often limited to the col-
laboration’s end result and doesn’t include the
steps that were taken or discussions that took
place. We can DEMO up archiving systems to store
all the information together with relevant syn-
chronization information. Content-based analy-
sis and indexing of these archives based on
DEMO information streams enable the retrieval
of segments of the collaborative process. Such a
process lets users not only access the end result
but also DEMO process that led to those results.
When the communication links used for the col-
laborative session are established by a conferenc-
ing bridge, we can use the available data in the
indexing process, thereby reducing the process-
ing required to identify each stream’s source.
Consumer domain applications
Video-content DEMO research is geared
toward large video archives. However, the widest
audience DEMO video-content analysis is consumers.
We all have video content pouring through
broadcast TV and cable. Also, as consumers, we
own unlabeled home video DEMO recorded tapes.
To capture the consumer’s perspective, the man-
agement of DEMO information in the home enter-
tainment area will require sophisticated yet
feasible techniques for analyzing, filtering, and
browsing video by content.
The methods DEMO with video information
in the consumer domain will have different
requirements. In large archives, we store data in
ﬁles, and we can access DEMO repeatedly and slower
than in real time. Therefore, the algorithms for
DEMO extraction can operate at rates slower
than 30 fps. In consumer devices, however, the
content may be available only during real-time
display (recording or playback). Consequently,
we can analyze video data only in DEMO time. In
large archives, we assume that the workstation
for video DEMO has considerable power
(and possibly hardware support) to run video-
content analysis algorithms. In the consumer
domain, recording and display devices are
impoverished from the point of view of infor-
mation processing—devices normally have lim-
DEMO memory and processor power. Therefore, the
algorithms must run with all DEMO constraints in
real time. In addition, these algorithms have dif-
ferent DEMO of accessory information available.
Large video archives will probably have the infor-
mation about the author, actors, and storyboard.
However, in the consumer domain, we can prob-
ably expect metadata to be available in the
broadcast stream or in the electronic program
guide. Researchers are developing standards DEMO
50
IEEE MultiMedia
this area, such as digital video brocasting service
information (DVB-SI) and MPEG-7, which will
add descriptions and accessory data to the stored
and streamed content.
Consumer devices can use many additional
features for video cataloging, advanced video con-
trol, personalization, proﬁling, and time-saving
functions. Information ﬁltering functions for con-
verging PCs and TVs will add value to video DEMO
cations beyond digital capture, playback, and
interconnect. We can use these consumer appli-
cations in video editing, cataloging applications,
enhanced access, DEMO ﬁltering applications.
Video overview and access
An example of a video home library applica-
tion that performs VHS tape cataloging functions
is Video Indexing DEMO True Access and
Multimedia Information Navigation (Vitamin).28
In this system, the video-content analysis process
extracts visual information, which it then
archives and presents to the user as a visual table
of contents for the DEMO video. The purpose
is to later use this information for retrieval pur-
poses in a master index. This prototype has an
archival and a DEMO module to perform these
two functions.
During archiving, the system performs DEMO
scene change and static scene detection based on
a comparison of discrete cosine transform (DCT)
coefficients of subsequent frames (MPEG-1 and
MPEG-2)DEMO However, from the user’s perspective,
not all the keyframes are DEMO or necessary
to convey the video’s visual contents. We apply
a keyframe ﬁltering method to reduce the num-
ber of keyframes. We reduce the DEMO of
frames by filtering out noisy, blurry, unicolor,
and repetitive frames. For example, in a dialogue
scene, it’s likely that both DEMO will be shown
several times, requiring two frames to represent
the DEMO scene.
During the keyframe selection process, we use
frame signatures to DEMO keyframes and to
detect a particular frame’s content. A keyframe
signature representation is derived for each
grouping of similarly valued DCT blocks in a
DEMO By using different thresholds, we can con-
trol the number of DEMO keyframes. The signa-
tures are also used for spotting certain patterns,
which might correspond to objects of interest.
Selected keyframes are structured in DEMO temporal
hierarchy, which is flattened to aid in optimal
retrieval, even from slow storage devices. The sys-
tem presents this hierarchy to the DEMO in a visu-
al table of contents (see Figure 6.) The user can
browse and navigate through the visual index
and fast-forward or DEMO to a certain point on
the videotape or MPEG ﬁle.
Video content ﬁltering
In the consumer domain, some products
already perform video-content analysis and fil-
tering functions—for example, VCRs with the
automatic commercial skip feature. Most systems
work by detecting black frames and changes in
activity.
When people DEMO a TV program, such as
Seinfeld, they immediately recognize the non-
program segments in the broadcast. This is
because they recognize the broadcast’s DEMO in
context. Some characteristics of commercials are
therefore naturally different than the TV pro-
gram. Such special characteristics include rapid
scene changes, repetitive nature, use of text with
different sizes, transitional monochrome frames
into and DEMO of the commercial break, and
absence of the station logo. Based DEMO these char-
acteristics, suitable methods for advertisement
isolation are cut rate, average keyframe distance,
black frame, static frame rate, similar frame DEMO
tance, text location detection, logo detection,
audio analysis, and DEMO (autolearning)
advertisements. The commercial breaks are usu-
ally preceded and DEMO by a series of black
frames.
We can often detect commercials by deter-
mining when a high number of cuts per minute
Figure 6. DEMO snapshot
from the visual table of
contents user interface
in a home library
application.
51
July–September 2002
Time
Figure 7. Keyframes
from an area with a
high density of DEMO
representing a
commercial break.
52
occur in conjunction with the identification of
the black-frame series. Specifically, by using the
relative times between cuts, DEMO can determine
the cut density. However, action movies may
also have DEMO scenes with a large number
of cuts per minute. For a more reliable commer-
cial isolation, we can analyze a total distribution
of the cuts and black frames in the source video
in an attempt to DEMO their frequency and
length.
We analyze the keyframes for color uniformi-
ty and similarity to previously selected key-
frames. We use the DCT coefficients DEMO create a
frame signature for the keyframe clustering
process. During this process, we use the frame
signatures to compare keyframes and detect a
particular frame’s content. In addition, we ana-
lyze the video cut rate and determine the dura-
tion of the cut-rate change. We also identify the
DEMO frame boundaries and analyze the total dis-
tribution of the cut-rate change in the program
to deduce the frequency and length of the cuts.
DEMO can help us determe the likelihood of com-
mercial segments.
Figure 7 shows the keyframes extracted from
a commercial break. The vertical lines at DEMO bot-
tom represent the cuts, and a high density of
cuts-per-unit DEMO (cut rate) may represent a
commercial break, as Figure 7 DEMO The cut
rate alone produces many false positives. To
reduce the number of false positives, we can
examine the false-positive sections of the com-
mercials for the presence or absence of text. All
in this example DEMO a signiﬁcant amount of text.
The text varies significantly in the position on
the TV screen as well as size. The type of text DEMO
sent in some of these areas was scene text (for
example, text on cars, helicopters, or police vehi-
cles), buildings with DEMO, or a product logo.
For consumer applications, we must detect
commercials on a constrained platform.29 We’ve
also developed methods that use features pro-
DEMO during MPEG compression. We developed
an algorithm that uses features as triggers and
verifiers to detect commercials. Our algorithm
first looks for a triggering DEMO, such as a black
frame, to mark a potential commercial start.
Once a potential start is found, it uses other char-
acteristic features to verify the commercial break.
We’ve achieved a recall of 93 percent DEMO a pre-
cision of 95 percent when station logos and trail-
ers are excluded and a precision of 99 percent
when station logos and DEMO are regarded as a
part of the commercial break.
Enhanced access to broadcast video
Intelligent access and enhanced search tools
for broadcast content is DEMO important area where
content-based analysis can have a strong contri-
bution. The existing high-definition TV, video
compression, and transmission standards have
made it DEMO to transmit a large number of
high-quality TV channels to the consumer over
the broadcast networks and the Internet.
However, the only available tools to grapple with
the growing number of TV channels are the
scrolling DEMO guide (at least in the US) and
the old-fashioned paper guide. The program
guide is envisioned under the analog paradigm
of passively receiving DEMO entertainment. The
scrolling isn’t interactive and is valid for only a
limited time. Within the Society of Motion,
Picture, and Television Engineers (DEMO),
TVAnytime, MPEG, and DVB, there are ongoing
efforts to provide auxiliary information to the
regular broadcast stream. Current personal video
recorders DEMO the market use an electronic pro-
gram guide for an interactive selection of pro-
grams to watch or store. There could be layers of
DEMO personalization of content where new
video-content analysis algorithms could be
employed. The combined information extracted
by this video-content processing will generate
powerful indexing capabilities DEMO would enable
content-based retrieval of different segments of
TV programs online or ofﬂine.
We developed Video Scout, a content-based
retrieval system for personalizing TV at a sub-
program level.6 Users make content requests in
their user DEMO and then Scout begins record-
ing TV programs. In addition, Scout DEMO
watches the TV programs it records and person-
alizes program segments. Scout analyzes the visu-
al, audio, and transcript data to segment and
DEMO the programs. When viewing full pro-
grams, users see a high-level DEMO as well as
topic-speciﬁc starting points. (For example, users
IEEE MultiMedia
can quickly ﬁnd and play Dolly Parton’s musical
performance within an episode DEMO Late Night with
David Letterman.) In addition, users can access
video segments organized by topic (such as ﬁnd-
ing all the segments on Philips Electronics that
Scout has recorded from various financial news
programs). DEMO divide Scout’s interface into two
sections—program guide and TV magnets. The
program guide lets users interact with whole TV
programs that they can segment DEMO different
ways. TV magnets (see Figure 8) let users access
their proﬁles and video clips organized by topic.
Users navigate the interface on DEMO TV screen using
a remote control.
The archiving module employs a three-layered,
multimodal integration framework to segment,
analyze, characterize, and classify DEMO The
multimodal segmentation and indexing incorpo-
rates a Bayesian framework that integrates infor-
mation from the audio, visual, and transcript
(closed-caption) domains. DEMO framework uses
three layers to process low-, mid-, and high-level
multimedia information. The retrieval module
relies on users’ personal preferences to deliver
both DEMO programs and video segments. In addi-
tion to using electronic program guide metadata
and a user proﬁle, Scout lets users request speciﬁc
topics within a program. For example, users can
request the video clip of the US President speak-
ing from a half-hour news program. The high-
level DEMO generates semantic information about
TV program topics used during retrieval.
Advanced access to broadcast video must take
into account user preferences captured in a DEMO
profile. Ferman et al.30 proposed automatic user
proﬁling and ﬁltering agents. The proﬁling agent,
based on fuzzy reasoning, automatically gener-
ates the users’ proﬁle on the basis of their usage
history. Given program description metadata,DEMO
the ﬁltering agent ﬁlters programs on the basis of
the user’s profile. The agents developed by
Ferman et al.30 can generate MPEG-7 and TV-
DEMO compliant usage history and user pref-
erence descriptions as well as ﬁltering programs
that are described by MPEG-7 and TV-Anytime
compliant descriptions.
Conclusions
To DEMO things in perspective, it’s important
to distinguish between research activities, exper-
iments, and real applications that have made, or
are likely to DEMO, the transition from research
labs into the real world. Researchers and DEMO
nologists, who are constantly reminded of the
level of difﬁculty in DEMO certain technologi-
cal challenges, are more likely to be excited by
DEMO technologies that may not yet be ready for
use. The targeted users are the ultimate judges of
the technology’s usefulness in meeting their
needs. DEMO the other hand, accepting new ways of
doing things often involves DEMO change in mindset.
Users accustomed to performing a certain task by
using existing tools and methods might have a
tendency to resist new tools DEMO methods. For
example, most people who are accustomed to
text-based information DEMO techniques may
not feel as comfortable with the notion of per-
forming image and video searches by nonlin-
guistic queries. Well-designed prototype
applications can DEMO bring about the necessary
change in mindset for users to accept these appli-
cations. Such prototypes also serve the purpose
of making technologists aware DEMO users’ require-
ments and preferences. MM
References
1. M.G. Brown et al., “Automatic Content-Based
Retrieval of Broadcast News,” Proc. 3rd Int’l Conf.
Multimedia (ACM Multimedia 95), ACM Press, New
York, 1995, pp. DEMO
2. B. Shahraray and D.C. Gibbon, “Automatic
Generation of Pictorial Transcripts,DEMO Proc. SPIE Conf.
Multimedia Computing and Networking 1995, SPIE
Press, Bellingham, Wash., 1995, pp. 512-518.
3. B. Shahraray and D.C. Gibbon, DEMO
Authoring of Hypermedia Documents of Video
Programs,” Proc. 3rd Int’l Conf. Multimedia (ACM
Figure 8. Content
magnets attracting
story segments in the
content-based personal
video recorder Video
Scout application.
53
July–September 2002
Multimedia 95), ACM Press, New York, 1995, pp.
401-409.
4. B. Shahraray and D.C. Gibbon, “Efﬁcient Archiving
and Content-Based Retrieval of Video Information
on the Web,” Proc. AAAI Symp. Intelligent Integration
and Use DEMO Text, Image, Video, and Audio Corpora,
1997, pp. 133-136.
5. B. Shahraray, “Multimedia Information Retrieval
using Pictorial Transcripts,” Handbook of Multimedia
Computing, B. Furht, ed., CRC Press, Boca Raton,
DEMO, 1999, pp. 345-359.
6. R.S. Jasinschi et al., “Integrated Multimedia
DEMO for Topic Segmentation and
Classiﬁcation,” Proc. IEEE Int’l Conf. Image Processing
(ICIP 2001), IEEE CS Press, Los Alamitos, Calif., 2001.
DEMO H.J. Zhang et al., “Video Parsing, Retrieval, and
Browsing: An Integrated and Content-Based
Solution,” Proc. Third Int’l Conf. Multimedia (ACM
Multimedia 95), ACM Press, New York, 1995,
pp.15-24.
8. R. DEMO, K. Mai, and J. Miller, “A Robust Method
for Detecting DEMO and Dissolves in Video
Sequences,” Proc. 3rd Int’l Conf. Multimedia (DEMO
Multimedia 95), ACM Press, New York, 1995.
9. H.J. Zhang et al., “Video Parsing Using Compressed
Data,” Proc. SPIE 94 Image and Video Processing II,
SPIE Press, Bellingham, Wash., 1994, DEMO
10. D. Swanberg, C.-F. Shu, and R. Jain, “Knowledge-
Guided DEMO in Video Databases,” Proc. SPIE
Conf. Storage and Retrieval for Image and Video
Databases, SPIE Press, Bellingham, Wash., 1993.
11. H.J. DEMO et al., “Automatic Parsing and Indexing
of News Video,” Multimedia DEMO, vol. 2, no. 6,
1995, pp. 256-265.
12. Q. DEMO et al., “Automated Generation of News
Content Hierarchy by Integrating Audio, Video, and
Text Information,” Proc. IEEE Int’l Conf. Acoustics,
Speech, and Signal Processing (ICASSP 99), IEEE CS
Press, Los Alamitos, Calif., 1999.
13. P. Aigrain and P. Joly, “The Automatic Real-Time
Analysis of Film Editing and Transition Effects and
Its Applications,” Computers & Graphics, vol. 18, no.
1, Jan./Feb. 1994, pp. 93-103.
14. B. Shahraray, “Scene Change Detection and
Content-Based Sampling of Video Sequences,”
IS&T/SPIE Symp. Digital Video Compression:
Algorithm and Technologies, SPIE Press, Bellingham,
Wash., vol. 2419, 1995, pp. 2-13.
DEMO Y. Rui, A. Gupta, and A. Acero, “Automatically
Extracting Highlights DEMO TV Baseball Programs,”
Proc. 8th Int’l Conf. Multimedia (ACM Multimedia
DEMO), ACM Press, New York, 2000, pp. 105-115.
16. B. DEMO and M.I. Sezan, “Event Detection and
Summarization in American Football Broadcast
DEMO,” Proc. IS&T/SPIE Conf. Storage and Retrieval for
Media Databases, SPIE Press, Bellingham, Wash.,
2002, pp. 202-215.
17. B. DEMO and M.I. Sezan, “Event Detection and
Summarization in Sports Video,” DEMO IEEE Workshop
on Content-Based Access to Video and Image Libraries,
IEEE CS Press, Los Alamitos, Calif., 2001, CD-ROM.
18. H. Pan, B. Li, and M.I. Sezan, “Automatic Detection
of Replay Segments in DEMO Sports Programs
by Detection of Logos in Scene Transitions,” Proc.
IEEE Int’l Conf. Acoustics, Speech, and Signal
Processing (ICASSP 2002), IEEE CS Press, Los
Alamitos, Calif., 2002, CD-ROM.
19. H. Pan, P. van Beek, and M.I. Sezan, “Detection of
Slow-Motion Replay Segments DEMO Sports Video for
Highlights Generation,” Proc. IEEE Int’l Conf. Acoustics,
Speech, and Signal Processing (ICASSP 2001), IEEE CS
Press, Los Alamitos, Calif., 2001, CD-ROM.
20. B. Li et al., “Sports DEMO Summarization,” IEEE
Computer Vision and Pattern Recognition (CVPR)
Conf. DEMO Session, IEEE CS Press, Los
Alamitos, Calif., 2001, CD-ROM.
DEMO D.C. Gibbon and J. Segen, “Video Based Detection
of People from DEMO,” Proc. Virtual Reality
Systems Conf., 1993.
22. H. Sundaram and DEMO Chang, “Constrained Utility
Maximization for Generating Visual Skims,” Proc.
IEEE DEMO on Content-Based Access of Image and
Video Libraries (CBAIVL-2001), IEEE DEMO Press, Los
Alamitos, Calif., 2001.
23. T. Liu and J.R. DEMO, “A Hidden Markov Model
Approach to the Structure of Documentaries,” DEMO
Computer Vision and Pattern Recognition: Workshop
on Content-Based Access of Image DEMO Video Libraries
(CVPR 00), IEEE CS Press, Los Alamitos, DEMO, 2000.
24. L. Agnihotri et al., “Summarization of Video
Programs Based on Closed Captioning,” Proc. SPIE
Conf. Storage and Retrieval in Media DEMO, SPIE
Press, Bellingham, Wash., 2001, pp. 599-607.
25. H.J. DEMO, J.Y.A. Wang, and Y. Altunbasak,
“Content-Based Video Retrieval and Compression:
A Uniﬁed Solution,” Proc. IEEE Int’l Conf. Image
Processing, IEEE CS Press, Los Alamitos, Calif., 1997.
26. S.F. Chang et al., “VideoQ: An Automated Content-
Based Video Search System Using Visual Cues,DEMO
Proc. 5th Int’l Conf. Multimedia (ACM Multimedia
97), ACM Press, New York, 1997, pp. 313-324.
27. D. Gibbon et al., “Browsing and Retrieval of Full
Broadcast-Quality Video,” Proc. Packet Video Conf.,
DEMO, CD-ROM.
28. N. Dimitrova, T.McGee, and H. Elenbaas, “Video
Keyframe Extraction and Filtering: A Keyframe Is
Not a Keyframe to Everyone,” Proc. ACM Conf.
Knowledge and Information Management, ACM
54
IEEE MultiMedia
Press, New York, 1997, pp.113-120.
29. N. Dimitrova et al., DEMO Commercial
Detection Using MPEG Features,” to appear in Proc.
9th Int’l Conf. Information Processing and
Management of Uncertainty in Knowledge-Based
Systems (IPMU 2002), July 2002.
30. A.M. Ferman et al., “Content-Based Filtering and
Personalization Using Structured Metadata,” to
appear in Joint Conf. Digital Libraries, 2002.
Nevenka Dimitrova is a research
staff member at Philips Research.
Her main DEMO interests are in
content information manage-
ment, digital TV, content synthe-
sis, video content navigation and
retrieval, MPEG-7, and advanced multimedia systems.
She has a BS in mathematics and computer science from
the University DEMO Kiril and Metodij, Skopje, Macedonia,
and an MS and PhD in computer science from Arizona
State University. She is on the editorial DEMO of IEEE Mul-
tiMedia, ACM Multimedia Systems Journal, ACM Transac-
tions of Information Systems, and IEEE Communications
Society (an online magazine). DEMO is a program chair of
ACM Multimedia 2002 in the area of content processing.
Hong-Jiang Zhang is a senior
researcher and assistant managing
director DEMO Microsoft Research
Asia. He has a BS from Zhengzhou
University, China, and PhD from
the Technical University of Den-
mark, both in electrical engineering. He is a senior IEEE
member and an ACM member. He DEMO serves on
the editorial boards of ﬁve professional journals and a
dozen committees of international conferences.
Behzad Shahraray is a division
manager at AT&DEMO Labs Research,
where he heads the Multimedia
Processing Research Department.
His work focuses on multimedia
indexing, multimedia data min-
ing, content-based video DEMO, and automated
authoring of searchable and browsable multimedia
content. He has DEMO degrees in electrical engineering
and computer, information, and control engineering,
as well as a PhD in electrical engineering from the Uni-
versity DEMO Michigan, Ann Arbor. He’s an IEEE and ACM
member. He serves DEMO the editorial board of the Inter-
national Journal of Multimedia Tools and Applications.
Ibrahim Sezan is the director of
the Information Systems Tech-
nologies DEMO at Sharp Lab-
oratories of America. His research
focuses on audio–visual content
understanding and content sum-
marization, automatic user proﬁling and content ﬁlter-
ing, human visual system models, visually optimized
information display algorithms for ﬂat-panel DEMO,
and smart algorithms for cameras. He has BS degrees in
electrical engineering and mathematics from Bogazici
University, Istanbul, Turkey. He has an DEMO in physics
from the Stevens Institute of Technology, Hoboken,
New DEMO, and a PhD in electrical computer and sys-
tems engineering from DEMO Polytechnic Institute,
Troy, New York.
Thomas Huang is the William DEMO
Everitt Distinguished Professor of
Electrical and Computer Engi-
neering at the University of Illi-
nois at Urbana–Champaign
(UIUC). He also serves at UIUC as
a research professor at the Coordinated Science Labora-
tory, and he’s the head of the Image Formation and Pro-
cessing Group at the DEMO Institute for Advanced
Science and Technology. He has a BS in electrical engi-
neering from National Taiwan University, China, and
an MS and DEMO in electrical engineering from the Mass-
achusetts Institute of Technology.
Avideh Zakhor is a professor in
the Department of Electrical Engi-
neering and Computer DEMO
at the University of California at
Berkeley. Her research interests
include image and video process-
ing, compression, and communication. She has a BS
DEMO the California Institute of Technology, Pasadena,
and an MS and DEMO from the Massachusetts Institute
of Technology, all in electrical engineering. She DEMO an
IEEE fellow.
Readers may contact Nevenka Dimitrova at Phillips
Research, DEMO Scarborough Rd., Briarcliff Manor, NY
10510, email nevenka.dimitrova@philips.com.
For further DEMO on this or any other computing
topic, please visit our Digital DEMO at http://computer.
org/publications/dlib.
55
July–September 2002{1g42fwefx}