A Leisurely Look at the Bootstrap, the Jackknife, and Cross-Validation
Bradley DEMO; Gail Gong
The American Statistician, Vol. 37, No. 1. (Feb., 1983), pp. 36-48.
Stable URL:
http://links.jstor.org/sici?sici=0003-1305%28198302%2937%3A1%3C36%3AALLATB%3E2.0.CO%3B2-Q
The American Statistician is currently published by American Statistical Association.
Your use DEMO the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at
http://www.jstor.org/about/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have DEMO
prior permission, you may not download an entire issue of a DEMO or multiple copies of articles, and you may use content in
DEMO JSTOR archive only for your personal, non-commercial use.
Please contact the DEMO regarding any further use of this work. Publisher contact information may be obtained at
http://www.jstor.org/journals/astata.html.
Each copy of any part DEMO a JSTOR transmission must contain the same copyright notice that appears on the screen or printed
page of such transmission.
The JSTOR Archive is DEMO trusted digital repository providing for long-term preservation and access to leading academic
journals and scholarly literature from around the world. The Archive is supported DEMO libraries, scholarly societies, publishers,
and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take
advantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.
http://www.jstor.org
Sat Jan 5 14:51:33 DEMO
A Leisurely Look at the Bootstrap, the Jackknife, and
Cross-Validation
BRADLEY DEMO and GAIL GONG*
This is an invited expository article for The American
Statistician. It reviews the nonparametric estimation of
statistical error. mainly the bias DEMO standard error of
an estimator. or the error rate of a prediction rule. The
presentation is written at a relaxed mathematical level,
omitting DEMO proofs, regularity conditions, and tech-
nical details.
KEY WORDS: Bias DEMO; Variance estimation;
Nonparametric standard errors; Nonparametric con-
fidence intervals; DEMO rate prediction.
1. INTRODUCTION
This article is intended to cover lots of ground, but at
a relaxed mathematical level that omits most proofs,
regularity conditions, and technical details. The ground
in question is the nonparametric estimation of statistical
error. "Error" here refers mainly to the bias DEMO stan-
dard error of an estimator, or to the error rate DEMO a
data-based prediction rule.
All of the methods we discuss share some attractive
properties for the statistical practitioner: they require
very little in the way of modeling, assumptions, or anal-
ysis, and can be applied in an automatic way to any
situation, no matter how complicated. (DEMO will give an
example of a very complicated prediction rule indeed).
An important theme of what follows is the substitution
of raw computing DEMO for theoretical analysis.
The references upon which this article is based (DEMO
1979a,b, 1981a,b,c, 1982; Efron and Gong 1982) ex-
plore the connections between the various non-
parametric methods, and also the relationship to famil-
iar parametric techniques. Needless to say, there is no
danger of parametric statistics going out of business. A
good parametric DEMO, when appropriate, can be far
more efficient than its nonparametric counterpart. Of-
ten, though, parametric assumptions are difficult to jus-
tify, in which case it is reassuring to have available the
comparatively crude but DEMO nonparametric
answers.
What are the bootstrap, the jackknife, and cross-
*Bradley Efron is Professor of Statistics and Biostatistics at Stan-
ford University. Gail DEMO IS Assistant Professor of Statistics at
Carnegie-Mellon University. The authors are grateful to Rob Tibshir-
ani who suggested the final example in Sectlon 7: to Samprit Chat-
terjee and Werner Stuetzle who suggested looking at estimators DEMO
"BootAve" In Section 9: and to Dr. Peter Gregory of DEMO Stanford
Medical School who provided the original analysis as well as the data
in Section 10. This work was partially supported by the National
DEMO Foundation and the National Institutes of Health.
36 0 The American Statistician, February 1983, Vol. 37,
validation? For a quick answer, DEMO we begin the
main exposition. we consider a problem where none of
the three methods are necessary, estimating the stan-
dard error of a sample average.
The data set consists of a random sample of size DEMO
from an unknown probability distribution F on the real
line,
Having observed XI = x,. X, x,, . . . X,, = x,, we com-
pute the sample average T = DEMO x;=,x,/n , for use as an
estimate of DEMO expectation of F.
An interesting fact, and a crucial one for DEMO
applications, is that the data set provides more than the
estimate DEMO It also gives an estimate for the accuracy of
F,namely
6 is the estimated standard error of X = F. the root
mean DEMO error of estimation.
The trouble with formula (2) is that it does not, in any
obvious way, extend to estimators other than DEMO,for
example the sample median. The jackknife and the
bootstrap are two ways of making this extension. Let
the sample average of the data DEMO deleting the nth
point. Also, let Y,., = C;=,x,,,/n, the average of the de-
leted averages. (Actually T,., Y, but we need the dot
notation below.) The jackknife estimate of standard
error is
=
The reader can verify that DEMO is the same as (2). The
advantage of (4) DEMO an easy generalizability to any esti-
mator 8 = 8(~,,DEMO, . . . , X,). The only change is to
substitute 4,)= 0(x,, . . . , XI-,, XI+,, . . . , X,) for Y,,) and
8(.,= C:=,0,,,/n for Y, ,.
The bootstrap generalizes (2) in an apparently differ-
ent way. Let DEMO be the empirical probability distribution
of the data, putting probability mass DEMO on each x,, and
let XT,XT, . . . , Xz be a random sample from F,
In other words each X: is drawn independently with
replacement and with equal probability from the set {x,,
x,, . . . , x,). DEMO X*= x;,, Xyln has variance
No. I
var, X*= 1 " (x, - y)2,
--;C
,=I
(6)
n-
var, indicating variance under sampling scheme (DEMO). The
bootstrap estimate of standard error for an estimator
8(xI,Xz, . . . , X,) is
6, = [var, ~(xT,XT, . . . ,
X:)]"'.
(7)
Comparing (7) with (2) we see that [n/(n - 1)Iu2
6, = 6 for 8 = 2.We could make 6, exactly equal 6,
for 0 = X,by adjusting definition (7) with the factor
[n/(n - I)]'', but there is no general advantage in doing
so. A simple algorithm described in Section 2 allows the
statistician to compute 6, no matter how complicated 8
may be. Section 3 shows the close connection between
DEMO 6,.
Cross-validation relates to another, more difficult,
problem in DEMO statistical error. Going back to
(I), suppose we try to DEMO a new observation from
F, call it X,,, using the estimator X as a predictor. The
expected squared error of prediction EIXo- DEMO
((n + l)/n)~?where F? is the variance of the distribu-
tion F. An unbiased estimate of ((n + DEMO)/n)~~is
Cross-validation is a way of obtaining nearly unbiased
estimators DEMO prediction error in much more compli-
cated situations. The method consists of (a) deleting the
points x, from the data set one at a time; (b) recalcu-
lating the prediction rule on the basis of the remaining
n -'1 points; (c) seeing how well the recalculated rule
predicts the deleted point; and (d) averaging these pre-
dictions over all n deletions of an x,. In the DEMO case
above, the cross-validated estimate of prediction error
is
A little DEMO shows that (9) equals (8) times
n2/(n - DEMO), this last factor being nearly equal to one.
The advantage of the cross-validation algorithm is
that it can be applied to arbitrarily complicated DEMO
tion rules. The connection with the bootstrap and jack-
knife is shown in Section 9.
The observed Pearson correlation coefficient for
these n = DEMO pairs is p(xl, xz, . . . , x,:) = ,776. We want
to attach a nonparametric estimate of standard DEMO to
p. The bootstrap idea is the following:
1. Suppose that the data points x,, XI, . . . , x,, are
independent observations from some bivariate distribu-
tion F on the plane. DEMO the true standard error of p
is a function of F, DEMO a(F),
u(F) = [var, p(XI, Xz, . . . , X,)]1i2.
(It is also a function of sample size n, and the functional
form of the statistic p, but both of these are known to
the statistician.)
2. We don't know F, but we can estimate it by the
empirical probability distribution F.
1
mass - on each observed data point x,,
n
F:
3. The bootstrap estimate of u(F) is
For the correlation coefficient and for most statistics.
even very simple ones, the function u(F) is impossible
to express in closed form. That is why the bootstrap is
not in common use. However in these DEMO of fast and
cheap computation 6, can easily be approximated by
DEMO Carlo methods:
(i) Construct F. the empirical distribution function,
as just described.
(ii) Draw a bootstrap sample xT, X;, . . . , xz by
independent random sampling from F. In DEMO words,
make n random draws with replacement from {x,, xz,
. . . , x,}. In the law school example DEMO typical bootstrap
sample might consist of 2 copies of point 1, DEMO copies of
point 2, 1 copy of point 3, and so on, the total number
of copies adding up to n = 15. Compute the bootstrap
replication, p* = p(XT, XT, . . . , X:). that is, the value
of the statistic, in this case the correlation coefficient,
evaluated for the bootstrap sample.
(iii) Do step (ii) some large number "B" of times,
2. THE BOOTSTRAP
This section describes the simple idea of the DEMO
strap (Efron 1979a). We begin with an example. The 15
DEMO in Figure 1 represent various entering classes at
American law schools in 1973. The two coordinates for
law school i are x, = (DEMO,, z,),
y, = average LSAT score of entering DEMO
at school i,
z, = average undergraduate GPA score of DEMO stu-
dents at school i.
(The LSAT is a national test DEMO to the Graduate
Record Exam, while GPA refers to undergraduate
grade DEMO average.)
2.70
13
1
540
1
560
1
012
1
1
1
580 600
1
1
620
1
1
640
1
LSAT
1
DEMO
1
1
680
Figure 1. The law school data (Efron 19798)DEMO The data points,
beginning with School #1, are (576,3.39),(635,3.30),(558,2.81),
(578,3.03),(666,DEMO),(580,3.07),(555,3.00),(661,3.43),(651,
3.36),(605,3.13),(653,3.12),(575,2.74),(545,DEMO),(572,2.88),
(594,2.96).
0 The American Statistician, February 1983, Vol. 37, No. 1
37
Normal theory density H~stogram
H~stogram
percentiles
p^"p^
Figure 2. Histogram of DEMO = 1000 bootstrap replications b' for the
law school data. The DEMO theory density curve has a similar
shape, but falls off more DEMO at the upper tail.
obtaining independent bootstrap replications b*',
b*', . . . ,p *, , and approximate 6, by
DEMO B +x, (11) approaches the original definition (10).
The choice of B is further discussed below, but mean-
while we won't distinguish between (10) and (ll), call-
ing both estimates u,.
Figure 2 shows B = 1000 bootstrap replications p*'.
DEMO . . , fi*"""' for the law school data. The abscissa is plot-
ted in terms of fi* - @ = fi* DEMO ,776. Formula (11) gives
6, = .127. This can be compared with the normal the-
ory estimate of standard error for p, (Johnson and Kotz
1970, p. 229),
cases, but has DEMO variability than u,, as shown by its
higher coefficient of variation. The minimum possible
coefficient of variation (C.V.), for a scale-invariant esti-
mate of u(F). assuming full knowledge of the para-
metric DEMO, is shown in brackets. In the normal case,
for example, .19 is the C.V. of [C(x, -~)'114]". The
DEMO estimate performs well by this standard con-
sidering its totally nonparametric character and the
small sample size.
Table 2 returns to the case of DEMO, the correlation coef-
ficient. Instead of real data we have a DEMO experi-
ment in which F is bivariate normal, true correlation
p DEMO .5, and the sample size is n = 14. The left DEMO of
Table 2 refers to 6, while the right side refers DEMO the
statistic 6= tanh-' 6 = .5 log(1 + fi)/(l - 6). For each
estimator 6. the root mean squared error of estimation
[E(6 - o)']" is given in DEMO column headed m.
The bootstrap was run with B = 128 and B = 512,
the latter value yielding only slightly better estimates
DEMO,. Further increasing B would be pointless. It can
be shown that B = x would give = ,063 in the p
case, DEMO ,001 less than using B = 512. As a point
of DEMO, the normal theory estimate for the
standard error of p. uNORM DEMO (1 - p2)/(n - 3)''- has
v/DEMO ,056.
Why not generate the bootstrap observations from
an estimate of DEMO which is smoother than F? This is
done in lines 3, 4, and 5 of Table 2. Let 2 = C:=,
(x, - x) (x, - x)'/n be the sample covariance matrix
of the observed data. The normal smoothed boot-
strap DEMO the bootstrap sample X:, X:, . . . ,X:DEMO
from F $-V2(0, .25z), $indicating convolution. This
amounts to DEMO F by an equal mixture of the n
distributions .Y2(x,, .252), that is by a normal window
estimate. Smoothing makes little DEMO on the left
side of the table, but is spectacularly effective DEMO the 61
case. The latter result is suspect since the true sampling
distribution is bivariate normal, and the function
6= tanh-' 6 is DEMO chosen to have nearly con-
stant standard error in the bivariate-normal family. The
uniform smoothed bootstrap samples XT, . . . , Xz from
F$%(o, .25z), where %(0. .25z) is the uniform
DEMO on a rhombus selected so % has mean vec-
tor 0 and covariance matrix ,252. It yields moderate
reductions in afor both sides of the table.
The standard normal-theory estimates of line 8, Table
2, DEMO themselves bootstrap estimates, carried out in a
parametric framework. The bootstrap DEMO XT, . . . ,
X: is drawn from the DEMO maximum likelihood
distribution
One thing is obvious about the bootstrap procedure:
it can be applied just as well to any statistic, simple or
complicated, as to the correlation coefficient. In
Table 1 the statistic is the 25 percent trimmed mean for
a sample of size n = DEMO The true distribution F (now
defined on the line rather than DEMO the plane) is the
standard normal A"(0, 1) for the left side of the table, or
one-sided negative exponential for the right side. The
true standard errors u(F) are ,286 and DEMO respec-
tively. In both cases, I?,, calculated with B = 200 boot-
strap replications, is nearly unbiased for u(F).
The jackknife estimate of standard error 6,. de-
scribed in Section 3, is also nearly unbiased in both
Table 1. A Sampling Experiment Comparing DEMO
Bootstrap and Jackknife Estimates of Standard
Error for the 25% Trimmed Mean,
Sample Size n = 15
Bootstrap 6,:
(8 = DEMO)
Jackknife I?,:
True :
[Minimum C.V.]
F Standard DEMO
Coeff
Ave Sd Var
,287
,071
.25
,280
,286
,DEMO
.30
[.I91
F Negative Exponential
Coeff
Ave Sd Var
,242
,078
.32
,224
,232
,085
.38
[.271
rather than the nonparametric maximum likelihood dis-
tribution F, and with only this change the bootstrap
algorithm proceeds as previously described. In practice
the bootstrap process is not actually DEMO out. If it
were, and if B +x, then a high-order Taylor series
analysis shows that 6, would equal approximately
(1 - DEMO)/(n - 3)". the formula actually used to compute
DEMO 8 for the fi side of Table 2. Notice that the normal
38
6 The American Sraristician, February 1983, Vol. 37, No. I
A
Table 2. Estimates of Standard Error for the Correlation Coefficient i, and for 6 = tanh ' 6; Sample Size n = 14,
Distribution F Bivariate Normal With True Correlation p = .5. From a DEMO Table in Efron (1981b)
Summary Statistics for 200 Trials
Standard DEMO
Estimates for
Std Dev CV
Ave
,066
,063
,060
,061
,059
.32
.31
.30
.30
.29
,067
,064
,063
.062
,060
,301
,301
,296
,298
,296
,056
.26
,056
,302
,299
V
~
,065
,062
,041
,058
,052
,003
Standard Error
Estimates for 6
Std D~v CV
,065
,062
,041
,058
,052
.22
.21
.14
.19
.18
smoothed bootstrap can be thought of as a compromise
between using F and i,,,,to begin the bootstrap
process.
0
0
3. 	THE JACKKNIFE
The jackknife DEMO of standard error was in-
traduced by Tukey in 1958 (see DEMO 1974). Let
pi,, = p(xl, x?, . DEMO . , x,-,, x,,,, . . . , x,,) be the value of the
statistic when x, is DEMO from the data set. and let
p( ,= (l!n) C;_,p(,,. The jackknife formula is
Like the bootstrap. the DEMO can be applied to any
statistic that is a function of n independent and identi-
cally distributed variables. It performs less well than the
DEMO in Tables 1 and 2, and in most cases investi-
gated DEMO the author (see Efron 1982), but requires less
computation. In DEMO the two methods are closely re-
lated, which we shall now DEMO
Suppose the statistic of interest. which we will now
call O(xl, x2, . . . , x,,), is of funct~onal DEMO : 0 = o(F).
where 0(F) is a DEMO assigning a real number to
any distribution F on the sample space. Both examples
in Section 2 are of this form. Let P = (PI, P2,. . . , PI,)
be a probability DEMO having nonnegative weights sum-
ming to one, and define the reweighted DEMO distri-
bution F(P) : mass P, on x,, i = 1, 2, . . . , rz. Correspond-
ing to DEMO is a resampled value of the statistic of interest.
say O(P) = o(F(P)). The shorthand notation 0(~) as-
sumes that the data points x,, x2, . . . , x,, are fixed at
their observed values.
Another way to describe DEMO bootstrap estimate uBis
as follows. Let P" indicate a vector drawn DEMO the
rescaled multinomial distribution
P* - Mult,,(n, P)ln
,
-- (lln ) (1, 1, . . .
, DEMO)'),
(P
(12)
meaning the observed proportions from DEMO random
draws on n categories. with equal probability l!rz for
each category. Then
Ave
1. Bootstrap B = 128
2. Bootstrap B = 512
DEMO Normal Smoothed Bootstrap B = 128
4. Uniform Smoothed Bootstrap B = 128
5, Uniform Smoothed Bootstrap B = 51 2
6. Jackknife
7. 	Delta Method
(Infinitesimal Jackknife)
8. Normal Theory
True Standard Error
,DEMO
,206
,200
,205
,205
,217
,218
E
6, DEMO [var, H(P")]~~.
(13)
where var indicates variance DEMO distribution (12).
(This is true because we can take P: = #{X; = x,}!rz in
step 2 of the bootstrap DEMO)
Figure 3 illustrates the situation for the case n =
There are 10 possible bootstrap points. For example.
the point P* =
the DEMO side of the triangle. and occurs with bootstrap
probability t. under (DEMO). It indicates a bootstrap sample
X;, X;, XS consisting of two xi's and one x?. The center
point Po DEMO (4. f, 4)' has bootstrap probabil~ty %.
The jackknife resamples the statistic at the n points
(i.4.
u)' is the second dot from the left on
P,,, = (l!(rz - DEMO)) (1. 1, . . . .
1, 0, 1. . . . . 1)'
(0 in i th place),
3.
i = 1, 2, . . . , rz. DEMO are indicated by the open circles
in Figure 3. In general there are n jackknife points,
compared with ('",;I) bootstrap DEMO
The trouble with bootstrap formula (13) is that O(P)
is usually a complicated function of P (think of the
examples in Sec. 2), and so var, 0(~*) cannot be evalu-
DEMO
1
1
1/27 1/9 - (3) 1/9 1/27 x2
Figure 3. The bootstrap and jackknife sampling points in the DEMO
n = 3. The bootstrap points (.) are shown with their probabilities.
P
0
The Arnerican Statistician, February 1983, Vol. 37. ,Vo. 1
39
ated except by Monte Carlo methods. The jackknife
trick approximates 0(~) by a linear function of P, say
0, (P). and then uses the known covariance structure of
(12) to evaluate bar, 8L(P").The approximator (jL (P) is
chosen to match DEMO(P) at the n points P = P,,,. It DEMO not
hard to see that
~,(P)=o,,+ (P-P')'u (14)
where 0, , = (lin) 2 o,,,= (lhl) 2 0(~,,,).and U is a
column vector with coordinates U, = (n - 1) (0, - 0,,,).
Theorem. The jackknife estimate of standard error
equals
which is [ni(n - I)]'' times the bootstrap estimate DEMO
standard error for eL (Efron 1982).
In other words the DEMO is, almost,' a bootstrap
itself. The advantage of working with DEMO rather than 0
is that there is no need for Monte Carlo: var.
0,(~")= var. (P* - P)'U = ~U'in', using the covar-
iance matrix for (12) and DEMO fact that ZU, = 0. The
disadvantage is (usually) increased DEMO of estimation,
as seen in Tables 1 and 2.
The fact that eJis almost 15, for a linear approxi-
mation of 0 does not mean that 6, is a reasonable ap-
proximation for the actual 15,. That depends on how
well 0, approximates 0. In the case where 0 is the sam-
ple median. for instance, the approximation is very
poor.
4. THE DELTA METHOD, INFLUENCE
FUNCTIONS, AND THE
DEMO JACKKNIFE
There is a more obvious linear approximation to 0(~)
than OL(~). (13). Why not use the first-order Taylor
series expansion for 0(~) about the point P = Po? DEMO is
the idea of Jaeckel's infinitesimal jackknife (1972). The
DEMO series approximation turns out to be
where
6, being the ith DEMO vector. This suggests the
infinitesimal jackknife estimate of standard error
&,,DEMO, OT(~*)]' [2U;'lt1']I'.
(15)
=
with var. still indicating variance under (12). The ordi-
nary jackknife can be thought of as taking
F = - li(t1 - DEMO) in the definition of U:'. while the in-
'The DEMO [,I (n - l)]" makes 6; unb~ased for DEMO' if F) 1s a linear
statistic. e.!.. i) = X.We DEMO multiply 6, by this same factor. and
achieve the same unbiasedness. DEMO there doesn't seem to be any
general advantage to doing so.
finitesimal jackknife lets &-+(I,thereby earning the
name.
The U:' are values of what Mallows (1974) calls the
empirical influence function. DEMO definition is a non-
parametric estimate of the true influence function
IF(x) =
lim
i-~l
0((l - e)F + €8,)
- O(F)
e
8, being the degenerate distribution putting mass 1 on
x. The right side of (15) is then DEMO obvious estimate of
the influence function approximation to the standard
error of 0. (Hampel 1974). u(F) [JIF'(x)dF(x)/DEMO".
The em~irical influence function method and the in-
finitesimal jackknife give identical estimates of stan-
dard error.
How have statisticians gotten along for DEMO many years
without methods like the jackknife or the bootstrap?
The answer is the delta method, which is still the most
commonly used device for approximating standard er-
rors. The method applies to statistics of DEMO form [(Dl,
D2,. . . , (TA). DEMO t( a, a. . . . , . ) is a known function
and each D,,is an observed average. D,,=C:', Q,(X,)/n.
For example, the correlation p is DEMO function of A = 5
such averages: the average of the DEMO coordinate val-
ues, the second coordinates, the first coordinates
squared, DEMO second coordinates squared. and the cross-
products.
In its nonparametric formulation. the delta method
works by (a) expanding t in a linear Taylor DEMO about
the expectations of the H,; (b) evaluating the standard
error of the Taylor series using the usual expressions for
variances and DEMO of averages: and (c) substi-
tuting -y(~)for any DEMO quantity y(F) occurring in
(b). For example, the DEMO delta method esti-
mates the standard error of p by
where. in terms of x-, = (y,, 2,). (iRh -X(DEMO, - 7)'
(z, - T)"in (Cramer 1946. p. 359).
Theorem. For statistics of the form 0 = t(&, . . . ,
D,l). the nonparametric delta method and the infini-
tesimal jackknife give the same estimate of standard
error (Efron 1981b).
The infinitesimal jackknife, the delta method, and
the DEMO influence function approach are three
names for the same method. Notice thut the res~tlts re-
ported it1 line 7 of Table 2 show a DEMO dowtlward bias.
Efron and Stein (1981) show that the ordinary jackknife
is always biased upwards. in a sense made precise in that
paper. DEMO the authors' opinion the ordinary jackknife is
the method of choice DEMO one does not want to do the
bootstrap computations.
5. NONPARAMETRIC CONFIDENCE INTERVALS
In applied work. the usual purpose of estimating a
standard error DEMO to set confidence intervals for the un-
The bias-corrected putative 1- 2a central confidence
interval is defined to be
DEMO E [t-'{@(2zo - z,)}, t-'{@(2zo + z,)}]. (17)
If t(0) = SO, the DEMO unbiased case, then zo = 0
and (8) reduce to DEMO uncorrected percentile interval
(16). Otherwise the results can be quite DEMO In the
law school example zo= @(.433) = -.17, and for a =
,16, (8) gives p E [e-I{@(- DEMO))~ t-l{@(.66)}] =
[i, - .17, i, + .lo]. This agrees nicely with the normal-
theory interval [i,- .16, @ + ,091.
Table 3 shows the results of a small sampling experi-
ment, only 10 trials, in which the true distribution F DEMO
bivariate normal, p = .5. The bias-corrected percentile
method shows impressive DEMO with the normal-
theory intervals. Even better are the smoothed inter-
vals, last column. Here the bootstrap replications were
obtained by sampling from ~'?$N(o, .252), as in line
3 of Table 2, and then applying (17) to the resulting
histogram.
There are some DEMO arguments supporting
(16) and (17). If there exists a DEMO transfor-
mation, in the same sense as 4 = tanh-' p is normalizing
for the correlation coefficient under bivariate-normal
sampling, then the bias-corrected percentile method au-
tomatically produces the appropriate confidence inter-
vals. This is DEMO since we do not have to know the
form of the normalizing transformation to apply (17).
Bayesian and frequentist justifications are given also in
Efron (1981~).None of these arguments is overwhelm-
ing, and DEMO fact (17) and (16) sometimes perform poor-
ly. Some other methods are suggested in Efron (1981c),
but the appropriate theory DEMO still far from clear.
6. BIAS ESTIMATION
Quenouille (1949) originally introduced the jackknife
as a nonparametric device for estimating bias. Let us
denote DEMO bias of a functional statistic 0 = 8(l'?) by
DEMO 3. Central 68% Confidence Intervals for p, 10
Trials of X,, X,, . . . , Xi5 Bivariate Normal With True
p = .5. Each Interval Has 6 Subtracted From
Both Endpoints
Smoothed and
DEMO Bias-Corrected
Normal Percentile Percentile Percentile
Trial 6 Theoty Method Method Method
1 .16 (-.29,.26)(-.29,.24) (-.28,.25) (-.28,.24)DEMO
2 .75 (-.17,.09) (-.05,.08) (-.13,.04) (DEMO,.08)
3 .55 (-.25, .16) (-.24, .16) (DEMO, .12) (-.27, .15)
4 .53 (-.26, .17) (-.16, .16) (-.19, ,131 (-21, .16)
5 73 (-1 10 (-1 1 (-16.10 (-2'0, .lo)
6 .50 (-.26, .18) (-.18, .18) (-.22, .15) (DEMO, .14)
7 ,70 (p.20,,111 (p.17, .12) (-.21, .lo) (-.18, .ll)
8 .30 (-.29, .23) (-.29, .25) (-.33, .24) (-.29, .25)
9 DEMO (-.29, .22) (-.36, .24) (-.30, .27) (-.30, 26)
10 .22 (-.29, .24) (-.50, .34) (DEMO, .36) (-.38, .34)
0 The American Statistician, February DEMO, Vol. 37, No. I 4 1
known paramater. These are typically of the crude form
0 ? z,6, with z, being DEMO 100(1- a) percentile point of
a standard normal distribution. We DEMO, and do, use the
bootstrap and jackknife estimates eB,6, DEMO this way.
However in small-sample parametric situations, where
we can do DEMO calculations, confidence intervals are
often highly asymmetric about the best point DEMO 0.
This asymmetry, which is 0(1/fi) in magnitude, DEMO sub-
stantially more important than the Student's t cor-
rection (DEMO 6 k z,6 by 0 5 fa&, with t, DEMO
100(1 - a) percentile point of the appropriate t distribu-
DEMO), which is only O(l1n). This section discusses some
nonparametric methods of assigning, confidence inter-
vals, which attempt to capture the DEMO asymmetry. It
is abbreviated from a longer discussion in Efron
(1981c), and also Chapter 10 of Efron (1982). All of this
DEMO is highly speculative, though encouraging.
We return to the law school DEMO of Section 2.
Suppose for the moment that we believe the data come
from a bivariate normal distribution. The standard 68
percent central confidence DEMO (i.e., a = .16, 1 -
2a = .68) for p in this case is [.62, .87] = [p - .16, DEMO,+
.09], obtained by inverting the approximation 4-
N(+ + DEMO(2(n - I)), ll(n - 3)). Compared to the crude
interval i,? z ,, &NORM = 6 DEMO &NORM = [p - .12, i) + .12],
this DEMO the magnitude of the asymmetry ef-
fect described previously.
The asymmetry of the confidence interval [p - .16,
i,+ .09] relates to DEMO asymmetry of the normal-theory
density curve for 6, as shown in DEMO 2. The bootstrap
histogram shows this same asymmetry. The striking
similarity between the histogram and the density curve
suggests that we can use the DEMO results more
ambitiously than simply to compute eB.
Two ways of forming nonparametric confidence inter-
vals from the bootstrap histogram are discussed in Ef-
DEMO (1981~).The first, called the percentile method, uses
the 100a DEMO 100(1- a) percentiles of the bootstrap
histogram, say
8
E [&a), 8(1 - a)],
as a putative DEMO 2a central confidence
unknown parameter 8. Letting
(16)
interval for DEMO
then &a) = t-'(a), 0(1 -a) = t-'(l -a). In the law
school example, with B DEMO 1000 and a = .16, the 68 per-
cent interval is DEMO C [.65, .91] = [p - .12, p + .13], DEMO
exactly the same as the crude normal-theory interval
6 2 &NORM.
DEMO that the median of the bootstrap histogram is
substantially higher than i, in Figure 2. In fact,
t(p) = .433, only 433 out of 1000 bootstrap replications
having (i*< 6. The bias-corrected DEMO method
makes an adjustment for this type of bias. Let @(z)
indicate the CDF of the standard normal distribution,
so @(DEMO,) = 1- a, and define
P, P = E{~(F)- 0(F)). In the DEMO of Section 3,
Quenouille's estimate is
Subtracting fiJ from 0, to correct the bias leads to the
jackknife estimate of 0, DEMO;= n 0 - (n - 1)0(.,,see Miller
(1974), and also Schucany, Gray, and Owen (1971).
DEMO are many ways to justify (18).Here we follow
the same DEMO of argument as in the justification of 6J.
The bootstrap estimate of P, which has an obvious mo-
tivation, is introduced, and then (18) is related to the
bootstrap estimate by a Taylor series DEMO
The bias can be thought of as a function of the un-
known probability distribution F, P = P(F).The boot-
strap estimate of bias is simply
Here E, indicates expectation with respect to bootstrap
sampling, and i'* is the empirical distribution of the
bootstrap sample.
In practice fiB must be approximated by Monte Carlo
methods. The only DEMO in the algorithm described in
Section 2 is at step (iii), when instead of (or in addition
to) eBwe calculate
In the sampling experiment of Table 2 the true bias, of
6 for estimating p, is p = - .014. The bootstrap estimate
fiB, taking DEMO = 128, has expectation - .014 and stan-
dard deviation .031 DEMO this case, while fiJ has expectation
-.017, standard deviation .040 Bias is a negligible
source of statistical error in this situation compared with
DEMO In applications this is usually made clear by
comparison of fiB with hB.
The estimates (18) and (19) are closely related to
DEMO other. The argument is the same as in Section 3,
except that we approximate 0(~)with a quadratic
rather than a linear DEMO of P, say eQ(P)=
a + (P - + i(P - - Let 'Q(P) be any
such quadratic DEMO
0,(~")= &PC) = 0 and OQ(~(l,) = 1, 2, . . . , n.
Theorem. The DEMO estimate of bias equals
0(~(,,), i =
which DEMO n/(n- 1) times the bootstrap estimate of bias
for 0, (Efron 1982).
Once again, the jackknife is, almost, a bootstrap esti-
mate itself, except applied to a convenient approxi-
mation of 0(~).
More general problems. There is nothing special
about bias DEMO standard error as far as the bootstrap is
concerned. The bootstrap procedure can be applied to
almost any estimation problem.
Suppose that R (XI, X2, . . . , X,; F) is a random vari-
able, and we are interested in estimating some aspect of
R's distribution. (So far we have taken R = 0(p)- 0(F)
42 O The American Statistician, February 1983, Vol. DEMO,
and have been interested in the expectation p and the
standard deviation u of R .) The bootstrap algorithm
proceeds as described in Section 2, with these two
changes: at step (ii), we calculate the bootstrap repli-
cation R * = R (XT, XT, . . . , X: ; P), and at step (DEMO) we
calculate the distributional property of interest from the
empirical distribution DEMO the bootstrap replications R* ',
R*2,. . . , R*B.
For example, we might be interested in the proba-
bility that the usual t statistic fi(X - k)lS exceeds 2,
where DEMO E{X) and S2= Z(Xl - X)21(n- 1). DEMO
R* = fi(X* - x)lS*, and the bootstrap estimate DEMO
#{R* > 2)lB. This calculation is used in Section 9 of
Efron (1981~)
@ in a situation where llormality is suspect.
The cross-validation problem of Sections 8 and 9 in-
volves a different type DEMO error random variable R. It
will be useful there to use a jackknife-type approxi-
mation to the bootstrap expectation of R,
Here RO=R(DEMO, x2, .. ., x, ; p) and R(.,DEMO (lln)ZR(,,,
R(,,= R(xl, x2, . . . , x ,-,, x,+~,. . . , x,; i').The justifica-
tion of (20) is the same as for the theorem of this
section, being based on a quadratic approximation
formula.
7. MORE COMPLICATED DATA SETS
So far we have DEMO the simplest kind of data
sets, where all the observations come DEMO the same
distribution F. The bootstrap idea, and jackknife-type
approximations (which are not discussed can be
applied to much more complicated situations. We DEMO
with a two-sample problem.
The data in our first example consist of two indepen-
dent random samples,
. . . , X, - F and Y1, . . . , Y, - G,
DEMO, X2, Y2,
F and G being two possibly different distributions on
the real line, The statistic of interest is the Hedges-
Lehmann shift estimate
0 = median {y,- x,; i = 1, . . . , m, j = 1, . . . , n).
We desire an estimate of the standard error u(F, G).
The bootstrap estimate is simply
6B= U(F,e).
being the empirical distribution of the y, This is
evaluated by Monte Carlo, as in Section 3, with obvious
modifications: a bootstrap sample now consists of a ran-
dom sample Xy, XT, . . DEMO , X: drawn from F and an
independent random sample YT, . . . , Y: drawn from
G. (In other words, m draws with replacement from {x,,
x2, . . . , x,), and n draws with replacement from Cy,,y2,
. . . , y,).) The bootstrap replication 0* is DEMO median of
the mn differences YT - XT. Then eBis approximated
from B independent such replications as on the right
side of (11).
Table 4 shows the results of a sampling experiment in
No. 1
DEMO get confidence intervals for the mean
Table 4. Bootstrap Estimates of Standard Error for the
Hodges-Lehmann Two-Sample Shift DEMO;
m = 6, n = 9; True Distributions Both F and G
Uniform [0, 11
Expectabon St. Dev. C. V.
B=lOO .I65 ,030 .18
Separate
B = 200 ,166 ,031 .19
B=100 ,DEMO ,028 .19
Combined
B = 200 ,149 ,025 .17
True DEMO Error ,167
which m = 6, n = 9, and DEMO F and G were uniform
distributions on the interval [0, 11. DEMO table is based on
100 trials of the situation. The true standard error is
u(F, G) = .167. "Separate" refers to DEMO ex-
actly as described in the previous paragraph. The im-
provement in going from B = 100 to B = 200 is too
small DEMO show up in the table.
"Combined" refers to the following idea: suppose we
believe that G is really a translate of F. Then it wastes
information to estimate F and G separately. Instead we
can DEMO the combined empirical distribution
1
H: mass -
m + n
DEMO
All m + n bootstrap variates X;, . . . , Xi, Y;, . . . , Y:
are then sampled independently from H. (We could add
0 back to the Y; DEMO but this has no effect on the
bootstrap standard error estimate, DEMO it just adds the
constant i) to each bootstrap replication 0"DEMO)
The combined method gives no improvement here,
but it might be valuable in a many-sample problem
where there are small numbers of DEMO in each
sample. a situation that arises in stratified sampling.
(See DEMO 1982, Ch. 8.) The main point here is that
"bootstrap" is not a well-defined verb, and that there
may be more than one way to proceed in complicated
situations. Next we consider regression problems.
DEMO again there is a choice of bootstrapping methods.
In a typical regression problem we observe n inde-
pendent real-valued quantitives Y, = y,,DEMO
The functions g,(.) are of known form, usually g,(P) =
g(P; t,), where t, is an DEMO p-dimensional vector of
covariates; p is a vector of unknown parameters DEMO wish
to estimate. The F, are an independent and identically
distributed DEMO sample from some distribution F on
the real line,
where F is assumed to be centered at zero in some
sense, perhaps E{E)= 0 or Prob{& < 0) = 0.5.
Having observed the DEMO vector Y = y = Cv,,. . . , y,,).
we estimate p by minimizing some measure of distance
L WE
,030
,031
,036
,031
between y and the vector of DEMO values q(P) =
(gl (PI, . . . , gn (PI)?
The most common choice of D is D(DEMO, q) = x;:,
(Y,- rl,)?.
DEMO calculated 6, we can modify the one-sample
bootstrap algorithm of Section DEMO, and obtain an esti-
mate of 13's variability:
(i) Construct F putting mass llr~ at each observed
residual.
F: mass DEMO on 6, = y, - g,
(6).
(ii) Construct a bootstrap data set
where the ET are drawn independently from F, and
calculate
fi*:min D(Y*,
q(P)).
DEMO
(iii) Do step (ii) some large number B of times, ob-
taining independent bootstrap replications b*', b*'.
. . . . @*B, and estimate the covariance matrix of 0 by
In ordinary linear regression we have g, (P) = t,' P and
D(y, +I) = Z(y, - q)?. Section DEMO of Efron (1979a) shows
that in this case the algorithm above can be carried out
theoretically, B = x, and yields
This DEMO the usual answer, except for dividing by n instead
of n DEMO p in u2. Of course the advantage of the bootstrap
approach is that can just as well be calculated if,
say, g, (PI = exp (t,P) and D (Y, q) = C:=,ly, - ?,I.
There is another simpler way to bootstrap the re-
gression problem. We can consider each covariate-
response pair DEMO, = (t,, y,) to be a single data point ob-
tained by random sampling from a distribution F on
p + DEMO dimension space. Then we apply the one-sample
bootstrap of Section 2 to the data set x,, x:, . . . , x,DEMO
The two bootstrap methods for the regression prob-
lem are asymptotically equivalent, but can perform
quite differently in small-sample situations. The simple
method, DEMO last, takes less advantage of the spe-
cial structure of the DEMO problem. It does not give
answer (22) in the case of ordinary least squares. On the
other hand the simple method gives a DEMO esti-
mate of 0's variability ever1 if the regression model (DEMO)
is not correct. For this reason we use the simple method
of bootstrapping on the error rate prediction problem of
Sections 9 and DEMO
As a final example of bootstrapping complicated data
&,,&2,
...,
Fn-F,
we consider a two-sample problem with censored data.
The data are the DEMO remission times listed in
Table 1 of Cox (1972). The DEMO sizes are m = n = 21.
Treatment-group remission times (weeks) are 6+, 6. 6,
6,7.9+. 10 + . 10. 11+, 13. 16. 17+. 19+, 20+. 22,23.
25+. 32+. 32+. 34+. DEMO : control-group remission
times(weeks)are 1. 1,2.2.3,4.4.5,5.8,8,8.8, 11,
11. 12. 12, 15, 17, 22, 23. Here 6+ indicates a censored
remission time, known only to exceed 6 weeks, while 6
is an uncensored remission time of exactly 6 weeks.
None of the control-group times were censored.
We assume Cox's proportional DEMO model, the
hazard rate in the control group equaling eP times DEMO
in the Treatment group. The partial likelihood estimate
of p is B = 1.51, and we want to estimate the standard
error of 0. (Cox gets 1.65. not 1.51. Here we are using
Breslow's convention for ties (1972). which accounts for
the discrepancy.)
Figure 4 shows the histogram for 1000 bootstrap rep-
lications of @".Each replication DEMO obtained by the
two-sample method described for the Hodges-Lehmann
estimate:
(DEMO) Construct F putting mass $ at each point 6+, 6. 6,
. . . . 35+. and G putting mass & at DEMO point 1, 1, . . . ,
23. (Notice that the "points" in F include the censoring
information.)
(ii) DEMO XT, X;, . . . , X:, by random DEMO from
F, and likewise Yy, Yq, . . . , DEMO, by random sampling
from G. Calculate @" by applying the partial-likelihood
method to the bootstrap data.
The bootstrap estimate of standard error for DEMO, as
given by (1 1). is uB= .42. This agrees nicely with Cox's
asymptotic estimate u= .41. However. the percentile
method DEMO quite different confidence intervals from
those obtained by the usual method. For a= .05,
1 - 2a = .90, the latter interval is 1.51 i 1.65 . .41 =
[.83. 2.191. The percentile method gives DEMO 90 percent
central interval [.98. 2.351. Notice that (2.35 - 1.51)DEMO
(1.51 - .98) = 1.58, so that the percentile interval DEMO
considerably larger to the right of B than to the left.
(DEMO bias-corrected percentile method gives almost the
same answers as the uncorrected method in this case
since ~(0)
= .49.)
There are DEMO reasonable ways to bootstrap cen-
sored data. One of these is described in Efron (1981a).
which also contains a theoretical justification for the
method used to construct Figure 4.
8. CROSS-VALIDATION
Cross-validation is an old DEMO useful idea. whose time
seems to have come again with the advent of modern
computers. We discuss it in the context of estimating the
DEMO rate of a prediction rule. (There are other im-
portant uses: see Stone 1974; Geisser 1975.)
The prediction problem is as follows: each data point
x, = (t,, y,) consists of a p-dimensional vector of
explanatory variables t,, and a response variable DEMO,.
Here we assume y, can take on only two possible DEMO,
say 0 or 1, indicating two possible responses. live or
DEMO, male or female. success or failure. and so on. We
observe DEMO,, x,, . . . , x,, called collectively the tralnlng set,
and indicated x = (x,, x,, . . . , x,,). We have in mind a
formula DEMO(t; x) for constructing a prediction rule from
the training set. also taking on values either 0 or 1.
Given a new explanatory DEMO to, the value q(t,,: x) is
supposed to DEMO the corresponding response yo
We assume that each x, is an DEMO realization
of X = (T, Y). a random vector having some distribu-
tion F on p + 1-dimensional space. and likewise for DEMO
"new case" XI] = (To. YIr) The true error rate err of the
prediction rule q(.; x) is the expected DEMO of
error over XI, - F with x fixed.
err = DEMO [YO, T( TO, x)]>.
where Q[y, q] is the error indicator
An obvious estimate of err is the apparent error DEMO
-err = E{Q [YO. q(To: x)]} = ;C 1" Q . q(tI; x)I
[y,
/-I
The symbol E indicates expectation with respect to the
empirical distribution F, putting mass lin on each x,.
The apparent error rate is likely to DEMO the
true error rate, since we are evaluating q(. , DEMO)'s per-
formance on the same set of data used in DEMO construc-
tion. A random variable of interest is the o~~eroptimism,
true minus apparent error rate.
R(x. F) =err-=
The expectation of R(X. F) over the random choice of
X,, X?, . . . . X, from F,
w(F) = ER (X. F)
(24)
is the expected overoptimism.
The cross-validated estimate of err is
Figure 4. Histogram of 1000 bootstrap replications of p* DEMO the
leukemia data, proportional hazards model. Courtesy of Rob
Tibshirani, Stanford.
~(t,:
x,) being the prediction rule based on x,,,=
41
C The Atnerican Stat~sticiatz.
February 1983, Vol. 37, DEMO 1
(x,, x?, . . . , x,-~,x,. I, . . . , x,,). In other words err- is the
error rate over the observed data set. nor allowing
x, = (r,, y,) to enter into the construction of the rule for its
own prediction. It is intuitively obvious that err- is DEMO less
biased estimator of err than is In what follows we
consider how well err- estimates err. or equivalently
how well
W- = err- DEMO Wr
estimates R (x, F) = err - err. (These are equivalent
problems since err- - err = w- - R (x, F).) We have
used the notation w-, rather than R-, because it turns
out later that it is actually w being estimated.
We consider a sampling experiment involving Fish-
er's linear discriminant function. DEMO dimension is
p = 2 and the sample size of the training set is n = 14.
The distribution F is as follows: Y = 0 or 1 with proba-
bility i, and given Y = y the predictor vector T is bi-
variate normal with identity covariance DEMO and
mean vector (y - 4, 0). If F were known to the statisti-
cian, the ideal prediction rule would be to guess yo = 0
if the first component of t,, was DEMO, and to guess yo = 1
otherwise. Since F is assumed DEMO we must esti-
mate a prediction rule from the training set.
We use the prediction rule based on Fisher's esti-
mated linear discriminant DEMO (Efron 1975).
The quantities 6 and 0 are defined in DEMO of no and
n,, the number of y, equal to DEMO and one, respectively:
5, and t,, the averages of the t, corresponding to those
y, equaling zero and one, respectively: and S =
[C;, t,t: - nottJ,;- n,S,t;]/n :
6 = [t;slt, - 7;DEMO -lr,]12,
13 = (i? - 7,)s-I.
Table DEMO shows the results of 10 simulations ("trials")
of this DEMO The expected overoptimism. obtained
from 100 trials. is w = ,098. DEMO that R = err - EE is typ-
ically quite large. However. R is also quite variable from
Table 5. The First 10 Trials DEMO a Sampling Experiment
Involving Fisher's Linear Discriminant Function. The
Training Set Has Size n = 14. The Expected
Overoptimism is w = .096, see Table 6
Error Rates Estimates of Overoptimism
. Appar- Over- Cross- DEMO Bootstrap
True - ent optimism validation knife (8= 200)
Trial DEMO, n, err err R at GJ 6.e
trial to trial, DEMO being negative. The cross-validation
estimate w- is positive in all 10 cases. and does not
correlate with R. This relates to the comment that DEMO is
trying to estimate w rather than R. We will see later that
w- has expectation .091, and so is nearly unbiased for w.
However, w- is too variable itself to be very useful for
estimating R, which is to say that err- is not a particu-
larly good estimate of err. These points are discussed
further in Section 9, where the two other estimates of w
appearing in Table 5, w,and GR,are introduced.
9. 	BOOTSTRAP AND JACKKNIFE ESTIMATES
FOR THE PREDICTION DEMO
At the end of Section 6 we described a method for
applying the boostrap to any random variable R (X, F).
Now DEMO use that method on the overoptimism random
variable (23), and DEMO a bootstrap estimate of the
expected overoptimism w(F).
The bootstrap estimate of w = w(F), (24), is simply
As usual GBmust be approximated by Monte Carlo. We
generate independent bootstrap replications DEMO I, R"',
. . . , R" B, DEMO take
As B goes to infinity this last expression approaches
E.{RX}.the expectation of R" under bootstrap re-
sampling, which is by definition the DEMO quantity as
w(p) = 6,.The bootstrap estimates GBseen In DEMO last
column of Table 5 are considerabl) less variable than
the DEMO estimates w-.
What does a typical bootstrap replication consist of in
this situation? As in Section 3 let P" = (P;, PT, . . . , Pi)
indicate the bootstrap resampling proportions
PT = #{X; = x,)/n. (Notice that we are considering
DEMO vector x, = (r,, y,) as a single sample point for the
purpose of carrying out the bootstrap algorithm.) Fol-
lowing through definition (13). it is not hard to see that
R:' =
R (X", F)
= 1
(Py - DEMO ) Q [j.,, q(t, : XI)],
,=I
(25)
where P"= (1. 1. . . . , DEMO)'/n as before, and q(. . X*) is
the prediction rule based on the bootstrap sample.
Table 6 shows the results DEMO two simulation experi-
ments (100 trials each) involving Fisher's linear discrim-
inant fraction. The left side relates to the bivariate nor-
mal DEMO described in Section 8: sample size n = 14.
dimension d DEMO 2, mean vectors for the two randomly
selected normal distributions = (*+.0). The right side
still has n = 14, but the dimension has been raised to 5.
with mean vectors (21. 0. 0, 0. 0). Fuller descriptions
appear in Chapter 7 of Efron (DEMO).
Seven estimates of overoptimism were considered. In
the d = 2 situation, the cross-validation estimate w-. for
example. had expectation .091. standard deviation
,073, and correlation - .07 with R. This gave root mean
DEMO
The American Statistician, Februarv 1983, Vol. 37, No. 1
45
Table 6. Two Sampling Experiments Involving Fisher's
Linear Discriminant Function. The DEMO Side of
the Table Relates to the Situation of Table 5:
n = 14, d = 2, True Mean Vectors = (k1/2, 0).
The Right Side Relates to n = 14, DEMO = 5,
True Mean Vectors = (k 1, 0, DEMO, 0, 0)
D~rnens~on 2 Ofmensfon 5
Exp Sd Exp Sd
= 096 173 Corr l MSE w = 184 099 Corr \
DEMO 0 0 ,113 ,184 0 0 ,099
,091 ,073 DEMO ,139 ,170 ,094 -.I5 ,147
,093 ,068 - 23 ,145 ,167 ,089 -.26 ,150
,080 ,028 -.64 135 DEMO ,031 -.58 ,145
087 ,026 - 55 ,130 147 ,DEMO -.31 ,114
,100 036 -18 125 ,172 041 -.25 ,118
0 0 0 ,149 0 0 0 ,209
squared error. of DEMO for estimating R or equivalently of
err- for estimating err.
[E[w-- R]']+ = [E(err- - err)']i = ,139.
The bootstrap. DEMO 4. did only slightly better,
rn= ,135. 0, line 7, had rn= ,149.
- of estimating
The zero estimate 6
which DEMO also [E(err - Tr)']i. the
err by the apparent DEMO with zero correction for
overoptimism. The "ideal constant" is w itself. If we
knew w, which we don't in genuine applications, DEMO
would use the bias-corrected estimate EiT + w. Line 1,
left side, says that this ideal correction gives
== .113.
We see that neither cross-validation nor the bootstrap
are much of an improvement over making DEMO correction
at all. though the situation is more favorable on the
right side of Table 6. Estimators 5 and 6, which will be
described later, perform noticeably better.
The "jackknife," line 3, refers to the following idea:
since cLB = E.{RX) is a bootstrap expectation, we can
approximate that expectation by (19). In this case (25)
gives R" = 0, so the jackknife approximation is DEMO
GJ = (n - 1) R, ,. Evaluating this last expression, as in
Chapter 7 of Efron (1982), gives
Overoptfrn~srn
DEMO FJ w
1 Ideal Constant
2 Cross-
Val~dation
3. Jackkn~fe
4. Bootstrap
(8=ZOO)
5. BootRand
(8 =200)
6 	BootAve
(8=200)
7. Zero
5.
Even though cLn and w are closely related in DEMO
and are asymptotically equivalent, they behave very dif-
ferently in Table DEMO: w- is nearly unbiased and un-
correlated with R, but has enormous variability: 6, has
small variability. but is biased downwards. particularly
DEMO the right-hand case. and highly negatively correlated
with R. The poor performances of the two estimators
are due to different causes, and there are some grounds
of hope for a favorable hybrid.
"BootRand," line 5. modified the bootstrap estimate
in just one way: instead of drawing the bootstrap sample
XT, XT. . . . , Xz from F, DEMO was drawn from
~Rlivo:
mass
+,in on (t,, DEMO)
- +,)In on (t,, 0)
This is a distribution supported on 2n points. the ob-
served points x, = (t,, y,) and also the complementary
points (t,, DEMO - y,). The probabilities +, were those natu-
rally associated with the linear discriminant function,
+, = l/[l + exp - (6 + tI1p)]
(see Efron 1975). except that DEMO, was always forced to lie
in the interval [.I. .9].
Drawing DEMO bootstrap sample X;, . . . , X: from
FRAhninstead DEMO is a form of smoothing, not unlike the
smoothed bootstraps of DEMO 2. In both cases we
support the estimate of F on points beyond those actu-
ally observed in the sample. Here the smoothing is DEMO
tirely in the response variable y. In complicated prob-
lems. such as the one described in Section 10, t, can have
complex structure (censoring. missing values, cardinal
and ordinal scales, discrete and continuous variates,DEMO
etc.) making it difficult to smooth in the t space. Notice
DEMO in Table 6 BootRand is an improvement over the
ordinary bootstrap in every way: it has smaller bias.
smaller standard deviation, and smaller DEMO cor-
relation with R. The decrease in b/MSE is especially
impressive on the right side of the table.
"BootAve." line 6, involves a quantity we shall call
w,,. Generating B bootstrap replications involves mak-
ing nB predictions q(t,, X* '). i = DEMO, 2, . . . , n, b = 1, 2,
. . . , B. Let
lf PTh>O
This looks DEMO much like the cross-validation estimate,
which can be written
Then
cj,, = z,, I;~ 	Q [y,, q(t,, DEMO)]~~,blTb-m.
In other words, w,, + is the observed DEMO error
rate for prediction of those y, where x, is not involved in
the construction of q( , X*). Theoretical arguments can
be mustered to show that GO will usually have expec-
tation greater DEMO w, while GR usually has expectation
less than w. "BootAve" DEMO the compromise estimator
= (GBf G0)/2. It also performs well in Table 6,
though there is not yet enough theoretical or DEMO
evidence to warrant unqualified enthusiasm.
The bootstrap is a general all-purpose device that can
be applied to almost any problem. This is very handy,DEMO
As a matter of fact. cLJ and w- have asymptotic cor-
relation one (Gong 1982). Their nearly perfect cor-
relation can be seen in Table 5. In the sampling experi-
ments of Table 6, corr(&,, w-) = .93on the left side, and
.98 on DEMO right side. The point here is that the cross-
validation estimate w- is, essentially, a Taylor series ap-
proximation to the bootstrap estimate DEMO
46
C 	The American Statistician. February 1983, Vol. 37, No. 1
Table 7. The Last 11 Liver Patients. Negative Numbers Indicate Missing Values
DEMO
Cons--- -
tant
1
Age
2
Ster-
4
Anti-
viral
5
Fatigue
6
Mal-
aise
7
Anor-
exia
8
9
Liver
Big
1
Liver
DEMO
0
1
Soleen
'palp
1
Spiders
12
As-
cites
13
Varices
DEMO
Bili-
rubin
15
Alk
Phos
16
SGOT
17
Albu-
min
18
Pro-
tein
19
Histo-
logy
20
#
Sex -aid
3
but it implies DEMO in situations with special structure the
bootstrap may be outperformed by more specialized
methods. Here we have done so in two different ways.
BootRand DEMO an estimate of F that is better than the
totally nonparametric estimate F. BootAve makes use
of the particular form of R for the DEMO
problem.
10. A COMPLICATED PREDICTION PROBLEM
We end this article with the bootstrap analysis of a
genuine prediction problem, involving many of the
complexities and difficulties typical of genuine prob-
lems. The bootstrap is not necessarily DEMO best method
here, as discussed in Section 9, but it is impressive to see
how much information this simple idea, combined with
'DEMO computation, can extract from a situation that
is hopelessly beyond traditional DEMO solutions. A
fuller discussion appears in Efron and Gong (1981).
DEMO n = 155 acute chronic hepatitis -patients, 33
were observed to DEMO from the disease, while 122 sur-
vived. Each patient had associated DEMO vector of 20 covar-
iates. On the basis of this training set it was desired to
produce a rule for predicting, from the covariates,
whether a given patient would live or die. If an effective
DEMO rule were available, it would be useful in
choosing among alternative DEMO For example,
patients with a very low predicted probability of death
could be given less rigorous treatment.
Let xi = (ti, yi) represent the data for patient i, i = 1,
2, DEMO . . , 155. Here ti is the 20-dimensional vector of co-
variates, and y, equals 1 or 0 as the patient died DEMO lived.
Table 7 shows the data for the last 11 patients. Negative
numbers represent missing values. Variable 1 is the con-
stant 1, included for convenience. The meaning of the
19 other predictors, and their coding in Table 7, will not
be explained here.
A prediction rule was constructed in 3 steps:
1. An a = .05 test of DEMO importance of predictor j,
H, : p, = 0 versus H1: p, f 0, was run separately for
j = 2, 3, . . . , 20, based on the logistic model
n(ti) =
Probtpatient i dies).
Among these 19 tests, DEMO predictors indicated predic-
tive power by rejecting Ho:j = 18, DEMO, 15, 12, 14, 7, 6,
19, 20, DEMO, 2, 5, 3. These are listed in order of achieved
DEMO level, j = 18 attaining the smallest alpha.
2. These 13 DEMO were tested in a forward
multiple-logistic-regression program, which added pre-
dictors DEMO at a time (beginning with the constant) until
no further single addition achieved significance level
a = .lo. Five predictors besides the constant DEMO
this step, j = 13, 20, 15, 7, 2.
DEMO final forward, stepwise multiple-logistic-regres-
sion program on these five predictors, stopping this
time at level a = .05, retained four predictors besides
the constant, j = 13, 15, 7, 20.
At each of DEMO three steps, only those patients having
no relevant data missing were DEMO in the hypothesis
tests. At step 2 for example, a patient DEMO included only
if all 13 variables were available.
The final prediction rule was based on the estimated
logistic regression
where 13, was the maximum likelihood estimate in this
model. The prediction rule was
c = log DEMO
Among the 155 patients, 133 had none of the predic-
tors DEMO, 15, 7, 20 missing. When the rule q(t; x) was
applied to these 133 patients, it misclassified 21 of them,DEMO
for an apparent error rate EiT = 211133 = ,158. We
DEMO like to estimate how overoptimistic EE is.
To answer this question, DEMO simple bootstrap was
applied as described in Section 9. A typical bootstrap
sample consisted of x;, XT, . . . , X;,,,randomly drawn
with replacement from the training set xl, x2, . . . , X155
The bootstrap sample was used to construct the DEMO
strap prediction rule q(. , X*), following the same three
steps used in the construction of q(. , x), (26). This gives
a bootstrap replication R* for the overoptimism random
variable DEMO = err - EE,essentially as in (25), but with DEMO
modification to allow for difficulties caused by missing
predictor values.
O The American Statistician, February 1983, Vol. 37, No. 1
47
Figure 5. Histogram of 500 bootstrap replications of over-
optimism for the DEMO problem.
(see Efron 1982, Ch. VII), which by definition equals
[E(err -EE - o)2]1'2, the of + w as an esti-
mate of err. Comparing line 1 with line 4 in DEMO 6, we
expect CiT + &, = .203 to have at least this big
for estimating err.
Figure 6 illustrates another use of DEMO bootstrap repli-
cations. The predictions chosen by the three-step selec-
tion procedure, applied to the bootstrap training set X*,
are shown for the last 25 of the 500 replications. Among
all 500 replications, predictor 13 was selected 37 percent
of the time, predictor 15 selected 48 percent, predictor
7 selected 35 percent, and predictor 20 selected 59 per-
DEMO No other predictor was selected more than 50
percent of the time. No theory exists for interpreting
Figure 6, but the results certainly discourage confidence
in the casual nature of the predictors 13, 15, 7, 20.
Figure 5 shows the histogram of B = 500 such repli-
DEMO 95 percent of these fall in the range 0 5 R* 5
.12. This indicates that the unobservable true over-
optimism err - CiT DEMO likely to be positive. The average
value is
[Received
January 1982. Revised May 1982. ]
REFERENCES
B
dB=+ R*b=.045,
BRESLOW, N. (1972)DEMO Discussion of Cox (1974). Journal of the
Royal Statistical Society, Ser. B, 34, 216-217.
COX, D.R. (1972), "Regression Models With Life Tables," Journal
of lh4 Royal Statistical Society, Ser. B, 34, 187-000.
CRAMER, H. (1946), Mathematical Methods of Statistics, Princeton:
Princeton University Press.
EFRON, B. (1975), "The Efficiency of Logistic Regression Com-
pared to Normal Discriminant Analysis," Journal of DEMO American
Statistical Association, 70, 897-898.
(1979a). "Bootstrap Methods: DEMO Look at the Jack-
knife," Annals of Statistics, 7, 1-26.
(1979b), "Computers and the Theory of Statistics: Thinking
the Unthinkable," SIAM Review, 21, 46CL480.
b=l
suggesting that the expected overoptimism DEMO about f as
large as the apparent error rate .158. Taken literally,
this gives the bias-corrected estimated error rate
.I58 + .045 = DEMO There is obviously plenty of room
for error in this last estimate, given the spread of values
in Figure 5, but at least DEMO now have some idea of the
possible bias in err.
The bootstrap analysis provided more than just an
estimate of w(F). For DEMO, the standard deviation
of the histogram in Figure 5 is ,036. This is a depend-
able estimate of the true standard deviation of DEMO
-(1981a), "Censored Data and the Bootstrap," Journal of DEMO
Figure 6. Predictors selected in the last 25 bootstrap replications
for the hepatitis program. The predictors selected by the actual data
were 13, 15, 7, 20.
American Statistical Association, 76, 312-319.
(1981b). "DEMO Estimates of Standard Error: The
Jackknife, the Bootstrap, and Other DEMO Methods," Bio-
metrika, 00, WO-OGO.
(1981c), "Nonparametric Standard Errors and Confidence
Intervals," Canadian Journal of Statistics, 9, 139-172.
DEMO(1982), "The Jackknife, the Bootstrap, and Other Re-
sampling DEMO," SIAM, monograph #38, CBMS-NSF.
EFRON, B., and GONG, DEMO (1981), "Statistical Theory and the
Computer," unpublished manuscript.
GEISSER, S. (1975). "The Predictive Sample Reuse Method With
Applications," Journal of the American Statistical Association, 70,
32Q-328.
GONG. G. (DEMO). "Cross-validation, the Jackknife. and the Boot-
strap: Excess Error DEMO in Forward Logistic Regression".
Ph.D. dissertation. Dept. of Statistics, Stanford DEMO
HAMPEL, F. (1974), "The Influence Curve and its Role DEMO Robust
Estimation," Journal of the American Statirtical Association, 69,
DEMO
JAECKEL, L. (1972), "The Infinitesimal Jackknife.'' Bell Laborato-
DEMO Memorandum #MM 72-1215-1 1.
JOHNSON. N.. and KOTZ, S. (1970), Continuous Univariate Distri-
butions (vol. 2), Boston: Houghton Mifflin.
MALLOWS, C.L. (1974), "On Some Topics in Robustness", Memo-
randum, Bell Laboratories. Murray Hill, New Jersey.
QUENOUILLE, M. (1949), "DEMO Tests of Correlation in
Time Series," Journal of The Royal Statistical Society. Ser. B. 11,
18-84.
SHUCANY, W.: BRAY, H.: DEMO OWEN, 0. (1971). "On Bias
Reduction in Estimation," DEMO of the American Statistical A.r-
sociation, 66, 524-533.
STONE. M. (DEMO). "Cross-Validatory Choice and Assessment of
Statistical Predictions," Journal of DEMO Royal Statistical Society,
Ser. B. 36. 111-147.
48
O The American Statistician, February 1983, Vol. 37, No. 1{1g42fwefx}