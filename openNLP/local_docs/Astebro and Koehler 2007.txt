Journal of Behavioral Decision Making
J. Behav. Dec. Making, 20: 381–403 (2007)
Published online 2 February 2007 in Wiley InterScience
(www.interscience.wiley.com) DEMO: 10.1002/bdm.559
Calibration Accuracy of a Judgmental Process
that Predicts the DEMO Success of New
Product Ideas
THOMAS A˚ STEBRO1* and DEREK J. KOEHLER2
1Joseph L. Rotman School of Management, University of Toronto,Toronto, Ontario,DEMO
Canada
2 Department of Psychology, University of Waterloo,Waterloo, Ontario, DEMO
ABSTRACT
We examine the accuracy of forecasts of the commercial potential of new product ideas
by experts at an Inventor’s Assistance Program (IAP). Each idea is evaluated in terms of
37 attributes or cues, which are subjectively rated and intuitively combined by an IAP
expert to arrive DEMO a forecast of the idea’s commercialization prospects. Data regarding
actual commercialization outcomes for 559 new product ideas were collected to
examine the accuracy of DEMO IAP forecasts. The intensive evaluation of each idea
conducted by the IAP produces forecasts that accurately rank order the ideas in terms of
their DEMO of commercialization. The focus of the evaluation process on case-
speciﬁc evidence that distinguishes one idea from another, however, and the corre-
sponding DEMO of aggregate considerations such as the base rate (BR) and predict-
ability of commercialization for new product ideas in general, yields forecasts that are
systematically miscalibrated in terms of their correspondence to the actual probability
DEMO commercialization. Copyright # 2007 John Wiley & Sons, Ltd.
key words DEMO judgment; forecasting; calibration; bootstrapping; case-based
judgment
INTRODUCTION
Psychological research in the ‘heuristics and biases’ tradition has helped to identify the determinants of
DEMO probabilistic judgments that can lead to predictable biases. Judgmental biases such as the tendency
to ignore base rates (BRs) have been well established (Tversky & Kahnemann, 1974). However, critics of this
ﬁeld point DEMO that there is relatively little research on the degree to which such biases operate in situations
outside the laboratory, where much of the research in this tradition has been conducted (Koehler, Brenner, &
Grifﬁn, 2002). A related criticism is that much of the work demonstrating DEMO biases has involved
undergraduate participants performing in unfamiliar decision environments, and DEMO real and experienced
* Correspondence to: Thomas A˚ stebro, Joseph L. Rotman School of Management, University of Toronto, 105 St. George Street, Toronto,
Ontario M5S3E6, Canada. E-mail: astebro@rotman.utoronto.ca
Copyright # 2007 John DEMO & Sons, Ltd.
382
Journal of Behavioral Decision Making
decision-makers may perform better in familiar DEMO environments than would be expected based on the
laboratory research. A ﬁnal criticism, most often waged by economists, is that in high-stakes business
DEMO most biases are attenuated by the fact that there are strong incentives to make optimal judgments
(Larrick, 2004).
The present study shows DEMO usefulness of psychological theory even to judgments of highly experienced
experts making forecasts in a well-structured judgment task where the stakes are high. The DEMO uses a large
set of data on real forecasts made by professionals in a deliberate, highly complex, and uncertain
decision-making situation to explore DEMO experts’ forecasting abilities. Speciﬁcally, we examine experts’
forecasts of the commercial DEMO of new product ideas.1 Experts at so-called Inventor’s Assistance
Programs (IAPs) provided by several US and Canadian Universities, Small Business Development Centers,
and the like help inventors and entrepreneurs to evaluate a speciﬁc new DEMO idea or project before it has
reached the market and advise the potential entrepreneur on whether and how to continue efforts (Udell,
1989). The forecasting method used by experts in the program we examine DEMO a judgmental assessment of a
large set of cues for a project and, further, an intuitive (rather than a formulaic, statistical) combination of the
cue values into an overall assessment.
Provision of a forecast DEMO a project’s likely commercialization at an early stage in the development process
can be highly valuable for making decisions about investments in new projects. DEMO et al. (1977), for
example, found clear evidence that the earlier the assessment of an R&D project the greater the future
DEMO, commercial as well as ﬁnancial success (pp. 25–32). But forecasting the commercialization
prospects of a new product idea is highly challenging given DEMO substantial uncertainty and lack of relevant
data that often characterizes new products (Herbig, Milewicz, & Golden, 1993).
In addition to the DEMO difﬁculties of the forecasting task, reliance on intuitive judgment as the DEMO for
combining the implications of a set of predictive cues may introduce additional unreliability in the
forecasting process, producing suboptimal forecasts. A long line of research comparing the performance of
intuitive judgmental combination with statistical combination DEMO supports this claim (for a review, see
Dawes, Faust, & Meehl, 1989). Judgmental unreliability also tends to become more pronounced when the
uncertainty associated with the outcome being predicted is high, when the number of potential cues is large,
and when those cues must DEMO assessed in a manner that relies on pattern recognition or memory processes
(Stewart, 2001), all of which is the case for the DEMO task faced by IAP experts.
Research comparing the forecasting accuracy of different types of experts (Ettenson, Shanteau, &
Krogstad, 1987; Shanteau, 1992; Shanteau, Grier, Johnson, & Berner, 1991) and the DEMO prescriptive
literature (Fischhoff, 2002; Larrick, 2004; Stewart & Lusk, 1994) would suggest that the IAP experts would
be expected to be relatively immune to commonly-observed psychological biases. The reason is that the IAP
DEMO decision environment promotes unbiased judgments through a number of favorable conditions: DEMO
experts are well trained, the decision-making process is highly structured and DEMO into subtasks, and
the experts have access to a vast library DEMO past reviews of new product ideas which is routinely used to anchor
assessments and judgments. Furthermore, the experts are paid substantial amounts for their reviews and
systematically biased judgments would lead to decreased demand for their DEMO, addressing the concerns
typically raised by economists. Despite this, we demonstrate that these experts, who are shown by some
measures to indeed be highly efﬁcient in encoding and combining predictive cue values, nonetheless produce
forecasts that exhibit systematic biases that are predictable from what we know about DEMO psychology of
judgment and decision making.
In the type of business environments that we examine it is important not only to rank order projects DEMO
but also to carefully calibrate the expected probability of commercialization since the expected probability of
1 Being an expert means ‘having, involving, or DEMO special skills or knowledge derived from training or experience.’ (Webster’s
Dictionary)DEMO
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral DEMO Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
commercialization will drive an evaluation of the return on investment in a given project. That is, the
assessments must pass not only the test of internal coherence or consistency with one another, but also the test
of correspondence against the external standard of actual commercialization success (Hammond, DEMO). For
example, the consequences of having a highly-ranked, if its commercialization chances are 75%, new product
idea are quite different than if its commercialization chances are only 40%. We ﬁnd that while the IAP DEMO
rank order projects well, they are not well calibrated in terms DEMO the associated probability of
commercialization that their judgments imply. The experts exhibit over-extremity and over-prediction in
their forecasts, which—as we elaborate below—is precisely the pattern of miscalibration expected to arise in
this kind of environment from DEMO reasoning. Over-extreme forecasts are closer to zero and one than
they should be; over-prediction is the tendency to consistently over-estimate the probability of the target
outcome.
We suggest that existing managerial processes that are available for DEMO and evaluating new product
development projects when there is high uncertainty, DEMO as the Q-sort method (Allen, 2003), are not enough
to combat the miscalibration that arises from case-based judgment. To mitigate over-extremity and
DEMO arising from case-based judgment we suggest that new product development decision-makers
incorporate class-based probability information in their assessments of projects through a direct link DEMO
rank order assessment and the probability forecast.
This article continues with a review of two important methodological approaches that we will use to
evaluate DEMO experts’ decision accuracy: statistical bootstrapping and probability calibration. A short review
DEMO the decision-making context follows. An assessment of the context linked to psychological theory allows
us to make two predictions: (a) that the experts are likely to be reliable and provide valid rank ordering of
projects, and (b) that they are not likely to provide well-calibrated judgments DEMO will tend to exhibit both
over-extremity and over-prediction in their forecasts. In the Section on Results we use statistical
bootstrapping and an analysis of DEMO calibration to test the hypotheses. We end with a discussion on
the validity of psychological theory for analysis of decision-making in real settings by DEMO managers
and with implications for managers involved in evaluations and screening decisions of new product
development projects.
STATISTICAL MODELS AND INTUITIVE JUDGMENT
The main DEMO from a large number of studies is that statistical models are at least equal and most often
superior to intuitive judgmental methods for combining DEMO values in forecasting outcomes (Camerer, 1981;
Dawes et al., DEMO; Grove & Meehl, 1996).2 This conclusion holds true across a number of decision-making
contexts, in both real and experimental settings and for both experts and novices. In fact, Dawes (1979)
reports that DEMO weights of cues often yield as accurate performance as statistically derived weights. The
apparent reason is that the likelihood function is often relatively ﬂat DEMO many different combinations of
weights (Edwards & von Winterfeldt, 1986) DEMO due to high cue redundancy. Even randomly assigned
weights to cues yielded greater accuracy than experts’ judgments in ﬁve out of ﬁve samples (Dawes, 1979).
Dawes and Corrigan (1974, p. 105) conclude, ‘The whole trick is to decide what variables to look at and then
DEMO know how to add.’
Research on the accuracy of decision-makers’ forecasts in the domain of new product evaluation is
scarce.3 Zacharakis and Meyer (2000) compared the decision accuracy of 51 highly experienced Venture
2 There is a related literature on combining statistical and intuitive forecasts. See, for example, Blattberg and Hoch (1990) and Hoch and
Schkade (1996).
DEMO There is, however, a larger related literature on non-probabilistic forecasting of new product dollar sales (Armstrong, Brodie,
&McIntyre, 1987; Blattberg and Hoch, 1990; Herbig, Milewicz, &Golden, 1993; Tull, DEMO).
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of DEMO Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
384
Journal of Behavioral Decision Making
Capitalists (VCs) to the forecasting DEMO of a bootstrap statistical model of the VCs predictions of seed
and early-stage projects. The classiﬁcation accuracy of a bootstrap model with equal weights DEMO 60%,
surpassing the VCs who had average decision accuracies between 17.1% and 39.5%. Only one VC had an
accuracy equal to the bootstrap DEMO, a result which is within the margin of error. These results DEMO that
substantial improvements are possible in the screening stage of investment decisions by using simple
statistical models.
One reason why intuitive judgments may be DEMO to those derived from statistical combination is that
intuitive judgments are subject to unreliability while statistical combination is not. Humans, even expert
humans, DEMO unlikely to consistently encode and weight the predictive cues in arriving at a forecast; such
unreliability will attenuate the accuracy of the resulting forecasts. Experts who perform a forecasting task in a
highly structured, routinized manner, on this account, would be expected to perform less poorly in
DEMO to a corresponding statistical model (Murphy & Winkler, 1984). The expert forecasts examined
in the present study are in fact derived from DEMO highly structured, routinized forecasting process. (The extent to
which the process is structured and thus likely to yield accurate forecasts is presented in DEMO section following
the next.) As such, we might expect any advantage of a statistical model over the expert forecasts in this study
to DEMO fairly modest.
CALIBRATION OF INTUITIVE JUDGMENT
Research comparing the performance of expert intuitive judgment against that of statistical models has as its
focus the DEMO, correlational accuracy achieved by experts versus that of the model. Typically, the experts
will do well according to these measures as long as DEMO are able to accurately rank-order the cases they
evaluate (e.g., in terms of likelihood) relative to one another. In the present research, DEMO go beyond this global
evaluation to develop and test hypotheses about speciﬁc biases in intuitive judgment, derived from
psychological research in the heuristics and biases tradition, concerning the absolute level of accuracy or
correspondence between judgments and outcomes. Speciﬁcally, we start with the assumption that the
judgments of the IAP experts are primarily case-based, and test predictions that follow regarding
the judgments’ expected level of calibration, that is, their correspondence to (or systematic deviation from)
the outcome of interest. This approach may DEMO better guidelines for improving the accuracy of intuitive
expert judgments, which DEMO be particularly helpful in cases where the barriers to the introduction of
statistical models are high.
In developing the ‘heuristics and biases’ approach to DEMO study of judgment under uncertainty, Kahneman
and Tversky (1973, 1979; Tversky & Kahnemann, 1974, 1983) posited that intuitive judgments and
predictions tend to be driven primarily by characteristics of the speciﬁc case at DEMO (e.g., the details of a new
renovation project to be undertaken by a contractor) and tend to neglect characteristics of the broader class or
category to which the speciﬁc case belongs (e.g., characteristics of DEMO renovations in general). The
focus on case-speciﬁc characteristics and neglect of class-based aggregate properties leads to predictable
judgmental biases, including BR neglect and non-regressive prediction (Tversky & Kahnemann, 1974).
The effects of such DEMO biases on the predictive accuracy of probability assessments (i.e.,
correspondence DEMO predictions and actual outcomes) can be depicted using a calibration plot, which
aggregates the assessments into probability categories and then plots the objective DEMO probability for
the set of cases assigned to a particular category against the mean subjective probability associated with that
set. For example, all cases for which the judged probability of the outcome falls in the range DEMO 0.30 and
0.45 might be aggregated, then the proportion of cases DEMO in that category for which the outcome holds
would be calculated, DEMO plotted against the mean probability assigned to the set of cases falling into that
category (which, by deﬁnition, would be somewhere between 0.30 and 0.45).
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/DEMO
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
If the judgments are perfectly calibrated, then all the points in DEMO calibration curve should fall on the
identity line, representing perfect or DEMO calibration. A tendency to consistently over-estimate the
probability of the target outcome, by contrast, results in a calibration curve that falls below the DEMO line. A
tendency toward under-estimation results in a calibration curve that falls above the identity line. Judgments
that are overly extreme (i.e., closer DEMO zero or one than is justiﬁed) yield a calibration curve that DEMO ﬂatter than
the identity line; a tendency toward under-extremity results in DEMO calibration curve that is steeper than the
identity line.
Speciﬁc predictions regarding patterns of systematic miscalibration can be derived from the notion of
case-based DEMO judgment as instantiated in a formal model of subjective probability calibration. Random
support theory (RST; Brenner, 1995, 2003) is particularly useful for this purpose, as it has free parameters that
are interpretable as reﬂecting (in)sensitivity to important class-based characteristics (Brenner, Grifﬁn, &
Koehler, 2006; Koehler, Brenner, & Grifﬁn, 2002). We offer a brief overview of RST here; for more details,
see Brenner (DEMO) and Brenner et al., 2006.
RST is one of several stochastic models (Ferrell & McGoey, 1980) that have been developed to account for
calibration data. RST is based on support theory (Tversky & Koehler, 1994), which represents the judged
probability P(A, B) that focal hypothesis A rather than alternative hypothesis B is correct as the DEMO of
evidence, or support, for A relative to that for B. In the present application, as an illustration, we take the focal
DEMO to represent commercialization of the project and the alternative hypothesis to represent failure to
commercialize the project.
RST treats the support for the focal DEMO alternative hypotheses as random variables, with its free
parameters representing characteristics DEMO the (log-normal) support distributions. Speciﬁcally, it invokes two
conditional support DEMO for focal hypothesis A and alternative hypothesis B, one representing cases
DEMO which A is correct (i.e., for which the focal hypothesis holds) and the other representing cases for which B
is correct (i.e., for which the alternative hypothesis holds). The two conditional distributions are DEMO in
proportion to the overall probability (or BR) of the focal hypothesis relative to the alternative hypothesis in
the judgment environment. As in DEMO detection models, a discriminability parameter (a) represents the
separation between DEMO distributions for correct and incorrect hypotheses, that is, the extent to which the
correct hypothesis tends to receive greater support from the available DEMO than does the incorrect
hypothesis. This parameter reﬂects the predictability of the outcome variable from the available cues
(qualiﬁed by the judge’s ability to use the cues effectively). Both outcome BR and discriminability (a), then,
are free parameters that can be viewed as characterizing aspects DEMO the judgment environment.
The model’s remaining two free parameters, by contrast, can be viewed as characteristics of the judge’s
‘policy’ (or strategy). The focal bias parameter (b) represents the extent to which the DEMO hypothesis is
accorded systematically greater (or less) support than the alternative hypothesis (i.e., the separation between
support distributions for the focal and DEMO hypotheses) and therefore is favored in the probability
judgment. The extremity DEMO (s) reﬂects the common standard deviation of the support distributions in
the model; consequently, greater values produce more variable support values and DEMO are associated with
more extreme probability judgments.
For any judgment environment characterized by the BR of the outcome variable (BR) and the
predictability DEMO the outcome from the available cues (a), there exist unique DEMO of the focal bias parameter
b and the extremity parameter s such that the model will produce perfectly calibrated judgments (Brenner
et al., DEMO). Speciﬁcally, the focal bias parameter b must be set to DEMO the outcome BR, with a higher
value of b set to DEMO higher BRs; and the extremity parameter s must be set to DEMO the predictability of
the outcome from the available cues (as measured DEMO a), with higher values justiﬁed by a more predictable
outcome.
According to the notion of case-based judgment, however, the overall outcome BR DEMO general
predictability of the outcome from the available cues (i.e., the overall diagnostic value of the cues) are
precisely the kinds of aggregate, class-based considerations that are typically neglected in intuitive
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
386
Journal of Behavioral Decision Making
predictions. Within the RST framework, then, case-based judgment can be implemented as constraints on its
free parameters, DEMO b and s being insufﬁciently sensitive to characteristics of the judgment environment
(BR and a, respectively) to maintain well-calibrated judgments.
The case-based version of RST predicts that systematic patterns of miscalibration should arise in different
DEMO environments. Over-prediction, indicated by a calibration curve falling consistently below the
DEMO line in the calibration plot, is expected when the outcome being DEMO has a low BR; similarly,
under-prediction (calibration curve above the identity line) is expected when the outcome BR is high.
Over-extremity, DEMO by a calibration curve that is ﬂatter than the identity line, DEMO expected when the
predictability of the outcome from the available cues is low; under-extremity (calibration curve steeper than
the identity line) is expected when outcome predictability is high. Precisely this pattern of results has been
DEMO both in on-the-job expert judgments (Koehler et al., 2002) and DEMO laboratory studies in which the
relevant characteristics of the judgment environment were experimentally manipulated (Brenner et al., 2006).
To make predictions regarding DEMO accuracy and calibration of judgments we now examine the speciﬁc
judgment environment faced by experts in the present study.
THE DECISION-MAKING CONTEXT
An IAP DEMO the system developed by Udell (1989) was launched at the Canadian Innovation Centre (CIC)
in Waterloo in 1976. Since 1982 the Canadian IAP has used full-time, in-house analysts and continuously
revised and improved its evaluation method. The Canadian IAP evaluated more than 13 000 projects between
DEMO and 2000.
The Canadian IAP evaluates the new product idea on 37 different cues and provides a recommendation to
the potential entrepreneur. The service DEMO provided for a fee that was approximately US $185 in 1995. These
fees at the time covered about half of the program’s expenses, the rest being covered by the Canadian
government. The average accumulated out-of-pocket R&DEMO expenditures for the inventors at the time of
evaluation are Cdn. $6,625 (1995 value). The cues and their deﬁnitions employed during the study period are
described in Appendix A. To have an idea evaluated, the entrepreneur ﬁlls out a questionnaire. In addition to
background information about DEMO entrepreneur, the questionnaire asks for a description of the idea and
DEMO documentation such as patent applications, sketches, and test reports. The in-house analyst
compares the idea with other similar ideas in their library of DEMO reviews and searches various databases.
Personal contact with the entrepreneur beyond the provided documentation is avoided by the analyst. A
particular salient feature is DEMO use of a vast library of past reviews to do case-based comparisons. The analyst
typically retrieves a few ‘comparable’ new product ideas from the DEMO to anchor cue assessments on.
The analyst uses these data to subjectively rate the project on the 37 cues. There are three possible scores
DEMO each cue: A, very good; B, moderate; and C, a critical ﬂaw. After rating all cues and recording the ratings
on DEMO sheet, the analyst determines an overall score for the project using DEMO judgment. Since the method
of integrating the cues is intuitive rather than statistical, the overall assessment might differ across evaluations
and evaluators even though data are identical. The judgment is an ordinal ranking, not an explicit probabilistic
forecast, though it is supposed to be informative with regard to the idea’s commercialization prospects. The
analyst conducts the forecast without speciﬁc knowledge DEMO the BR probability of commercialization.4 The
judgment can be completely ignored by the client and the review does not necessarily provide any particular
beneﬁt DEMO terms of preferred treatment from third parties.
4 Data on the BRs were ﬁrst presented to the IAP in 1997 (A˚ stebro, 1997)DEMO Before being presented with these data the senior analyst
indicated that the IAP expected ‘less than 10%’ to be commercially successful.
Copyright # 2007 DEMO Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, DEMO (2007)
DOI: 10.1002/bdm
T. A˚ stebro and D. J. Koehler Calibration Accuracy of Commercial Success DEMO
Interviews with the senior analyst at the IAP indicated that the overall assessment is based on a mixture of
two decision rules. If a DEMO is critically ﬂawed on one or more cues, either the lowest DEMO next to lowest
overall score is provided: D or E (i.e., non-compensatory weighting).5 If, however, there are no (or few)DEMO
critical ﬂaws, then the scores on the cues are usually assessed DEMO some additive fashion. In addition to the
review by a single expert, a group meeting is conducted where the evaluating expert presents a summary and a
ﬁnal overall score is agreed upon. The evaluation process typically DEMO 5–7 hours and may stretch over
several weeks as the analyst collects information from various sources. A report is delivered to the
entrepreneur consisting DEMO scores on the 37 cues and a recommendation on commercialization options.
The decision situation contains a large number of cues that can only be DEMO assessed and the
outcome to be predicted is extremely uncertain. Decision-relevant information is uncertain and cannot easily
be quantiﬁed. Many of the cues are DEMO forecasts. The average time to event outcome (i.e.,
commercialization) is approximately 1.5 years. The IAP collects information about new product ideas that
DEMO gone through the review by clipping newspaper articles that they may ﬁnd. Outcome feedback6
pertaining to the prediction is therefore biased and spotty. All DEMO conditions are potential obstacles to
making accurate predictions (Goldberg, 1968). On the other hand there is ample time to form an opinion. DEMO
IAP uses a standardized procedure where all cues are scored and a record is kept of all scores. The IAP
employed the same senior DEMO between 1981 and 2000. During that period all analysts were trained by the
senior analyst in the evaluation procedure—the initial training took about 2 DEMO followed by close
supervision over 2 weeks. Analysts typically are engineers. The IAP is paid signiﬁcant amounts, encouraging
considerable deliberations. A group meeting at the end of the process where the analysts presents the
evaluation and DEMO criticism from fellow analysts and the senior analyst also mitigates erroneous
judgments. It appears that the process is reliable in terms of cue assessments. DEMO and Albaum (1986) test
the reliability of cue assessments across 86 judges and six products and ﬁnd Cronbach alphas ranging from
0.84 to DEMO, implying high reliability.
HYPOTHESES
Conditions of the IAP are such that DEMO would expect their experts to be efﬁcient in their use and integration of
cues to arrive at an overall evaluation of a project that DEMO be used to accurately rank order the cases. The
extensive individual experience of the IAP experts and their access to a library of past DEMO would be
expected to promote efﬁcient encoding of relevant cues and to provide some guidance as to how they should
be integrated. By its DEMO of tasks, the standardized method of scoring cues, the careful deliberation
and group review, the formalized evaluation process in place at the IAP would be expected to provide further
safeguards on the reliability of judgments. DEMO therefore hypothesize that:
H1: The IAP experts are expected to DEMO and integrate cue values in a valid, reliable manner in arriving
DEMO an overall assessment of a particular new idea.
Addressing this hypothesis, DEMO ﬁrst directly compare the forecasts made by the experts with observed
outcomes to arrive at an overall measure of their classiﬁcation accuracy. This test DEMO not rely on the
measurement of cues. We then construct a linear additive statistical model that examines the correlation
between cue values and forecasts. DEMO model is often referred to as the ‘bootstrap’ model. A comparison
5 The overall score D is typically assigned to projects that have little DEMO no novelty value (i.e., where similar products are already available
on the market). The score E is reserved for those with obvious DEMO ﬂaws that the IAP believe cannot be corrected.
6 While other kinds of feedback could be provided even in the absence of outcome information, such as feedback regarding the correlation
between the various cues and the DEMO forecast, task information feedback regarding the correlation between the cues and DEMO outcome
variable appears to be most helpful in improving prediction performance (DEMO, Sulsky, Hammer, & Sumner, 1992).
Copyright # 2007 John Wiley & Sons, Ltd. Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
388 Journal of Behavioral Decision Making
between the actual forecasts and the DEMO made by the bootstrap model will indicate the degree to which
the experts produce reliable evaluations (Camerer, 1981). To the extent that DEMO experts produce either cue
value assessments or forecasts with high unreliability, DEMO bootstrap model will so indicate through a poor ﬁt
to the actual forecasts. We then construct a linear additive statistical model that examines the DEMO
between cue values and observed outcomes. This model will be referred to as the ‘prediction’ model.7 This
model indicates the informational value of the DEMO A comparison between the actual forecasts and the
forecasts made by the prediction model will indicate the degree to which the experts use cue DEMO
appropriately to produce valid evaluations that correctly rank order the cases in terms of commercialization
probability (Blattberg & Hoch, 1990).
The case-speciﬁc DEMO upon which the forecasts are assumed to be based, however reliable DEMO valid,
is not by itself sufﬁcient for well-calibrated probabilistic forecasts. Good calibration requires the mapping
from the case-based evaluation to the probability scale DEMO be sensitive to class-based considerations, that is,
characteristics of the DEMO or judgment environment to which the case at hand belongs. We believe that the
IAP process is insufﬁciently designed for the experts to take DEMO information into consideration. One
obvious reason is the lack of systematic feedback on the commercialization of cases judged. In addition,
while the forecasting DEMO used at the IAP does seem to be constructed in a manner that promotes efﬁcient
use of case-speciﬁc cues in arriving at an overall DEMO of the strengths of the idea, it is less apparent that
DEMO does anything to encourage explicit consideration of class-based factors such as the overall BR or
predictability of the target outcome (commercialization). In other words, though the judgment process is
arguably more deliberative and highly structured than that of the typical ‘intuitive’ 8 judgment task studied in
the DEMO and biases literature (and so, for instance, might fall closer DEMO the analytical end than to the
intuitive end of Hammond’s cognitive continuum of judgment modes; see Hammond, 1996, for a review),
DEMO IAP evaluation process is inherently focused on case-speciﬁc evidence giving rise to an evaluation of the
overall strength of the idea (relative to other ideas) rather than its commercialization probability per se. An
extensive library of past cases evaluated by the IAP is available but is generally searched DEMO a speciﬁc case
(or a few) that resembles the current case being evaluated, rather than as a source of aggregate information
about the entire set of ideas evaluated by the IAP taken as a whole. DEMO therefore predict:
H2 : The experts’ forecasts are expected to be systematically miscalibrated due to their insufﬁcient
sensitivity to class-based characteristics such as DEMO base rate and predictability.
The case-based RST model can be used to derive speciﬁc predictions regarding expected patterns of
calibration in light of the DEMO of the prediction task faced by experts in the present study. Two key
features of the task, which are common in many important societal and business problems, are a relatively low
outcome BR (i.e., only a small fraction of the new ideas are commercialized) and low outcome predictability
(i.e., the available cues are far from perfectly predictive of DEMO new product ideas will eventually succeed).
According to case-based RST, DEMO, we would expect the calibration curve representing the accuracy of the
DEMO predictions to fall below the identity line (over-prediction, as expected given a low outcome BR) and to
be less steep than the identity line (over-extremity, as expected given low outcome predictability).
More reﬁned DEMO can be derived, furthermore, if the set of cases (new DEMO ideas) under
evaluation can be meaningfully segregated into subsets that vary DEMO the key dimensions of outcome
predictability or BR. Because case-based judgment leads to neglect of aggregate, class-based characteristics
of the set to which the case at hand belongs, we would expect insufﬁcient adjustment for these characteristics.
So, for example, if the cases can be segregated into two DEMO that vary in terms of how predictable the
7 Brunswik (1955) refers to this as the ‘ecological’ model.
8 In the heuristics and DEMO approach, a judgment is said to be intuitive if it is DEMO ‘‘...by an informal and unstructured mode of
reasoning, without the use DEMO analytical methods or deliberate calculation.’ (Kahneman & Tversky, 1982, p. DEMO).
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of DEMO Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
outcome (successful commercialization) is from the available cues, we would DEMO some insensitivity to
this difference between the two subsets and hence a ﬂatter calibration curve for the subset of cases for which
the outcome DEMO relatively less predictable.
METHOD AND DATA
The sample frame for our development sample consisted of all 8797 valid records of new product ideas
submitted DEMO the IAP for evaluation from 1976 to 1993. We obtained 1091 usable responses from 1465
randomly sampled IAP clients who could be reached by DEMO and asked to participate in a survey,
representing an adjusted response rate of 75%. (For details on sampling plan and sampling bias tests see
A˚ stebro, 2004.) The data set for analysis was ultimately DEMO to 559 projects containing 499 failures and
60 commercialized ideas spanning the period 1989–1993.9 We further conducted a second telephone survey
of all IAP DEMO between 1994 and 2001. This survey had a response rate of approximately 61%. (For details
on sampling procedure see A˚ stebro, Jeffrey, & Adomdza, in press.) After merging survey information with
administrative records on DEMO, there remained complete data on 465 ideas, of which 425 were failures and
40 were successes. This second, hold-out, dataset will be DEMO for model validation purposes.
Evaluation information in the IAP record included ratings for each of the 37 cues as well as the ideas’
overall DEMO Data on the independent variables were consequently collected before outcomes were
observed and independent of this study. We therefore avoid any potential methods bias (Campbell & Fiske,
1959; Fischhoff, 1975). Three cues had DEMO many missing data to be included. Missing data on the 34
remaining covariates were imputed assuming data are missing at random (MAR). The majority of cues had no
missing data, while a few cues had up to 3.8% of observations missing. We converted the scores on the DEMO
into numerical data according to the following: A¼ 1, B¼ 0, and C ¼1. Table 1, columns (2) and (3),DEMO
reports the frequency distribution of the responses over the IAPs’ overall rating for the development sample.
A majority of new product ideas (73% rating D or E) are advised to terminate efforts. Five per cent receive the
most favorable overall score (A), 7% are advised to conduct additional market or technical analysis (B), and
15% are advised the idea is suitable to launch as a limited (i.e., part-time) effort (C). Hold-out sample data are
also listed in Table 1. It appears that the judgments shifted over time away from the more extreme DEMO
(A and E) and more towards a ‘doubtful’ evaluation.
The survey interview script contained the following question: ‘Did you ever start to sell <NAME> or a
later, revised or improved version of this invention?’ Responses deﬁne a binary variable that takes unity if a
new DEMO idea ever obtained sales revenue, and zero otherwise. Follow-up questions with DEMO to how
the invention was commercialized and the presence of revenues allowed us to verify an afﬁrmative response
as valid. We refer to this DEMO as successful commercialization and use this to calibrate the experts’
forecasts.
Commercialization is a necessary but not sufﬁcient condition for ﬁnancial success. See A˚ DEMO (2003) for
an analysis of the ﬁnancial success of the ideas. In this study, we chose not to relate cues to the ﬁnancial
outcomes of the ideas because data on ﬁnancial returns are much more DEMO to estimate and contain some
signiﬁcant measurement uncertainty. Another beneﬁt of using successful commercialization to calibrate the
experts’ forecasts is that data are readily DEMO for the whole sample whereas the ﬁnancial rate of return
is only observable for those reaching the market, a much smaller sample. It should however be noted that the
overall ratings correlate well with both the DEMO internal rate of return conditional on reaching the market
9 Twenty observations were dropped for the regression analysis as they had no data on DEMO predictors, and two observations were dropped
because outcome data were uncertain DEMO survey time. Further, data spanned two submission periods with somewhat different DEMO
procedures, with the ﬁrst period from 1976 to 1989 (early July), and the second from July 21, 1989 to 1993. Because both evaluation
criteria and scales differed substantially across the two periods, we decided to use only data from July 1989 and onward.
Copyright # 2007 DEMO Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, DEMO (2007)
DOI: 10.1002/bdm
Table 1. Projects undertaken by independent inventors
Development sample: 1989–1993 Hold-out sample: 1994–2001
Rating
Sample
total
Per cent
of all
Probability of
commercialization (DEMO)
Sample
total
Per cent
of all
Probability of
commercialization (%)DEMO
Median return
among commercial (%)
(1) (2) (3) (4) (5) (6) (7) (8)
A-recommended DEMO development. 28 5 71 2 0 0 26.0
B-may go forward, DEMO need to 38 7 24 36 8 17 26.0
collect more data
C-recommended to go forward, 82 15 20 38 8 21
returns likely modest
D-doubtful, further development 341 61 4 383 83 7
not recommended
E-strongly recommend to stop 70 12 1 6 1 0 N/A
DEMO development
Weighted Average 11 9 7.3
Total 559 100 465 100
13.2
28.5
N/A: Return on investment IS not applicable, or alternatively, negative inﬁnity.

Data for inventions rated A and B not possible DEMO compute separately. Numbers on returns for A and B combined. Data from A˚
stebro (2003).
390
Journal of Behavioral Decision Making
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, DEMO, 381–403 (2007)
DOI: 10.1002/bdm
T. A˚ stebro and D. J. Koehler Calibration Accuracy of Commercial Success DEMO
and the probability of commercialization (Table 1, columns 4 and 8). The median rate of return for those
reaching the market is DEMO for those rated A and B, it is 13.2% for those DEMO C and 28.5% for the few
rated D that reach the market. These results suggest that the advice dispensed by the IAP to inventors DEMO
considerable predictive value both in terms of predicting whether they will reach the market and in terms of
their ﬁnancial success conditional on reaching DEMO market. That the IAP provides net valuable advice to the
inventors has indeed been shown by A˚ stebro and Bernhardt (1999) and A˚ DEMO and Gerchak, (2001).
A concern is that the outcome data may be affected by a self-fulﬁlling prophecy. The advice provided by
the DEMO may affect inventors’ efforts unduly. Hypothetically, if the cues are completely DEMO of
commercialization likelihood while the recommendation turns out to be highly correlated with
commercialization efforts (for example, due to affectation), we would DEMO positive and biased correla-
tions between cues and the likelihood of commercialization when the correlations, in fact, should be zero. We
therefore investigate DEMO potential bias in depth.
RESULTS
Experts’ decision accuracy
We ﬁrst tabulate the decision accuracy of the experts by assuming that an idea rated ‘A,DEMO ‘B,’ or ‘C’ can be
classiﬁed as a predicted ‘commercialization’ and those rated ‘D’ or ‘E’ can be classiﬁed as a predicted failure
DEMO commercialize. This information is then compared to the actual commercialization outcome as described
above.
Table 2, columns (2) and (3), shows DEMO, by this analysis, experts at the IAP predicted 441 out of the 559
outcomes correctly (78.9%). This breaks down into predicting 45 of 60 commercial ideas correctly (75.0%)
Table 2. Predictive accuracies
Experts’ judgments Bootstrap modelþ Prediction modelþ
Failure Success Failure Success Failure Success
Development DEMO
Failed 396 103 392 107 407 92
Succeeded 15 45 15 45 17 43
# % #% #%
Overall predictive accuracy 441 78.9 437 DEMO 450 80.5
Correctly predicts success (Sens.) 45 75.0 45 75.0 43 71.7
Correctly predicts failure (Spec.) 396 79.3 392 78.6 407 81.6
DEMO positive’s 103 69.6 107 70.4 92 68.1
False negative’s 15 3.6 15 3.7 17 4.0
Correlation with outcome 0.38 0.40 0.41
LR (x2) DEMO 78.82
p >x2 0.000 0.000
Area under the ROC curve (AUC) 0.813 0.815
Pseudo-R2 0.257 0.205
Hold-out sample
Overall predictive accuracy 81.1 DEMO 66.2
 Rating A, B, and C classiﬁed as ‘success,’ rating D and E classiﬁed as ‘failure.’ Test statistics are for a DEMO model with dummy
variables that exactly replicates the experts’ decisions.
Hand (DEMO).
Domencich and McFadden (1975).
þOnly variables signiﬁcant at p < 0.05 were included using stepwise backward variable elimination.
Copyright # 2007 John Wiley & Sons, Ltd. Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
392
Journal of Behavioral Decision Making
and predicting 396 of the 499 DEMO correctly (79.3%). There is a surprisingly even ability of the DEMO at
predicting both failures and commercializations. Consider that the baseline probability of commercialization
is low, around 0.11. Therefore, even though experts see approximately DEMO commercialization in 10 reviews
and thus have signiﬁcantly less opportunity to obtain training on reviewing these, they are almost as able to
make correct judgments on the high quality as the low quality ideas. It should DEMO recognized, however, that
the experts make a signiﬁcant number of false positive predictions. Out of 148 new product ideas predicted to
be commercial, 103 (69.6%) were actually failures.
The classiﬁcation accuracy of the experts DEMO be analyzed within the decision-making context. One
important contextual feature is the low probability of commercialization: 0.11, suggesting that a classiﬁcation
rule based DEMO predicting all ideas as failures would be high. Indeed, 499 out DEMO 559 ideas (89.3%) would then
be correctly classiﬁed as failures while the percentage of correctly classiﬁed commercializations would be
0% (0 out of 62). While accurate in the aggregate, this rule yields non-diagnostic advice that completely
defeats the purpose of the IAP to identify and encourage DEMO commercial new product ideas and would
not provide a useful service to inventors. Another simple rule (probability matching), which also yields
non-diagnostic predictions, would be to use the BR of 11% commercializations to forecast 60 randomly
chosen ideas to be commercial. This rule correctly classiﬁes 444 of DEMO failures (89.0%) and 7 out of 60
commercializations (11%) for an overall classiﬁcation accuracy of 80.7%. To compare the experts’
forecasting accuracy DEMO that of appropriate base-line models, given the goal of producing diagnostic
DEMO, we next calibrate the statistical models such that the proportion of DEMO classiﬁed
commercializations is maintained at approximately 75%.
Bootstrap model
A bootstrap model will give a sense of how well the experts’ ‘policy’ as captured DEMO a model of the
relationship between cue information and judgments performs, DEMO for a moment that the experts’
‘policy’ contains only main and linear effects on the odds of commercialization. A logistic regression model
is ﬁtted DEMO stepwise backward variable elimination with the overall rating as the outcome variable,
assuming ideas rated ‘A,’ ‘B,’ or ‘C’ are forecasted DEMO commercialized (¼1) and those rated ‘D’ or ‘E’ are
10
forecasted as failures (¼0), using the cues as independent variables. Using a p-value of 0.05 to determine
inclusion of predictors, the resulting bootstrap model has a pseudo-R2 of 0.59 [LR x2(11) ¼ 378.14] and
contains 10 of the possible 34 explanatory cues. The cues predicting experts’ forecasts DEMO, ‘Proﬁtability,’
‘Functional Performance,’ ‘Protection,’ ‘Appearance,’ ‘Duration of DEMO,’ ‘Size of Investment,’ ‘Tooling
Cost,’ ‘Development Risk,’ ‘Potential sales,’ and ‘Function.’ The bootstrap model is a good descriptor of DEMO
experts’ judgments, correctly classifying 503 of the 559 decisions (90.0%). This result indicates that even
without knowing the experts’ decision rules an DEMO statistical meta-model of these rules can be
created.11 The bootstrap model correctly classiﬁes 78.2% of the outcomes within-sample and 72.0%
out-of-sample. The within-sample accuracy DEMO are approximately equal to those for the experts’
judgments themselves [Table 2, columns (4) and (5)]. The relatively high agreement between DEMO simple
bootstrap model and the experts’ judgments suggests that the expert judgments are made with fairly high
10 An ordinal logit model was also DEMO that did not provide greater classiﬁcation accuracy than that here reported.
11 This model overestimates the reliability of the experts’ policy by using within-sample DEMO The model’s out-of-sample prediction
accuracy of decisions is lower, 80.9%. The DEMO in model accuracy is traced in part to the apparent change in decision policy between
the two sample periods, as evident from Table 1. Further, the bootstrap model is sensitive to data and alternative model reduction
techniques, indicating a ﬂat maximum likelihood and high cue redundancy. Much more work could be done to further optimize the
boostrap model given these DEMO For such work see A˚ stebro and Elhedhli (2006). Since DEMO this article the bootstrap model is not
used to predict future decisions but simply to describe the experts’ within-sample policy, we were content with presenting a simple linear
additive model representation, as long as its descriptive accuracy was reasonably high.
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: DEMO/bdm
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
reliability; the trivial difference in predictive accuracy between the bootstrap model DEMO the expert
judgments, in turn, suggests that the cost of any unreliability in the expert judgments is relatively low.
The experts and the DEMO model typically agree in their predictions, with 503 cases on the DEMO
The experts’ deviations from the bootstrap model are spread between 26 predicted commercial successes
when the model predicts failure, and 30 predicted failures when the model predicts commercial success. The
prediction accuracies of the off-diagonal elements DEMO about equal showing neither an informational
advantage nor bias by the experts. On the ideas where the experts disagree with the bootstrap model are DEMO
probabilities of success, 0.09 and 0.11, respectively, none statistically signiﬁcantly DEMO from the BR.
Apparently, any ability of the experts to exploit DEMO cue-outcome relations is offset by the bootstrap
model’s greater reliability.
Prediction model
The prediction model describes the ‘true’ relationship between cue information and outcomes DEMO is
estimated to benchmark the calibration accuracy of the experts’ judgments. Typically the prediction model is
reduced in complexity to linear and additive effects, which will capture a majority of the variance (Dawes
et al., DEMO). For the purpose of obtaining robust statistical prediction, a backward DEMO variable
elimination procedure is used including only predictors that are signiﬁcant at the 5% level and the logistic
speciﬁcation, which is linear in effects on the odds of commercialization. Results are robust to alternative
elimination procedures DEMO as forward elimination and to alternative model speciﬁcations such as the
probit.12 The cut-off for classiﬁcation is calibrated such that the proportion of correctly DEMO successes is
similar to the judges’ accuracy at approximately 75%.
The statistical procedure selects the following four cues as predictors: ‘Proﬁtability,’ ‘Development risk,’
‘Functional Performance,’ and ‘Protection.’ These together correctly predict 450 out DEMO 559 outcomes
(80.5%), while the out-of-sample forecasting accuracy is reduced DEMO 66.2%. Although arriving at a slightly
higher classiﬁcation accuracy, the prediction DEMO used fewer cues than did the bootstrap model. Columns 6
and 7 of Table 2 indicate that the model correctly predicts 43 of 60 DEMO (71.7%) and 407 of the 499
failures (81.6%) within sample. The rates of false negatives and false positives are very close to DEMO experts’
rates.13 The incorrect classiﬁcations by the model could be attributable either to the difﬁculty of predicting
the outcome from the cues or the DEMO of correctly coding the cues themselves in the ﬁrst place.
We cannot reject the hypothesis that the experts come close to an optimal use DEMO cue information when the
basis of comparison is linear additive effects. The experts’ judgments are close in accuracy to a linear
bootstrap model of DEMO policy and the bootstrap model is, in turn, a close representation of the (linear)
12The likelihood function for this model is steeper than that for the bootstrap model leaving less room for alternative model DEMO
based on slight changes to data or estimation procedure. One cue, DEMO Performance’ gets interchanged for ‘Function’ between the
two methods while the pseudo-R2 changes only on the third decimal when using forward instead of backward DEMO elimination.
Reviewing Appendix A it is seen that the two cues are almost identical.
13It might be argued that the backwards elimination and use DEMO only main and linear effects does not capture all useful information. We
examined this argument using split samples, one covering 1989–1992 (estimation sample) and one covering 1993 (prediction sample).
First, it should be DEMO that the calibration accuracy of the experts was 83.8% in 1993, DEMO the forecasting accuracy of a prediction model
estimated on the 1989–1992 sample was 82.6% for 1993. Using all 34 predictors we ﬁnd a better DEMO model with a within-sample
pseudo-R2 of 0.283 and an area under the ROC curve of 0.851. However, when applying this model to the 1993 sample it had an overall
prediction accuracy of merely 71.9%. This indicates DEMO Another method is to include predictors with higher p-values, but not DEMO
34. We explore this using stepwise regression with an inclusion criterion of p < 0.10. In addition to the previous predictors, this allows
entry of ﬁve more cues for a within-sample pseudo-R2 of 0.21 and an DEMO under the ROC curve of 0.820 for the 1989–1992 pool. Applied
to the 1993 pool, the model correctly predicts 12 successes (70.6%) and 122 failures (81.3%) for an overall forward prediction accuracy
of 80.2%. DEMO model does not improve prediction accuracy. Finally, including all two-way interactions DEMO the four signiﬁcant
predictors does not increase the 1989–1992 within-sample pseudo-R2 or the area under the ROC curve and the out-of-sample predictions
are identical DEMO the simple linearly additive model (82.6%).
Copyright # 2007 John DEMO & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
394
Journal of Behavioral Decision Making
predictive value of available data. Predictions DEMO the experts deviating from the bootstrap model are neither
biased nor particularly effective, but simply random. The experts pay attention to the cues indicated by the
statistical model as most important in predicting commercialization and, in addition, use several cues that are
not predictive when making their judgments.
To get a sense of the small differences in accuracy between expert DEMO, the bootstrap model and the
prediction model, compare these results to the summary of ﬁve similar comparisons in Dawes (1979). The
average marginal improvement in forecast accuracy of the bootstrap model over experts’ judgments DEMO
those ﬁve studies is 23% while in the current study the experts perform 1% better. The average marginal
improvement in forecast accuracy of a DEMO model over experts’ judgments across the ﬁve studies is
73% while it is 2% in this study.
Given that this is not an experimental DEMO with randomized assignment but an analysis of real decisions
there is a concern that the data may be affected by a partial self-fulﬁlling prophecy DEMO may affect results. The
self-fulﬁlling prophecy can affect our results in two ways. It may bias upwards the estimated predictive ability
of the judges. DEMO also poses a problem when one would like to estimate the relationship between cues
and outcomes. The latter relationships are potentially contaminated by the DEMO that judgments have on
outcomes. The self-fulﬁlling prophecy, however, does not bias reported results on the relationship between
cues and judgments. That is, our bootstrap analysis that examines whether the experts’ decision rules are
reliable DEMO unaffected. A˚ stebro and Chen (2004) estimate the potential bias, DEMO as the average
effect of the judgment on the probability of commercialization, controlling for the expected commercial
quality of the idea. The expected quality of the invention is estimated econometrically by an index measuring
the likelihood DEMO reaching the market, while controlling for the selection bias. A more DEMO way of
saying this (and a restricted case) is that they estimate the degree to which there is a bias in the relationship
DEMO cues and outcomes if the outcomes are 100% determined by the IAP recommendation, and not at all
by the underlying quality of the ideas. The authors ﬁnd that most of the efforts by the inventors are DEMO by
the underlying quality of their ideas and that the IAP advice accurately reﬂects this quality. The most likely
bias is a rather small DEMO (decrease) in the inventor’s expectation of the probability of success as a
function of a positive (negative) review by the IAP, while controlling for the expected quality of the idea.
The detected bias is DEMO large enough to invalidate the conclusion that the statistically estimated model
describing the relation between the cues and the probability of commercialization is relatively DEMO for
the majority of new product ideas. Therefore, the tests of DEMO validity of the experts’ forecasting accuracy
remain valid.
Whereas the experts continue to produce accurate judgmental forecasts for the period 1994–2001 it
becomes evident DEMO both the bootstrap and prediction models developed on data for the 1989–1993 period
deteriorate in out-of-sample tests. There can be many reasons for this DEMO The policies of the
experts appear to change over time, as DEMO above, and so the bootstrap model would need to be revised. DEMO
distribution of outcomes is also different for the hold-out sample, but DEMO of this may be due to random
variation, and the sample DEMO of success are not signiﬁcantly different. In any case, our analysis DEMO not
rely on strong out-of-sample prediction accuracies. The bootstrap model is merely intended to replicate the
experts’ decisions and the prediction model is intended DEMO identify valid predictors within sample. The key
pieces of analysis are within-sample comparisons of accuracy across these two models and with the judgment
accuracy DEMO the experts.
Calibration
Here we investigate the calibration of the IAP experts’ judgments, that is, their correspondence to the actual
commercialization outcomes of DEMO ideas. In our dataset, analysis of the calibration of expert predictions DEMO
somewhat complicated by the nature of the IAP classiﬁcation system, which DEMO not directly elicit subjective
probability assessments from the experts. The bootstrap model discussed above, however, usefully captures
Copyright # 2007 John Wiley & DEMO, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
0.90
0.80
observed
RST
perfect
smoothed
0.70
0.60
0.50
0.40
395
0.30
0.20
0.10
0.00
0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 DEMO 1.00
expert prediction (bootstrap)
Figure 1. Calibration plot showing correspondence DEMO IAP expert predictions of commercialization probability
derived from the bootstrap model and the actual probability of commercialization (as derived both from observed
commercialization probabilities and ‘smoothed’ estimates of commercialization probabilities estimated from the
prediction model). DEMO of RST model to the smoothed expert calibration curve is also shown
the predictive policy of the experts and produces, for each case, DEMO predicted commercialization probability
that can be subjected to conventional calibration analysis. In the analysis reported below, then, the probability
assigned to a case DEMO the bootstrap model of the experts’ judgments, along with an outcome DEMO
associated with that case, is the dependent variable.
In Figure 1, expert predictions (in the form of a probability derived from the bootstrap model of the
experts’ judgment policy) were aggregated into 7 probability categories uniformly spanning the unit interval;
outcome probability (proportion of ideas commercialized) is then plotted for each probability category
associated with the expert predictions (curve labeled ‘observed,’ round dots). The use of 7 probability
categories, while somewhat arbitrary, was chosen as it is large enough DEMO provide a sufﬁcient number of points
for an informative characterization of calibration performance but small enough that each point is based on a
reasonably DEMO number of observations. As expected if the expert judgments are case based, as elaborated in
the introduction, the calibration curve falls consistently below (over-prediction, expected given the low
outcome BR) and is ﬂatter than (over-extremity, expected given the relatively low level of predictability of the
outcome from the cues) the identity line.
The observed calibration curve in Figure 1 was constructed using the dichotomous outcome variable
(commercialized vs. not commercialized). For smaller sample sizes, as in analyses to be reported below, it is
useful to construct a less noisy variant of the outcome variable. To do this, the prediction model is used to
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, DEMO, 381–403 (2007)
DOI: 10.1002/bdm
outcome (commercialization) probability
396
Journal of Behavioral Decision Making
estimate the probability of commercialization for DEMO idea, and then the aggregate estimated probability of
commercialization is calculated DEMO each set of ideas assigned to each judged probability category and used as
the outcome variable in the calibration analysis (instead of simply using the proportion of those cases that
were commercialized). For comparison, the calibration curve for the full dataset that results from this
approach is DEMO shown in Figure 1 (curve labelled ‘smoothed,’ square dots). DEMO ﬁgure shows that the
observed pattern of calibration is quite similar to that found using the more conventional method of analysis,
but with DEMO pronounced non-monotonicities.
The RST model was ﬁt to the smoothed expert calibration data (curve is labeled RST in Figure 1), which
associates each case (idea) with a forecast probability (from the bootstrap model of the judge) and an outcome
probability (from the prediction model). DEMO parameter values14 were a ¼ 0.806, b ¼0.364,
s ¼ DEMO, with the sample outcome BR set to 0.107 (see Table 1, bottom row). The observation that s >a
conﬁrms that the DEMO are too extreme (as indexed by s ) relative to the DEMO of the outcome (as
indexed by a). The observed negative DEMO bias parameter b indicates some accommodation of the very low
outcome BR, but not nearly enough; a much more negative value of b (2.626) would be required to maintain
good calibration. As a result, DEMO judgments show a distinct over-prediction bias. Fitting the RST model to the
noisier observed expert calibration curve (instead of the ‘smoothed’ curve) yielded DEMO similar results:
a ¼ 0.979, b ¼0.271 (ideal b ¼2.163), s ¼ 1.228.
As a further investigation of the consequences of DEMO case-based judgment, the new product ideas
under evaluation were split into DEMO subsets differing in the overall level of uncertainty associated with their
future prospects. This split was based on two cues, demand predictability (‘How DEMO will it be possible to
predict sales?’) and development risk (‘What degree of uncertainty is associated with complete successful
development from the DEMO condition of the innovation to the market ready state?’). Recall that each idea
was rated as an A, B, or C DEMO each cue which, as noted above, was translated for our analyses into a score of 1,
0, or  1, respectively; higher scores on demand predictability and development risk indicate lower
uncertainty. New DEMO with a total score on these two cues of 0 or higher (n ¼ 358) were assigned to the low
subset. Broadly speaking, the cues characterizing a particular new idea would be expected to be DEMO subset, and those with a score of 1 or lower (n ¼ 201) were assigned to the high uncertainty
predictive of its eventual commercialization likelihood under conditions of greater uncertainty. If intuitive
expert judgments of DEMO new idea’s commercialization prospects are primarily case based, we would expect
DEMO to be insufﬁciently sensitive to aggregate characteristics such as the overall predictability of the
outcome variable from the available cues. Because they are not DEMO ‘corrected’ for the overall level of
predictability, therefore, we would expect the calibration curve for the expert predictions regarding the high
uncertainty ideas DEMO be ﬂatter than that for the low uncertainty ideas.
To test this possibility, separate calibration curves were constructed for the low and high uncertainty ideas,
and the RST model ﬁt to each. Figure 2 shows DEMO results. As predicted, the calibration curve for the high
uncertainty ideas DEMO ﬂatter than that for the low uncertainty ideas. For the low uncertainty ideas, the estimated
RST parameter values were a¼ 0.706, b ¼0.049, and s ¼ 1.299, with an outcome BR of 0.147 and ideal
b ¼2.489 required for good calibration. For the high uncertainty ideas, the estimated RST paramter values
good calibration. The lower value of alpha for DEMO high uncertainty ideas conﬁrms our assumption that thewere a¼ 0.473, b DEMO, and s ¼ 0.873, with an outcome BR of 0.37 and ideal b ¼6.924 required for
commercialization outcome is less predictable than it DEMO for the low uncertainty ideas. The corresponding
difference in sigma indicates that the experts did attenuate the extremity of their predictions for the high
DEMO ideas, but not sufﬁciently in light of the lower predictability of DEMO outcome in these cases relative
14 The model was ﬁt to the conditional probability distributions (i.e., distribution of forecast probabilities given commercialization and
DEMO no commercialization), such that alpha and beta reproduced the means of the distributions and sigma their pooled variance. See
Brenner, Grifﬁn, & DEMO (2006) for RST model-ﬁtting details.
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)DEMO
DOI: 10.1002/bdm
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
Low Uncertaint ty aintyHigh Uncer
1.00
0.90
0.80
RST
perfect
smoothed
1.00
0.90
0.80
RST
perfect
smoothed
0.70
0.70
0.60
0.50
0.60
0.50
0.40
DEMO
0.20
0.10
0.00
0.40
0.30
0.20
0.10
0.00
0.00
0.10
0.20
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
expert prediction (bootstrap)
0.90
1.00
0.30
0.40
0.50
0.60
0.70
0.800
expert prediction (bootstrap)
0.90
1.0
Figure 2. Calibration plots showing correspondence between IAP expert predictions DEMO commercialization probability
derived from the bootstrap model and the actual probability of commercialization (‘smoothed’) for low and high
uncertainty ideas. Fit of RST DEMO to each calibration curve is also shown
to that for the low uncertainty ideas. The lower outcome BR for the high uncertainty ideas indicates DEMO not
only was the outcome less predictable, but also the overall DEMO of commercialization were lower than for
the low uncertainty ideas. The estimated value of beta for the two subsets of ideas indicates that experts
DEMO adjust substantially for the poorer prospects of the high uncertainty ideas, DEMO again this adjustment
does not appear to have been sufﬁcient, with DEMO tendency toward overly optimistic predictions (i.e.,
over-prediction) being present in both subsets and somewhat more pronounced for the high uncertainty
subset in DEMO
CONCLUDING REMARKS
Experts at the Canadian IAP are reasonably accurate forecasters of the future commercialization of new
product ideas. Over a period of 5 DEMO the experts were able to, ex ante, correctly classify 79% of the ideas.
Such performance is impressive in light of the inherent uncertainty DEMO predicting such notoriously
unpredictable outcomes in a setting where feedback on their decisions is not readily available and the BR of
commercialization success was DEMO precisely known.
The most notable strength of the IAP experts is their highly effective use of the available predictive cues.
The experts are marginally DEMO at predicting outcomes than a (linearly additive) bootstrap model and
achieve 98.0% of the accuracy achieved by an optimal linear statistical prediction model, showing high
predictive validity and support of H1. The IAP experts appear DEMO be signiﬁcantly better at using the available
predictive information than other experts, such as clinicians predicting psychosis, graduate admission ofﬁcers
predicting graduate student’ DEMO, faculty members predicting the performance of other faculty
members, and VCs predicting the likelihood that new ventures will succeed (Dawes, 1979; Zacharakis &
Meyer, 2000).
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
outcome (commercialization) probability
outcome (commercialization) probability
398
Journal of Behavioral Decision Making
Consistent with the notion that intuitive DEMO (even those of experts) tend to be case-based, however,
DEMO commercialization forecasts of the IAP experts exhibited systematic miscalibration of the form that
would be expected to arise from insensitivity to aggregate, class-based characteristics of the judgment
environment. Speciﬁcally, their forecasts are biased in ways typical for settings with low BRs and high
uncertainty, exhibiting both over-prediction and over-extremity, showing support for H2. Miscalibration can
be costly in this context, even when the assessments accurately rank order the ideas in terms of their
prospects, as a highly-ranked idea may have a substantially lower probability of commercialization (even if it
is higher than that of many of the other ideas under assessment) than the assessment suggests.
Potential explanations for the experts’ performance
In searching for explanations to the IAP experts’ forecasting DEMO as well as their biases, we consider
the application (or lack thereof) by the IAP of decision-making processes that have been recommended by
researchers as ‘good practices’ (Fischhoff, 1982, 2002; Larrick, 2004; DEMO & Lusk, 1994).
A ﬁrst observation is that a fair DEMO of training is provided to the experts. Second, the procedure is
DEMO and decomposed, something that has been argued to reduce the cognitive DEMO and increase
reliability (Dawes & Corrigan, 1974; Einhorn, 1972). Except for forecasting weather (Murphy & Winkler,
1984), this particular process seems to be more standardized than most other real-world forecasting tasks
DEMO in the literature (Fischhoff, 2002). Although the cues are in many cases forecasts themselves (such
as the degree of competition from new ﬁrms expected to enter after launch), and thus subject to error DEMO
measurement, Armstrong, Brodie, & McIntyre (1987) and York, Doherty, &Kamouri, (1987) demonstrate
that measurement error in cues may not be critical, especially if there are many redundant cues. Further
analysis of the data indeed revealed large amounts of multicollinearity across the whole matrix—if DEMO analyst
tends to assign an A to one cue she tends to assign an A to a cue of a related dimension. Selecting the DEMO
cue for making the overall judgment is therefore not that detrimental to accuracy.15
Third, the group meeting at the end of the review may encourage careful evaluation of the idea by the IAP
expert that could DEMO accurate predictions. Hagafors and Brehmer (1983), for example, suggest that
reliability increases if forecasters are asked to verbally justify forecasts, especially when no outcome
feedback is available. Larrick (2004) argues that the principal DEMO by which such accountability
improves decision-making is pre-emptive self-criticism. In addition, DEMO length of the deliberation regarding
the assessment decision suggests that the analysts would not tend to make snap judgments, which otherwise
are prone to be associated with greater decision-making biases than deliberate choices (Frederick, 2002;DEMO
Slovic, Finucane, Peters, &MacGregor, 2002).
Fourth, the experts take steps to remain personally detached in their evaluations, for example, DEMO not
talking to the inventor. They see the primary value of their service as providing an impartial, outsider’s view
of the idea’s commercialization prospects that the inventor, potentially prone to an overly optimistic
assessment, may DEMO have difﬁculty obtaining.
Finally, the IAP experts’ method of comparing a DEMO new product idea to a ‘similar’ new product idea
archived in their extensive library of reviews may have both beneﬁts and drawbacks. Edwards and DEMO
Winterfeldt (1986) argue that such comparisons may work well. For example, diamond evaluators reduce the
decision problem to assessing similarities and differences with other remembered or currently available
diamonds on key criteria. The IAP analysts DEMO such case-based comparisons to sort a new product idea into
15 Note that the overall forecasting accuracy which we document the IAP experts to DEMO does not rely on cue measurement. It is only
when we construct the bootstrap and prediction models that we rely on cue measurement. To DEMO extent that there is unreliable cue
measurement, the corresponding bootstrap and DEMO models would account for that by increased standard errors of estimates.
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
an ordinal ranking scheme without considering BRs. It is likely that some judgment error is avoided by this
simplifying scheme. However, similar decision-making heuristics that focus on the case at hand rather than
on class-based data DEMO been shown in experiments to produce biased judgments (Kahneman & Tversky,DEMO
1973; Tversky & Kahnemann, 1983). We indeed ﬁnd that the experts still exhibit over-prediction and
over-extremity, the expected result of case-based reasoning in this judgment environment, despite their
apparently effective use of the predictive cues. This situation is possible since accurate rank-ordering into ﬁve
bins, each with a relatively wide probability-range, allows for considerable over (or under) prediction of true
probabilities, while still conserving correct rank order.
Managerial implications
When the goal of an evaluation process such as the one reported DEMO is simply to rank order cases (e.g., as
part of an initial screening), intuitive expert forecasts are likely to do a reasonably DEMO job, given
circumstances such as those of the IAP: extensive experience, structured decision process, library of past
cases. Indeed, under these conditions, the introduction of linear statistical models may not offer much in the
way of gains in accuracy, though it might still produce savings in time and effort. In this respect our results
echo those found in DEMO, reviewed by Armstrong et al. (1987). But when the goal of the evaluation
process is to associate a probability of commercialization with DEMO particular case, rather than just rank
ordering, the insensitivity of intuitive expert forecasts to class-based considerations in the mapping from the
case-based evaluation DEMO the probability scale can be costly. Indeed, in the type of DEMO environments that
we examine it is important not only to rank order ideas well but also to carefully calibrate the forecast
probability of commercialization DEMO the expected probability of commercialization will drive an
evaluation of the return on investment in a given idea.
We suggest that existing managerial processes DEMO are available for screening and evaluating new product
development projects when there is high uncertainty, such as the Q-sort method (Allen, 2003), are not enough
to combat the typical biases that can arise from DEMO judgment. Prescriptions for avoiding
miscalibration biases include the use of statistical models to carry out the mapping from experts’ evaluations
to the probability scale, as well as changes to the structured forecasting process that encourage explicit
DEMO of outcome BR and predictability. Using statistical models for decision support is advocated by
many (Armstrong et al., 1987; Blattberg & Hoch, DEMO; Hoch & Schkade, 1996). However, research has
shown that DEMO is resistance to the use of computer aids for decision making (DEMO, Van Bruggen, &
Staelin, 1999). If an organization can develop a decision environment that is as efﬁcient as the IAP, the
introduction of a regression model for decision support may not be necessary. DEMO, in its simplest form the
mapping from rank ordering to the DEMO scale could be accomplished by a table, such as that presented
DEMO Table 1, that takes into account the relevant outcome BR and DEMO from the available cues. Such
tables are not difﬁcult to develop for most organizations as they are based on historical aggregate new product
development DEMO And they ﬁt the recommendation by Larrick (2004) of being simple to use and therefore
more likely to be adopted.
When predictions are DEMO made for cases that differ in the classes from which they are drawn (i.e., in
aggregate characteristics such as outcome BR and predictability), the judgment process could also be
structured in ways that encourage their explicit evaluation. It is likely that the predictability of cue
information varies DEMO stages in the development process or across various groups of projects such as
small-scale enhancements versus large-scale projects. In addition, Herbig et al. (DEMO) suggest that
predictability is greater for consumer product outcomes than for DEMO products. An evaluation of the
degree of predictability of the cue information is also possible using historical aggregate data. The same is
true for DEMO outcome BR, which may differ across product categories (e.g., consumer DEMO industrial
products). Changing the decision process such that these variables are explicitly encoded for each new case
may encourage experts to place greater DEMO on class-based characteristics in their forecasts.
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)DEMO
DOI: 10.1002/bdm
400
Journal of Behavioral Decision Making
APPENDIX A
Brief explanations of cues DEMO by the IAP from 1989 and onward
Cue name Explanation
Technical feasibility Is the technical solution sound and complete?
Functional performance Does this DEMO work better than the alternatives?
Research and development How great a burden is the remaining research and development
required to bring the innovation DEMO a marketable stage?
Technology signiﬁcance How signiﬁcant a contribution to technology or to its application is
proposed?
Technology of production Are the DEMO and skills required to produce the new product
idea available?
Tooling cost How great a burden is the cost of production tooling required DEMO meet
the expected demand?
Cost of production Does production at a reasonable cost level appear possible?
Need Does the innovation solve a DEMO, ﬁll a need or satisfy a want
for the customer?
DEMO market How large and how enduring is the total market for all products
serving this function?
Trend of demand Will the demand for DEMO an innovation be expected to rise, remain
steady, or fall in the lifetime of this idea?
Duration of demand Is the demand DEMO the innovation expected to be ‘long term?’
Demand predictability How closely will it be possible to predict sales?
Product line potential Can DEMO innovation lead to other proﬁtable products or services?
Societal beneﬁts Will the innovation be of general beneﬁt to society?
Compatibility Is the DEMO compatible with current attitudes and ways of
doing things?
Learning How easily can the customer learn the correct use of the innovation?
DEMO How evident are the advantages of the innovation to the prospective
customer?
Appearance Does the appearance of the innovation convey a message of DEMO
qualities?
Function Does this innovation work better than the alternatives?—or fulﬁll a
function not now provided?
Durability Will this innovation endure DEMO usage?’
Price Does this innovation have a price advantage over its competitors?
Existing competition Does this innovation already face competition in the DEMO that
will make its entry difﬁcult and costly?
New competition Is this innovation likely to face new competition in the marketplace from
other DEMO that must be expected to threaten its market share?
Marketing research How great an effort will be required to deﬁne the product and DEMO that
the ﬁnal market will ﬁnd acceptable?
Promotion cost Is the cost and effort of promotion to achieve market acceptance of the
innovation DEMO line with expected earnings?
Distribution How difﬁcult will it be to develop or access distribution channels for
the innovation?
Legality Does the DEMO product idea meet the requirements of applicable laws,
regulations and product standards and avoid exposure to product liability?
Development risk What degree DEMO uncertainty is associated with complete
commercializationful development from the present condition of the
innovation to the market ready state?
Dependence To what degree DEMO this innovation lose control of its market and sales
due to its dependence on other products, processes, systems or services?
(Continues)
Copyright # 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
T. A˚
stebro and D. J. Koehler Calibration Accuracy of Commercial Success
DEMO
Table . (Continued)
Appendix A (Continued)
Cue name Explanation
Protection Is it likely that worthwhile commercial protection will be obtainable for
DEMO innovation through patents, trade secrets or other means?
Size of DEMO Is the total investment required for the project likely to be obtainable?
Potential sales Is the sales volume for this particular innovation likely DEMO be sufﬁcient
to justify initiating the project?
Payback period Will the initial investment be recovered in the early life of the innovation?
DEMO Will the expected revenue from the innovation provide more proﬁts than
other investment opportunities?
ACKNOWLEDGEMENTS
A˚ stebro acknowledges partial ﬁnancial support through a DEMO Grant from the Natural Sciences and
Engineering Research Council of Canada, DEMO support from the MINE program, Ecole Polytechnique, and
in-kind support from the Canadian Innovation Centre. Koehler also acknowledges support from a Natural
Sciences DEMO Engineering Research Council of Canada Discovery Grant.
REFERENCES
Allen, K. R. (2003). Bringing new technology to market. Upper Saddle River, NJ: DEMO Hall.
Armstrong, J. S., Brodie, R. J., & McIntyre, DEMO H. (1987). Forecasting methods for marketing: Review of empirical
research. International Journal of Forecasting, 3, 355–346.
A˚ stebro, T. (1997)DEMO The economics of invention and inventor’s assistance programs. Waterloo: University of DEMO
A˚ stebro, T. (2003). The return to independent invention: DEMO of risk seeking, extreme optimism or skewness-loving?
The Economic Journal, 113, 226–239.
stebro, T. (2004). Key success factors for technological entrepreneurs’ R&D projects. IEEE Transactions on
Engineering Management, 51, 314–321.
DEMO, T., & Bernhardt, I. (1999). The social rate of return to Canada’s Inventor’s Assistance Program. The Engineering
A˚
A˚
Economist, 44, 348–361.
stebro, T., & Chen, G. (2004). Statistical decision-making models and treatment effects. Manuscript, available from
http://papers.ssrn.com/sol3/papers.cfm?abstract_id¼578525.
stebro, T., & Elhedhli, S. (2006). The effectiveness DEMO simple decision heuristics: Forecasting commercial success for
early-stage venures. Management Science, 52, 395–409.
A˚ stebro, T., & Gerchak, Y. (2001). Proﬁtable advice: The value of information provided by Canada’s Entrepreneur’s
Assistance Program. Economics of Innovation and New Technology, 10, 45–72.
A˚ stebro, T., Jeffrey, S. A., & Adomdza, G. K. (in press). Inventor perseverance after being told to quit: The role of
cognitive biases. Journal of Behavioral Decision Making. DOI: 10.1002/bdm.554
Baker, K. G., & Albaum, G. S. (1986). Modeling new product screening decision. DEMO of Product Innovation
Management, 32–39.
Balzer, W. K., Sulsky, L. M., Hammer, L. B., & Sumner, K. E. (1992). Task information, cognitive information, or
functional validity information: Which components of cognitive feedback affect performance? Organizational
Behavior and Human Decision Processes, 53, 35–54.
Blattberg, R. C., & Hoch, S. J. (1990). DEMO models and managerial intuition: 50% model R 50% manager.
Management Science, 36, 887–899.
Brenner, L. (1995). A stochastic model of the calibration of subjective probabilities. Unpublished doctoral dissertation,
Stanford University.
Brenner, L. (2003). A random support model of the calibration of subjective probabilities. Organizational Behavior and
Human Decision Processes, 90, 87–110.
A˚
A˚
Copyright DEMO 2007 John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
402
Journal of Behavioral Decision Making
Brenner, L., Grifﬁn, D., & Koehler, D. J. (2006). Modeling patterns of probability calibration with random support theory:
Diagnosing case-based judgment. Organizational Behavior and Human Decision DEMO, 97, 64–81.
Brunswik, E. (1955). Representative design and probabilistic theory in a functional psychology. Psychological Review,
62, 193–217.
Camerer, DEMO (1981). General conditions for the success of bootstrapping models. Organizational DEMO and Human
Decision Processes, 27, 411–422.
Campbell, D. T., & Fiske, D. W. (1959). Convergent and discriminant validation by the DEMO Matrix.
Psychological Bulletin, 56, 81–105.
Dawes, R. (1979). The robust beauty of improper linear models in decision making. American Psychologist, 34, 571–582.
Dawes, R., & Corrigan, B. (1974). Linear models in decision making. Psychological Bulletin, 81, 95–106.
Dawes, R., Faust, D., & Meehl, P. E. (1989). Clinical versus actuarial judgment. Science, 243, 1668–1674.
Domencich, T., & McFadden, D. (1975)DEMO Urban travel demand. Amsterdam: North-Holland.
Edwards, W., & von Winterfeldt, D. (1986). Cognitive illusions and their implications for the law. Southern California
Law Review, 59, 225–276.
Einhorn, H. (1972). Expert DEMO and mechanical combination. Organizational Behavior and Human Perform-
ance, 7, 86–106.
Ettenson, R., Shanteau, J., & Krogstad, J. (1987). DEMO judgment: Is more information better? Psychological Reports,
60, 227–238.
DEMO, W. R., & McGoey, P. J. (1980). A model of calibration for subjective probabilities. Organizational Behavior and
Human Performance, 26, DEMO
Fischhoff, B. (1975). Hindsight is not equal to foresight: DEMO effect of outcome knowledge on judgment under uncertainty.
Journal of Experimental Psychology: Human Perception and Performance, 1, 288–299.
Fischhoff, B. (1982). Debiasing. In D. Kahneman, P. Slovic, & A. Tversky (Eds.), Judgment under uncertainty:
Heuristics and biases. New York: Cambridge University Press.
Fischhoff, B. (2002). Heuristics and biases in application. In T. DEMO, D. Grifﬁn, & D. Kahneman (Eds.), Heuristics
and biases: The psychology of intuitive judgment (pp. 730–762). Cambridge, UK: Cambridge University Press.
Frederick, S. (2002). Automated choice heuristics. In T. DEMO, D. Grifﬁn, & D. Kahneman (Eds.), Heuristics and
biases: The psychology of intuitive judgment (pp. 548–558). Cambridge, UK: Cambridge University Press.
Goldberg, L. R. (1968). Simple models or simple DEMO? Some research on clinical judgment. American Psychologist,
23, 483–496.
Grove, W. M., & Meehl, P. E. (1996). Comparative efﬁciency DEMO informal (subjective, impressionistic) and formal
(mechanical, algorithmic) prediction procedures: The clinical-statistical controversy. Psychology, Public Policy, and
Law, 2, 293–323.
Hagafors, R., & Brehmer, B. (1983). Does having to DEMO ones judgments change the nature of the judgment process?
Organizational Behavior and Human Decision Processes, 31, 223–232.
Hammond, K. R. (1996)DEMO Human judgment and social policy: Irreducible uncertainty, inevitable error, unavoidable
DEMO New York: Oxford University Press.
Hand, D. J. (2001). DEMO diagnostic accuracy of statistical prediction model. Statistica Neerlandica, 55, 3–16.
Herbig, P., Milewicz, J., & Golden, J. E. (1993). DEMO do’s and don’ts of sales forecasting. Industrial Marketing
Management, 22, 49–57.
Hoch, S. J., & Schkade, D. A. (1996). A DEMO approach to decision support systems. Management Science, 42,
51–64.
Kahneman, D., & Tversky, A. (1973). On the psychology of prediction. Psychological Review, 80, 234–251.
Kahneman, D., & Tversky, A. (DEMO). Intuitive prediction: biases and corrective procedures. Management Science, 12,
313–327.
Kahneman, D., & Tversky, A. (1982). On the DEMO of statistical intuition. Cognition, 11, 123–141.
Koehler, D. J., Brenner, L., & Grifﬁn, D. (2002). The calibration of expert DEMO: Heuristics and biases beyond the
laboratory. In T. Gilovic , D. DEMO , & D. Kahneman (Eds.), Heuristics and biases: The psychology of intuitive
judgment (pp. 685–715). Cambridge, UK: Cambridge University Press.
Larrick, R. P. (2004). Debiasing. In D. J. Koehler, & N. Harvey (Eds.), Blackwell handbook of judgment and decision
making (DEMO 316–334). Oxford, UK: Blackwell.
Mansﬁeld, E., Rapaport, J., Romeo, A., Villani, E., Wagner, S., & Husic, F. (1977). The production and application of new
industrial technology. New York: W.W. Norton.
Murphy, A. H., & Winkler, R. L. (1984). Probability forecasting in meteorology. Journal of the American Statistical
Association, 79, 489–500.
Shanteau, J. (1992). How much information does an expert use? Is it relevant? Acta Psychologica, 81, 75–86.
Copyright # DEMO John Wiley & Sons, Ltd.
Journal of Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm
T. A˚ stebro and D. J. Koehler Calibration Accuracy of Commercial Success DEMO
Shanteau, J., Grier, M., Johnson, J., & Berner, DEMO (1991). Teaching decision making skills to student nurses. In J. DEMO, &
R. Brown (Eds.), Teaching decision making to adolescents. DEMO, NJ: Erlbaum.
Slovic, P., Finucane, M., Peters, E., & MacGregor, D. (2002). The affect heuristic. In T. Gilovic, D. Grifﬁn, & D.
Kahneman (Eds.), Heuristics and biases: The psychology of intuitive judgment (pp. 397–420). Cambridge, UK:
Cambridge DEMO Press.
Stewart, T. R. (2001). Improving reliability of judgmental forecasts. In J. S. Armstrong (Ed.), Principles of forecasting: A
handbook DEMO researchers and practitioners (pp. 81–106). Norwell, MA: Kluwer.
Stewart, T. R., & Lusk, C. M. (1994). Seven components of judgmental forecasting skill: Implications for research and the
improvement of forecasts. Journal of Forecasting, 13, 575–599.
Tull, D. (1967). The relationship DEMO actual and predicted sales and proﬁts in new-product introductions. Journal of
Business, 40(3), 233–250.
Tversky, A., & Kahnemann, D. (1974). Judgment under uncertainty: Heuristics and biases. Science, 185, 1124–1131.
Tversky, A., & Kahnemann, D. (1983). Extensional vs. intuitive reasoning: The conjunction fallacy in probability
judgment. Psychological Review, 91, 293–315.
Tversky, A., & Koehler, D. J. (1994). Support theory: A DEMO representation of subjective probability.
Psychological Review, 101, 547–567.
Udell, G. (1989). Invention evaluation services: A review of the state of the art. Journal of Product Innovation
Management, 6, 157–168.
Wierenga, B., DEMO Bruggen, G. H., & Staelin, R. (1999). The commercialization of marketing management support
systems. Marketing Science, 18, 196–207.
York, K. M., Doherty, M. E., & Kamouri, J. (1987). The Inﬂuence of cue unreliability on judgment in a multiple-cue
probability learning task. DEMO Behavior and Human Decision Processes, 39, 303–317.
Zacharakis, A., & Meyer, D. (2000). The potential of actuarial decision models: Can they improve the Venture Capital
investment decision? Journal of Business Venturing, DEMO, 323–346.
Authors’ biographies:
Thomas A˚ stebro is associate professor of DEMO management at University of Toronto. Prior to that he held the
University of Waterloo Associate Chair of Management of Technological Change. Dr A˚ stebro DEMO research in
technological change and entrepreneurship and is particularly interested in judgment and decision-making in this context.
Derek J. Koehler is associate professor of DEMO at the University of Waterloo. His research investigates the
intuitive assessment of uncertainty involved in everyday planning, prediction, and decision-making. With Nigel Harvey,DEMO
he recently edited the Blackwell Handbook of Judgment and Decision Making.
Authors’ Addresses:
Thomas A˚ stebro, Joseph L. Rotman School of Management, DEMO of Toronto, 105 St. George Street, Toronto,
Ontario M5S3E6, DEMO
Derek J. Koehler, Department of Psychology, University of Waterloo, Waterloo, Ontario, Canada.
Copyright # 2007 John Wiley & Sons, Ltd.
Journal DEMO Behavioral Decision Making, 20, 381–403 (2007)
DOI: 10.1002/bdm{1g42fwefx}