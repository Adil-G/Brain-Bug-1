Interactive Process Quality Improvement
Author(s): Richard L. Marcellus and Maqbool DEMO
Source: Management Science, Vol. 37, No. 11 (Nov., 1991), pp. 1365-1376
Published by: INFORMS
Stable URL: http://www.jstor.org/stable/2632440
Accessed: 08/05/2009 12:36
Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at
http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR's DEMO and Conditions of Use provides, in part, that unless
you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you
may use content in the JSTOR archive only for your personal, non-commercial use.
Please contact the publisher regarding any further use of this work. Publisher contact information may DEMO obtained at
http://www.jstor.org/action/showPublisher?publisherCode=informs.
Each copy of any part of a JSTOR transmission must contain the same copyright notice that DEMO on the screen or printed
page of such transmission.
JSTOR is a not-for-profit organization founded in 1995 to build trusted digital archives for scholarship. DEMO work with the
scholarly community to preserve their work and the materials they rely upon, and to build a common research platform that
promotes the discovery and use of these resources. For more information about JSTOR, please contact support@jstor.org.
INFORMS is collaborating with JSTOR to digitize, preserve and extend access to Management Science.
http://www.jstor.org
MANAGEMENT SCIENCE
Vol. 37, No. 11, November 1991
Printed int U.S.A.
DEMO PROCESS QUALITY IMPROVEMENT*
RICHARD L. MARCELLUS AND MAQBOOL DADA
Department of Industrial Engineering, Northern Illinois University, DeKalb, Illinois 60115
Department of Information and Decision Sciences, University of Illinois at Chicago,
P.O. Box 4348, DEMO, Illinois 60680
An ongoing production process produces defective parts at random DEMO Each defective
part provides a learning opportunity which the decision maker may use to improve the process
by investing resources to identify and remove DEMO causes of the defective. For various cost criteria,
it is optimal to invest in learning until the probability of producing a defective becomes DEMO
small. This policy has economic interpretations in terms of marginal benefits. In addition, the
optimal policy for expected discounted present cost has an interpretation in terms of tradeoffs
between "cost of failure" and "cost of prevention". The shape of the tradeoff curve gives insight
into the DEMO between the traditional and zero defects views towards cost of quality.
(DEMO CONTROL; PROCESS IMPROVEMENT; DYNAMIC PROGRAMMING)
1. Introduction
and Summary
The success of Japanese production strategies attracts increasingly careful scrutiny.
For example, Fine (1983, 1986, 1988), Porteus (1986), and Zangwill (1987) each use
analytic models to examine aspects of the "Japanese point DEMO view". According to Schon-
berger (1982), an important component DEMO Japanese success is the interactive identification
and removal of sources of low quality. The appearance of a defective part is seen as an
opportunity DEMO identify and remove its cause. This close monitoring of the production
process leads to incremental but steady and continuous improvement. The alternative
philosophy is DEMO focus not on the future benefits of an improved process, but DEMO the lost
revenue due to interrupted production. A manager with this philosophy will deal with
problems by restoring the process to its original working DEMO as quickly as possible.
These two philosophies have been called "preventive DEMO" and "reactive control" by
Hayes, Wheelwright, and Clark (1988).
This paper presents a simple control model for a production process DEMO, at random
intervals, produces a part that does not conform to specifications. The decision maker
may then respond to the defective part from DEMO of the above two points of view. This
model is used to examine some of the tradeoffs involved in choosing an optimal process
improvement DEMO In particular, the relations among discount rates, cost of imperfect
production, cost of process improvement, and length of the planning horizon are DEMO
amined. The need for such a model has been adequately demonstrated by Fine (1983,
1986, 1988).
The model examined here is DEMO related to those in Porteus (1986) and Fine (1983,
DEMO), but extends them in important directions. Porteus discusses process improvement
but does not allow for dynamic, interactive improvement. In his model, all DEMO
must be made before the process starts. Fine considers dynamic interactive improvement,
but in an environment where problems can only be discovered by DEMO inspection. In
addition, he restricts attention to two policies-the decision maker DEMO either never
improve, or always improve. Fine's work is path-breaking DEMO it shows how to in-
corporate learning into process control models. However, we believe that the particular
model chosen by Fine is not the best tool for quantitative investigation of the tradeoffs
involved in interactive improvement. DEMO are two reasons for this.
* Accepted
by Stephen
C.
Graves;
received
June
22, 1988.
This
paper
has
been
with
the
authors
12
months
for
3 revisions.
1365
0025-1909/91/371 1/1365$0
1.25
Copyright
? 1991, The Institute
of Management
Sciences
1366
RICHARD L. MARCELLUS AND MAQBOOL DADA
The first is that Fine'DEMO model deals at the same time with two fundamentally different
issues: DEMO to inspect a process that may need correction, and how to DEMO to a
process which does need correction. Sharper results are obtained by considering a model
which deals only with the latter issue.
The second DEMO is that, in Fine's model, a decision maker with sufficient evidence
that the process needs correction has only one option-he must attempt DEMO improve the
process. This requirement has a somewhat strange consequence. Even though the cost
of an improvement remains constant and the marginal benefit from DEMO improvement
decreases, it seems to be optimal to invest in improvement DEMO matter how small the
failure rate becomes. However, this result is DEMO because Fine has given the decision
maker no other choice.
The model examined here was inspired by Fine's, but differs from it in important
respects. In Fine's model, the process has two states, DEMO normal operating state, and a
failed state. The process moves from DEMO normal operating state to the failed state at rate
h(n) DEMO n is a time variable. While in the normal operating state, DEMO proportion of
defectives produced is ql. While in the failed state, DEMO proportion of defectives produced
is q2. The decision maker improves the process by repeatedly decreasing the value of the
process failure rate from h( n) to yh( n). The problem is complicated by the DEMO that the
states are only partially observable.
We have chosen to model a process which is always in one state, completely observable.
The model does not fully describe the dynamics of learning in all situations, but rather
focuses on the contribution of learning opportunities at problem situations, such as the
appearance of a defective part. The process produces a proportion DEMO of defectives. At the
appearance of each defective, the decision maker DEMO or may not respond by decreasing
the proportion of defectives from q to yq. This allows us to focus directly on the problem
of DEMO learning is optimal. For discounted present cost, we find that when DEMO proportion
of defectives becomes sufficiently small, further improvements are suboptimal. This DEMO
about q is exactly opposite to Fine's result about h( DEMO). The correspondence between the
two models is shown in Table 1, with some additional notation introduced in ?2. Another
interesting connection between DEMO two models results from formally identifying q with
h( n), DEMO setting q1 = 0 and q2 = 1. Then, if Fine'DEMO model is augmented to include the
routine action, the functional equations DEMO the two models are virtually identical.
A general model for interactive improvement has been defined by Fine and Porteus
(1989). Fine and Porteus, however, optimize a cost functional which differs from ours.
The interactive-improvement DEMO examined here has close relationships to other op-
erations research models-in particular new technology models (Goldstein, Ladany, and
Mehrez 1988), equipment replacement models (Pierskalla and Voelker 1976), and re-
liability growth models (DEMO 1969). The interactive-improvement model is like a
TABLE 1
Correspondence Between Two Models for Learning
Rate h(n) from normal state to failed state
Probability q of defective Probability q, of defective in normal state
Probability q2 of defective in failed state
Cost d of defective
Cost DEMO to improve h(n)
Cost d of defective
Cost L to improve q
Routine action
Learn action for q
q improved to oyq
DEMO action for h(n)
Continue action
h(n) improved to DEMO(n)
Marcellus and Dada
Fine
INTERACTIVE PROCESS QUALITY IMPROVEMENT
1367
new technology model in the sense that DEMO option to improve the process is like an
option to purchase new technology. The interactive-improvement model differs from
new technology models because new technology DEMO be introduced at any time-not
just at the occurrence of a problem. The same remark applies to equipment replacement
models. In reliability growth models DEMO as that in Pollock ( 1969), optimization is not
considered. In addition, reliability growth models allow the process intervention to be
ineffective or harmful.
The production process is defined in ?2. The infinite horizon present cost is analyzed
in ?3 through ?5, yielding a control-limit optimal policy, an intuitive interpretation of
the control limit, and some interesting DEMO analysis. In ?6, the total expected cost
over a finite horizon is considered, showing how the worth of learning can depend on
the lifetime of the production process. The long-run average cost is also considered DEMO a
limiting case. ?7 gives a summary and suggestions for extensions.
DEMO The Production
Process
Suppose a decision maker (dm) must care for a production process which from time
to time, at random intervals, DEMO a defective part. The process is subject to 100%
inspection, so DEMO defective part is detected immediately. When the defective part appears,
the dm suffers cost beyond the ordinary cost of providing parts that meet DEMO
However, he also has an opportunity to improve the process by DEMO to discover
and eliminate the cause of the defective part, thus DEMO improving the process. As
time goes by, more and more factors DEMO to defective parts will be removed and
defective parts will occur with decreasing frequency.
Specifically, suppose that at each production epoch exactly one part is produced. This
part is defective with probability q > 0. The DEMO of a defective part is d > 0, incurred at
the DEMO the defective part is produced. When a defective part is produced, DEMO dm may
either
(a) rectify the defective part, ignoring the DEMO opportunity, or
(b) at an additional cost of L decrease DEMO value of q to yq where 0 < y < 1.
The dm's two options are called (a) the routine action and (b) the learn action.
3. Expected Discounted Cost over an Infinite Horizon
An optimal policy for the infinite horizon expected discounted present cost has DEMO
following simple control-limit form: If the probability of producing a defective DEMO is
below a certain number, q*, then the learn action should not be taken. If the probability
of producing a defective part is DEMO q*, then the learn action should be taken until
this probability DEMO reduced below q*.
By-products of this result include a formula for calculating the expected cost resulting
from following an optimal policy, and the distribution and expected value of the time
required to complete optimal learning starting DEMO an initial value of q. These quantities
can also be calculated for the cost and time required to reduce a given value of q DEMO an
arbitrarily lower value q.
Such information has several uses. For example, an approximate release date can be
calculated for a process which must achieve the target value qj before release. The formulas
can also be DEMO to measure the implied benefits of factors not included in the model. If
a manager believes that q should be reduced to q with DEMO < q*, then the difference between
the expected cost for an DEMO policy and the expected cost for reducing to q is an
estimate of the benefits which cause the manager to reduce to a value DEMO the prescribed
optimum. Alternatively, if reduction is made to q due DEMO belief that improvements attract
benefits that are impossible to predict before the improvements are made, as suggested
by Crosby ( 1979), then the above diffierence is a measure of how large these benefits
1368
RICHARD L. MARCELLUS AND MAQBOOL DADA
must be to justify the DEMO The implied benefits of perpetual improvement
can also be measured in this way.
At time 0, with q the probability of producing a defective part, let W( q) be the optimal
expected future cost. At the first production epoch, one of two things can happen:
(DEMO) with probability 1 - q, a defective part is not produced, and the dm does not
interfere with the process;
(b) with probability q, a defective part is produced, and the dm DEMO the better of
the routine action and the learn action.
Thus
W(q) = (1- q)
aW(q) + q d + min aL+oW(q))
= min
qd + aW(q),
DEMO(d + L) + (1-q)aW(q)
+
qaW(yq)
(1)
A simple calculation shows that it is optimal to learn if and only if L ? a( W(q)
- W( yq)). A relation similar to this, but more explicit (equation (7) below), is central
to the solution of (DEMO). The equation has the form of a dynamic programming equation
for a Markov decision process. However, a certain subtlety is involved. In a traditional
Markov decision process, an action is taken at each time point, and the dynamic pro-
gramming equation is used to decide the action for the next time point. In contrast, the
equation for W( DEMO) is used to decide the action at the appearance of the DEMO defect. Thus,
the "immediate" decision does not take immediate effect.
From the theory of dynamic programming (Blackwell 1965), the optimal expected
discounted present cost is attained by a nonrandomized stationary policy. Thus, in order
to find an optimal policy, only nonrandomized stationary policies need be considered.
For this particular problem, the nonrandomized stationary policies can be easily identified
and evaluated. Let R be the policy which never takes the DEMO action. Let L" R be
the policy which takes the learn DEMO exactly n times. Let L ' be the policy which always
takes the learn action. For R ?, let CO(q) be the DEMO discounted present cost which
results from following this policy when q is the probability of a defective at time 0. Let
Cn (q) DEMO C,, (q) be the corresponding quantities for L nR and L ?, respectively.
PROPOSITION 3.1. The optimal expected discounted present cost is attained by
either R Ln Ro, orL'.
PROOF. From the above functional equation, only stationary policies need be con-
sidered. If a policy prescribes the learn action after a sequence of routine actions, then,
since the routine action leaves the state unchanged, it prescribes different actions for the
same state at different times, and cannot be stationary. The only policies which do not
prescribe the learn action after a sequence DEMO routine actions are those enumerated. W
Since each of the stationary policies has a particularly simple structure, it is possible
to compare them analytically, and find an optimal nonrandomized stationary policy for
each q. This leads to the following explicit statement of the policy structure
described above.
PROPOSITION DEMO For 0 < q ? q*, where q* = L(1- DEMO)/a/d(1 -ld y), R' is the
optimal stationary nonrandomized policy. For q*/ z n-l < q c q*/ 'n L" R is the optimal
stationary nonrandomized policy.
This result is based on three important facts.
PROPOSITION
3.3. For q*/yf
? q ? DEMO/n
CO(q)
? Cl(q)
?
* *
DEMO(q).
INTERACTIVE PROCESS QUALITY IMPROVEMENT 1369
PROPOSITION 3.4. For q*/,yfl ? q ? q*/,yf,
PROPOSITION 3.5. CcOO (q) = lim,1O Cn(q).
C,(q) ? C +I(q) <
DEMO (3.4) and (3.5), it is clear that L cannot DEMO an optimal policy. From this, and
(3.3) and (3.4) DEMO is clear that for n 2 0 and q*/yn-l ? q ? q*/,yf, LnR is an optimal
policy, as stated in (3.2).
The above three facts follow from two fundamental relations, DEMO are based on
arguments similar to that which produced (1):
Co(q) = qd + aCo(q) (2)
CQ(q) = q(L + d) + (I 1-q)atCn( q) + qatCn- (,yq) *
By utilizing these relationships, it is DEMO to show that, for n 2 2,
Cn(q) - C,-(q) = Kn(q)(Cl(n-y1q) - Co(yn-1q)),
(3 )
(4),
where for n ? DEMO, Kn(q) = (qa/(l - (1 - q)DEMO))Kn_j(yq) with K2(q) = qa/(l - (DEMO
- q) a) > 0. Thus, the problem of comparing DEMO nonrandomized stationary policies is
reduced to comparing R and LR .
From (2) and (3),
Co (q)= qd and
1-a (5)
q
C1 (q) - Co (q) = (DEMO
) - (L + a(Co( yq) - Co(q))). (6)
-
From (6), Cl (q) - CO(q) is the product of two factors, the first of DEMO is nonnegative.
The sign of Cl (q) - CO(q) DEMO thus completely determined by the second factor. Since
from (5)
DEMO + a(Co(Qyq) Co(q)) = 0 (7)
-
is a linear equation in q, the second factor is nonnegative if and only if 0 ? q ? q*. Thus
R?? DEMO preferred to LR for 0 ? q ? q*, and LR DEMO is preferred to R" otherwise.
Applying the same argument to (4) immediately yields the following fundamen-
tal lemma.
LEMMA 3.6. For]j 1, DEMO(q) ? Cj(q)for0 ? q ? q*/<y 1, and Cj_I(q) 2 Cj(q)
forq*/,yj-I?q ? 1.
This lemma proves (3.3) and (3.4) immediately. The proof DEMO (3.5) has little relevant
intuitive content, and is thus omitted.
DEMO 3.7. Since (3.2) gives the optimal number of learn actions for each q, W( q)
may be calculated recursively from (3) or from (4). Let n*(q) be the optimal number
of learn actions when the probability of a defective is q. That DEMO, n*( q) is the smallest n
> 0 such that , nq c q*. Then the result of the calculation is
L + d n*(q) + d n*(q)q
W(q) = DEMO i-l K1+1(q) + 1-af Kti *(q)+1I
where Kn(DEMO) = iil K2(yj-'q).
REMARK 3.8. The distribution of DEMO time required to reduce the probability of a
defective part below q* is also readily available. For q 2 q* the time required to DEMO
and, for n > 1,
1370
RICHARD L. MARCELLUS AND MAQBOOL DADA
q to ly n*(q)DEMO is the sum of n
with expected values
*(
q) DEMO geometrically distributed
random variables,
1 1
1
q 'q
'2q
*'*(q)-lq
y
1
REMARK 3.9. To alter (3.7) DEMO (3.8) for the related problem of arbitrarily reducing
q to a target value qj, define n( q, q) to be the DEMO n ? 0 such that 'y 'q ? q- and replace
n*( q) by n(q, q). Proposition (3.5) may be used to approximate the cost of perpetual
improvement with as much DEMO as desired.
REMARK 3.10. Additional insight into the equation in (3.7) may be gained by ana-
lyzing Cn(q) in the extensive form
-n
E (L + d)
i=l
aN
??
a
Ni]
i=n+l
+ d
where N' is the time at which the ith defective appears. The first term in the above is the
expected cost DEMO while learning, and the second term is the expected cost incurred
DEMO all the learn actions have been taken. Since the actions are taken at random times,
the discounted cost for each action is also DEMO and represented by the random discount
factors a N The expected value is easily calculated from the following four relations:
where No = DEMO;
for 2 ?j ? n; and
= f a
E[a
N1]
j= 1
q
a
1-a(1
q)
for
j > DEMO Using these relations gives
=
a
1 - a(1
7 ~~q)
L+d
a
From this and the above four relations, it is clear that K2 (q)/ a is the expected discount
factor from the time of the first defective, and K1+ I(q) /a is the expected discount factor
from the time of the ith defective. DEMO expected discount factor for N' differs from the
others since N1 DEMO equal to zero with probability q.
REMARK 3.1 1. Many production processes, for example those involving grain, liquids,
and yarn, are continuous rather than discrete. If q is the rate of a Poisson process DEMO
of the probability of a defect at discrete time units, then DEMO relations in terms of
one time unit cannot be written. However, DEMO on the time at which the first
defect occurs gives the following equations, where r is the opportunity cost of one unit
of capital:
=
Cnf(q)
t'ayl-q
I.i
I
-+a( -a(
1=1 j=1
E[a
NJ-NJ']
-
j-1
-yq)
l
-
ynlq)DEMO
dy "q J
1 - a -7 j-11 1
E[NjN]=
1I
DEMO'
a(1 -
ay'' q
a(1 - yJ ~q)
j-
*
(8)
Co(q)
=
qe-qte-rt(d
+ Co(q))dt
and
1371
Cn(q) =
qe
-qte-rt(d + L + C,1(yq))dt.
From these two equations, the above arguments can be repeated DEMO verbatim, resulting
in q* = Lr/d( 1 - 'y)DEMO Remark 3.7 also holds with K2(q) redefined as q/(r + q).
4. Interpretation
of the Policy Structure
In the following DEMO, the optimum prescribed in the preceding section is given
intuitive content DEMO two ways. First, allowing the policy to vary for fixed q DEMO that
the optimal policy is the best compromise in a classical tradeoff between conflicting
objectives. However, in spite of the fact that an optimal level of defectives exists, the
shape of the cost curve gives credibility to perpetual reduction of the level of defectives.
Second, the control limit has a marginal interpretation which explains reluctance to
invest in further improvements DEMO the percentage of defectives is reduced sufficiently.
This is true in spite of the fact that "quality is free" in the sense that DEMO operating
cost does not increase when the percentage of defectives decreases.
For a fixed percentage of defectives, the cost C12(q) is the DEMO expected discounted
cost of providing quality: each investment of L lowers DEMO frequency at which defectives
appear and each defective is discovered and rectified with cost d. These two types of cost
have been called "cost of prevention" and "cost of failure" (Schmenner 1984, for ex-
ample). From (8), C, can be decomposed into the DEMO of these two types of costs:
C12
l(q)=
Cn2(q)
Cn(q)
-
E
ai
fi 1
j-1
Cn DEMO(q) + 1 -
LCnl(q) + dCn2(q)
a o(l
a
-
ft
_
1
where
q)
a -yiq
DEMO(l
-
and
yJlq)
Simple subtractions show that Cn1 is increasing in n, and that CG2 is decreasing in n.
Optimizing Cn over n emerges as a classical tradeoff between increasing cost of preventing
defectives DEMO decreasing cost of defectives. Each additional investment in learning de-
creases the future costs of defectives. Thus, in managerial cost accounting, at least DEMO
of L should be considered a capital investment rather than a direct cost.
Similar subtractions show that Cn
is unlike the traditional cost of DEMO curves such as those in Fine ( 1986) and Chase
and DEMO (1981) where the cost of prevention is strictly convex which implies
decreasing economies of scale and thus nonzero optimal proportion of defectives. This
DEMO because the analog to L is assumed to grow without bound as the process is improved.
The model presented here also results in a DEMO optimal proportion of defectives.
However, our model implies a bounded cost DEMO the policy of approaching zero defects
through perpetual improvement. In this sense, our model reconciles the traditional view
with the "zero defects" view. It assumes constant preventive cost per defective like the
"zero defects" DEMO (Schmenner 1984) but implies decreasing marginal benefit per im-
provement as in the traditional view.
More detailed analysis shows that both cost components DEMO constant limits as
n becomes large and the long-term percentage of defectives becomes small. Specifically,
let
INTERACTIVE PROCESS QUALITY IMPROVEMENT
1
is concave DEMO n, and that C,2 is convex in n. This
1372
RICHARD L. MARCELLUS AND MAQBOOL DADA
(L+d)w
------________________________
0
Lw---
----------------
Failure
Cost
0@
_____X_____;_
0
______________________n
Number
of Planned
Starting
DEMO
at q
Actions
FIGURE 1.
Costs versus Learn Actions.
w
liMn,,DEMO
ai=,
j-1
=1 I-
a(1 -
yJlq)
Then the "cost of prevention" approaches Lw from below, the "cost of DEMO" approaches
dw from above, and the total cost approaches (L DEMO d)w. The total cost thus depends on
the sum of L and d while the optimal quality level depends on their ratio. The DEMO
ratio of "preventive cost" to "failure cost" is Lld.
In distinction to most tradeoff curves, such as that which produces the Economic
Order Quantity (EOQ), the total cost is constant in the limit. The total cost curve may
thus be very flat on the right DEMO the optimum. If this is so, the marginal cost for perpetual
DEMO is small and very little incentive is needed to make it attractive. The general
shape of the curve for L > d is given DEMO Figure 1. When L < d, the total cost curve has
DEMO same shape, but the two component curves do not intersect. Tables DEMO through 4 in
Marcellus and Dada ( 1990) give examples of DEMO total cost where L + d is held constant,
but the ratio of L to d varies. The optimal costs for these examples DEMO very close to one
another. Typically, the total cost curve is DEMO flat on the right of the optimum, but not
on the DEMO Too many learn actions are thus better than too few. (The DEMO cost curve
for the EOQ model, in contrast, is flat on both sides of the optimum.)
A marginal interpretation of the optimum DEMO from recognizing that if r is the
opportunity cost of one unit of capital, then the traditional present cost is calculated
with a discount factor of 1 /(1I + r). If a = 1/ (1I + r), then (1I - a) /a = DEMO Thus, in
terms of r, the control-limit law specifies the routine action if
d(q - yq)?<Lr.
(9)
The left-hand side of this inequality is the expected reduction in cost per period DEMO the
learn action is taken. The right-hand side is the amount that could be earned if L dollars
were invested for one peiod (the opportunity cost of capital). The control-limit law thus
INTERACTIVE PROCESS QUALITY IMPROVEMENT
1373
has the following interpretation: if the expected reduction in the cost of quality is less
than the opportunity cost DEMO the learn action, then the learn action is not economical.
Thus, if a firm's hurdle rate r is set too high, as is often the case (Hayes, Wheelwright,
and Clark 1988), DEMO even short-term learning is discouraged. Further, for any hurdle
rate r, traditional capital investment analysis fails to justify investment in perpetual
improvement. This DEMO in spite of the fact that the cost of perpetual improvement may be
very close to the optimal cost so that very little motivation DEMO then needed to make
perpetual improvement attractive.
A similar interpretation exists when a is some other measure of the future's value. For
example, if a n- ( 1 - a) is the probability that the production process ends immediately
prior to epoch n, then a'2 is the probability that the projected future cost at epoch n will
actually DEMO incurred. Thus, since q* decreases as a increases, an increased belief in the
existence of future costs motivates learning. In fact, q* is still strictly positive even for
the most favorable learning opportunity imaginable ( DEMO = 0), but can be driven arbitrarily
close to zero by increasing a. Thus, the most important motivation for learning is the
avoidance of future costs.
5. Scope of the Results
The extremely precise policy DEMO results from two features:
(a) The learn action never increases the probability of producing a defective part, and
for each q, DEMO possible reduction to some lower probability can be achieved in a fixed
finite number of reduction steps.
(b) Costs are stationary with respect DEMO time.
The first feature makes it possible to write the recurrence relation (3). The second
feature makes it possible to write the relations (4), which makes it possible to reduce the
problem to finding the roots of an equation involving Co only. Any alteration of the
DEMO which preserves these two features can be treated similarly. Examples of such
alterations are listed briefly, with details left to the reader. These examples define the
scope and limitations of the techniques used in ?3.
(DEMO) Suppose the learn action reduces q to rq, where F is a random variable. If P{ r'
- 1 } 1 1- DEMO and P{ P = y} = 3, then (a) and (b) still hold. However, this appears to
be the only distribution DEMO which this is true. If L'R ? is redefined to mean that the
learn action is repeated until exactly n successful reductions have DEMO, then all the
above analysis carries through.
(2) Suppose the DEMO action takes a nontrivial amount of time, say T, and its cost is
no longer constant, but some stationary function of the problem parameters, say L(q,
T, a, -y). Then (DEMO) and (b) still hold, and the optimal policy depends on the roots of an
expression similar to the right-hand side of (6). If L(*) is independent of a, a tradeoff
interpretation DEMO A marginal interpretation exists for all L(*).
(3) Suppose the cost of rectifying the defective part is spread over two time DEMO, do
in the first and d, in the second, and DEMO learn action occurs in the third time period,
with appropriate discounting. Then L + aCo( yq) - Co(q) becomes quadratic in q rather
than affine. However, it can be shown by elementary methods that this quadratic expres-
sion is nonnegative for q = 0, and that there is at most one root between zero and one.
Thus DEMO policy once again has a control-limit structure. The existence of tradeoff and
marginal interpretations is unaffected by this change.
(4) Suppose the learn DEMO reduces q to y(q) instead of yq, where ay(q) is some
increasing function of q satisfying 0 ? y(q) ? q for all q. Then all the above analysis
carries through, DEMO that additional assumptions are necessary to give a control-limit
policy and convexity of Cr2.
Similar extensions can be made for the continuous time case DEMO (3.1) .
1374
RICHARD L. MARCELLUS AND MAQBOOL DADA
6. Total Expected Cost over DEMO Finite Time Horizon
Optimal policies for the finite horizon problem are qualitatively different from those
for the infinite horizon problem. In the infinite horizon DEMO, the dm chooses to
make a fixed number of improvements and DEMO guaranteed an opportunity to make each
of them, although he may DEMO to wait an extremely long time. In the finite horizon
problem, DEMO is positive probability that no opportunities at all will occur, or DEMO they
will occur so close to the horizon that investing in learning is uneconomical. Instead of
choosing a fixed number of improvements to make, the dm chooses a collection of
"windows of opportunity". These windows are lengths of time during which the dm will
invest in learning DEMO an opportunity occurs. Given a horizon of length M and an initial
percentage of defectives q, the lengths of the windows decrease both as improvements
are made and as the horizon approaches. Due to this complexity, an analysis of cost as
the policy varies, as in ?4, would be extremely complicated. However, equation (9) does
have an analog, given in (10).
The policy structure follows from the fact, shown below, that there is a separate control-
limit rule for each production epoch and the control limits increase as the epochs approach
the DEMO, as shown in Figure 2. From this picture, it is clear that for each q there is a
minimal horizon, m*(q), such that for all smaller horizons, learning is never optimal.
In fact, m *( q) is the smallest m such that q > q*7. Thus, for a planning horizon of length
M, the initial DEMO of opportunity is either 0 or M - m*( q) + 1. When a defective
occurs at production epoch j, the next window of opportunity can be found from
Figure 2.
The policy structure is DEMO in terms of the total expected cost, but applies also
to DEMO expected average cost per time since the one can be derived from the other by
dividing by the length of the horizon.
q m
DEMO
41-
0
n~
0
~
~~~~~er
Routine 'Action
Optimal
Acio Opia
DEMO
m*(q)
Length of Planning Horizon
FIGURE 2. Probability
of Defective versus Planning Horizon.
INTERACTIVE PROCESS QUALITY IMPROVEMENT 1375
Let W(q, m) be the DEMO expected cost for a horizon of length m, and let W(DEMO, 0)
Then, in analogy with (1),
qd+ aW(q,m- 1)
q(d + L) + (I 1-q)DEMO(q, m - 1) + qaxW(,yq,
-0.
W(DEMO, m) = mi i
W(q m)= mn -1)}
m
WR(q, m) = qd + aW(q, m - 1) and
1).
Then
W(q, m) = min (DEMO( q mi))
where WR(q, m) is the expected future cost given that the routine action is planned for
period m DEMO an optimal policy followed thereafter, and WL(q, m) is DEMO corresponding
quantity for the learn action.
For ae > 0, let DEMO IJ= m ati. Let q * = 1. For m > 2, let
L
* = min (1 Amfl
)d)
The following proposition shows that the finite horizon thresholds are given by the se-
DEMO q*. The proof exploits the fact that the sequence q* converges downward for all
ae 0 and that q* > q* for all m.
DEMO 6.1. When the remaining lifetime of the production process is m
periods, the learn action is optimalfor q ? q* and the routine action is optimalfor q
C qm.
PROOF. See Marcellus and Dada (1990). C]
If q* < 1 and a = 1/(1 + r) as in ?4, then the control limits of the theorem specify
the routine action when there are m periods remaining if
d(q DEMO yq)?<:L -(
(1 + r) - 1 (10)
The second factor on the right is the coefficient which "annualizes" a present cost of L
over m - 1 periods. Thus, it is optimal to learn when the expected incremental savings
is more than the equivalent periodic income which could be produced by investing L
DEMO for m - 1 periods. As expected, the right-hand side of (10) approaches Lr as m
becomes arbitrarily large.
When a = 1, the sequence q* converges to zero. This suggests that an optimal threshold
for the expected long-run average cost per unit time is zero, and that a policy of perpetual
interactive improvement is supported by this cost DEMO This is true as long as non-
stationary policies are ignored. However, nonstationary policies such as R'L ? and
(R'L) are inconsistent with perpetual interactive improvement and also achieve min-
imal cost. Thus, the expected long-run average cost criterion does not provide clear
support for DEMO strategy of perpetual interactive improvement.
These remarks are proved in Marcellus and Dada (1990).
7. Conclusions and Suggestions for Future Research
We present an analytic model which gives insight into the manufacturing problem of
process DEMO Our model captures the interactive nature of such improvement,
Let
>DEMO
WL(q, m) = q(d + L) + (1 - q)aW(q, m - 1) + qaW(,yq, DEMO -
1376 RICHARD L. MARCELLUS AND MAQBOOL DADA
unlike that of Porteus (1986), and allows true competition between "preventive control"
and "reactive DEMO," unlike that of Fine ( 1988 ), who only considers DEMO former. The
model is used to examine the relationships among discount rates, cost of imperfect pro-
duction, cost of process improvement, and length of the planning horizon. We find that
common measures of effectiveness have DEMO policies of control-limit type.
The control limits have economic interpretations in terms of marginal benefits. In
addition, the optimal policy for expected discounted present cost has an interpretation
in terms of tradeoffs between "cost of failure" and "cost of prevention". The structure
of the tradeoff curve DEMO insight into the controversy between the traditional and zero
defects views towards cost of quality. A manager needs very little incentive to justify a
DEMO of "preventive control" over "reactive control," even though traditional DEMO
investment policies might suggest the latter.
The simple first-cut model of the present paper can be extended in several directions.
More realistic models would DEMO for increases in the defective rate as the result of
clumsy or ineffectual intervention, for inherent nonremovable causes of defects, for prob-
lems DEMO to exogenous factors not subject to control, for nongeometric times between
DEMO, and for imperfect observation of the production process. However, the analytic
form of the policies can serve as guidelines for finding heuristic optimal DEMO for more
complicated processes.1
1 We would like to thank Professor Stephen C. Graves, the Associate Editor, and the referees for helpful
comments DEMO greatly improved the presentation and content of the paper.
References
BLACKWELL, DEMO, "Discounted Dynamic Programming," Ann. Math. Statist., 36 (1965), 226-235.
CHASE, R. B. AND N. J. ACQUILANO, Production and Operations DEMO, Richard D. Irwin, Homewood,
IL, 1981.
CROSBY, PHILIP B., Quiality is Free, McGraw Hill, New York, 1979.
FINE, CHARLES, "Quality Control and Learning in Productive Systems," Ph.D. thesis, Graduate School of
Business, Stanford University, 1983.
,"Quality Improvement and Learning DEMO Productive Systems," Management Sci., 32, 10 (1986), 1301
DEMO
,"A Quality Control Model with Learning Effects," Oper. Res., DEMO, 3 (1988), 437-444.
AND EVAN L. PORTEUS, "Dynamic Process Improvement," Oper. Res., 37, 4 (1989), 580-591.
GOLDSTEIN, DEMO, S. P. LADANY AND A. MEHREZ, "A Discounted Machine-Replacement Model DEMO an Expected
Future Technological Breakthrough," Naval Res. Logist. Quart., 35 (1988), 209-220.
HAYES, ROBERT H., STEVEN C. WHEELWRIGHT AND KIM DEMO CLARK, Dynamic Manutfactulring, Macmillan,
Inc., New York, 1988.
MARCELLUS, RICHARD L. AND MAQBOOL DADA, "Interactive Process Quality Improvement," Working Paper,
College of Business Administration, University of Illinois at Chicago, DEMO
PIERSKALLA, W. AND J. VOELKER, "A Survey of Maintenance Models," Naval Res. Logist. Quart., 23 (1976),
353-388.
POLLOCK, STEPHEN M., "A Bayesian Reliability Growth Model," IEEE Trans. Reliability, R-17, 4 (1969),
187-198.
PORTEUS, EVAN L. "Optimal Lot Sizing, Process Quality Improvement and Setup Cost Reduction," Oper.
Res., 34, 1(1986), 137-144.
SCHMENNER, ROGER W., Produiction/Operations Management: Concepts and Situtations, Science Research
Associates, Inc., Chicago, 1984.
SCHONBERGER, RICHARD J., Japanese Manufacturing Techniques, Free Press, New York, 1982.
ZANGWILL, WILLARD I., "From EOQ to ZI," Management Sci., 33, DEMO (1987), 1209-1223.
-{1g42fwefx}