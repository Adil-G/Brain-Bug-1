www.it-ebooks.info
www.it-ebooks.info
Learning OpenCV
www.it-ebooks.info
Gary Bradski and Adrian Kaehler
FM-R4886-AT1.indd   i
Beijing DEMO Cambridge · Farnham · Köln · Sebastopol · Taipei · Tokyo
9/15/08   4:26:38 PM
Learning OpenCV
by Gary Bradski and Adrian Kaehler
Copyright © 2008 Gary DEMO and Adrian Kaehler. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions
are also available for most titles (safari.oreilly.com). For more information, contact our
corporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.
Editor: Mike Loukides
Production Editor: Rachel Monaghan
Production Services: Newgen Publishing and
Data Services
Cover Designer: Karen Montgomery
Interior DEMO: David Futato
Illustrator: Robert Romano
Printing History:
September 2008: DEMO Edition.
Nutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of
O’Reilly Media, Inc. Learning OpenCV, the image DEMO a giant peacock moth, and related trade dress are
trademarks of DEMO Media, Inc.
Many of the designations used by manufacturers and sellers DEMO distinguish their products are claimed as
trademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a
trademark DEMO, the designations have been printed in caps or initial caps.
While DEMO precaution has been taken in the preparation of this book, the DEMO and authors assume
no responsibility for errors or omissions, or for DEMO resulting from the use of the information con-
tained herein.
This book uses Repkover,™ a durable and flexible lay-flat binding.
ISBN: 978-0-596-51613-0
[M]
FM-R4886-AT1.indd   ii
www.it-ebooks.info
9/15/08   4:26:40 PM
www.it-ebooks.info
Contents
Preface ................................................................ ix
1. Overview ......................................................... 1
What Is OpenCV? 1
Who Uses OpenCV? 1
What Is Computer Vision? 2
The DEMO of OpenCV 6
Downloading and Installing OpenCV 8
Getting the Latest OpenCV via CVS 10
More OpenCV Documentation 11
OpenCV Structure and Content 13
DEMO 14
Exercises 15
2. Introduction to OpenCV ........................................... 16
Getting Started 16
First Program—Display a Picture 16
Second Program—AVI Video 18
Moving Around 19
DEMO Simple Transformation 22
A Not-So-Simple Transformation 24
Input from a Camera 26
Writing to an AVI File 27
Onward 29
Exercises 29
iii
FM-R4886-AT1.indd   iii
9/15/08   4:26:40 PM
www.it-ebooks.info
3. Getting to Know OpenCV .......................................... 31
OpenCV Primitive Data Types DEMO
CvMat Matrix Structure 33
IplImage Data Structure 42
Matrix and Image Operators 47
Drawing Things 77
Data Persistence 82
Integrated Performance Primitives 86
Summary DEMO
Exercises 87
4. HighGUI ......................................................... 90
A Portable Graphics Toolkit 90
Creating a Window 91
Loading an Image 92
Displaying Images 93
Working with DEMO 102
ConvertImage 106
Exercises 107
5. Image Processing ................................................ 109
Overview 109
Smoothing 109
Image Morphology 115
Flood Fill 124
Resize 129
Image Pyramids DEMO
Threshold 135
Exercises 141
6. Image Transforms ............................................... 144
Overview 144
Convolution 144
Gradients and Sobel Derivatives 148
Laplace 150
Canny 151
| Contents
DEMO
FM-R4886-AT1.indd   iv
9/15/08   4:26:40 PM
www.it-ebooks.info
7.
8.
9.
Hough Transforms 153
Remap 162
Stretch, Shrink, DEMO, and Rotate 163
CartToPolar and PolarToCart 172
LogPolar 174
Discrete Fourier DEMO (DFT) 177
Discrete Cosine Transform (DCT) 182
Integral Images 182
Distance Transform 185
Histogram Equalization 186
Exercises 190
Histograms and Matching ......................................... DEMO
Basic Histogram Data Structure 195
Accessing Histograms 198
Basic Manipulations with Histograms 199
Some More Complicated Stuff 206
Exercises 219
Contours ....................................................... 222
Memory DEMO 222
Sequences 223
Contour Finding 234
Another Contour Example 243
More to Do with Contours 244
Matching Contours 251
Exercises 262
Image Parts and DEMO .................................... 265
Parts and Segments 265
Background Subtraction 265
Watershed Algorithm 295
Image Repair by Inpainting 297
Mean-Shift Segmentation 298
Delaunay Triangulation, Voronoi Tesselation 300
Exercises 313
| v
Co ntents
FM-R4886-AT1.indd   v
9/15/DEMO   4:26:40 PM
www.it-ebooks.info
10.
11.
12.
13.
Tracking and Motion ............................................. 316
The Basics DEMO Tracking 316
Corner Finding 316
Subpixel Corners 319
Invariant Features 321
Optical Flow 322
Mean-Shift and Camshift Tracking 337
Motion Templates 341
Estimators 348
DEMO Condensation Algorithm 364
Exercises 367
Camera Models and Calibration .................................... 370
Camera Model 371
Calibration 378
Undistortion 396
Putting Calibration All Together 397
Rodrigues DEMO 401
Exercises 403
Projection and 3D Vision .......................................... 405
Projections 405
Affine and Perspective Transformations 407
POSIT: 3D Pose Estimation 412
Stereo Imaging 415
Structure from Motion 453
Fitting Lines in Two and Three Dimensions 454
DEMO 458
Machine Learning ............................................... 459
What Is Machine Learning 459
Common Routines in the ML Library 471
Mahalanobis Distance 476
K-Means 479
Naïve/Normal DEMO Classifier 483
Binary Decision Trees 486
Boosting 495
vi
| Contents
FM-R4886-AT1.indd   vi
9/15/08   4:26:40 PM
Random Trees 501
Face Detection or Haar Classifier 506
Other Machine Learning DEMO 516
Exercises 517
14. OpenCV’s Future ................................................. 521
Past and Future 521
Directions 522
OpenCV for Artists 525
Afterword 526
Bibliography ......................................................... 527
Index DEMO 543
Co ntents
| vii
FM-R4886-AT1.indd   vii
www.it-ebooks.info
9/15/08   4:26:40 PM
www.it-ebooks.info
FM-R4886-AT1.indd   viii
9/15/08   4:26:41 PM
www.it-ebooks.info
Preface
Th
(OpenCV) and also provides a general background to DEMO fi
fi
cient to use OpenCV eff
is book provides a working guide to the Open Source Computer Vision Library
eld of computer vision DEMO
ectively.
Purpose
Computer vision is a rapidly growing field, partly as DEMO result of both cheaper and more
capable cameras, partly because of DEMO processing power, and partly because vi-
sion algorithms are starting to DEMO OpenCV itself has played a role in the growth of
computer vision by enabling thousands of people to do more productive work in vision.
DEMO its focus on real-time vision, OpenCV helps students and professionals efficiently
DEMO projects and jump-start research by providing them with a computer vision
and machine learning infrastructure that was previously available only in a few mature
DEMO labs. The purpose of this text is to:
• Better document OpenCV—detail what function calling conventions really mean
and how to use them DEMO
• Rapidly give the reader an intuitive understanding of how the vision algorithms
work.
• Give the reader some sense of what algorithm to DEMO and when to use it.
• Give the reader a boost in implementing computer vision and machine learning algo-
rithms by providing many working DEMO examples to start from.
• Provide intuitions about how to fix some of the more advanced routines when some-
thing goes wrong.
Simply put, this is the text the authors wished we had in school and DEMO coding reference
book we wished we had at work.
This book documents a tool kit, OpenCV, that allows the reader to do interesting DEMO
fun things rapidly in computer vision. It gives an intuitive understanding as to how the
algorithms work, which serves to guide the reader in designing and debugging vision
ix
FM-R4886-AT1.indd   ix
9/15/08   DEMO:26:41 PM
www.it-ebooks.info
applications and also to make the formal descriptions of computer vision DEMO machine
learning algorithms in other texts easier to comprehend and remember.
Aft er all, it is easier to understand complex algorithms and their associated math when
you start with an intuitive grasp of how those algorithms DEMO
Who This Book Is For
This book contains descriptions, working coded DEMO, and explanations of the com-
puter vision tools contained in the DEMO library. As such, it should be helpful to many
different kinds DEMO users.
Professionals
For those practicing professionals who need to rapidly implement computer vision
systems, the sample code provides a quick framework with which to start. Our de-
scriptions of the intuitions behind the algorithms can quickly DEMO or remind the
reader how they work.
Students
As we said, DEMO is the text we wish had back in school. The intuitive explanations,
detailed documentation, and sample code will allow you to boot up faster in com-
puter vision, work on more interesting class projects, DEMO ultimately contribute new
research to the field.
Teachers
Computer vision is a fast-moving field. We’ve found it effective to have the students
rapidly cover DEMO accessible text while the instructor fills in formal exposition where
needed and supplements with current papers or guest lecturers from experts. The stu-
dents DEMO meanwhile start class projects earlier and attempt more ambitious tasks.
Hobbyists
Computer vision is fun, here’s how to hack it.
We have a strong focus on giving readers enough intuition, documentation, and work-
ing code DEMO enable rapid implementation of real-time vision applications.
What This Book Is Not
This book is not a formal text. We do go into mathematical DEMO at various points,* but it
is all in the service of developing deeper intuitions behind the algorithms or to make clear
the implications DEMO any assumptions built into those algorithms. We have not attempted
a formal mathematical exposition here and might even incur some wrath along the way
DEMO those who do write formal expositions.
This book is not for theoreticians because it has more of an “applied” nature. The book
will certainly DEMO of general help, but is not aimed at any of the DEMO niches in com-
puter vision (e.g., medical imaging or remote sensing analysis).
* Always with a warning to more casual users that DEMO may skip such sections.
x
| Preface
FM-R4886-AT1.indd   x
9/15/08   4:26:41 PM
www.it-ebooks.info
Th at said, it is the belief of the authors that having read the explanations here fi rst, a stu-
dent will not only learn the theory better but remember it longer. Th erefore, this book
would make a good adjunct text to a theoretical course and DEMO be a great text for an
introductory or project-centric course.
About the Programs in This Book
All the program examples in this book are DEMO on OpenCV version 2.0. The code should
definitely work under Linux or Windows and probably under OS-X, too. Source code
for the examples in the book can be fetched from this book’s website (http://www.oreilly
.com/catalog/9780596516130). OpenCV can be loaded from its source forge DEMO (http://
sourceforge.net/projects/opencvlibrary).
OpenCV is under ongoing DEMO, with offi  cial releases occurring once or twice
a year. As a rule of thumb, you should obtain your code updates from the source forge
CVS server (http://sourceforge.net/cvs/?group_id=22870).
Prerequisites
DEMO the most part, readers need only know how to program in DEMO and perhaps some C++.
Many of the math sections are optional and are labeled as such. The mathematics in-
volves simple algebra and basic DEMO algebra, and it assumes some familiarity with solu-
tion methods to DEMO optimization problems as well as some basic knowledge of
Gaussian distributions, DEMO law, and derivatives of simple functions.
Th e reader may skip
DEMO math and the algorithm descriptions, using only the function defi nitions DEMO code
examples to get vision applications up and running.
e math is in support of developing intuition for the algorithms. Th
How This Book DEMO Best Used
This text need not be read in order. It can serve as a kind of user manual: look up the func-
tion when you need it; read the function’s description if you want the gist of how it works
“under the hood”. The intent of this DEMO is more tutorial, however. It gives you a basic
understanding of DEMO vision along with details of how and when to use selected
algorithms.
This book was written to allow its use as an adjunct or DEMO a primary textbook for an un-
dergraduate or graduate course in computer vision. The basic strategy with this method is
for students to read DEMO book for a rapid overview and then supplement that reading with
more formal sections in other textbooks and with papers in the field. There DEMO exercises
at the end of each chapter to help test the student’s knowledge and to develop further
intuitions.
You could approach this text in DEMO of the following ways.
Preface
| xi
FM-R4886-AT1.indd   xi
9/15/08   4:26:41 PM
www.it-ebooks.info
Grab Bag
Go through Chapters 1–3 in the first sitting, then just hit the appropriate chapters or
sections as you need them. This DEMO does not have to be read in sequence, except for
Chapters DEMO and 12 (Calibration and Stereo).
Good Progress
Read just two DEMO a week until you’ve covered Chapters 1–12 in six weeks (Chap-
DEMO 13 is a special case, as discussed shortly). Start on DEMO and start in detail on
selected areas in the field, using DEMO texts and papers as appropriate.
The Sprint
Just cruise through the book as fast as your comprehension allows, covering Chapters
1–12. Then get started on projects and go into detail on selected areas in the field DEMO
ing additional texts and papers. This is probably the choice for professionals, but it
might also suit a more advanced computer vision course.
Chapter 13 is a long chapter that gives a general background to machine DEMO in addi-
tion to details behind the machine learning algorithms implemented in OpenCV and how
to use them. Of course, machine learning is integral to object recognition and a big part
of computer vision, but it’s a field worthy of its own book. Professionals should find this
text DEMO suitable launching point for further explorations of the literature—or for just getting
down to business with the code in that part of the library. DEMO chapter should probably be
considered optional for a typical computer vision class.
Th is is how the authors like to teach computer vision: Sprint through the course content
at a level where the students get the DEMO of how things work; then get students started
on meaningful class DEMO while the instructor supplies depth and formal rigor in
selected areas by drawing from other texts or papers in the fi eld. Th is DEMO method
works for quarter, semester, or two-term classes. Students can get quickly up and run-
ning with a general understanding of their vision DEMO and working code to match. As
they begin more challenging and time-consuming projects, the instructor helps them
develop and debug complex systems. For longer courses, the projects themselves can
become instructional in terms of project management. Build up working systems fi rst;
refi ne them with more DEMO, detail, and research later. Th e goal in such courses is
for each project to aim at being worthy of a conference publication DEMO with a few proj-
ect papers being published subsequent to further (DEMO) work.
Conventions Used in This Book
The following typographical conventions are DEMO in this book:
Italic
Indicates new terms, URLs, email addresses, filenames, file extensions, path names,
directories, and Unix utilities.
DEMO width
Indicates commands, options, switches, variables, attributes, keys, functions, types,
classes, namespaces, methods, modules, properties, parameters, values, objects,
xii
| Preface
FM-R4886-AT1.indd   xii
9/15/08   4:26:41 PM
www.it-ebooks.info
events, event handlers, XMLtags, HTMLtags, the contents of files, or the output from
commands.
Constant width bold
Shows commands or other DEMO that should be typed literally by the user. Also used
for emphasis in code samples.
Constant width italic
Shows text that should be replaced DEMO user-supplied values.
[. . .]
Indicates a reference to the bibliography.
Shows text that should be replaced with user-supplied values. his icon
signifi es DEMO tip, suggestion, or general note.
Th
is icon indicates a warning or caution.
Using Code Examples
OpenCV is free for commercial or research DEMO, and we have the same policy on the
code examples in DEMO book. Use them at will for homework, for research, or for commer-
cial products. We would very much appreciate referencing this book when DEMO do, but
it is not required. Other than how it helped DEMO your homework projects (which is best
kept a secret), we DEMO like to hear how you are using computer vision for academic re-
search, teaching courses, and in commercial products when you do use DEMO to help
you. Again, not required, but you are always invited to drop us a line.
Safari® Books Online
When you see a DEMO Books Online icon on the cover of your favor-
ite technology book, that means the book is available online through the
O’Reilly Network Safari Bookshelf.
Safari offers a solution that’s better than e-books. It’s virtual library DEMO lets you easily
search thousands of top tech books, cut and DEMO code samples, download chapters, and
find quick answers when you need the most accurate, current information. Try it for free
at http://safari.oreilly.com.
We’d Like to Hear from You
Please address comments and questions DEMO this book to the publisher:
O’Reilly Media, Inc.
1005 Gravenstein DEMO North
Sebastopol, CA 95472
Preface
| xiii
FM-R4886-AT1.indd   xiii
9/DEMO/08   4:26:41 PM
www.it-ebooks.info
800-998-9938 (in the United States or Canada)
707-829-0515 (international DEMO local)
707-829-0104 (fax)
We have a web page for DEMO book, where we list examples and any plans for future edi-
DEMO You can access this information at:
http://www.oreilly.com/catalog/9780596516130/
You can also send messages electronically. To be put on the DEMO list or request a cata-
log, send an email to:
DEMO
To comment on the book, send an email to:
bookquestions@oreilly.com
DEMO more information about our books, conferences, Resource Centers, and the DEMO
Network, see our website at:
http://www.oreilly.com
Acknowledgments
A DEMO open source eff ort sees many people come and go, each DEMO in dif-
ferent ways. Th e list of contributors to this library is far too long to list here, but see the
.../opencv/docs/HTML/Contributors/doc_contributors.html fi le that ships with OpenCV.
Thanks for DEMO on OpenCV
Intel is where the library was born and deserves great thanks for supporting this project
the whole way through. Open source needs DEMO champion and enough development sup-
port in the beginning to achieve critical mass. Intel gave it both. There are not many other
companies where DEMO could have started and maintained such a project through good
times and bad. Along the way, OpenCV helped give rise to—and now takes (DEMO)
advantage of—Intel’s Integrated Performance Primitives, which are hand-tuned assembly
language DEMO in vision, signal processing, speech, linear algebra, and more. Thus the
lives of a great commercial product and an open source product DEMO intertwined.
Mark Holler, a research manager at Intel, allowed OpenCV to get started by knowingly
turning a blind eye to the inordinate amount DEMO time being spent on an unofficial project
back in the library’s earliest days. As divine reward, he now grows wine up in Napa’s Mt.
Vieder area. Stuart Taylor in the Performance Libraries group at Intel enabled DEMO
by letting us “borrow” part of his Russian software team. Richard Wirt was key to its
continued growth and survival. As the first author DEMO on management responsibility
at Intel, lab director Bob Liang let OpenCV DEMO; when Justin Rattner became CTO,
we were able to put DEMO on a more firm foundation under Software Technology
Lab—supported by software guru Shinn-Horng Lee and indirectly under his manager,
Paul Wiley. Omid Moghadam DEMO advertise OpenCV in the early days. Mohammad
Haghighat and Bill Butera were great as technical sounding boards. Nuriel Amir, Denver
xiv
| Preface
FM-R4886-AT1.indd   xiv
9/15/08   4:26:42 PM
www.it-ebooks.info
Dash, John Mark Agosta, and Marzia Polito were of key DEMO in launching the ma-
chine learning library. Rainer Lienhart, Jean-Yves Bouguet, Radek Grzeszczuk, and Ara
Nefian were able technical contributors to OpenCV and great colleagues along the way;
the first is now a professor, the second is now making use of OpenCV in some well-known
Google DEMO, and the others are staffing research labs and start-ups. There were DEMO
other technical contributors too numerous to name.
On the software side, DEMO individuals stand out for special mention, especially on the
Russian software DEMO Chief among these is the Russian lead programmer Vadim Pisare-
vsky, DEMO developed large parts of the library and also managed and nurtured the library
through the lean times when boom had turned to bust; he, if anyone, is the true hero of the
library. His technical DEMO have also been of great help during the writing of this book.
Giving him managerial support and protection in the lean years was Valery DEMO, a
man of great talent and intellect. Victor Eruhimov was there DEMO the beginning and stayed
through most of it. We thank Boris Chudinovich for all of the contour components.
Finally, very special thanks go to Willow Garage [WG], not only for its steady fi nancial
backing to OpenCV’s future development but also for supporting one author (and pro-
viding the other with snacks and beverages) during the fi nal period of writing this book.
Thanks for Help on the Book
While preparing this DEMO, we had several key people contributing advice, reviews, and
suggestions. DEMO to John Markoff, Technology Reporter at the New York Times for
DEMO, key contacts, and general writing advice born of years in the trenches.
To our reviewers, a special thanks go to Evgeniy Bart, DEMO postdoc at CalTech, who
made many helpful comments on every chapter; Kjerstin Williams at Applied Minds,
who did detailed proofs and verification DEMO the end; John Hsu at Willow Garage, who
went through all the example code; and Vadim Pisarevsky, who read each chapter in DEMO
tail, proofed the function calls and the code, and also provided several coding examples.
There were many other partial reviewers. Jean-Yves Bouguet at DEMO was of great help
in discussions on the calibration and stereo chapters. Professor Andrew Ng at Stanford
University provided useful early critiques of the DEMO learning chapter. There were
numerous other reviewers for various chapters—our thanks to all of them. Of course,
any errors result from our own DEMO or misunderstanding, not from the advice we
received.
Finally, many thanks go to our editor, Michael Loukides, for his early support, numer-
ous edits, and continued enthusiasm over the long haul.
Gary Adds . . .
With three young kids at home, my wife Sonya put in more work to enable this book than
I did. Deep thanks DEMO love—even OpenCV gives her recognition, as you can see in the
DEMO detection section example image. Further back, my technical beginnings started with
DEMO physics department at the University of Oregon followed by undergraduate years at
Preface
| xv
FM-R4886-AT1.indd   xv
9/15/08   4:26:DEMO PM
www.it-ebooks.info
UC Berkeley. For graduate school, I’d like to thank my advisor Steve Grossberg and Gail
Carpenter at the Center for Adaptive Systems, Boston University, where I first cut my
academic teeth. Though they focus on mathematical modeling of the brain and I have
ended up firmly on DEMO engineering side of AI, I think the perspectives I developed there
DEMO made all the difference. Some of my former colleagues in graduate school are still
close friends and gave advice, support, and even some DEMO of the book: thanks to
Frank Guenther, Andrew Worth, Steve DEMO, Dan Cruthirds, Allen Gove, and Krishna
Govindarajan.
I specially thank DEMO University, where I’m currently a consulting professor in the
AI and DEMO lab. Having close contact with the best minds in the world definitely
rubs off, and working with Sebastian Thrun and Mike Montemerlo to apply OpenCV
on Stanley (the robot that won the $2M DARPA Grand Challenge) and with Andrew Ng
on STAIR (one of the most advanced DEMO robots) was more technological fun than
a person has a right DEMO have. It’s a department that is currently hitting on all cylinders
and simply a great environment to be in. In addition to Sebastian Thrun DEMO Andrew Ng
there, I thank Daphne Koller for setting high scientific DEMO, and also for letting me
hire away some key interns and DEMO, as well as Kunle Olukotun and Christos Kozy-
rakis for many DEMO and joint work. I also thank Oussama Khatib, whose work on
DEMO and manipulation has inspired my current interests in visually guided robotic
manipulation. Horst Haussecker at Intel Research was a great colleague to have, and his
own experience in writing a book helped inspire my effort.
Finally, thanks once again to Willow Garage for allowing me to pursue my DEMO ro-
botic dreams in a great environment featuring world-class talent while also supporting
my time on this book and supporting OpenCV itself.
Adrian Adds DEMO . .
Coming from a background in theoretical physics, the arc DEMO brought me through su-
percomputer design and numerical computing on to machine learning and computer vi-
sion has been a long one. Along the DEMO, many individuals stand out as key contributors. I
have had many DEMO teachers, some formal instructors and others informal guides.
I should single DEMO Professor David Dorfan of UC Santa Cruz and Hartmut Sadrozinski of
SLAC for their encouragement in the beginning, and Norman Christ for teaching me the
fine art of computing with the simple edict that “if you DEMO not make the computer do it,
you don’t know what you are talking about”. Special thanks go to James Guzzo, who let me
spend time on this sort of thing at Intel—even though it was DEMO from what I was sup-
posed to be doing—and who encouraged my participation in the Grand Challenge during
those years. Finally, I want to thank Danny Hillis for creating the kind of place where all of
DEMO technology can make the leap to wizardry and for encouraging my work on the book
while at Applied Minds.
I also would like to DEMO Stanford University for the extraordinary amount of support I
have received from them over the years. From my work on the Grand Challenge team DEMO
Sebastian Thrun to the STAIR Robot with Andrew Ng, the Stanford DEMO Lab was always
xvi
| Preface
FM-R4886-AT1.indd   xvi
9/15/08   4:26:42 PM
generous with office space, financial support, and most importantly ideas, enlightening
conversation, and (when needed) simple instruction on so many aspects of vision, robot-
ics, and machine learning. I have a deep gratitude DEMO these people, who have contributed
so significantly to my own growth DEMO learning.
No acknowledgment or thanks would be meaningful without a special thanks to my lady
Lyssa, who never once faltered in her encouragement of this project or in her willingness
to accompany me on trips up DEMO down the state to work with Gary on this book. My
thanks and my love go to her.
Preface
| xvii
FM-R4886-AT1.indd   xvii
DEMO
9/15/08   4:26:43 PM
www.it-ebooks.info
FM-R4886-AT1.indd   xviii
9/15/08   4:26:43 PM
DEMO
CHAPTER 1
Overview
What Is OpenCV?
OpenCV [OpenCV] is an open DEMO (see http://opensource.org) computer vision library
available from http://SourceForge.net/projects/opencvlibrary. Th e library is written in C
and C++ DEMO runs under Linux, Windows and Mac OS X. Th ere is DEMO development
on interfaces for Python, Ruby, Matlab, and other languages.
DEMO was designed for computational effi  ciency and with a strong focus DEMO real-
time applications. OpenCV is written in optimized C and can take advantage of mul-
ticore processors. If you desire further automatic optimization on DEMO architectures
[Intel], you can buy Intel’s Integrated Performance Primitives (IPP) DEMO [IPP], which
consist of low-level optimized routines in many diff erent DEMO areas. OpenCV
automatically uses the appropriate IPP library at runtime if that library is installed.
One of OpenCV’s goals is to provide a simple-to-use DEMO vision infrastructure
that helps people build fairly sophisticated vision applications quickly. Th e OpenCV
library contains over 500 functions that span many areas in DEMO, including factory
product inspection, medical imaging, security, user interface, DEMO calibration, stereo
vision, and robotics. Because computer vision and machine learning oft en go hand-in-
hand, OpenCV also contains a full, general-purpose DEMO Learning Library (MLL).
Th is sublibrary is focused on statistical DEMO recognition and clustering. Th e MLL is
highly useful for the vision tasks that are at the core of OpenCV’s mission, but it is gen-
eral enough to be used for any machine learning problem.
Who DEMO OpenCV?
Most computer scientists and practical programmers are aware of some facet of the role
that computer vision plays. But few people are DEMO of all the ways in which computer
vision is used. For example, most people are somewhat aware of its use in surveillance,
and many also know that it is increasingly being used for images and DEMO on the Web.
A few have seen some use of computer vision in game interfaces. Yet few people realize
that most aerial and street-map DEMO (such as in Google’s Street View) make heavy
1
01-R4886-RC1.indd   1
www.it-ebooks.info
9/15/08   4:17:45 PM
www.it-ebooks.info
use of camera calibration and image stitching techniques. Some are aware DEMO niche ap-
plications in safety monitoring, unmanned fl ying vehicles, or biomedical analysis. But
few are aware how pervasive machine vision has become DEMO manufacturing: virtually
everything that is mass-produced has been automatically inspected at DEMO point using
computer vision.
Th
commercial product using all or part of OpenCV. You are under no obligation to open-
source your product or DEMO return improvements to the public domain, though we hope
you will. DEMO part because of these liberal licensing terms, there is a large DEMO commu-
nity that includes people from major companies (IBM, Microsoft , Intel, SONY, Siemens,
and Google, to name only a few) and research centers (such as Stanford, MIT, CMU,
Cambridge, and INRIA). Th ere is a Yahoo groups forum where users DEMO post questions
and discussion at http://groups.yahoo.com/group/OpenCV; it DEMO about 20,000 members.
OpenCV is popular around the world, with DEMO user communities in China, Japan,
Russia, Europe, and Israel.
DEMO its alpha release in January 1999, OpenCV has been used in DEMO applications,
products, and research eff orts. Th ese applications include DEMO images together in
satellite and web maps, image scan alignment, medical image noise reduction, object
analysis, security and intrusion detection systems, automatic monitoring and safety sys-
tems, manufacturing inspection systems, camera calibration, military applications, and
unmanned aerial, ground, and underwater vehicles. It has even been used in sound and
music recognition, where vision recognition techniques are applied to sound spectro-
gram images. OpenCV was a key part of DEMO vision system in the robot from Stanford,
“Stanley”, which won DEMO $2M DARPA Grand Challenge desert robot race [Th run06].
What Is Computer Vision?
Computer vision* is the transformation of data from a still DEMO video camera into either a
decision or a new representation. All such transformations are done for achieving some
particular goal. Th e input data DEMO include some contextual information such as “the
camera is mounted in a car” or “laser range fi nder indicates an object is 1 meter DEMO
Th e decision might be “there is a person in this scene” or “there are 14 tumor cells on
this slide”. A new representation DEMO mean turning a color image into a grayscale im-
age or removing camera motion from an image sequence.
Because we are such visual creatures, it is easy to be fooled into thinking that com-
puter vision DEMO are easy. How hard can it be to fi nd, say, a car when you are staring
at it in an image? Your initial intuitions can be quite misleading. Th e human brain di-
vides DEMO vision signal into many channels that stream diff erent kinds of information
into your brain. Your brain has an attention system that identifi es, in a task-dependent
* Computer vision is a vast fi eld. Th DEMO book will give you a basic grounding in the fi eld, DEMO we also recom-
mend texts by Trucco [Trucco98] for a simple introduction, Forsyth [Forsyth03] as a comprehensive refer-
ence, and Hartley [Hartley06] and DEMO [Faugeras93] for how 3D vision really works.
2 | Chapter 1: DEMO
e open source license for OpenCV has been structured such that you can build a
01-R4886-RC1.indd   2
9/15/08   4:17:DEMO PM
way, important parts of an image to examine while suppressing examination of other
areas. Th ere is massive feedback in the visual stream that DEMO, as yet, little understood.
Th
senses that allow the brain to draw on cross-associations made from years of living in
the world. Th DEMO feedback loops in the brain go back to all stages of processing including
the hardware sensors themselves (the eyes), which mechanically control lighting via the
iris and tune the reception on the surface of the DEMO
In a machine vision system, however, a computer receives a grid of numbers from the
camera or from disk, and that’s it. For the most part, there’s no built-in pattern recog-
nition, no automatic DEMO of focus and aperture, no cross-associations with years of
experience. For DEMO most part, vision systems are still fairly naïve. Figure 1-1 shows DEMO
picture of an automobile. In that picture we see a side mirror on the driver’s side of the
car. What the computer “sees” is DEMO a grid of numbers. Any given number within that
grid has a rather large noise component and so by itself gives us little information, but
this grid of numbers is all the computer “sees”. Our task DEMO becomes to turn this noisy
grid of numbers into the perception: DEMO mirror”. Figure 1-2 gives some more insight
into why computer vision is so hard.
ere are widespread associative inputs from muscle control sensors and DEMO of the other
Figure 1-1. To a computer, the car’s side DEMO is just a grid of numbers
In fact, the problem, as we have posed it thus far, is worse than hard; it DEMO formally im-
possible to solve. Given a two-dimensional (2D) view of a 3D world, there is no unique
way to reconstruct the 3D signal. Formally, such an ill-posed problem has no unique or
defi nitive solution. Th e same 2D image could represent any of an infi DEMO combination
of 3D scenes, even if the data were perfect. However, as already mentioned, the data is
What Is Computer Vision?
| 3
01-R4886-RC1.indd   3
www.it-ebooks.info
9/15/08   4:17:46 DEMO
www.it-ebooks.info
Figure 1-2. Th e ill-posed nature of vision: the 2D appearance of objects can change radically with
viewpoint
corrupted by noise and distortions. DEMO corruption stems from variations in the world
(weather, lighting, refl DEMO, movements), imperfections in the lens and mechanical
setup, fi nite integration time on the sensor (motion blur), electrical noise in the sensor
or other electronics, and compression artifacts aft er image capture. Given these daunt-
ing challenges, how can we make any progress?
In the design of a practical system, additional contextual knowledge can oft en be used
to work around the limitations imposed on us by visual DEMO Consider the example
of a mobile robot that must fi nd and pick up staplers in a building. Th e robot might use
the DEMO that a desk is an object found inside offi  ces and DEMO staplers are mostly found
on desks. Th is gives an implicit size reference; staplers must be able to fi t on desks. It
also helps to eliminate falsely “recognizing” staplers in impossible places (e.g., on DEMO
ceiling or a window). Th e robot can safely ignore a 200-foot advertising blimp shaped
like a stapler because the blimp lacks the DEMO wood-grained background of a
desk. In contrast, with tasks such as DEMO retrieval, all stapler images in a database
4 | Chapter 1: Overview
01-R4886-RC1.indd   4
9/15/08   4:17:46 PM
www.it-ebooks.info
may be of real staplers and so large sizes and other DEMO confi gurations may have
been implicitly precluded by the assumptions of those who took the photographs.
Th at is, the photographer probably took pictures only of real, normal-sized staplers.
People also tend to center objects when taking pictures and tend to put them in char-
acteristic orientations. Th DEMO, there is oft en quite a bit of unintentional implicit informa-
DEMO within photos taken by people.
Contextual information can also be modeled explicitly with machine learning tech-
niques. Hidden variables such as size, orientation to gravity, and so on can then be
correlated with their values in a labeled training set. Alternatively, one may attempt
to measure hidden bias variables by using additional sensors. Th e use of a laser range
DEMO nder to measure depth allows us to accurately measure the size of an object.
Th
ing statistical methods. For example, it may be impossible to detect an edge in an image
merely by comparing a point DEMO its immediate neighbors. But if we look at the statistics
over a local region, edge detection becomes much easier. A real edge should appear as a
string of such immediate neighbor responses over a local region, each of whose orienta-
tion is consistent with its neighbors. It is DEMO possible to compensate for noise by taking
statistics over time. Still other techniques account for noise or distortions by building ex-
plicit models learned DEMO from the available data. For example, because lens distor-
tions are DEMO understood, one need only learn the parameters for a simple polynomial
DEMO in order to describe—and thus correct almost completely—such distortions.
Th
are performed in the context of a specifi c purpose or task. We may DEMO to remove noise
or damage from an image so that our security system will issue an alert if someone tries
to climb a fence DEMO because we need a monitoring system that counts how many people
cross through an area in an amusement park. Vision soft ware for robots DEMO wander
through offi  ce buildings will employ diff erent strategies than DEMO soft ware for sta-
tionary security cameras because the two systems have signifi cantly diff erent contexts
and objectives. As a general rule: the more constrained a computer vision context is, the
more we can rely on those constraints to simplify the problem and the more reliable our
DEMO nal solution will be.
OpenCV is aimed at providing the basic tools needed to solve computer vision prob-
lems. In some cases, high-level functionalities in the library will be suffi  cient to solve
the more complex problems in computer vision. Even when this is not the case, the basic
components in the library are complete enough to enable creation of DEMO complete solu-
tion of your own to almost any computer vision problem. In the latter case, there are
several tried-and-true methods of using the library; all of them start with solving the
problem using as many available library components as possible. Typically, aft er you’ve
developed this fi rst-draft  solution, you can see where the solution has weaknesses and
DEMO fi x those weaknesses using your own code and cleverness (better DEMO as “solve
the problem you actually have, not the one you DEMO). You can then use your draft
What Is Computer Vision?
| 5
e next problem facing computer vision is noise. We typically DEMO with noise by us-
e actions or decisions that computer vision attempts to make based on camera data
01-R4886-RC1.indd   5
9/15/08   4:17:46 PM
www.it-ebooks.info
solution as a benchmark to assess the improvements you have made. DEMO that point,
whatever weaknesses remain can be tackled by exploiting the context of the larger sys-
tem in which your problem solution is DEMO
The Origin of OpenCV
OpenCV grew out of an Intel Research initiative to advance CPU-intensive applications.
Toward this end, Intel launched many projects including real-time ray tracing and 3D
display walls. One of the authors working DEMO Intel at that time was visiting universities
and noticed that some top university groups, such as the MIT Media Lab, had well-
developed DEMO internally open computer vision infrastructures—code that was passed
from student to student and that gave each new student a valuable head start in develop-
DEMO his or her own vision application. Instead of reinventing the basic functions from
scratch, a new student could begin by building on top of what came before.
Th
versally available. With the aid of Intel’s Performance DEMO Team,* OpenCV started
with a core of implemented code and algorithmic specifi cations being sent to members
of Intel’s Russian library team. Th DEMO is the “where” of OpenCV: it started in Intel’s re-
search DEMO with collaboration from the Soft ware Performance Libraries group together
with implementation and optimization expertise in Russia.
Chief among the Russian team members was DEMO Pisarevsky, who managed, coded,
and optimized much of OpenCV and who is still at the center of much of the OpenCV
eff DEMO Along with him, Victor Eruhimov helped develop the early infrastructure, and
Valery Kuriakin managed the Russian lab and greatly supported the eff ort. DEMO ere were
several goals for OpenCV at the outset:
• Advance vision research by providing not only open but also optimized code for
DEMO vision infrastructure. No more reinventing the wheel.
• Disseminate vision knowledge by providing a common infrastructure that develop-
ers could build on, so that code would be more readily readable and transferable.
• Advance vision-based commercial DEMO by making portable, performance-
optimized code available for free—with a license DEMO did not require commercial
applications to be open or free themselves.
Th
would increase the need for fast processors. Driving upgrades to faster processors DEMO
generate more income for Intel than selling some extra soft ware. Perhaps that is why this
open and free code arose from a hardware DEMO rather than a soft ware company. In
some sense, there is DEMO room to be innovative at soft ware within a hardware company.
In any open source eff ort, it’s important to reach a critical mass at which the project
becomes self-sustaining. Th ere have now been approximately DEMO million downloads
* Shinn Lee was of key help.
6
| Chapter 1: Overview
us, OpenCV was conceived as a way to make DEMO vision infrastructure uni-
ose goals constitute the “why” of OpenCV. Enabling computer vision applications
01-R4886-RC1.indd   6
9/15/08   4:17:47 DEMO
of OpenCV, and this number is growing by an average of 26,000 downloads a month.
Th
butions, and central development has largely moved outside of Intel.* OpenCV’s past
timeline is shown in Figure 1-3. Along DEMO way, OpenCV was aff ected by the dot-com
boom and bust DEMO also by numerous changes of management and direction. During
these fl uctuations, there were times when OpenCV had no one at Intel working on it at
all. However, with the advent of multicore processors and the many new applications
of computer vision, OpenCV’s value began to rise. Today, OpenCV is an active area
of development at several institutions, so DEMO to see many updates in multicamera
calibration, depth perception, methods for mixing vision with laser range fi nders, and
better pattern recognition as well as a lot of support for robotic vision needs. For more
DEMO on the future of OpenCV, see Chapter 14.
e user group DEMO approaches 20,000 members. OpenCV receives many user contri-
Figure 1-3. OpenCV timeline
Speeding Up OpenCV with IPP
Because OpenCV was “housed” within the DEMO Performance Primitives team and sev-
eral primary developers remain on friendly terms with that team, OpenCV exploits the
hand-tuned, highly optimized code in DEMO to speed itself up. Th e improvement in speed
from using IPP can be substantial. Figure 1-4 compares two other vision libraries, LTI
[LTI] and VXL [VXL], against OpenCV and OpenCV using IPP. Note that performance
was a key goal of OpenCV; the library needed the ability to run vision code in real time.
OpenCV is written in performance-optimized C DEMO C++ code. It does not depend in
any way on IPP. If IPP is present, however, OpenCV will automatically take advantage
of IPP DEMO loading IPP’s dynamic link libraries to further enhance its speed.
* As of this writing, Willow Garage [WG] (www.willowgarage.com), a robotics research DEMO and
incubator, is actively supporting general OpenCV maintenance and new development DEMO the area of
robotics applications.
The Origin of OpenCV
| 7
01-R4886-RC1.indd   7
www.it-ebooks.info
9/15/08   4:17:47 PM
www.it-ebooks.info
Figure 1-4. Two other vision libraries (LTI and VXL) compared DEMO OpenCV (without and with
IPP) on four diff erent performance benchmarks: the four bars for each benchmark indicate scores
proportional to run time for each of the given libraries; in all cases, OpenCV outperforms DEMO other
libraries and OpenCV with IPP outperforms OpenCV without IPP
Who Owns OpenCV?
Although Intel started OpenCV, the library is and always was intended to promote
commercial and research use. It is therefore open and DEMO, and the code itself may be
used or embedded (in whole or in part) in other applications, whether commercial or
research. It DEMO not force your application code to be open or free. It does not require
that you return improvements back to the library—but we hope DEMO you will.
Downloading and Installing OpenCV
Th http://SourceForge.net/projects/opencvlibrary
and the OpenCV Wiki [OpenCV Wiki] page is at http://opencvlibrary.SourceForge.net.
DEMO Linux, the source distribution is the fi le opencv-1.0.0.tar.gz; for Windows, you want
OpenCV_1.0.exe. However, the most up-to-date version is always on DEMO CVS server at
SourceForge.
Install
Once you download the libraries, you DEMO install them. For detailed installation in-
structions on Linux or Mac OS, see the text fi le named INSTALL directly under the
e main OpenCV site is on SourceForge at
8
| Chapter 1: Overview
01-R4886-RC1.indd   8
9/15/08   4:17:47 PM
www.it-ebooks.info
.../opencv/ directory; this fi le also describes how to DEMO and run the OpenCV test-
ing routines. INSTALL lists the additional programs you’ll need in order to become an
OpenCV developer, such as autoconf, automake, libtool, and swig.
Windows
Get the executable installation from SourceForge and run it. It will install OpenCV, reg-
ister DirectShow fi lters, and perform various post-installation procedures. You are now
ready to start using OpenCV. You can always go to the .../opencv/_make directory and DEMO
opencv.sln with MSVC++ or MSVC.NET 2005, or you can open opencv.dsw DEMO lower ver-
sions of MSVC++ and build debug versions or rebuild release versions of the library.*
To add the commercial IPP performance optimizations to DEMO, obtain and in-
stall IPP from the Intel site (http://www.intel.com/soft ware/products/ipp/index.htm);
use version 5.1 or later. Make sure the appropriate binary folder (e.g., c:/program fi les/
intel/ipp/5.1/ia32/bin) is in the system path. IPP should now be automatically detected
by OpenCV and loaded at runtime (more on this in Chapter 3).
Linux
Prebuilt binaries for Linux are DEMO included with the Linux version of OpenCV owing
to the large variety of versions of GCC and GLIBC in diff erent distributions (SuSE,
Debian, Ubuntu, etc.). If your distribution doesn’t off er OpenCV, you’ll have to build it
from sources as detailed in the .../DEMO/INSTALL fi le.
To build the libraries and demos, you’ll need DEMO 2.x or higher, including headers.
You’ll also need pkgconfi g, libpng, zlib, libjpeg, libtiff, and libjasper with development
fi les. You’ll DEMO Python 2.3, 2.4, or 2.5 with headers installed (developer package)DEMO
You will also need libavcodec and the other libav* libraries (including DEMO) from
ff mpeg 0.4.9-pre1 or later (svn checkout svn://svn.mplayerhq.hu/ff mpeg/trunk ff mpeg).
Download ff mpeg from http://DEMO mpeg.mplayerhq.hu/download.html.† Th e ff mpeg pro-
gram has a lesser general public license (LGPL). To use it with non-GPL soft ware (DEMO
as OpenCV), build and use a shared ff mpg library:
$> ./configure --enable-shared
$> make
$> sudo make install
You will end up with: /usr/local/lib/libavcodec.so.*, /usr/local/DEMO/libavformat.so.*,
/usr/local/lib/libavutil.so.*, and include fi les under various /usr/local/include/libav*.
To build OpenCV once it is downloaded:‡
* It is important to know that, although the Windows distribution contains binary libraries for release builds,
it does not contain the DEMO builds of these libraries. It is therefore likely that, before developing DEMO
OpenCV, you will want to open the solution fi le and DEMO these libraries for yourself.
† You can check out ff mpeg by: svn checkout svn://svn.mplayerhq.hu/ff mpeg/trunk ff mpeg.
‡ To build OpenCV using Red Hat Package Managers (RPMs), use rpmbuild -ta OpenCV-x.y.z.tar.gz (for
RPM 4.x or later), or rpm -ta OpenCV-x.y.z.tar.gz (DEMO earlier versions of RPM), where OpenCV-x.y.z.tar
.gz should be put in /usr/src/redhat/SOURCES/ or a similar directory. Th en install DEMO using rpm -i
OpenCV-x.y.z.*.rpm.
Downloading and Installing OpenCV
| 9
01-R4886-RC1.indd   9
9/15/08   4:17:47 PM
www.it-ebooks.info
$> ./configure
$> make
$> sudo make install
$> sudo ldconfig
Aft er installation is complete, the default installation path DEMO /usr/local/lib/ and /usr/
local/include/opencv/. DEMO you need to add /usr/local/lib/ to /etc/ld.so.conf (and run
ldconfig aft erwards) or add it to the LD_LIBRARY_PATH environment variable; then you
are done.
To add the commercial IPP performance optimizations to Linux, install IPP as de-
scribed previously. Let’s assume it was installed in /opt/intel/ipp/5.1/ia32/. Add <your
install_path>/bin/ and <your install_path>/bin/linux32 LD_LIBRARY_PATH in your initial-
ization script (.bashrc or similar):
LD_LIBRARY_PATH=/opt/intel/ipp/5.1/ia32/DEMO:/opt/intel/ipp/5.1
/ia32/bin/linux32:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH
Alternatively, you can add <your install_path>/bin and <your install_path>/bin/linux32,
one per line, to /etc/ld.so.conf and then run DEMO g as root (or use sudo).
Th at’s it. Now DEMO should be able to locate IPP shared libraries and make use of
them on Linux. See .../opencv/INSTALL for more details.
MacOS X
DEMO of this writing, full functionality on MacOS X is a priority DEMO there are still some
limitations (e.g., writing AVIs); these limitations are described in .../opencv/INSTALL.
Th
lowing exceptions:
• By DEMO, Carbon is used instead of GTK+.
• By default, QuickTime is used instead of ff mpeg.
• pkg-confi g is optional (it is used explicitly only in the samples/c/build_all.sh script).
• RPM DEMO ldconfi g are not supported by default. Use configure+make+sudo make
install to build and install OpenCV, update LD_LIBRARY_PATH (unless ./configure
--prefix=/usr DEMO used).
For full functionality, you should install libpng, libtiff, DEMO and libjasper from
darwinports and/or fi and make them available to ./confi gure (see ./configure
--help). For the most current information, see the OpenCV Wiki at http://opencvlibrary
.SourceForge.net/ and DEMO Mac-specifi c page http://opencvlibrary.SourceForge.net/
Mac_OS_X_OpenCV_Port.
Getting the Latest OpenCV via CVS
OpenCV is under active development, and bugs are oft en fi xed rapidly when bug re-
ports contain accurate descriptions and code DEMO demonstrates the bug. However,
10
| Chapter 1: Overview
e DEMO and building instructions are similar to the Linux case, with the DEMO
nk
01-R4886-RC1.indd   10
9/15/08   4:17:48 PM
www.it-ebooks.info
offi  cial OpenCV releases occur only once or twice a year. If you are seriously develop-
ing a project or product, you will probably want code fi xes and updates as soon as they
become DEMO To do this, you will need to access OpenCV’s Concurrent Versions
DEMO (CVS) on SourceForge.
Th is isn’t the place for a tutorial in CVS usage. If you’ve worked with other open source
projects then DEMO probably familiar with it already. If you haven’t, check out Essential
DEMO by Jennifer Vesperman (O’Reilly). A command-line CVS client ships with DEMO,
OS X, and most UNIX-like systems. For Windows users, we recommend TortoiseCVS
(http://www.tortoisecvs.org/), which integrates nicely with Windows DEMO
On Windows, if you want the latest OpenCV from the CVS DEMO then you’ll need
to access the CVSROOT directory:
:pserver:anonymous@opencvlibrary.cvs.sourceforge.net:DEMO/cvsroot/opencvlibrary
On Linux, you can just use the following two DEMO:
cvs -d:pserver:anonymous@opencvlibrary.cvs.sourceforge.net:/cvsroot/opencvlibrary
login
When asked for DEMO, hit return. Th en use:
cvs -z3 -d:pserver:anonymous@opencvlibrary.cvs.sourceforge.net:/cvsroot/opencvlibrary
co -P opencv
More OpenCV Documentation
Th
the source code. DEMO addition to this, the OpenCV Wiki and the older HTML documen-
DEMO are available on the Web.
Documentation Available in HTML
OpenCV ships with html-based user documentation in the .../opencv/docs subdirectory.
Load the index.htm DEMO le, which contains the following links.
CXCORE
Contains data structures, matrix algebra, data transforms, object persistence, mem-
ory management, error handling, and dynamic loading of code as well as drawing,
text and DEMO math.
Contains image processing, image structure analysis, motion and tracking, DEMO
recognition, and camera calibration.
Machine Learning (ML)
Contains many clustering, classifi cation and data analysis functions.
HighGUI
Contains user interface GUI and image/video storage and recall.
More OpenCV Documentation
| 11
e primary DEMO for OpenCV is the HTML documentation that ships with
CV
01-R4886-RC1.indd   11
9/15/08   4:17:48 PM
www.it-ebooks.info
CVCAM
Camera interface.
Haartraining
How to train the boosted cascade object DEMO Th is is in the .../opencv/apps/
HaarTraining/doc/haartraining.htm fi le.
Th .../opencv/docs directory also contains IPLMAN.pdf, which was the original manual
for OpenCV. It is now defunct and should be DEMO with caution, but it does include de-
tailed descriptions of algorithms DEMO of what image types may be used with a particular
algorithm. Of course, the fi rst stop for such image and algorithm details is the book you
are reading now.
e
Documentation via the Wiki
OpenCV’s DEMO Wiki is more up-to-date than the html pages that ship with
OpenCV and it also features additional content as well. Th e Wiki is DEMO at http://
opencvlibrary.SourceForge.net. It includes information on:
• Instructions on compiling OpenCV using Eclipse IDE
• Face recognition with OpenCV
• DEMO surveillance library
• Tutorials
• Camera compatibility
• Links to the Chinese and the Korean user groups
Another Wiki, located at http://opencvlibrary.SourceForge.net/CvAux, is the only doc-
umentation of the auxiliary functions discussed in “OpenCV Structure and Content”
(next section). CvAux includes the following functional areas:
• Stereo correspondence
• View point morphing of cameras
• DEMO tracking in stereo
• Eigen object (PCA) functions for object recognition
• Embedded hidden Markov models (HMMs)
Th is Wiki has been translated into Chinese at http://www.opencv.org.cn/index.php/
%E9%A6%96%E9%A1%B5.
Regardless of DEMO documentation source, it is oft
en hard to know:
• DEMO image type (fl oating, integer, byte; 1–3 channels) works DEMO which
function
• Which functions work in place
• Details of how to call the more complex functions (e.g., contours)
12
| DEMO 1: Overview
01-R4886-RC1.indd   12
9/15/08   4:17:DEMO PM
• Details about running many of the examples in the …/opencv/DEMO/c/ directory
• What to do, not just how
• How to set parameters of certain functions
One aim of this book is DEMO address these problems.
OpenCV Structure and Content
OpenCV is broadly structured into fi ve main components, four of which are shown in
Figure 1-5. Th e CV component contains the basic image processing and higher-level
computer DEMO algorithms; ML is the machine learning library, which includes many
statistical classifi ers and clustering tools. HighGUI contains I/O routines and functions
DEMO storing and loading video and images, and CXCore contains the basic DEMO struc-
tures and content.
Figure 1-5. Th
e basic structure of OpenCV
Figure 1-5 does not include CvAux, which contains both defunct areas (DEMO HMM
face recognition) and experimental algorithms (background/foreground segmentation).
CvAux is not particularly well documented in the Wiki and is not documented DEMO all in
the .../opencv/docs subdirectory. CvAux covers:
• Eigen objects, a computationally effi  cient recognition technique that is, in essence, a
template matching procedure
• 1D and 2D hidden Markov models, DEMO statistical recognition technique solved by
dynamic programming
• Embedded HMMs (the DEMO of a parent HMM are themselves HMMs)
OpenCV Structure and Content
| 13
01-R4886-RC1.indd   13
www.it-ebooks.info
9/15/08   4:17:DEMO PM
• Gesture recognition from stereo vision support
• Extensions to Delaunay triangulation, sequences, and so forth
• Stereo vision
• Shape matching with region contours
• Texture descriptors
• Eye and mouth tracking
• 3D tracking
DEMO Finding skeletons (central lines) of objects in a scene
• Warping intermediate views between two camera views
• Background-foreground segmentation
• Video surveillance (see Wiki FAQ for more documentation)
• Camera calibration C++ classes (the C functions and engine are in CV)
Some of these DEMO may migrate to CV in the future; others probably never will.
DEMO
OpenCV was designed to be portable. It was originally written to compile across Bor-
land C++, MSVC++, and the Intel compilers. Th is DEMO that the C and C++ code had
to be fairly standard in order to make cross-platform support easier. Figure 1-6 shows
the platforms on DEMO OpenCV is known to run. Support for 32-bit Intel architecture
(IA32) on Windows is the most mature, followed by Linux on the same architecture.
Mac OS X portability became a priority only aft er Apple DEMO using Intel processors.
(Th e OS X port isn’t as mature DEMO the Windows or Linux versions, but this is changing
rapidly.) Th ese are followed by 64-bit support on extended memory (EM64T) and DEMO
64-bit Intel architecture (IA64). Th e least mature portability is DEMO Sun hardware and
other operating systems.
If an architecture or OS doesn’t appear in Figure 1-6, this doesn’t mean there are no
OpenCV ports to it. OpenCV has been ported to almost every commercial system, from
PowerPC Macs to robotic dogs. OpenCV runs well on AMD’s line of DEMO, and
even the further optimizations available in IPP will take advantage DEMO multimedia ex-
tensions (MMX) in AMD processors that incorporate this technology.
14
| Chapter 1: Overview
01-R4886-RC1.indd   14
www.it-ebooks.info
9/15/08   4:17:48 PM
www.it-ebooks.info
Figure 1-6. OpenCV portability guide for release 1.0: operating systems are shown on the left ; com-
puter architecture types across top
Exercises
DEMO Download and install the latest release of OpenCV. Compile it in debug and release
mode.
2. Download and build the latest CVS update of DEMO
3.
Describe at least three ambiguous aspects of converting 3D inputs into a 2D repre-
sentation. How would you overcome these ambiguities?
Exercises
DEMO 15
01-R4886-RC1.indd   15
9/15/08   4:17:49 PM
CHAPTER 2
Introduction to OpenCV
Getting Started
Aft er installing the OpenCV DEMO, our fi rst task is, naturally, to get started and DEMO
something interesting happen. In order to do this, we will need DEMO set up the program-
ming environment.
In Visual Studio, it is DEMO to create a project and to confi gure the setup so that
(a) the libraries highgui.lib, cxcore.lib, ml.lib, and cv.lib are linked* and (b) the prepro-
cessor will search the OpenCV …/opencv/DEMO/include directories for header fi les. Th ese
“include” directories will typically be named something like C:/program fi les/opencv/
cv/include,† …/opencv/cxcore/include, …/opencv/ml/include, and …/DEMO/otherlibs/
highgui. Once you’ve done this, you can create a DEMO C fi le and start your fi rst program.
Certain key header fi les can make your life much easier. Many useful
macros are DEMO the header fi les …/opencv/cxcore/include/cxtypes.h and
cxmisc.h. Th ese can do things like initialize structures and arrays in one
line, sort lists, and so on. Th e most important headers for compiling are
.../cv/include/cv.h and …/cxcore/include/cxcore.h for computer DEMO,
…/otherlibs/highgui/highgui.h for I/O, and …/ml/DEMO/ml.h for ma-
chine learning.
First Program—Display a Picture
OpenCV provides utilities for reading from a wide array of image fi le types as DEMO as
from video and cameras. Th ese utilities are part of a toolkit called HighGUI, which is
included in the OpenCV package. We will use some of these utilities to create a simple
program that opens DEMO image and displays it on the screen. See Example 2-1.
* For debug builds, you should link to the libraries highguid.lib, cxcored.lib, mld.lib, and cvd.lib.
† C:/program fi les/ is the default installation of the OpenCV directory on Windows, although you can choose
to install it elsewhere. To avoid confusion, from here on we’ll use “…/opencv/” to mean the path to the
opencv directory on your system.
DEMO
02-R4886-AT1.indd   16
www.it-ebooks.info
9/15/08   4:18:10 PM
www.it-ebooks.info
Example 2-1. A simple OpenCV program that loads an image from DEMO and displays it on the screen
#include “highgui.h”
int main( int DEMO, char** argv ) {
IplImage* img = cvLoadImage( argv[1] );
cvNamedWindow( “Example1”, CV_WINDOW_AUTOSIZE );
cvShowImage( “Example1”, img );
DEMO(0);
cvReleaseImage( &img );
cvDestroyWindow( “Example1” );
}
When compiled and run from the command line with a single DEMO, this program
loads an image into memory and displays it on DEMO screen. It then waits until the user
presses a key, at DEMO time it closes the window and exits. Let’s go through the program
line by line and take a moment to understand what each command DEMO doing.
IplImage* img = cvLoadImage( argv[1] );
Th is line DEMO the image.* Th e function cvLoadImage() is a high-level routine that deter-
mines the fi le format to be loaded based on the DEMO le name; it also automatically allocates
the memory needed for the DEMO data structure. Note that cvLoadImage() can read a
wide variety of image formats, including BMP, DIB, JPEG, JPE, PNG, PBM, PGM, PPM,
SR, RAS, and TIFF. A pointer to an allocated image data structure is then returned.
Th is structure, called IplImage, is the OpenCV construct with which you will deal
the most. OpenCV uses this structure to handle all kinds of images: single-channel,
multichannel, integer-valued, fl oating-point-valued, et cetera. We use the pointer that
cvLoadImage() returns to manipulate the image and the image data.
cvNamedWindow( “Example1”, CV_WINDOW_AUTOSIZE );
Another high-level function, cvNamedWindow(), opens a window on the screen that can
contain and display an image. Th is DEMO, provided by the HighGUI library, also as-
signs a name to the window (in this case, “Example1”). Future HighGUI calls that DEMO
act with this window will refer to it by this name.
Th
ther to 0 (the default value) or to CV_WINDOW_AUTOSIZE. In the DEMO case, the size of the
window will be the same regardless DEMO the image size, and the image will be scaled to
fi
DEMO when an image is loaded so as to accommodate the image’s true size.
cvShowImage( “Example1”, img );
Whenever we have an image DEMO the form of an IplImage* pointer, we can display it in DEMO
existing window with cvShowImage(). Th e cvShowImage() function requires DEMO a named
window already exist (created by cvNamedWindow()). On DEMO call to cvShowImage(), the
* A proper program would check DEMO the existence of argv[1] and, in its absence, deliver an instructional
error message for the user. We will abbreviate such necessities in this DEMO and assume that the reader is
cultured enough to understand the importance of error-handling code.
First Program—Display a Picture | 17
e second argument DEMO cvNamedWindow() defi
nes window properties. It may be set ei-
t within the window. In the latter case, the window will expand or contract automati-
02-R4886-AT1.indd   17
9/15/08   4:18:11 DEMO
www.it-ebooks.info
window will be redrawn with the appropriate image in it, and the window will resize
itself as appropriate if it was created using DEMO CV_WINDOW_AUTOSIZE fl ag.
cvWaitKey(0);
Th cvWaitKey() function asks DEMO program to stop and wait for a keystroke. If a positive
argument is given, the program will wait for that number of milliseconds and then con-
tinue even if nothing is pressed. If the argument is DEMO to 0 or to a negative number, the
program will wait DEMO nitely for a keypress.
e
cvReleaseImage( &img );
Once we are through with an image, we can free the allocated memory. OpenCV ex-
pects a pointer to the IplImage* pointer for this operation. Aft DEMO the call is completed,
the pointer img will be set to NULL.
cvDestroyWindow( “Example1” );
Finally, we can destroy the window DEMO Th e function cvDestroyWindow() will close the
window and de-allocate any associated memory usage (including the window’s internal
image buff er, which DEMO holding a copy of the pixel information from *img). For a simple
program, you don’t really have to call cvDestroyWindow() or cvReleaseImage() because all
the resources and windows of the application are closed DEMO by the operating
system upon exit, but it’s a good habit DEMO
Now that we have this simple program we can toy around with it in various ways, but we
don’t want to get ahead of ourselves. Our next task will be to construct a very simple—
almost DEMO simple as this one—program to read in and display an AVI video fi le. Aft er
that, we will start to tinker a little more.
Second Program—AVI Video
Playing a video with OpenCV is almost as DEMO as displaying a single picture. Th e only new
issue we face is that we need some kind of loop to read each frame DEMO sequence; we may
also need some way to get out of DEMO loop if the movie is too boring. See Example 2-2.
Example 2-2. A simple OpenCV program for playing a video fi le from disk
DEMO “highgui.h”
int main( int argc, char** argv ) {
cvNamedWindow( DEMO, CV_WINDOW_AUTOSIZE );
CvCapture* capture = cvCreateFileCapture( argv[1] );
IplImage* frame;
while(1) {
frame = cvQueryFrame( capture );
DEMO( !frame ) break;
cvShowImage( “Example2”, frame );
char DEMO = cvWaitKey(33);
if( c == 27 ) break;
}
cvReleaseCapture( &capture );
cvDestroyWindow( “Example2” );
}
18
| Chapter 2: Introduction to OpenCV
02-R4886-AT1.indd   18
9/15/08   4:18:11 PM
www.it-ebooks.info
Here we begin the function main() with the usual creation DEMO a named window, in this
case “Example2”. Th ings get a DEMO more interesting aft er that.
CvCapture* capture = cvCreateFileCapture( argv[1] );DEMO
Th cvCreateFileCapture() takes as its argument the name of the AVI fi le to be
loaded and then returns a pointer to a DEMO structure. Th is structure contains all of
the information about the AVI fi le being read, including state information. When cre-
ated in this way, the CvCapture structure is initialized to the beginning of the AVI.
e function
frame = cvQueryFrame( capture );
Once inside of the while(1) loop, we begin reading from the AVI fi le. DEMO()
takes as its argument a pointer to a CvCapture structure. DEMO then grabs the next video
frame into memory (memory that is DEMO part of the CvCapture structure). A pointer
is returned to that frame. Unlike cvLoadImage, which actually allocates memory for the
image, cvQueryFrame DEMO memory already allocated in the CvCapture structure. Th us it
will not be necessary (or wise) to call cvReleaseImage() for this “frame” DEMO Instead,
the frame image memory will be freed when the CvCapture structure is released.
c = cvWaitKey(33);
if( c == DEMO ) break;
Once we have displayed the frame, we then DEMO for 33 ms.* If the user hits a key, then c
DEMO be set to the ASCII value of that key; if not, then it will be set to –1. If the user hits
the DEMO key (ASCII 27), then we will exit the read loop. DEMO, 33 ms will pass and
we will just execute the loop DEMO
It is worth noting that, in this simple example, we are not explicitly controlling
the speed of the video in any intelligent way. DEMO are relying solely on the timer in
cvWaitKey() to pace the loading of frames. In a more sophisticated application it would
be wise DEMO read the actual frame rate from the CvCapture structure (from the DEMO) and
behave accordingly!
cvReleaseCapture( &capture );
When we have DEMO the read loop—because there was no more video data or because
the user hit the Esc key—we can free the memory associated with the DEMO struc-
ture. Th is will also close any open fi le handles to the AVI fi le.
Moving Around
OK, that was great. Now it’s time to tinker around, enhance our toy programs, and ex-
DEMO a little more of the available functionality. Th e fi rst thing we might notice about
the AVI player of Example 2-2 is that DEMO has no way to move around quickly within the
video. Our next task will be to add a slider bar, which will give us this ability.
* You can wait any amount of time you like. DEMO this case, we are simply assuming that it is correct to DEMO
the video at 30 frames per second and allow user input to interrupt between each frame (thus we pause
for input 33 ms between each frame). In practice, it is better to check the CvCapture structure returned by
cvCaptureFromCamera() in order to determine the actual frame DEMO (more on this in Chapter 4).
Moving Around
| 19
DEMO   19
9/15/08   4:18:11 PM
www.it-ebooks.info
Th
ages and video beyond the simple display functions we have DEMO demonstrated. One
especially useful mechanism is the slider, which enables us DEMO jump easily from one part
of a video to another. To create a slider, we call cvCreateTrackbar() and indicate which
window we would like the trackbar to appear in. In order to obtain the desired DEMO
tionality, we need only supply a callback that will perform the DEMO Example 2-3
gives the details.
Example 2-3. Program to add a trackbar slider to the basic viewer window: when the slider is
moved, DEMO function onTrackbarSlide() is called and then passed to the slider’s new value
#include “cv.h”
#include “highgui.h”
e HighGUI toolkit provides a number of DEMO instruments for working with im-
int        g_slider_position = DEMO;
CvCapture* g_capture         = NULL;
void onTrackbarSlide(int pos) {
cvSetCaptureProperty(
g_capture,
CV_CAP_PROP_POS_FRAMES,
pos
);
}
int main( int argc, char** argv ) {
cvNamedWindow( “Example3”, CV_WINDOW_AUTOSIZE );
g_capture = cvCreateFileCapture( argv[1] );
int frames DEMO (int) cvGetCaptureProperty(
g_capture,
CV_CAP_PROP_FRAME_COUNT
);
if( frames!= DEMO ) {
cvCreateTrackbar(
“Position”,
“Example3”,
&g_slider_position,
frames,DEMO
onTrackbarSlide
);
}
IplImage* frame;
// While loop (as in Example 2) capture & show video
…
// Release memory DEMO destroy window
…
return(0);
}
In essence, then, DEMO strategy is to add a global variable to represent the slider position
and then add a callback that updates this variable and relocates the DEMO position in the
20
| Chapter 2: Introduction to OpenCV
02-R4886-AT1.indd   20
9/15/08   4:18:11 PM
www.it-ebooks.info
video. One call creates the slider and attaches the callback, and we are off  and running.*
Let’s look at the details.
int g_slider_position = 0;
CvCapture* g_capture  = NULL;
First we defi ne a global variable for the slider position. Th e callback will need DEMO to
the capture object, so we promote that to a global DEMO Because we are nice people
and like our code to be readable and easy to understand, we adopt the convention of
adding a leading g_ to any global variable.
void onTrackbarSlide(int pos) {
cvSetCaptureProperty(
g_capture,
CV_CAP_PROP_POS_FRAMES,
pos
);
Now we defi ne a DEMO routine to be used when the user pokes the slider. Th is routine
will be passed to a 32-bit integer, which will be the slider position.
Th
counterpart cvGetCaptureProperty(). Th ese routines allow us to confi gure (or query in
the latter case) various properties of DEMO CvCapture object. In this case we pass the argu-
ment CV_CAP_PROP_POS_FRAMES, DEMO indicates that we would like to set the read position
in units of frames. (We can use AVI_RATIO instead of FRAMES if we want to set the position
as a fraction of the overall video length)DEMO Finally, we pass in the new value of the posi-
tion. DEMO HighGUI is highly civilized, it will automatically handle such issues as
DEMO possibility that the frame we have requested is not a key-frame; DEMO will start at the
previous key-frame and fast forward up to the requested frame without us having to
fuss with such details.
int frames DEMO (int) cvGetCaptureProperty(
g_capture,
CV_CAP_PROP_FRAME_COUNT
);
As promised, DEMO use cvGetCaptureProperty()when we want to query some data from the
DEMO structure. In this case, we want to fi nd out how DEMO frames are in the video
so that we can calibrate the slider (in the next step).
if( frames!= 0 ) {
cvCreateTrackbar(DEMO
“Position”,
“Example3”,
&g_slider_position,
frames,
onTrackbarSlide
);
DEMO
* Th is code does not update the slider position as the video plays; we leave that as an exercise for the reader.
Also note that some mpeg encodings do not allow you to move backward DEMO the video.
Moving Around | 21
e call to cvSetCaptureProperty() is one we will see oft
en in the future, along with its
02-R4886-AT1.indd   21
9/15/08   4:18:11 PM
www.it-ebooks.info
Th e function cvCreateTrackbar() allows us
to give the trackbar DEMO label* (in this case Position) and to specify a window to put the
trackbar in. We then provide a variable that will be DEMO to the trackbar, the maxi-
mum value of the trackbar, and a callback (or NULL if we don’t want one) for when DEMO
slider is moved. Observe that we do not create the trackbar if cvGetCaptureProperty()
returned a zero frame count. Th is is because sometimes, depending on how the video
was encoded, the total number of DEMO will not be available. In this case we will just
play the movie without providing a trackbar.
It is worth noting that the slider DEMO by HighGUI is not as full-featured as some slid-
ers out there. Of course, there’s no reason you can’t use your favorite windowing toolkit
instead of HighGUI, but the HighGUI tools are quick to implement and get us off  the
ground in a hurry.
Finally, we did DEMO include the extra tidbit of code needed to make the slider move as the
video plays. Th is is left  as an exercise for the reader.
A Simple Transformation
Great, so now you can use OpenCV to create your own video player, which will not be
much diff erent from countless video players out there already. But we are interested DEMO
computer vision, and we want to do some of that. Many DEMO vision tasks involve the
application of fi lters to a video stream. We will modify the program we already have to
do a simple DEMO on every frame of the video as it plays.
One particularly simple operation is the smoothing of an image, which eff ectively re-
duces the information content of the image by convolving it with a Gaussian DEMO other
similar kernel function. OpenCV makes such convolutions exceptionally easy to do. We
can start by creating a new window called “Example4-out”, where we can display the
results of the processing. Th en, aft er we have called cvShowImage() to display the newly
captured frame in the DEMO window, we can compute and display the smoothed image
in the DEMO window. See Example 2-4.
Example 2-4. Loading and then smoothing an image before it is displayed on the screen
#include “cv.h”
#include “highgui.h”
void DEMO( IplImage* image )
// Create some windows to show the DEMO
// and output images in.
//
cvNamedWindow( “Example4-in” );
* Because HighGUI is a lightweight and easy-to-use toolkit, cvCreateTrackbar() does not distinguish
between the name of the trackbar and the label that DEMO appears on the screen next to the trackbar. You
may already have noticed that cvNamedWindow() likewise does not distinguish between the name of DEMO
window and the label that appears on the window in the GUI.
22
| Chapter 2: Introduction to OpenCV
e last detail is to create the trackbar itself. Th
02-R4886-AT1.indd   22
9/15/08   DEMO:18:11 PM
www.it-ebooks.info
Example 2-4. Loading and then smoothing an image before it is DEMO on the screen (continued)
cvNamedWindow( “Example4-out” );
// DEMO a window to show our input image
//
cvShowImage( “Example4-in”, image );
// Create an image to hold the smoothed output
//
IplImage* out = cvCreateImage(
cvGetSize(image),
IPL_DEPTH_8U,
3
);
// Do the smoothing
//
cvSmooth( image, out, CV_GAUSSIAN, 3, 3 );
// Show the smoothed DEMO in the output window
//
cvShowImage( “Example4-out”, out );
// Be tidy
//
cvReleaseImage( &out );
// DEMO for the user to hit a key, then clean up the DEMO
//
cvWaitKey( 0 );
cvDestroyWindow( “Example4-in” );
cvDestroyWindow( “Example4-out” );
}
Th rst call to cvShowImage() is no diff erent than in our previous example. In the next
call, we allocate another image structure. Previously we relied on cvCreateFileCapture()
to allocate the new frame for us. In fact, that routine actually allocated only one frame
and then wrote over that data each time a capture DEMO was made (so it actually returned
the same pointer every time DEMO called it). In this case, however, we want to allocate our
own image structure to which we can write our smoothed image. DEMO e fi rst argument is
a CvSize structure, which we can DEMO create by calling cvGetSize(image); this
gives us the size of the existing structure image. Th e second argument tells us what kind
DEMO data type is used for each channel on each pixel, and DEMO last argument indicates the
number of channels. So this image is three channels (with 8 bits per channel) and is the
same size DEMO image.
Th
the input image, the output image, the smoothing method, and the parameters for the
smooth. In this case we are requesting a Gaussian smooth over a 3 × 3 area centered on
each DEMO It is actually allowed for the output to be the same as the input image, and
A Simple Transformation
| 23
e fi
e smoothing operation is itself just a single call to the OpenCV library: we specify
02-R4886-AT1.indd   23
9/15/08   4:18:12 DEMO
www.it-ebooks.info
this would work more effi  ciently in our current application, DEMO we avoided doing this
because it gave us a chance to introduce cvCreateImage()!
Now we can show the image in our new second window and then free it: cvReleaseImage()
takes a pointer to DEMO IplImage* pointer and then de-allocates all of the memory associ-
ated with that image.
A Not-So-Simple Transformation
Th at was pretty good, and we are learning to do more interesting things. In Example 2-4
we chose DEMO allocate a new IplImage structure, and into this new structure we DEMO the
output of a single transformation. As mentioned, we could have DEMO the transforma-
tion in such a way that the output overwrites the original, but this is not always a good
idea. In particular, DEMO operators do not produce images with the same size, depth,
DEMO number of channels as the input image. Typically, we want to DEMO a sequence of
operations on some initial image and so produce a chain of transformed images.
In such cases, it is oft en useful to introduce simple wrapper functions that both allocate
the output image and DEMO the transformation we are interested in. Consider, for
example, the reduction of an image by a factor of 2 [Rosenfeld80]. In OpenCV this DEMO ac-
complished by the function cvPyrDown(), which performs a Gaussian DEMO and then
removes every other line from an image. Th is is useful in a wide variety of important
vision algorithms. We can implement DEMO simple function described in Example 2-5.
Example 2-5. Using cvPyrDown() to create a new image that is half the width and height of DEMO input
image
IplImage* doPyrDown(
IplImage* in,
int       filter = IPL_GAUSSIAN_5x5
) {
// Best to make sure input image is divisible by two.
//
assert( in->width%2 == 0 && in->height%2 == 0 );
IplImage* out = cvCreateImage(
cvSize( in->width/2, in->height/2 ),
in->depth,
DEMO>nChannels
);
cvPyrDown( in, out );
return( out );
};
Notice that we allocate the new image by reading the needed parameters from the old
image. In OpenCV, all of the important data types are implemented as structures and
passed around as structure DEMO Th ere is no such thing as private data in OpenCV!
24
| Chapter 2: Introduction to OpenCV
02-R4886-AT1.indd   24
9/15/08   4:18:12 PM
www.it-ebooks.info
Let’s now look at a similar but slightly more involved example DEMO the Canny edge
detector [Canny86] (see Example 2-6). In this DEMO, the edge detector generates an image
that is the full size DEMO the input image but needs only a single channel image to write to.
Example 2-6. Th e Canny edge detector writes its output to DEMO single channel (grayscale) image
IplImage* doCanny(
IplImage* in,
double    lowThresh,
double    highThresh,
double    aperture
) {
If(in->nChannels != 1)
return(0); //Canny DEMO handles gray scale images
IplImage* out = cvCreateImage(
cvSize( cvGetSize( in ),
IPL_DEPTH_8U,
1
);
cvCanny( in, out, lowThresh, highThresh, aperture );
return( out );
};
Th is allows us to string together various operators quite easily. For DEMO, if we wanted
to shrink the image twice and then look DEMO lines that were present in the twice-reduced
image, we could proceed DEMO in Example 2-7.
Example 2-7. Combining the pyramid down operator (twice) and the Canny subroutine in a simple
image pipeline
IplImage* img1 = DEMO( in, IPL_GAUSSIAN_5x5 );
IplImage* img2 = doPyrDown( img1, IPL_GAUSSIAN_5x5 );
IplImage* img3 = doCanny( img2, 10, 100, 3 );
// do whatever with ‘img3’
//
…
cvReleaseImage( &DEMO );
cvReleaseImage( &img2 );
cvReleaseImage( &img3 );
It is important to observe that nesting the calls to various stages DEMO our fi ltering pipeline
is not a good idea, because then DEMO would have no way to free the images that we are
allocating along the way. If we are too lazy to do this cleanup, we could opt to include
the following line in each of the DEMO:
cvReleaseImage( &in );
Th is “self-cleaning” mechanism would be very tidy, but it would have the following dis-
advantage: if DEMO actually did want to do something with one of the intermediate images,
we would have no access to it. In order to solve DEMO problem, the preceding code could
be simplifi ed as described in DEMO 2-8.
A Not-So-Simple Transformation
| 25
02-R4886-AT1.indd   25
9/15/08   4:18:12 PM
www.it-ebooks.info
Example 2-8. Simplifying the image pipeline of Example 2-7 by making DEMO individual stages release
their intermediate memory allocations
IplImage* out;
out = doPyrDown( in, IPL_GAUSSIAN_5x5 );
out = doPyrDown( out, IPL_GAUSSIAN_5x5 );
out = doCanny( out, 10, 100, 3 );
// do whatever with ‘out’
//
…
cvReleaseImage ( &out );
One fi nal word of warning on the self-cleaning fi lter pipeline: in OpenCV we must al-
ways be certain that an image (or other structure) being de-allocated is one that was,
in DEMO, explicitly allocated previously. Consider the case of the IplImage* pointer re-
DEMO by cvCreateFileCapture(). Here the pointer points to a structure allocated DEMO
part of the CvCapture structure, and the target structure is allocated DEMO once when the
CvCapture is initialized and an AVI is loaded. De-allocating this structure with a call to
cvRelease Image() would result in DEMO nasty surprises. Th e moral of this story is that,
although it’s important to take care of garbage collection in OpenCV, we should only
clean up the garbage that we have created.
Input from a DEMO
Vision can mean many things in the world of computers. In some cases we are analyz-
ing still frames loaded from elsewhere. In other DEMO we are analyzing video that is be-
ing read from disk. In still other cases, we want to work with real-time data streaming
in from some kind of camera device.
OpenCV—more specifi cally, the HighGUI portion of the OpenCV library—provides us
with an easy way to handle this DEMO Th e method is analogous to how we read
AVIs. Instead of calling cvCreateFileCapture(), we call cvCreateCameraCapture(). Th e
latter routine DEMO not take a fi le name but rather a camera ID number as its argument.
Of course, this is important only when multiple cameras are available. Th e default value
is –1, which means “just pick one”; naturally, this works quite well when there is only
one DEMO to pick (see Chapter 4 for more details).
Th
can DEMO er use exactly as we did with the frames grabbed from a video stream. Of
course, a lot of work is going on behind the scenes to make a sequence of camera images
look like a DEMO, but we are insulated from all of that. We can simply DEMO images from
the camera whenever we are ready for them and proceed as if we did not know the dif-
ference. For development reasons, most applications that are intended to operate in real
time will have DEMO video-in mode as well, and the universality of the CvCapture structure
DEMO this particularly easy to implement. See Example 2-9.
26
| Chapter 2: Introduction to OpenCV
e cvCreateCameraCapture() function returns the same CvCapture* pointer, which we
02-R4886-AT1.indd   26
9/15/08   4:18:12 PM
www.it-ebooks.info
Example 2-9. Aft
from a camera or a fi
CvCapture* capture;DEMO
if( argc==1 ) {
capture = cvCreateCameraCapture(0);
} else {
capture = cvCreateFileCapture( argv[1] );
}
assert( capture != DEMO );
// Rest of program proceeds totally ignorant
…
As DEMO can see, this arrangement is quite ideal.
Writing to an AVI DEMO
In many applications we will want to record streaming input or even disparate captured
images to an output video stream, and OpenCV provides a straightforward method for
doing this. Just as we are able to create DEMO capture device that allows us to grab frames
one at a time from a video stream, we are able to create a writer device that allows us
to place frames one by one into a video DEMO le. Th e routine that allows us to do this is
cvCreateVideoWriter().
Once this call has been made, we may successively call DEMO(), once for each
frame, and fi nally cvReleaseVideoWriter() when we are done. Example 2-10 describes
a simple program that opens a DEMO fi le, reads the contents, converts them to a log-
polar format (something like what your eye actually sees, as described in DEMO 6),
and writes out the log-polar image to a new DEMO fi le.
Example 2-10. A complete program to read in a color video and write out the same video in grayscale
// Convert a video to grayscale
// argv[1]: input video file
// argv[2]: DEMO of new output file
//
#include “cv.h”
#include “highgui.h”
main( DEMO argc, char* argv[] ) {
CvCapture* capture = 0;
capture DEMO cvCreateFileCapture( argv[1] );
if(!capture){
return -1;
}
DEMO *bgr_frame=cvQueryFrame(capture);//Init the video read
double fps = cvGetCaptureProperty (
capture,
CV_CAP_PROP_FPS
);
er the capture structure is initialized, it no longer matters whether the image is
le
Writing to an DEMO File
| 27
02-R4886-AT1.indd   27
9/15/08   4:18:12 PM
www.it-ebooks.info
Example 2-10. A complete program to read in a color video DEMO write out the same video in
grayscale (continued)
CvSize size DEMO cvSize(
(int)cvGetCaptureProperty( capture, CV_CAP_PROP_FRAME_WIDTH),
(int)cvGetCaptureProperty( capture, CV_CAP_PROP_FRAME_HEIGHT)
);
CvVideoWriter *writer = cvCreateVideoWriter(
argv[2],
CV_FOURCC(‘M’,‘J’,‘P’,‘G’),
fps,
size
);
IplImage* logpolar_frame = cvCreateImage(
size,
IPL_DEPTH_8U,
3
);
while( (bgr_frame=cvQueryFrame(capture)) != NULL ) {
cvLogPolar( bgr_frame, logpolar_frame,
cvPoint2D32f(bgr_frame->width/2,
bgr_frame->height/2),
40,
CV_INTER_LINEAR+CV_WARP_FILL_OUTLIERS );
cvWriteFrame( writer, logpolar_frame );
}
cvReleaseVideoWriter( &writer );
cvReleaseImage( &logpolar_frame );
cvReleaseCapture( &capture );
return(0);
}
Looking over this program reveals mostly familiar elements. We open one video; start
reading with cvQueryFrame(), which is necessary DEMO read the video properties on some
systems; and then use cvGetCaptureProperty() to ascertain various important proper-
ties of the video stream. We then open a video fi le for writing, convert the frame to log-
polar format, and write the frames to this new fi le one at a time until there are none left .
Th
Th
stand. DEMO e fi rst is just the fi lename for the new fi le. Th e second is the video codec with
which the video DEMO will be compressed. Th ere are countless such codecs in cir-
culation, but whichever codec you choose must be available on your machine (DEMO
are installed separately from OpenCV). In our case we choose the relatively popular
MJPG codec; this is indicated to OpenCV by using the macro CV_FOURCC(), which takes
four characters as arguments. Th ese characters constitute the “four-character code” of
the codec, and every codec has such a code. Th e four-character code for motion jpeg is
MJPG, so we specify that as CV_FOURCC(‘M’,‘J’,‘P’,‘G’).
Th
using. DEMO our case, we set these to the values we got from DEMO original (color) video.
28 | Chapter 2: Introduction to OpenCV
DEMO we close up.
e call to cvCreateVideoWriter() contains several parameters that we should under-
e next two arguments are the replay frame rate, and the size of the images we will be
02-R4886-AT1.indd   28
DEMO/15/08   4:18:12 PM
www.it-ebooks.info
Onward
Before moving on to the next chapter, we should take a moment to take stock of where
we are and look ahead DEMO what is coming. We have seen that the OpenCV API provides
us with a variety of easy-to-use tools for loading still images from fi DEMO, reading video
from disk, or capturing video from cameras. We have also seen that the library con-
tains primitive functions for manipulating these DEMO What we have not yet seen are
the powerful elements of the library, which allow for more sophisticated manipulation
of the entire set of abstract data types that are important to practical vision problem
solving.
In DEMO next few chapters we will delve more deeply into the basics and come to under-
stand in greater detail both the interface-related functions and DEMO image data types. We
will investigate the primitive image manipulation operators and, later, some much more
advanced ones. Th ereaft er, we will be ready to explore the many specialized services
that the API provides DEMO tasks as diverse as camera calibration, tracking, and recogni-
tion. Ready? Let’s go!
Exercises
Download and install OpenCV if you have not already done so. Systematically go
through the directory structure. Note in particular the DEMO directory; there you can
load index.htm, which further links to the main documentation of the library. Further
explore the main areas of the DEMO Cvcore contains the basic data structures and algo-
rithms, cv contains DEMO image processing and vision algorithms, ml includes algorithms
for machine learning DEMO clustering, and otherlibs/highgui contains the I/O functions.
Check out DEMO _make directory (containing the OpenCV build fi les) and also the sam-
ples directory, where example code is stored.
1. Go to the …/opencv/_make directory. On Windows, open the solution fi le opencv
.sln; on Linux, open the appropriate makefi le. Build the library DEMO both the debug
and the release versions. Th is may take some time, but you will need the resulting
library and dll fi les.
2. Go to the …/opencv/samples/c/ directory. Create a project or make fi le and
then import and build lkdemo.c (this is an example motion tracking program).
Attach a camera to your system DEMO run the code. With the display window se-
lected, type “r” DEMO initialize tracking. You can add points by clicking on video po-
sitions with the mouse. You can also switch to watching only the points (and not
the image) by typing “n”. Typing “n” again will toggle between “night” and “day”
views.
3. Use the capture and store code DEMO Example 2-10, together with the doPyrDown() code
of Example 2-5 DEMO create a program that reads from a camera and stores downsam-
pled color images to disk.
Exercises
| 29
02-R4886-AT1.indd   29
9/15/DEMO   4:18:13 PM
4. Modify the code in exercise 3 and combine it with the DEMO display code in
Example 2-1 to display the frames as they are processed.
5. Modify the program of exercise 4 with a slider control DEMO Example 2-3 so that the
user can dynamically vary the pyramid downsampling reduction level by factors
of between 2 and 8. You may skip DEMO this to disk, but you should display the
results.
30
| DEMO 2: Introduction to OpenCV
02-R4886-AT1.indd   30
www.it-ebooks.info
9/15/08   4:18:13 PM
CHAPTER 3
Getting to Know OpenCV
OpenCV Primitive Data Types
OpenCV has DEMO primitive data types. Th ese data types are not primitive from the
point of view of C, but they are all simple structures, DEMO we will regard them as atomic.
You can examine details of the structures described in what follows (as well as other
structures) in DEMO cxtypes.h header fi le, which is in the .../OpenCV/cxcore/DEMO direc-
tory of the OpenCV install.
Th
members, x and y. DEMO has two siblings: CvPoint2D32f and CvPoint3D32f. Th e former
has the DEMO two members x and y, which are both fl oating-point numbers. DEMO e latter
also contains a third element, z.
CvSize is more DEMO a cousin to CvPoint. Its members are width and height, which DEMO
both integers. If you want fl oating-point numbers, use CvSize’s cousin DEMO
CvRect is another child of CvPoint and CvSize; it contains four DEMO: x, y, width, and
height. (In case you were DEMO, this child was adopted.)
Last but not least is CvScalar, which is a set of four double-precision numbers. When
memory is not DEMO issue, CvScalar is oft en used to represent one, two, DEMO three real num-
bers (in these cases, the unneeded components are simply ignored). CvScalar has a
single member val, which is a pointer to an array containing the four double-precision
fl
All of these DEMO types have constructor methods with names like cvSize() (generally*
the DEMO has the same name as the structure type but with the fi rst character
not capitalized). Remember that this is C and not DEMO, so these “constructors” are just
inline functions that take a list DEMO arguments and return the desired structure with the
values set appropriately.
* We say “generally” here because there are a few oddballs. In particular, we have cvScalarAll(double) and
cvRealScalar(double); the former returns a CvScalar with all four values set to the argument, while the
latter returns a CvScalar with the fi rst value set and the other DEMO 0.
31
e simplest of these types is CvPoint. CvPoint is a simple structure with two integer
oating-point numbers.
03-R4886-RC1.indd   31
www.it-ebooks.info
9/DEMO/08   4:18:36 PM
www.it-ebooks.info
Th cvPointXXX(), cvSize(),
cvRect(), and cvScalar()—are extremely useful because they make your code not only
easier to DEMO but also easier to read. Suppose you wanted to draw a white rectangle
between (5, 10) and (20, 30); you could simply call:
cvRectangle(
myImg,
cvPoint(5,10),
cvPoint(20,30),
cvScalar(255,255,255)
);
Table 3-1. Structures for points, size, rectangles, and scalar tuples
Structure Contains Represents
CvPoint int x, y Point in image
CvPoint2D32f float x, y DEMO in ℜ2
CvPoint3D32f float x, y, z Points in ℜ3
CvSize int width, height Size of image
CvRect int x, y, width, height Portion of image
CvScalar double val[4] RGBA value
cvScalar() is a special case: it has three constructors. Th e fi rst, DEMO cvScalar(), takes
one, two, three, or four arguments and assigns those arguments to the correspond-
ing elements of val[]. Th e DEMO constructor is cvRealScalar(); it takes one argu-
ment, which it assigns to val[0] while setting the other entries to 0. Th e DEMO nal variant is
cvScalarAll(), which takes a single argument but DEMO all four elements of val[] to that
same argument.
Matrix and Image Types
Figure 3-1 shows the class or structure hierarchy of the three DEMO types. When using
OpenCV, you will repeatedly encounter the IplImage data DEMO You have already seen
it many times in the previous chapter. IplImage is the basic structure used to encode
what we generally call “images”. DEMO ese images may be grayscale, color, four-channel
(RGB+alpha), and DEMO channel may contain any of several types of integer or fl oating-
point numbers. Hence, this type is more general than the ubiquitous three-channel 8-bit
RGB image that immediately comes to mind.*
OpenCV provides a vast DEMO of useful operators that act on these images, including
tools to DEMO images, extract individual channels, fi nd the largest or smallest value of
a particular channel, add two images, threshold an image, and so on. In this chapter we
will examine these sorts of operators DEMO
* If you are especially picky, you can say that OpenCV DEMO a design, implemented in C, that is not only object-
oriented but also template-oriented.
32 | Chapter 3: Getting to Know OpenCV
e inline constructors for the data types listed in Table 3-1—
03-R4886-RC1.indd   DEMO
9/15/08   4:18:37 PM
www.it-ebooks.info
Figure 3-1. Even though OpenCV is implemented in C, the structures used in OpenCV have an
object-oriented design; in eff ect, IplImage DEMO derived from CvMat, which is derived from CvArr
Before we can DEMO images in detail, we need to look at another data type: CvMat,
the OpenCV matrix structure. Th ough OpenCV is implemented entirely DEMO C, the rela-
tionship between CvMat and IplImage is akin to DEMO in C++. For all intents and
purposes, an IplImage can be DEMO of as being derived from CvMat. Th erefore, it is
best DEMO understand the (would-be) base class before attempting to understand the added
complexities of the derived class. A third class, called CvArr, can DEMO thought of as an
abstract base class from which CvMat is itself derived. You will oft en see CvArr (or, more
accurately, CvArr*) in function prototypes. When it appears, it is acceptable to pass
DEMO or IplImage* to the routine.
CvMat Matrix Structure
Th ere are two things you need to know before we dive into the matrix business. DEMO,
there is no “vector” construct in OpenCV. Whenever we want a vector, we just use a
matrix with one column (or one DEMO, if we want a transpose or conjugate vector).
Second, the concept of a matrix in OpenCV is somewhat more abstract than the DEMO
cept you learned in your linear algebra class. In particular, the DEMO of a matrix
need not themselves be simple numbers. For example, DEMO routine that creates a new
two-dimensional matrix has the following prototype:
cvMat* cvCreateMat ( int rows, int cols, int type );
DEMO type can be any of a long list of predefi ned types of the form: CV_<bit_depth>(S|U|F)
C<number_of_channels>. Th DEMO, the matrix could consist of 32-bit fl oats (CV_32FC1), of un-
signed integer 8-bit triplets (CV_8UC3), or of countless other elements. An element of a
CvMat is not necessarily a single number. Being DEMO to represent multiple values for a
single entry in the matrix allows us to do things like represent multiple color channels
in an RGB DEMO For a simple image containing red, green and blue channels, most im-
age operators will be applied to each channel separately (unless otherwise noted).
Internally, the structure of CvMat is relatively simple, as DEMO in Example 3-1 (you can
see this for yourself by opening DEMO …/opencv/cxcore/include/cxtypes.h). Matrices have
CvMat Matrix Structure | 33
03-R4886-RC1.indd   33
9/15/08   4:18:37 PM
www.it-ebooks.info
a width, a height, a type, a step (the DEMO of a row in bytes, not ints or floats), and DEMO
pointer to a data array (and some more stuff  that we won’t talk about just yet). You can
access these members directly DEMO de-referencing a pointer to CvMat or, for some more
popular elements, by using supplied accessor functions. For example, to obtain the size
of a matrix, you can get the information you want either by calling cvGetSize(CvMat*),
which returns a CvSize structure, or by accessing DEMO height and width independently
with such constructs as matrix->height and matrix->width.
Example 3-1. CvMat structure: the matrix “header”
typedef struct CvMat {
int type;
int step;
int* refcount;    // for internal use only
union {
uchar*  ptr;
short*  s;
DEMO    i;
float*  fl;
double* db;
} data;
union {
int rows;
int height;
};
union DEMO
int cols;
int width;
};
} CvMat;
Th is information is generally referred to as the matrix header. Many routines DEMO
guish between the header and the data, the latter being the DEMO that the data ele-
ment points to.
Matrices can be created in one of several ways. Th e most common way is to use
DEMO(), which is essentially shorthand for the combination of the more DEMO
functions cvCreateMatHeader() and cvCreateData(). cvCreateMatHeader() creates the
CvMat DEMO without allocating memory for the data, while cvCreateData() handles
the DEMO allocation. Sometimes only cvCreateMatHeader() is required, either because you
have DEMO allocated the data for some other reason or because you are not yet ready
to allocate it. Th e third method is to use DEMO cvCloneMat(CvMat*), which creates a new
matrix from an existing one.* When the matrix is no longer needed, it can be released
by calling cvReleaseMat(CvMat**).
Th
others that are closely related.
* cvCloneMat() and other OpenCV functions containing the word “clone” not only create a new header that
is identical to the input header, they also allocate a separate data area and copy the data from the source to
DEMO new object.
34 | Chapter 3: Getting to Know OpenCV
e DEMO in Example 3-2 summarizes the functions we have just described as well as some
03-R4886-RC1.indd   34
9/15/08   4:18:38 DEMO
www.it-ebooks.info
Example 3-2. Matrix creation and release
//  Create a new DEMO by cols matrix of type ‘type’.
//
CvMat* cvCreateMat( int DEMO, int cols, int type );
//  Create only matrix header without allocating data
//
CvMat* cvCreateMatHeader( int rows, int DEMO, int type );
//  Initialize header on existing CvMat DEMO
//
CvMat* cvInitMatHeader(
CvMat* mat,
int   rows,
int   cols,
int   type,
void* data = NULL,DEMO
int   step = CV_AUTOSTEP
);
//  Like cvInitMatHeader() but allocates CvMat as well.
//
CvMat cvMat(
int   DEMO,
int   cols,
int   type,
void* data = NULL
);
//  Allocate a new matrix just like the DEMO ‘mat’.
//
CvMat* cvCloneMat( const cvMat* mat );
//  Free the matrix ‘mat’, both header and data.
//
void DEMO( CvMat** mat );
Analogously to many OpenCV structures, there is a constructor called cvMat() that cre-
ates a CvMat structure. Th DEMO routine does not actually allocate memory; it only creates the
header (this is similar to cvInitMatHeader()). Th ese methods are a DEMO way to take some
data you already have lying around, package DEMO by pointing the matrix header to it as in
Example 3-3, DEMO run it through routines that process OpenCV matrices.
Example 3-3. Creating an OpenCV matrix with fi
xed data
// Create an OpenCV Matrix containing some fixed data.
//
float vals[] = { 0.866025, -0.500000, DEMO, 0.866025 };
CvMat rotmat;
cvInitMatHeader(
&rotmat,
2,
CvMat Matrix Structure
| 35
03-R4886-RC1.indd   35
9/15/08   4:18:38 PM
www.it-ebooks.info
Example 3-3. Creating an OpenCV matrix with fi
xed data (continued)
2,
CV_32FC1,
vals
);
Once we have a DEMO, there are many things we can do with it. Th e DEMO operations
are querying aspects of the array defi nition and data access. To query the matrix, we have
cvGetElemType( const CvArr* arr ), cvGetDims( const CvArr* arr, int* sizes=NULL ),
and cvGetDimSize( const CvArr* arr, int index ). Th e fi rst returns an DEMO constant
representing the type of elements stored in the array (this DEMO be equal to something
like CV_8UC1, CV_64FC4, etc). Th e second takes the array and an optional pointer to an
integer; it returns the number of dimensions (two for the cases we are considering, but
later on we will encounter N-dimensional matrixlike objects). If the integer pointer is
not null then it will store the height and DEMO (or N dimensions) of the supplied array.
Th
turns the extent of the matrix in that dimension.*
Accessing Data in Your Matrix
Th
DEMO right way.
ere are three ways to access the data in your matrix: the easy way, the hard way, and
The easy way
Th CV_MAT_ELEM() macro. Th is
macro (see Example 3-4) takes DEMO matrix, the type of element to be retrieved, and the
row and column numbers and then returns the element.
Example 3-4. Accessing a DEMO with the CV_MAT_ELEM() macro
CvMat* mat = cvCreateMat( 5, 5, CV_32FC1 );
float element_3_2 = CV_MAT_ELEM( *mat, float, 3, 2 );
e easiest way to get at a member element DEMO an array is with the
“Under the hood” this macro is just calling the macro CV_MAT_ELEM_PTR(). CV_MAT_ELEM_
PTR() (see Example 3-5) takes as arguments the matrix and the row and column of the
DEMO element and returns (not surprisingly) a pointer to the indicated element. One
important diff erence between CV_MAT_ELEM() and CV_MAT_ELEM_PTR() is that DEMO()
actually casts the pointer to the indicated type before de-referencing DEMO If you would
like to set a value rather than just read it, you can call CV_MAT_ELEM_PTR() directly; in this
case, however, you must cast the returned pointer to the appropriate type yourself.
Example 3-5. Setting a single value in a matrix using the CV_MAT_ELEM_PTR() DEMO
CvMat* mat = cvCreateMat( 5, 5, CV_32FC1 );
float DEMO = 7.7;
*( (float*)CV_MAT_ELEM_PTR( *mat, 3, 2 ) ) = element_3_2;
* For the regular two-dimensional matrices discussed here, dimension zero (0) is always the “width” and
dimension one (DEMO) is always the height.
36
| Chapter 3: Getting to Know OpenCV
e last function takes an integer indicating the dimension of interest DEMO simply re-
03-R4886-RC1.indd   36
9/15/08   4:18:38 PM
www.it-ebooks.info
Unfortunately, these macros recompute the pointer needed on every call. Th is means
looking up the pointer to the base element of the DEMO area of the matrix, computing an
off set to get the DEMO of the information you are interested in, and then adding that
DEMO set to the computed base. Th us, although these macros are DEMO to use, they may not
be the best way to access DEMO matrix. Th is is particularly true when you are planning to ac-
cess all of the elements in a matrix sequentially. We will come DEMO to the best
way to accomplish this important task.
The hard way
Th e easy way” are suitable only for accessing one- and
two-dimensional DEMO (recall that one-dimensional arrays, or “vectors”, are really just
n-by-1 DEMO). OpenCV provides mechanisms for dealing with multidimensional ar-
rays. In fact OpenCV allows for a general N-dimensional matrix that can have as many
DEMO as you like.
For accessing data in a general matrix, we DEMO the family of functions cvPtr*D and
cvGet*D… listed in Examples 3-6 and 3-7. Th e cvPtr*D family contains cvPtr1D(),
cvPtr2D(), cvPtr3D(), and cvPtrND() . . . . Each of the fi rst three takes a CvArr* matrix
pointer argument followed by the appropriate DEMO of integers for the indices, and
an optional argument indicating the DEMO of the output parameter. Th e routines return
a pointer to the element of interest. With cvPtrND(), the second argument is a pointer to
an array of integers containing the appropriate number of indices. We DEMO return to this
function later. (In the prototypes that follow, you will also notice some optional argu-
ments; we will address those when we need them.)
Example 3-6. Pointer access to matrix structures
uchar* DEMO(
const CvArr* arr,
int          idx0,DEMO
int*         type = NULL
);
uchar* cvPtr2D(
const CvArr* arr,
int          idx0,
int          idx1,
int*         type = NULL
);
uchar* cvPtr3D(
const CvArr* arr,
DEMO          idx0,
int          idx1,
int          idx2,
int*         type = NULL
);
uchar* cvPtrND(
CvMat Matrix DEMO
| 37
e two macros discussed in “Th
03-R4886-RC1.indd   37
9/15/08   4:18:38 PM
www.it-ebooks.info
Example 3-6. Pointer access to matrix structures (continued)
const CvArr* arr,
int*         idx,
int*         type            = NULL,
int          create_node     = 1,
unsigned*    precalc_hashval = NULL
);
For merely reading the data, there is another family of functions cvGet*D, listed in Ex-
ample 3-7, that DEMO analogous to those of Example 3-6 but return the actual value of the
matrix element.
Example 3-7. CvMat and IplImage element functions
double cvGetReal1D( const CvArr* arr, int idx0 );
double cvGetReal2D( const CvArr* DEMO, int idx0, int idx1 );
double cvGetReal3D( const CvArr* DEMO, int idx0, int idx1, int idx2 );
double cvGetRealND( const CvArr* arr, int* idx );
CvScalar cvGet1D( const CvArr* DEMO, int idx0 );
CvScalar cvGet2D( const CvArr* arr, int DEMO, int idx1 );
CvScalar cvGet3D( const CvArr* arr, int DEMO, int idx1, int idx2 );
CvScalar cvGetND( const CvArr* DEMO, int* idx );
Th cvGet*D is double for four of DEMO routines and CvScalar for the other
four. Th is means that there can be some signifi cant waste when using these functions.
Th
use DEMO
One reason it is better to use cvPtr*D() is that you can use these pointer functions to
gain access to a particular point DEMO the matrix and then use pointer arithmetic to move
around in the matrix from there. It is important to remember that the channels are DEMO
tiguous in a multichannel matrix. For example, in a three-channel two-dimensional DEMO
trix representing red, green, blue (RGB) bytes, the matrix DEMO is stored: rgbrgbrgb . . . .
Th
we wanted to DEMO to the next “pixel” or set of elements, we’d add and DEMO set equal to the
number of channels (in this case 3)DEMO
Th step element in the matrix array (see Examples 3-1 and
DEMO) is the length in bytes of a row in the matrix. DEMO that structure, cols or width alone
is not enough to move DEMO matrix rows because, for machine effi  ciency, matrix or
image DEMO is done to the nearest four-byte boundary. Th us a matrix of width three
bytes would be allocated four bytes with the last one DEMO For this reason, if we get
a byte pointer to a DEMO element then we add step to the pointer in order to step it to the
next row directly below our point. If we have DEMO matrix of integers or fl oating-point num-
bers and corresponding int or float pointers to a data element, we would step to the
next row by adding step/4; for doubles, we’d add step/8 (this is just to take into account
that C will automatically multiply DEMO off sets we add by the data type’s byte size).
38
| Chapter 3: Getting to Know OpenCV
e return type of
ey should be used only where convenient and effi  cient; otherwise, it is better just to
erefore, to move a pointer of the appropriate type to the next channel, we add 1. If
e other trick to know is that the
03-R4886-RC1.indd   38
9/15/08   DEMO:18:39 PM
www.it-ebooks.info
Somewhat analogous to cvGet*D is cvSet*D in Example 3-8, which sets a matrix or image
element with a single call, and the functions cvSetReal*D() and cvSet*D(), which can be
used to set the values of elements of a matrix or image.
Example 3-8. Set element DEMO for CvMat or IplImage.
void cvSetReal1D( CvArr* arr, int idx0, DEMO value );
void cvSetReal2D( CvArr* arr, int idx0, int DEMO, double value );
void cvSetReal3D(
CvArr* arr,
int DEMO,
int idx1,
int idx2,
double value
);
void cvSetRealND( CvArr* arr, int* idx, double value );
void cvSet1D( CvArr* arr, int idx0, CvScalar value );
void cvSet2D( DEMO arr, int idx0, int idx1, CvScalar value );
void DEMO(
CvArr* arr,
int idx0,
int idx1,
int idx2,
CvScalar value
);
void cvSetND( CvArr* arr, int* idx, CvScalar value );
As an added convenience, we also have cvmSet() and cvmGet(), which are used when
dealing with single-channel fl oating-point matrices. Th ey are very simple:
double cvmGet( const CvMat* mat, int row, int col )
void cvmSet( CvMat* mat, int row, int col, double value )
So the call to the convenience function cvmSet(),
cvmSet( mat, 2, 2, DEMO );
is the same as the call to the equivalent cvSetReal2D function,
cvSetReal2D( mat, 2, 2, 0.5000 );
The DEMO way
With all of those accessor functions, you might think that DEMO nothing more to say.
In fact, you will rarely use any DEMO the set and get functions. Most of the time, vision is
DEMO processor-intensive activity, and you will want to do things in the DEMO effi  cient way
possible. Needless to say, going through these interface functions is not effi  cient. Instead,
you should do your own pointer arithmetic and simply de-reference your way into the
matrix. Managing the DEMO yourself is particularly important when you want to do
something to every element in an array (assuming there is no OpenCV routine that can
perform this task for you).
For direct access to the innards DEMO a matrix, all you really need to know is that the DEMO
is stored sequentially in raster scan order, where columns (“x”) DEMO the fastest-running
CvMat Matrix Structure
| 39
03-R4886-RC1.indd   39
9/15/08   4:18:39 PM
www.it-ebooks.info
variable. Channels are interleaved, which means that, in the case DEMO a multichannel ma-
trix, they are a still faster-running ordinal. Example DEMO shows an example of how this
can be done.
Example 3-9. Summing all of the elements in a three-channel matrix
float sum( const CvMat* mat ) {
float s = 0.0f;
for(int row=0; row<mat->rows; row++ ) {
const float* ptr = (const float*)(mat->data.ptr + row * mat->step);
for( col=0; col<mat->cols; col++ ) {
s += *ptr++;
}
}
return( s );
}
When computing the pointer into the matrix, DEMO that the matrix element data
is a union. Th erefore, when DEMO this pointer, you must indicate the correct
element of the union DEMO order to obtain the correct pointer type. Th en, to off DEMO that
pointer, you must use the step element of the matrix. DEMO noted previously, the step ele-
ment is in bytes. To be DEMO, it is best to do your pointer arithmetic in bytes and DEMO
cast to the appropriate type, in this case float. Although the DEMO structure has the
concept of height and width for compatibility with the older IplImage structure, we use
the more up-to-date rows and cols instead. Finally, note that we recompute ptr for every
row rather than simply starting at the beginning and then incrementing that pointer
every read. Th DEMO might seem excessive, but because the CvMat data pointer could just
DEMO to an ROI within a larger array, there is no guarantee DEMO the data will be contigu-
ous across rows.
Arrays of Points
One issue that will come up oft en—and that is important to understand—is DEMO diff er-
ence between a multidimensional array (or matrix) of multidimensional objects and an
array of one higher dimension that contains only one-dimensional DEMO Suppose, for
example, that you have n points in three dimensions which you want to pass to some
OpenCV function that takes an DEMO of type CvMat* (or, more likely, cvArr*). Th ere
DEMO four obvious ways you could do this, and it is absolutely DEMO to remember that
they are not necessarily equivalent. One method would be to use a two-dimensional ar-
ray of type CV32FC1 with n rows DEMO three columns (n-by-3). Similarly, you could use a
two-dimensional array with three rows and n columns (3-by-n). You could also use an
array with n rows and one column (n-by-1) of type DEMO or an array with one row and
n columns (3-by-1). DEMO of these cases can be freely converted from one to the other
(meaning you can just pass one where the other is expected) DEMO others cannot. To un-
derstand why, consider the memory layout shown DEMO Figure 3-2.
As you can see in the fi gure, the DEMO are mapped into memory in the same way for three
of the four cases just described above but diff erently for the last. Th DEMO situation is even
40
| Chapter 3: Getting to Know OpenCV
DEMO   40
9/15/08   4:18:39 PM
www.it-ebooks.info
Figure 3-2. A set of ten points, each represented by three fl oating-point numbers, placed in four ar-
rays that each use a slightly diff erent structure; in three cases the resulting memory layout is identi-
cal, but one case is diff erent
more complicated for the case of an N-dimensional array of c-dimensional points. Th e
key thing DEMO remember is that the location of any given point is given by the formula:
δ=+ +() ( ) (row ⋅⋅ ⋅NN Ncols channels col chchannels annel)
where Ncols and Nchannels are the number DEMO columns and channels, respectively.* From
this formula one can see that, in general, an N-dimensional array of c-dimensional ob-
jects is not the same as an (N + c)-dimensional array of one-dimensional objects. In the
special case of N = 1 (i.e., vectors represented either DEMO n-by-1 or 1-by-n arrays), there is
a special degeneracy (specifi DEMO, the equivalences shown in Figure 3-2) that can some-
times be taken advantage of for performance.
Th
Th ned as C structures and DEMO have a strictly defi ned mem-
ory layout. In particular, the DEMO or fl oating-point numbers that these structures
comprise are “channel” sequential. As a result, a one-dimensional C-style array of these
objects has the same memory layout as an n-by-1 or a 1-by-n array of type CV32FC2. DEMO
lar reasoning applies for arrays of structures of the type CvPoint3D32f.
* In this context we use the term “channel” to refer to the DEMO index. Th is index is the one associ-
ated with the C3 part of CV32FC3. Shortly, when we talk about images, the “channel” DEMO will be exactly
equivalent to our use of “channel” here.
CvMat Matrix Structure | 41
e last detail concerns the OpenCV data types such DEMO CvPoint2D and CvPoint2D32f.
ese data types are defi
03-R4886-RC1.indd   41
9/15/08   4:18:39 PM
www.it-ebooks.info
IplImage Data Structure
With all of that in hand, it is now easy to discuss the IplImage data structure. In es-
sence this DEMO is a CvMat but with some extra goodies buried in it to make the matrix
interpretable as an image. Th is structure was originally DEMO ned as part of Intel’s Image
Processing Library (IPL).* Th DEMO exact defi nition of the IplImage structure is shown in
Example 3-10.
Example 3-10. IplImage header structure
typedef struct _IplImage {
int                  nSize;
int                  ID;
int                  nChannels;
int                  alphaChannel;
int                  depth;
char                 colorModel[4];
char                 channelSeq[4];
int                  dataOrder;
int                  origin;
int                  align;
int                  width;
int                  height;
struct _IplROI*      roi;
struct _IplImage*    maskROI;
void*                imageId;
struct _IplTileInfo* tileInfo;
int                  imageSize;
char*                imageData;
int                  widthStep;
int                  BorderMode[4];
int                  BorderConst[4];
char*                imageDataOrigin;
} IplImage;
As crazy as it sounds, we want to discuss the function of several of these variables. Some
are trivial, but many are very important to understanding how OpenCV interprets and
works DEMO images.
Aft er the ubiquitous width and height, depth and nChannels DEMO the next most crucial.
Th depth variable takes one of a set of values defi ned in ipl.h, which are (unfortunately)
not DEMO the values we encountered when looking at matrices. Th is is because for im-
ages we tend to deal with the depth and the DEMO of channels separately (whereas in
the matrix routines we tended to DEMO to them simultaneously). Th e possible depths are
listed in Table 3-2.
* IPL was the predecessor to the more modern Intel Performance DEMO (IPP), discussed in Chapter 1.
Many of the OpenCV functions DEMO actually relatively thin wrappers around the corresponding IPL or IPP
routines. Th is is why it is so easy for OpenCV to swap in DEMO high-performance IPP library routines when
available.
42
| Chapter 3: Getting DEMO Know OpenCV
e
03-R4886-RC1.indd   42
9/15/08   4:18:39 PM
www.it-ebooks.info
Table 3-2. OpenCV image types
Macro
IPL_DEPTH_8U
IPL_DEPTH_8S
IPL_DEPTH_16S
IPL_DEPTH_32S
IPL_DEPTH_32F
DEMO
Image pixel type
Unsigned 8-bit integer (8u)
Signed 8-bit integer (8s)
Signed 16-bit integer (16s)
Signed 32-bit integer (32s)DEMO
32-bit fl oating-point single-precision (32f)
64-bit fl oating-point double-precision (64f)
Th
nChannels are 1, 2, 3, or 4.
e possible values for
Th origin and dataOrder. Th e origin variable can
take DEMO of two values: IPL_ORIGIN_TL or IPL_ORIGIN_BL, corresponding to the origin of
coordinates being located in either the upper-left  or lower-left  corners of DEMO image, re-
spectively. Th e lack of a standard origin (upper versus lower) is an important source of
error in computer vision routines. In particular, depending on where an image came
from, the operating DEMO, codec, storage format, and so forth can all aff ect DEMO loca-
tion of the origin of the coordinates of a particular image. For example, you may think
you are sampling pixels from a face in the top quadrant of an image when you are really
sampling DEMO a shirt in the bottom quadrant. It is best to check the system the fi rst
time through by drawing where you think you DEMO operating on an image patch.
Th
indicates whether the data should be packed with multiple channels one aft er the other
for each pixel (interleaved, the usual case), or rather all of the channels clustered into
image planes with the planes placed one aft er another.
Th
DEMO and successive rows (similar to the “step” parameter of CvMat discussed DEMO).
Th
aligned with a certain number of bytes to achieve faster processing of the image; hence
there may be some gaps between the end of ith row and the start of (i + 1) DEMO Th e pa-
rameter imageData contains a pointer to the fi rst row of image data. If there are several
separate planes in the DEMO (as when dataOrder = IPL_DATA_ORDER_PLANE) then they are
placed consecutively as separate images with height*nChannels rows in total, but nor-
mally they are interleaved so that the number of rows is equal to height and DEMO each
row containing the interleaved channels in order.
Finally there is the practical and important region of interest (ROI), which is actually an
instance of another IPL/IPP structure, IplROI. An IplROI contains an xOffset, a yOffset,
* We say that dataOrder may be either IPL_DATA_ORDER_PIXEL or IPL_DATA_ORDER_PLANE, but in fact only
IPL_DATA_ORDER_PIXEL is supported by OpenCV. Both values are generally supported by IPL/IPP, but
OpenCV always uses interleaved images.
IplImage Data Structure
| 43
e next two important members are
DEMO dataOrder may be either IPL_DATA_ORDER_PIXEL or IPL_DATA_ORDER_PLANE.* Th
is value
e parameter widthStep contains the number of bytes between points in the same col-
DEMO variable width is not suffi
cient to calculate the distance because each row may be
03-R4886-RC1.indd   43
9/15/08   4:18:DEMO PM
www.it-ebooks.info
a height, a width, and a coi, where COI stands for channel of interest.* Th e idea behind the
ROI is that, once it is set, functions that would normally operate on the entire image will
instead act only on the subset of the image indicated DEMO the ROI. All OpenCV functions
will use ROI if set. If the COI is set to a nonzero value then some operators will act DEMO on
the indicated channel.† Unfortunately, many OpenCV functions ignore this parameter.
DEMO Image Data
When working with image data we usually need to do so quickly and effi  ciently. Th is
suggests that we should not subject ourselves to the overhead of calling accessor func-
tions like cvSet*D DEMO their equivalent. Indeed, we would like to access the data inside DEMO
the image in the most direct way possible. With our knowledge of the internals of the
IplImage structure, we can now understand how best to do this.
Even though there are oft en well-optimized routines in DEMO that accomplish many
of the tasks we need to perform on images, there will always be tasks for which there is no
prepackaged routine in the library. Consider the case of a three-channel HSV [Smith78]
image‡ DEMO which we want to set the saturation and value to 255 (DEMO maximal values
for an 8-bit image) while leaving the hue unmodifi DEMO We can do this best by handling
the pointers into the image ourselves, much as we did with matrices in Example 3-9.
However, DEMO are a few minor diff erences that stem from the diff erence between the
IplImage and CvMat structures. Example 3-11 shows the fastest way.
DEMO 3-11. Maxing out (saturating) only the “S” and “V” parts of an HSV image
void saturate_sv( IplImage* img ) {
for( int DEMO; y<img->height; y++ ) {
uchar* ptr = (uchar*) (
img->imageData + y * img->widthStep
);
for( DEMO x=0; x<img->width; x++ ) {
ptr[3*x+1] = 255;
ptr[3*x+2] = 255;
}
}
}
We simply compute the pointer DEMO directly as the head of the relevant row y. From
there, DEMO de-reference the saturation and value of the x column. Because this is a three-
channel image, the location of channel c in column x is 3*x+c.
* Unlike other parts of the ROI, the COI is not respected by all OpenCV functions. More on this later, but for
now you should keep in mind that COI is not as universally DEMO as the rest of the ROI.
† For the COI, the DEMO is to indicate the channel as 1, 2, 3, or DEMO and to reserve 0 for deactivating the
COI all together (something DEMO a “don’t care”).
‡ In OpenCV, an HSV image does DEMO diff er from an RGB image except in terms of how the channels are
interpreted. As a result, constructing an HSV image from an RGB image actually occurs entirely within the
“data” area; there is no representation in the header of what meaning is “intended” for the data DEMO
44 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   44
DEMO/15/08   4:18:40 PM
www.it-ebooks.info
One important diff erence between the IplImage case and the CvMat DEMO is the behav-
ior of imageData, compared to the element data DEMO CvMat. Th e data element of CvMat is a
union, so DEMO must indicate which pointer type you want to use. Th e imageData pointer
is a byte pointer (uchar*). We already know that the data pointed to is not necessarily of
type uchar, which means that—when doing pointer arithmetic on images—you can sim-
ply add widthStep (also measured in bytes) without worrying about the actual data type
until aft er the addition, when you cast the resultant pointer to the data type you need.
To recap: when working with matrices, you must scale DEMO the off set because the data
pointer may be of nonbyte type; when working with images, you can use the off set “as
DEMO because the data pointer is always of a byte type, so DEMO can just cast the whole thing
when you are ready to use it.
More on ROI and widthStep
ROI and widthStep have great practical DEMO, since in many situations they speed
up computer vision operations by DEMO the code to process only a small subregion of
the image. Support for ROI and widthStep is universal in OpenCV:* every function allows
DEMO to be limited to a subregion. To turn ROI on or off , use the cvSetImageROI()
and cvResetImageROI() functions. Given a rectangular subregion of interest in the form
of a CvRect, you may pass an image pointer and the rectangle to cvSetImageROI() to “turn
on” DEMO; “turn off ” ROI by passing the image pointer to cvResetImageROI().
void cvSetImageROI( IplImage* image, CvRect rect );
void cvResetImageROI( IplImage* image );
To see how ROI is used, let’s suppose we want to load an image and modify some region
of that DEMO Th e code in Example 3-12 reads an image and then sets the x, y, width,
and height of the intended ROI DEMO fi nally an integer value add to increment the ROI
region with. Th e program then sets the ROI using the convenience of the DEMO cvRect()
constructor. It’s important to release the ROI with cvResetImageROI(), for otherwise the
display will observe the ROI and dutifully display DEMO the ROI region.
Example 3-12. Using ImageROI to increment all of the pixels in a region
// roi_add <image> <x> <y> <width> <height> <add>
#include <cv.h>
#include <highgui.h>
int main(int argc, char** argv)
{
IplImage* src;
if( argc == 7 && ((src=cvLoadImage(argv[1],1)) != DEMO ))
{
int x = atoi(argv[2]);
int y DEMO atoi(argv[3]);
int width = atoi(argv[4]);
int height = atoi(argv[5]);
* Well, in theory at least. Any DEMO to widthStep or ROI is considered a bug and may be posted
as such to SourceForge, where it will go on a “to fi x” list. Th is is in contrast with color channel of interest,DEMO
“COI”, which is supported only where explicitly stated.
IplImage Data Structure
DEMO 45
03-R4886-RC1.indd   45
9/15/08   4:18:40 PM
Example 3-12. Using ImageROI to increment all of the pixels in a DEMO (continued)
int add = atoi(argv[6]);
cvSetImageROI(src, DEMO(x,y,width,height));
cvAddS(src, cvScalar(add),src);
cvResetImageROI(src);
cvNamedWindow( “Roi_Add”, 1 );
cvShowImage( “Roi_Add”, src );
cvWaitKey();
}
return 0;
}
Figure 3-3 shows the result of adding 150 to the blue channel DEMO the image of a cat with
an ROI centered over its face, using the code from Example 3-12.
Figure 3-3. Result of adding 150 to the face ROI of a cat
We can achieve the same DEMO ect by clever use of widthStep. To do this, we create DEMO im-
age header and set its width and height equal to the interest_rect width and height. We
also need to set the image origin (upper left  or lower left ) to be the same as the interest_
img. Next we set the widthStep of this subimage to be DEMO widthStep of the larger interest_
46
| Chapter 3: Getting to DEMO OpenCV
03-R4886-RC1.indd   46
www.it-ebooks.info
9/15/08   4:18:40 PM
www.it-ebooks.info
img; this way, stepping by rows in the subimage steps DEMO to the appropriate place at the
start of the next line of the subregion within the larger image. We fi nally set the subimage
DEMO pointer the start of the interest subregion, as shown in Example DEMO
Example 3-13. Using alternate widthStep method to increment all of the pixels of interest_img by 1
// Assuming IplImage *interest_img; and
//  DEMO interest_rect;
//  Use widthStep to get a region of interest
//
// (Alternate method)
//
IplImage *sub_img = DEMO(
cvSize(
interest_rect.width,
interest_rect.height
),
interest_img->depth,
interest_img->nChannels
);
sub_img->origin = interest_img->origin;
sub_img->widthStep = DEMO>widthStep;
sub_img->imageData = interest_img->imageData +
interest_rect.y * interest_img->widthStep   +
interest_rect.x * interest_img->nChannels;
cvAddS( sub_img, cvScalar(1), sub_img );
cvReleaseImageHeader(&sub_img);
So, why would you DEMO to use the widthStep trick when setting and resetting ROI seem
to be more convenient? Th e reason is that there are times when you want to set and per-
haps keep multiple subregions of an DEMO active during processing, but ROI can only
be done serially and DEMO be set and reset constantly.
Finally, a word should be said DEMO about masks. Th e cvAddS() function used in the
code examples allows the use of a fourth argument that defaults to NULL: const CvArr*
mask=NULL. Th is is an 8-bit single-channel array that allows you DEMO restrict processing to
an arbitrarily shaped mask region indicated by nonzero pixels in the mask. If ROI is set
along with a mask, processing will be restricted to the intersection of the ROI and the
mask. DEMO can be used only in functions that specify their use.
Matrix and Image Operators
Table 3-3 lists a variety of routines for matrix manipulation, most of which work equally
well for images. Th ey do all DEMO the “usual” things, such as diagonalizing or transpos-
ing a matrix, as well as some more complicated operations, such as computing image
statistics.
Matrix and Image Operators
| 47
03-R4886-RC1.indd   47
9/15/08   4:18:41 PM
www.it-ebooks.info
Table 3-3. Basic matrix and image operators
Function
cvAbs
cvAbsDiff
cvAbsDiffS
DEMO
cvAddS
cvAddWeighted
cvAvg
cvAvgSdv
cvCalcCovarMatrix
cvCmp
cvCmpS
cvConvertScale
cvConvertScaleAbs
cvCopy
cvCountNonZero
cvCrossProduct
cvCvtColor
cvDet
cvDiv
cvDotProduct
cvEigenVV
cvFlip
cvGEMM
cvGetCol
cvGetCols
cvGetDiag
cvGetDims
cvGetDimSize
DEMO
cvGetRows
cvGetSize
cvGetSubRect
cvInRange
cvInRangeS
cvInvert
Description
Absolute value of all elements in an array
Absolute value of diff erences between two arrays
Absolute DEMO of diff erence between an array and a scalar
Elementwise addition of two arrays
Elementwise addition of an array and a scalar
Elementwise weighted DEMO of two arrays (alpha blending)
Average value of all elements DEMO an array
Absolute value and standard deviation of all elements in an array
Compute covariance of a set of n-dimensional vectors
Apply selected comparison DEMO to all elements in two arrays
Apply selected comparison operator to an array relative to a scalar
Convert array type with optional rescaling of DEMO value
Convert array type after absolute value with optional rescaling
Copy elements of one array to another
Count nonzero elements in an array
Compute DEMO product of two three-dimensional vectors
Convert channels of an array from one color space to another
Compute determinant of a square matrix
Elementwise division DEMO one array by another
Compute dot product of two vectors
Compute eigenvalues and eigenvectors of a square matrix
Flip an array about a selected DEMO
Generalized matrix multiplication
Copy elements from column slice of an array
Copy elements from multiple adjacent columns of an array
Copy elements from an DEMO diagonal
Return the number of dimensions of an array
Return the sizes of all dimensions of an array
Copy elements from row slice of DEMO array
Copy elements from multiple adjacent rows of an array
Get size of a two-dimensional array and return as CvSize
Copy elements from subregion DEMO an array
Test if elements of an array are within values of two other arrays
Test if elements of an array are in range DEMO two scalars
Invert a square matrix
48
| Chapter 3: Getting DEMO Know OpenCV
03-R4886-RC1.indd   48
9/15/08   4:18:41 PM
www.it-ebooks.info
Table 3-3. Basic matrix and image operators (continued)
Function
cvMahalonobis
cvMax
cvMaxS
cvMerge
cvMin
cvMinS
cvMinMaxLoc
cvMul
cvNot
cvNorm
cvNormalize
cvOr
cvOrS
DEMO
cvRepeat
cvSet
cvSetZero
cvSetIdentity
cvSolve
cvSplit
cvSub
cvSubS
cvSubRS
cvSum
cvSVD
cvSVBkSb
cvTrace
cvTranspose
cvXor
cvXorS
cvZero
Description
Compute Mahalonobis distance between two vectors
DEMO max operation on two arrays
Elementwise max operation between an array and a scalar
Merge several single-channel images into one multichannel image
Elementwise min DEMO on two arrays
Elementwise min operation between an array and a scalar
Find minimum and maximum values in an array
Elementwise multiplication of two DEMO
Bitwise inversion of every element of an array
Compute normalized correlations between two arrays
Normalize elements in an array to some value
Elementwise bit-level DEMO of two arrays
Elementwise bit-level OR of an array and a scalar
Reduce a two-dimensional array to a vector by a given operation
Tile DEMO contents of one array into another
Set all elements of an array to a given value
Set all elements of an array to 0
DEMO all elements of an array to 1 for the diagonal and 0 otherwise
Solve a system of linear equations
Split a multichannel array into DEMO single-channel arrays
Elementwise subtraction of one array from another
Elementwise subtraction of a scalar from an array
Elementwise subtraction of an array from a DEMO
Sum all elements of an array
Compute singular value decomposition of a two-dimensional array
Compute singular value back-substitution
Compute the trace of an array
DEMO all elements of an array across the diagonal
Elementwise bit-level XOR between two arrays
Elementwise bit-level XOR between an array and a scalar
Set DEMO elements of an array to 0
cvAbs, cvAbsDiff, and cvAbsDiffS
void cvAbs(
const CvArr* src,
const        dst
);
Matrix and Image Operators
| 49
03-R4886-RC1.indd   49
9/15/DEMO   4:18:41 PM
www.it-ebooks.info
void cvAbsDiff(
const CvArr* src1,
const CvArr* src2,
DEMO        dst
);
void cvAbsDiffS(
const CvArr* DEMO,
CvScalar     value,
const        dst
);
Th erence between the
array and some reference. Th e cvAbs() function simply computes the absolute value of
the elements in src DEMO writes the result to dst; cvAbsDiff() fi rst subtracts src2 DEMO
src1 and then writes the absolute value of the diff erence to dst. Note that cvAbsDiffS()
is essentially the same as cvAbsDiff() except that the value subtracted from all of the
elements of src DEMO the constant scalar value.
ese functions compute the absolute value of an array or of the diff
cvAdd, cvAddS, cvAddWeighted, and alpha blending
void cvAdd(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
const CvArr* mask = NULL
);
void cvAddS(
const CvArr* src,
CvScalar     value,
CvArr*       dst,
const CvArr* mask = NULL
);
void  cvAddWeighted(
const CvArr* src1,
double       alpha,
const CvArr* src2,
double       beta,
double       gamma,
CvArr*       dst
);
cvAdd() is a simple addition function: it adds all of the elements in src1 to the corre-
sponding elements in src2 and puts the results DEMO dst. If mask is not set to NULL, then any
element DEMO dst that corresponds to a zero element of mask remains unaltered by this op-
eration. Th e closely related function cvAddS() does the DEMO thing except that the con-
stant scalar value is added to every element of src.
Th cvAddWeighted() is similar to cvAdd() except DEMO the result written to dst is
computed according to the following formula:
dst src src
xy
⋅⋅
12
xy
xy
,, ,
DEMO +αβ γ
50
| Chapter 3: Getting to Know OpenCV
e DEMO
03-R4886-RC1.indd   50
9/15/08   4:18:41 PM
www.it-ebooks.info
Th is function can be used to implement alpha blending [Smith79; Porter84]; that is, it
can be used to blend one image DEMO another. Th e form of this function is:
void  cvAddWeighted(DEMO
const CvArr* src1,
double       alpha,
const CvArr* src2,
double       beta,
double       DEMO,
CvArr*       dst
);
In cvAddWeighted() we have two source images, src1 and src2. Th ese images may be of
any pixel type so long as both are of the same DEMO Th ey may also be one or three chan-
nels (grayscale DEMO color), again as long as they agree. Th e destination result image, dst,
must also have the same pixel type as src1 and src2. Th ese images may be of diff erent
sizes, but their ROIs must agree in size or else OpenCV will issue an DEMO Th e param-
eter alpha is the blending strength of src1, DEMO beta is the blending strength of src2. Th e
alpha blending equation is:
dst src src
xy
⋅⋅
12
xy
xy
,, ,DEMO
=+ +αβ γ
You can convert to the standard alpha blend equation by choosing α between 0 and 1,
setting β = 1 DEMO α, and setting γ to 0; this yields:
dst src
xy
⋅⋅
() src xy
=+−αα
11 2
,, ,xy
However, cvAddWeighted() gives us a little more fl exibility—both in how we weight the
blended images and in the additional parameter γ, which allows for an additive off set to
the resulting destination image. For the DEMO form, you will probably want to keep
alpha and beta at DEMO less than 0 and their sum at no more than 1; DEMO may be set
depending on average or max image value to scale the pixels up. A program showing the
use of alpha blending is DEMO in Example 3-14.
Example 3-14. Complete program to alpha blend the ROI starting at (0,0) in src2 with the ROI
starting at (x,y) in src1
// alphablend <imageA> <image B> <x> <y> <width> <height>
//            <alpha> <beta>
#include <cv.h>
#include <DEMO>
int main(int argc, char** argv)
{
IplImage *src1, *src2;
if( argc == 9 && ((src1=cvLoadImage(argv[1],1)) != 0
)&&((src2=cvLoadImage(argv[2],1)) != 0 ))DEMO
{
int x = atoi(argv[3]);
int y = atoi(DEMO);
int width = atoi(argv[5]);
Matrix and Image Operators
| 51
03-R4886-RC1.indd   51
9/15/08   4:18:42 DEMO
www.it-ebooks.info
Example 3-14. Complete program to alpha blend the ROI starting at (0,0) in src2 with the ROI
starting at (x,y) in src1 (continued)
int height = atoi(argv[6]);
double DEMO = (double)atof(argv[7]);
double beta  = (double)atof(argv[8]);
cvSetImageROI(src1, cvRect(x,y,width,height));
DEMO(src2, cvRect(0,0,width,height));
cvAddWeighted(src1, alpha, src2, beta,0.0,src1);
cvResetImageROI(src1);
cvNamedWindow( “Alpha_blend”, 1 );
cvShowImage( “Alpha_blend”, src1 );
cvWaitKey();
}
return 0;
}
Th src1) and the one
to blend (src2). It reads in a rectangle ROI for src1 and applies an ROI of the same size to
src2, this time located at the origin. It reads in alpha and beta levels but sets gamma DEMO 0.
Alpha blending is applied using cvAddWeighted(), and the results DEMO put into src1 and
displayed. Example output is shown in Figure 3-4, where the face of a child is blended
onto the face and body of a cat. Note that the code took the same ROI DEMO in the ROI ad-
dition example in Figure 3-3. Th is time we used the ROI as the target blending region.
e code in DEMO 3-14 takes two source images: the primary one (
cvAnd and cvAndS
void cvAnd(
const CvArr* src1,
const CvArr* src2,
DEMO       dst,
const CvArr* mask = NULL
);
void cvAndS(
const CvArr* src1,
CvScalar     value,
DEMO       dst,
const CvArr* mask = NULL
);
Th src1. In the case of
cvAnd(), each element of dst is computed as the bitwise AND of the corresponding two
elements of DEMO and src2. In the case of cvAndS(), the bitwise AND DEMO computed with the
constant scalar value. As always, if mask is DEMO then only the elements of dst cor-
responding to nonzero entries in mask are computed.
Th src1 and src2 must have the same data DEMO for
cvAnd(). If the elements are of a fl oating-point DEMO, then the bitwise representation of
that fl oating-point number is used.
DEMO
| Chapter 3: Getting to Know OpenCV
ese two functions compute DEMO bitwise AND operation on the array
ough all data types are supported,
03-R4886-RC1.indd   52
9/15/08   4:18:42 PM
www.it-ebooks.info
Figure 3-4. Th
e face of a child is alpha blended DEMO the face of a cat
cvAvg
CvScalar cvAvg(
const CvArr* arr,
const CvArr* mask = NULL
);
cvAvg() computes the DEMO value of the pixels in arr. If mask is non-NULL then the aver-
age will be computed only over those pixels for which the DEMO value of mask
is nonzero.
Th
is function has the now deprecated alias cvMean().
cvAvgSdv
cvAvgSdv(
const CvArr* arr,
CvScalar*    mean,
CvScalar*    std_dev,
const CvArr* mask    = NULL
);
Matrix and Image Operators
| 53
03-R4886-RC1.indd   53
DEMO/15/08   4:18:42 PM
www.it-ebooks.info
Th is function is like cvAvg(), but in addition to the average it also computes the standard
deviation of the pixels.
Th DEMO function has the now deprecated alias cvMean_StdDev().
cvCalcCovarMatrix
void cvAdd(DEMO
const CvArr** vects,
int           count,
CvArr*        cov_mat,
CvArr*        avg,DEMO
int           flags
);
Given any number of vectors, cvCalcCovarMatrix() will compute the mean and covari-
ance matrix for the Gaussian approximation to the distribution of those points. Th is DEMO
be used in many ways, of course, and OpenCV has some additional fl ags that will help
in particular contexts (see Table 3-4). Th ese fl ags may be combined by the standard use
DEMO the Boolean OR operator.
Table 3-4. Possible components of fl ags argument to cvCalcCovarMatrix()
Flag in flags argument Meaning
CV_COVAR_NORMAL Compute mean and covariance
CV_COVAR_SCRAMBLED Fast PCA “scrambled” covariance
CV_COVAR_USE_AVERAGE Use avg as input instead DEMO computing it
CV_COVAR_SCALE Rescale output covariance matrix
In all cases, the DEMO are supplied in vects as an array of OpenCV arrays (i.e., a pointer
to a list of pointers to arrays), with the DEMO count indicating how many arrays are
being supplied. Th e results will be placed in cov_mat in all cases, but the exact meaning
of avg depends on the fl ag values (see Table 3-4).
Th ags CV_COVAR_NORMAL and CV_COVAR_SCRAMBLED are mutually exclusive; you should
use one or the other but not both. In the case of CV_COVAR_NORMAL, the function will sim-
ply compute the mean and covariance of the points provided.
DEMO L ⎤ ⎡ ⎤T
⎢ 00 0,,m 0 0⎥ ⎢ DEMO 0⎥
Σ2 = z ⎢ MO M ⎥ ⎢ ⎥
normal ⎢ L ⎥ ⎢ ⎥
⎣ 0 ⎦ ⎣ n ⎦
vv−−vv vv00 DEMO,,−−vvL m
MO M
vv−−vv vv−−vvL
,,nn mnn 0 ,,DEMO mn
us the normal covariance Σ2 normal is computed from the m vectors of length n, where
n is defi ned as the nth element of the average vector –v . Th e resulting covariance matrix
DEMO
–v
is an n-by-n matrix. Th e factor z is an optional scale factor; it will be set to 1 unless the
CV_COVAR_SCALE fl ag is used.
In the case of CV_COVAR_SCRAMBLED, cvCalcCovarMatrix() will compute the following:
54 | Chapter 3: Getting to Know OpenCV
e fl
03-R4886-RC1.indd   54
9/15/08   4:18:42 PM
www.it-ebooks.info
⎡
⎢
Σ2 = z ⎢
scrambled ⎢
⎣
L ⎤T
DEMO 0,,m 0 0⎥
MO M ⎥
⎥
⎦
vv−−vv
−v DEMO
vv0 ,,nn mn−vn
⎡
⎢
⎢
⎢
⎣
vv−−vv
L
00 0,,m
MO M
vv−v L
⎤
0 0⎥
⎥
⎥
0,nn m,, −v ⎦
nn
Th is matrix is not the DEMO covariance matrix (note the location of the transpose op-
erator). DEMO is matrix is computed from the same m vectors of length n, but the resulting
scrambled covariance matrix is an m-by-m matrix. Th is matrix is used in some specifi c
algorithms such as fast PCA DEMO very large vectors (as in the eigenfaces technique for face
recognition)DEMO
Th ag CV_COVAR_USE_AVG is used when the mean of the input vectors is already known.
In this case, the argument avg is used as an input rather than an output, which reduces
computation time.
Finally, DEMO fl ag CV_COVAR_SCALE is used to apply a uniform scale to the covariance matrix
calculated. Th is is the factor z in the preceding DEMO When used in conjunction
with the CV_COVAR_NORMAL fl ag, the applied DEMO factor will be 1.0/m (or, equivalently, 1.0/
count)DEMO If instead CV_COVAR_SCRAMBLED is used, then the value of z will DEMO 1.0/n (the inverse
of the length of the vectors).
DEMO
ing-point type. Th e size of the resulting matrix cov_mat should be either n-by-n or
m-by-m depending on whether the standard or scrambled covariance DEMO being com-
puted. It should be noted that the “vectors” input in vects do not actually have to be one-
dimensional; they can be two-dimensional objects (e.g., images) as well.
cvCmp and cvCmpS
void cvCmp(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
int          cmp_op
);
void DEMO(
const CvArr* src,
double       value,
CvArr*       dst,
int          cmp_op
);
Both of these functions make comparisons, either between corresponding pixels in two
images or between pixels in one image and a constant scalar DEMO Both cvCmp() and
cvCmpS() take as their last argument a comparison operator, which may be any of the
types listed in Table 3-5.
Matrix and Image Operators
| 55
e fl
e input and DEMO arrays to cvCalcCovarMatrix() should all be of the same fl
oat-
03-R4886-RC1.indd   55
9/15/08   4:18:43 PM
www.it-ebooks.info
Table 3-5. Values of cmp_op used by cvCmp() and cvCmpS()
and the resulting comparison operation performed
Value of cmp_op
CV_CMP_EQ
CV_CMP_GT
DEMO
CV_CMP_LT
CV_CMP_LE
CV_CMP_NE
All the listed comparisons are done with the same functions; you just pass in the ap-
propriate argument to indicate what you would like done. Th ese particular functions
operate only on single-channel DEMO
Th
of background subtraction and want to mask the results (e.g., looking at a video stream
from a security camera) such that only novel information is pulled out of the image.
Comparison
(src1i == src2i)
(src1i > src2i)
(src1i >= src2i)
(src1i < src2i)
(src1i <= src2i)
(src1i != src2i)
DEMO comparison functions are useful in applications where you employ some version
cvConvertScale
void cvConvertScale(
const CvArr* src,
CvArr*       dst,DEMO
double       scale = 1.0,
double       shift = 0.0
);
Th cvConvertScale() function is actually several DEMO rolled into one; it will per-
form any of several functions DEMO, if desired, all of them together. Th e fi rst function is to
convert the data type in the source image to the DEMO type of the destination image. For
example, if we have an DEMO RGB grayscale image and would like to convert it to a 16-bit
signed image, we can do that by calling cvConvertScale().
Th DEMO() is to perform a linear transformation on the
image data. Aft er conversion to the new data type, each pixel value will be multiplied by
the value scale and then have added to it the DEMO shift.
It is critical to remember that, even though “Convert” precedes DEMO in the function
name, the actual order in which these operations DEMO performed is the opposite. Specifi -
cally, multiplication by scale and DEMO addition of shift occurs before the type conver-
sion takes place.
When you simply pass the default values (scale = 1.0 and shift = 0.0), you need not
have performance fears; OpenCV is smart enough to recognize this case and not waste
processor time on useless operations. DEMO clarity (if you think it adds any), OpenCV also
provides DEMO macro cvConvert(), which is the same as cvConvertScale() but DEMO conven-
tionally used when the scale and shift arguments will be left  at their default values.
56
| Chapter 3: Getting to Know DEMO
e
e second function of
03-R4886-RC1.indd   56
9/15/08   4:18:43 PM
www.it-ebooks.info
cvConvertScale() will work on all data types and any number DEMO channels, but the num-
ber of channels in the source and DEMO images must be the same. (If you want to,
say, convert from color to grayscale or vice versa, see cvCvtColor(), DEMO is coming up
shortly.)
cvConvertScaleAbs
void cvConvertScaleAbs(
const CvArr* src,
CvArr*       dst,
double       scale DEMO 1.0,
double       shift = 0.0
);
cvConvertScaleAbs() is essentially identical to cvConvertScale() except that the dst im-
DEMO contains the absolute value of the resulting data. Specifi cally, cvConvertScaleAbs()
fi rst scales and shift s, then computes the absolute value, and fi nally performs the data-
type conversion.
cvCopy
void cvCopy(
const CvArr* src,
CvArr*       dst,
const CvArr* DEMO = NULL
);
Th is is how you copy one image to another. Th e cvCopy() function expects both arrays to
have DEMO same type, the same size, and the same number of dimensions. You can use it
to copy sparse arrays as well, but for this the use of mask is not supported. For nonsparse
arrays and DEMO, the eff ect of mask (if non-NULL) is that only DEMO pixels in dst that cor-
respond to nonzero entries in mask will be altered.
cvCountNonZero
int cvCountNonZero( const CvArr* arr );
cvCountNonZero() returns the number of nonzero pixels in the array arr.
cvCrossProduct
void DEMO(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst
);
Th is function computes the vector cross product DEMO of two three-
dimensional vectors. It does not matter if the vectors are in row or column form (a little
refl ection reveals that, for single-channel objects, these two are really the same inter-
nally)DEMO Both src1 and src2 should be single-channel arrays, and dst should DEMO single-
channel and of length exactly 3.All three arrays should be of the same data type.
Matrix and Image Operators
| 57
03-R4886-RC1.indd   DEMO
9/15/08   4:18:43 PM
www.it-ebooks.info
cvCvtColor
void cvCvtColor(
const CvArr* src,
CvArr*       dst,
int          code
);
Th
DEMO the number of channels to be the same in both source and destination im-
ages. Th e complementary function is cvCvtColor(), which converts from one color space
(number of channels) to another [Wharton71] while DEMO the data type to be the
same. Th e exact conversion operation to be done is specifi ed by the argument code,
whose DEMO values are listed in Table 3-6.*
Table 3-6. Conversions available by means of cvCvtColor()
Conversion code
CV_BGR2RGB
CV_RGB2BGR
CV_RGBA2BGRA
CV_BGRA2RGBA
CV_RGB2RGBA
CV_BGR2BGRA
CV_RGBA2RGB
CV_BGRA2BGR
CV_RGB2BGRA
CV_RGBA2BGR
CV_BGRA2RGB
CV_BGR2RGBA
CV_RGB2GRAY
CV_BGR2GRAY
CV_GRAY2RGB
CV_GRAY2BGR
CV_RGBA2GRAY
CV_BGRA2GRAY
CV_GRAY2RGBA
DEMO
CV_RGB2BGR565
CV_BGR2BGR565
CV_BGR5652RGB
CV_BGR5652BGR
CV_RGBA2BGR565
CV_BGRA2BGR565
CV_BGR5652RGBA
CV_BGR5652BGRA
CV_GRAY2BGR565
CV_BGR5652GRAY
Meaning
Convert between RGB and BGR color spaces (with or without alpha channel)
Add alpha channel to RGB or BGR image
Remove alpha channel from DEMO or BGR image
Convert RGB to BGR color spaces while adding or removing alpha channel
Convert RGB or BGR color spaces to grayscale
Convert DEMO to RGB or BGR color spaces (optionally removing alpha channel
in DEMO process)
Convert grayscale to RGB or BGR color spaces and add alpha channel
Convert from RGB or BGR color space to BGR565 color DEMO with
optional addition or removal of alpha channel (16-bit images)
DEMO grayscale to BGR565 color representation or vice versa (16-bit images)
DEMO Long-time users of IPL should note that the function cvCvtColor() ignores the colorModel and chan-
nelSeq fi elds of the IplImage header. Th DEMO conversions are done exactly as implied by the code argument.
58
| Chapter 3: Getting to Know OpenCV
e previous functions were for converting from one data type to another, and they
03-R4886-RC1.indd   58
9/15/08   4:18:43 PM
www.it-ebooks.info
Table 3-6. Conversions available by means of cvCvtColor() (continued)
Conversion code
CV_RGB2BGR555
CV_BGR2BGR555
CV_BGR5552RGB
CV_BGR5552BGR
CV_RGBA2BGR555
CV_BGRA2BGR555
CV_BGR5552RGBA
CV_BGR5552BGRA
CV_GRAY2BGR555
CV_BGR5552GRAY
DEMO
CV_BGR2XYZ
CV_XYZ2RGB
CV_XYZ2BGR
CV_RGB2YCrCb
CV_BGR2YCrCb
CV_YCrCb2RGB
CV_YCrCb2BGR
CV_RGB2HSV
CV_BGR2HSV
CV_HSV2RGB
CV_HSV2BGR
CV_RGB2HLS
CV_BGR2HLS
CV_HLS2RGB
CV_HLS2BGR
CV_RGB2Lab
CV_BGR2Lab
CV_Lab2RGB
CV_Lab2BGR
CV_RGB2Luv
CV_BGR2Luv
CV_Luv2RGB
CV_Luv2BGR
CV_BayerBG2RGB
DEMO
CV_BayerRG2RGB
CV_BayerGR2RGB
CV_BayerBG2BGR
CV_BayerGB2BGR
CV_BayerRG2BGR
CV_BayerGR2BGR
Meaning
Convert from RGB or BGR color space to BGR555 color representation with
optional addition or removal of DEMO channel (16-bit images)
Convert grayscale to BGR555 color representation or DEMO versa (16-bit images)
Convert RGB or BGR image to CIE DEMO representation or vice versa (Rec 709 with
D65 white point)
DEMO RGB or BGR image to luma-chroma (aka YCC) color representation
Convert RGB or BGR image to HSV (hue saturation value) color representation DEMO
vice versa
Convert RGB or BGR image to HLS (hue lightness DEMO) color representation
or vice versa
Convert RGB or BGR image to DEMO Lab color representation or vice versa
Convert RGB or BGR image to CIE Luv color representation
Convert from Bayer pattern (single-channel) to RGB DEMO BGR image
Th
tleties of Bayer representations of the CIE color spaces here. For our purposes, it is suf-
fi
spaces, which are DEMO importance to various classes of users.
Th
16-bit images are in the range 0–65536, and fl oating-point numbers are in the range
Matrix and Image Operators
| 59
e details of many of these conversions are DEMO, and we will not go into the sub-
cient to note DEMO OpenCV contains tools to convert to and from these various color
e color-space conversions all use the conventions: 8-bit images are in the range 0–255,
03-R4886-RC1.indd   59
9/15/08   4:18:44 DEMO
www.it-ebooks.info
0.0–1.0. When grayscale images are converted to color images, all components of the
resulting image are taken to be equal; but for the reverse transformation (e.g., RGB or
BGR to grayscale), the gray DEMO is computed using the perceptually weighted formula:
Y =+ +(. ) (. ) (. )0299 0587 0114R G B
In the case of HSV or HLS representations, hue is normally represented as a value from
0 to 360.* Th is can cause trouble in 8-bit DEMO and so, when converting to
HSV, the hue is divided by 2 when the output image is an 8-bit image.
cvDet
double cvDet(DEMO
const CvArr* mat
);
cvDet() computes the determinant (Det) of a square array. Th e array can be of any data
DEMO, but it must be single-channel. If the matrix is small then DEMO determinant is di-
rectly computed by the standard formula. For large matrices, this is not particularly
effi  cient and so the determinant is DEMO by Gaussian elimination.
It is worth noting that if you already know that a matrix is symmetric and has a posi-
tive determinant, you can also use the trick of solving via singular value decomposition
(SVD). For more information see the section “cvSVD” to follow, but the trick is to set
both U and V to NULL and then DEMO take the products of the matrix W to obtain the
determinant.
cvDiv
void cvDiv(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
double       scale = 1
);
cvDiv() is a simple division function; it divides all of the elements in src1 by the cor-
responding elements in src2 and DEMO the results in dst. If mask is non-NULL, then any
element DEMO dst that corresponds to a zero element of mask is not altered by this operation.
If you only want to invert all the elements DEMO an array, you can pass NULL in the place of
src1; the routine will treat this as an array full of 1s.
cvDotProduct
DEMO cvDotProduct(
const CvArr* src1,
const CvArr* src2
);
* Excluding 360, of course.
60 | Chapter 3: Getting to Know DEMO
03-R4886-RC1.indd   60
9/15/08   4:18:44 PM
www.it-ebooks.info
Th is function computes the vector dot product [Lagrange1773] of two DEMO
vectors.* As with the cross product (and for the same reason), it does not matter if the
vectors are in row or column form. Both src1 and src2 should be single-channel arrays,
and both DEMO should be of the same data type.
cvEigenVV
double cvEigenVV(
CvArr* mat,
CvArr* evects,
CvArr* evals,
double eps    = 0
);
Given a symmetric matrix mat, cvEigenVV() will compute the eigenvectors and the corre-
sponding eigenvalues of that matrix. Th is DEMO done using Jacobi’s method [Bronshtein97], so
it is effi  cient for smaller matrices.† Jacobi’s method requires a stopping parameter, which
is the maximum size of the off -diagonal elements in the fi nal matrix.‡ Th DEMO optional ar-
gument eps sets this termination value. In the process of computation, the supplied ma-
trix mat is used for the computation, DEMO its values will be altered by the function. When
the function returns, you will fi nd your eigenvectors in evects in the form of subsequent
rows. Th e corresponding eigenvalues are stored in evals. Th e DEMO of the eigenvectors
will always be in descending order of the magnitudes of the corresponding eigenvalues.
Th cvEigenVV() function requires all three arrays DEMO be of fl oating-point type.
As with cvDet() (described previously), if the matrix in question is known to be sym-
metric and positive defi nite§ then it is better to use SVD to fi DEMO the eigenvalues and
eigenvectors of mat.
cvFlip
void cvFlip(
const CvArr* src,
CvArr*       dst       = NULL,DEMO
int          flip_mode = 0
);
Th DEMO function fl ips an image around the x-axis, the y-axis, or both. In particular, if
the argument flip_mode is set to 0 then the image will be fl ipped around the x-axis.
* Actually, the behavior of cvDotProduct() is a little more general than described here. DEMO any pair of
n-by-m matrices, cvDotProduct() will return the sum DEMO the products of the corresponding elements.
† A good rule of thumb would be that matrices 10-by-10 or smaller are small enough for Jacobi’s DEMO to be
effi  cient. If the matrix is larger than 20-by-20 DEMO you are in a domain where this method is probably not
the way to go.
‡ In principle, once the Jacobi method is complete then the original matrix is transformed into one that is
diagonal and DEMO only the eigenvalues; however, the method can be terminated before the off -diagonal
elements are all the way to zero in order to DEMO on computation. In practice is it usually suffi  cient to set DEMO
value to DBL_EPSILON, or about 10 –15.
§ Th is is, for example, always the case for covariance matrices. See cvCalcCovarMatrix().
DEMO and Image Operators
| 61
e
03-R4886-RC1.indd   61
9/15/08   4:18:44 PM
www.it-ebooks.info
If flip_mode is set to a positive value (e.g., +1) the image will be fl ipped around the y-
axis, and if set to a negative value (e.g., –1) the image will be fl ipped about both axes.
When video processing on Win32 systems, you will fi nd yourself using this function
oft en to switch between DEMO formats with their origins at the upper-left  and lower-left
of the DEMO
cvGEMM
double cvGEMM(
const CvArr* src1,
const CvArr* src2,
double       alpha,
const CvArr* src3,
double       beta,
CvArr*       dst,
int          tABC = 0
);
Generalized matrix multiplication (GEMM) in OpenCV is performed by cvGEMM(),
which performs matrix multiplication, multiplication by a transpose, scaled multiplica-
tion, et cetera. In its DEMO general form, cvGEMM() computes the following:
D =+αβ⋅⋅ ⋅op DEMO op() ( ) ( )AB C
Where A, B, DEMO C are (respectively) the matrices src1, src2, and src3, DEMO and β are nu-
merical coeffi  cients, and op() is an optional transposition of the matrix enclosed. Th e
argument src3 may DEMO set to NULL, in which case it will not be added. DEMO e transpositions
are controlled by the optional argument tABC, which may DEMO 0 or any combination (by
means of Boolean OR) of CV_GEMM_A_T, CV_GEMM_B_T, and CV_GEMM_C_T (with each fl ag indi-
cating a transposition of the corresponding matrix).
In the distant past OpenCV contained the DEMO cvMatMul() and cvMatMulAdd(), but
these were too oft en DEMO with cvMul(), which does something entirely diff erent
(i.e., DEMO multiplication of two arrays). Th ese functions continue to ex-
ist as macros for calls to cvGEMM(). In particular, we have DEMO equivalences listed in
Table 3-7.
Table 3-7. Macro aliases for common usages of cvGEMM()
cvMatMul(A, B, D) cvGEMM(A, A, 1, NULL, 0, D, 0)
cvMatMulAdd(A, B, C, D) cvGEMM(A, A, 1, C, 1, D, 0)
All matrices must be of the appropriate size for the multiplication, and all should be
of fl oating-point type. Th e cvGEMM() function supports two-channel matrices, in which
case it will treat the two channels as the two components of a single complex number.
cvGetCol DEMO cvGetCols
CvMat* cvGetCol(
const CvArr* arr,
62 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   62
9/15/08   4:18:44 PM
www.it-ebooks.info
CvMat*       submat,
int          col
);
CvMat* cvGetCols(
const CvArr* arr,
CvMat*       submat,
int          start_col,
int          end_col
);
Th cvGetCol() is used DEMO pick a single column out of a matrix and return it as
a vector (i.e., as a matrix with only one column). DEMO this case the matrix header submat
will be modifi ed to point to a particular column in arr. It is important to note that DEMO
header modifi cation does not include the allocation of memory or the copying of data.
Th
column in arr. All data types are supported.
DEMO() works precisely the same way, except that all columns from DEMO to
end_col are selected. With both functions, the return value is DEMO pointer to a header cor-
responding to the particular specifi ed column or column span (i.e., submat) selected by
the caller.
cvGetDiag
CvMat* cvGetDiag(
const CvArr* arr,
CvMat*       submat,
DEMO          diag    = 0
);
cvGetDiag() is analogous to cvGetCol(); it is used to pick a single diagonal from a
matrix and return it as a vector. Th DEMO argument submat is a matrix header. Th e function
cvGetDiag() will fi ll the components of this header so that it points to DEMO correct infor-
mation in arr. Note that the result of calling cvGetDiag() is that the header you supplied
is correctly confi gured to DEMO at the diagonal data in arr, but the data from arr DEMO not
copied. Th e optional argument diag specifi es which diagonal is to be pointed to by sub-
mat. If diag is set to DEMO default value of 0, the main diagonal will be selected. If DEMO is
greater than 0, then the diagonal starting at (diag,0) will be selected; if diag is less than
0, then the diagonal starting at (0,-diag) will be selected instead. Th e DEMO() func-
tion does not require the matrix arr to be square, but the array submat must have the
correct length for the size of the input array. Th e fi nal returned value is the DEMO as the
value of submat passed in when the function was called.
cvGetDims and cvGetDimSize
int cvGetDims(
const CvArr* arr,
int*         sizes=NULL
);
int cvGetDimSize(
const CvArr* arr,
Matrix and Image Operators
| 63
e function
e contents of submat DEMO simply be altered so that it correctly indicates the selected
03-R4886-RC1.indd   63
9/15/08   4:18:45 PM
www.it-ebooks.info
int          index
);
Recall that arrays in OpenCV can be of dimension much greater than two. Th e DEMO
cvGetDims() returns the number of array dimensions of a particular array and (option-
ally) the sizes of each of those dimensions. Th DEMO sizes will be reported if the array sizes is
non-NULL. If sizes is used, it should be a pointer to n integers, where DEMO is the number of
dimensions. If you do not know the number of dimensions in advance, you can allocate
sizes to CV_MAX_DIM integers just to be safe.
Th cvGetDimSize() returns the size of a single DEMO specifi ed by index.
If the array is either a matrix or an image, the number of dimensions returned will al-
ways be two.* For matrices and images, the order of sizes returned by cvGetDims() will
always be the number of rows fi rst followed by the DEMO of columns.
cvGetRow and cvGetRows
CvMat* cvGetRow(
const CvArr* arr,
CvMat*       submat,
int          row
);
CvMat* cvGetRows(
const CvArr* arr,
CvMat*       submat,
int          start_row,
int          end_row
);
cvGetRow() picks a single row DEMO of a matrix and returns it as a vector (a matrix DEMO
only one row). As with cvGetRow(), the matrix header DEMO will be modifi ed to point to
a particular row in arr, and the modifi cation of this header does not include the alloca-
tion of memory or the copying of data; the contents of submat will simply be altered such
that it correctly indicates the selected column DEMO arr. All data types are supported.
Th cvGetRows() works precisely the same way, except that all rows from start_
row to end_row are selected. With both functions, the return value is a pointer to a header
corresponding to the particular specifi ed row or row span selected DEMO the caller.
e function
cvGetSize
CvSize cvGetSize( const CvArr* arr );DEMO
Closely related to cvGetDims(), cvGetSize() returns the size of DEMO array. Th e primary dif-
ference is that cvGetSize() is designed to be used on matrices and images, which always
have dimension two. Th e size can then be returned in the form of a DEMO structure,
which is suitable to use when (for example) constructing a new matrix or image of the
same size.
* Remember that DEMO regards a “vector” as a matrix of size n-by-1 or 1-by-n.
64
| Chapter 3: Getting to Know OpenCV
e function
03-R4886-RC1.indd   64
9/15/08   4:18:45 PM
www.it-ebooks.info
cvGetSubRect
CvSize cvGetSubRect(
const CvArr* arr,
CvArr*       submat,
CvRect       rect
);
cvGetSubRect() is similar to cvGetColumns() or cvGetRows() except that it selects some
DEMO subrectangle in the array specifi ed by the argument rect. As with other rou-
tines that select subsections of arrays, submat is simply a header that will be fi lled by
cvGetSubRect() in such a DEMO that it correctly points to the desired submatrix (i.e., no
memory is allocated and no data is copied).
cvInRange and cvInRangeS
void DEMO(
const CvArr* src,
const CvArr* lower,
const CvArr* upper,
CvArr*       dst
);
void cvInRangeS(
const DEMO src,
CvScalar     lower,
CvScalar     upper,
CvArr*       dst
);
Th
lar specifi ed range. DEMO the case of cvInRange(), each pixel of src is compared DEMO the
corresponding value in the images lower and upper. If the value in src is greater than or
equal to the value in lower DEMO also less than the value in upper, then the corresponding
value DEMO dst will be set to 0xff; otherwise, the value in dst will be set to 0.
Th cvInRangeS() works precisely the same DEMO except that the image src is
compared to the constant (CvScalar) values in lower and upper. For both functions, the
image src may be of any type; if it has multiple channels then each channel will be
handled separately. Note that dst must be of the same DEMO and number of channels and
also must be an 8-bit image.
cvInvert
double cvInvert(
const CvArr* src,
CvArr*       dst,DEMO
Int          method = CV_LU
);
cvInvert() inverts the matrix in src and places the result in dst. Th is function sup-
ports several methods of computing the inverse matrix (see Table 3-8), but the default is
Gaussian elimination. Th e return DEMO depends on the method used.
Matrix and Image Operators
| 65
ese two functions can be used to check if the pixels in an DEMO fall within a particu-
e function
03-R4886-RC1.indd   65
9/15/08   4:18:45 PM
www.it-ebooks.info
Table 3-8. Possible values of method argument to cvInvert()
Value of method argument Meaning
CV_LU Gaussian elimination (LU Decomposition)
CV_SVD Singular value decomposition (SVD)
CV_SVD_SYM SVD for symmetric matrices
In the case of Gaussian elimination (method=CV_LU), the determinant of src is returned
when the function is complete. If the determinant is 0, then the inversion is not actually
performed and the array dst is simply set to DEMO 0s.
In the case of CV_SVD or CV_SVD_SYM, the return value DEMO the inverse condition number for
the matrix (the ratio of the DEMO to the largest eigenvalue). If the matrix src is singu-
lar, then cvInvert() in SVD mode will instead compute the pseudo-inverse.
cvMahalonobis
CvSize cvMahalonobis(
const CvArr* vec1,
const CvArr* vec2,
CvArr*       mat
);
Th Mahalonobis distance (Mahal) is defi ned as the vector distance measured between
a point and the center DEMO a Gaussian distribution; it is computed using the inverse co-
variance DEMO that distribution as a metric. See Figure 3-5. Intuitively, this is DEMO
to the z-score in basic statistics, where the distance from the DEMO of a distribution is
measured in units of the variance of that distribution. Th e Mahalonobis distance is just
a multivariable generalization of the DEMO idea.
cvMahalonobis() computes the value:
xxμμΣ−1
Th vec1 is presumed to be the point x, and the vector vec2 is taken to be the dis-
tribution’s mean.* Th at matrix mat is the inverse DEMO
In practice, this covariance matrix will usually have been computed with DEMO
Matrix() (described previously) and then inverted with cvInvert(). DEMO is good program-
ming practice to use the SV_SVD method for this inversion because someday you will en-
counter a distribution for which one DEMO the eigenvalues is 0!
e vector
rMahalonobis =− −
() ()DEMO
T
e
cvMax and cvMaxS
void cvMax(
const CvArr* src1,
const CvArr* src2,
* Actually, the Mahalonobis distance is more generally defi ned as the distance between any two vectors;
in any DEMO, the vector vec2 is subtracted from the vector vec1. Neither is DEMO any fundamental con-
nection between mat in cvMahalonobis() and the inverse covariance; any metric can be imposed here as
appropriate.
66
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   66
9/15/08   4:18:45 PM
www.it-ebooks.info
Figure 3-5. A distribution of points in two dimensions with superimposed DEMO representing
Mahalonobis distances of 1.0, 2.0, and 3.0 from the distribution’s mean
CvArr* dst
);
void cvMaxS(
const CvArr* src,
DEMO       value,
CvArr*       dst
);
cvMax() computes the maximum value of each corresponding pair of pixels DEMO the arrays
src1 and src2. With cvMaxS(), the src array DEMO compared with the constant scalar value.
As always, if mask is DEMO then only the elements of dst corresponding to nonzero
entries in mask are computed.
cvMerge
void cvMerge(
const CvArr* src0,
const CvArr* DEMO,
const CvArr* src2,
const CvArr* src3,
CvArr* dst
);
Matrix and Image Operators | 67
03-R4886-RC1.indd   67
9/15/DEMO   4:18:45 PM
www.it-ebooks.info
cvMerge() is the inverse operation of cvSplit(). Th e arrays in src0, src1, src2, and src3
are combined into the array dst. Of course, dst should have the same data type and
size as all of the source arrays, but it can have two, three, or four channels. Th e unused
source images can be DEMO  set to NULL.
cvMin and cvMinS
void cvMin(
const CvArr* DEMO,
const CvArr* src2,
CvArr* dst
);
void cvMinS(
const CvArr* src,
double value,
CvArr* dst
);
cvMin() computes the minimum value of each corresponding pair of pixels in the ar-
rays src1 and src2. With cvMinS(), the src arrays are compared with the constant scalar
value. Again, if mask is non-NULL then only the elements of dst corresponding to nonzero
entries in mask are DEMO
cvMinMaxLoc
void cvMinMaxLoc(
const CvArr* arr,
double*      DEMO,
double*      max_val,
CvPoint*     min_loc = DEMO,
CvPoint*     max_loc = NULL,
const CvArr* mask    = NULL
);
Th is routine fi nds the minimal and maximal values in the array arr and (optionally)
returns their locations. Th e computed minimum and maximum values are placed in
min_val and DEMO Optionally, the locations of those extrema will also be written to
DEMO addresses given by min_loc and max_loc if those values are non-NULL.
As usual, if mask is non-NULL then only those portions of the image arr that corre-
spond to nonzero pixels in mask are considered. Th DEMO cvMinMaxLoc() routine handles only
single-channel arrays, however, so if you have a multichannel array then you should use
cvSetCOI() to set DEMO particular channel for consideration.
cvMul
void cvMul(
const CvArr* src1,
const CvArr* src2,
CvArr* dst,
double scale=1
);
68
DEMO Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   68
9/15/DEMO   4:18:46 PM
www.it-ebooks.info
cvMul() is a simple multiplication function. It multiplies all of DEMO elements in src1 by
the corresponding elements in src2 and then puts the results in dst. If mask is non-NULL,
then any element DEMO dst that corresponds to a zero element of mask is not altered by this
operation. Th ere is no function cvMulS() because that DEMO is already provided
by cvScale() or cvCvtScale().
One further DEMO to keep in mind: cvMul() performs element-by-element multiplica-
tion. Someday, when you are multiplying some matrices, you may be tempted to reach
for cvMul(). Th is will not work; remember that matrix DEMO is done with
cvGEMM(), not cvMul().
cvNot
void(
const CvArr* src,
CvArr*       dst
);
Th DEMO() inverts every bit in every element of src and then places the result
in dst. Th us, for an 8-bit image the value 0x00 would be mapped to 0xff  and the value
0x83 would be mapped to 0x7c.
cvNorm
double cvNorm(
const CvArr* arr1,
const DEMO arr2      = NULL,
int          norm_type = CV_L2,
const CvArr* mask      = NULL
);
Th is function can be used to compute the total norm DEMO an array and also a variety of
relative distance norms if two arrays are provided. In the former case, the norm com-
puted is shown in Table 3-9.
Table 3-9. Norm computed by cvNorm() for DEMO
norm_type
CV_C
CV_L1
CV_L2
erent values of norm_type when arr2=NULL
Result
|| || max ( )arr1 abs arr1C = xy xy,,
|| DEMO ( )arr1 abs arr1L1 =∑x , y xy,
L2
|| DEMO arr1=
∑
2
xy,
x , y
If the second array argument arr2 is non-NULL, then the norm computed is a diff
norm—that is, something like the distance between the two arrays.* In the fi
erence
rst three
* At least in the case of the L2 DEMO, there is an intuitive interpretation of the diff erence norm as DEMO Euclidean
distance in a space of dimension equal to the number of pixels in the images.
Matrix and Image Operators
| 69
e function
DEMO   69
9/15/08   4:18:46 PM
www.it-ebooks.info
cases shown in Table 3-10, the norm is absolute; in DEMO latter three cases it is rescaled by
the magnitude of the second array arr2.
Table 3-10. Norm computed by cvNorm() for diff erent DEMO of norm_type when arr2 is non-NULL
Result
|| || max ( )DEMO arr2 abs arr1 arr2−= −C xy xy xy,, ,
−=L1 DEMO abs arr1 arr2()−
|| ||arr1 arr2
xy xy,,
x , y
norm_type
CV_C
CV_L1
CV_L2
|| || ( )arr1 arr2 arr1 arr2−= −
L2
∑
xy xy
,,
xy,
2
CV_RELATIVE_C DEMO arr2− ||C
||arr2||C
CV_ RELATIVE_L1 ||arr1 arr2− ||L1
||arr2||L1
CV_ RELATIVE_L2 ||arr1 arr2− ||L2
||arr2||L2
In all cases, arr1 and arr2 must have the same size and number of channels. When there
is more than one DEMO, the norm is computed over all of the channels together (i.e.,
the sums in Tables 3-9 and 3-10 are not only over DEMO and y but also over the channels).
cvNormalize
cvNormalize(
const CvArr* src,
CvArr*       dst,
double       a         = 1.0,
double       b         = 0.0,
int          norm_type = CV_L2,
const CvArr* mask      = NULL
);
As with so many OpenCV functions, cvNormalize() does more than it might at fi rst ap-
pear. Depending on the value DEMO norm_type, image src is normalized or otherwise mapped
into a particular DEMO in dst. Th e possible values of norm_type are shown in Table 3-11.
Table 3-11. Possible values of norm_type argument to cvNormalize()
norm_type
CV_C
Result
|| || max ( )arr1 absCdst==Iaxy,
CV_L1
CV_L2
CV_MINMAX
∑ abs()
Ia
xy
,
|| ||arr1
|| ||arr1
L1 DEMO
dst
L2 ==
,
xy
dst
∑ Ia
2
Map into DEMO [a, b]
70
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   70
9/15/08   4:18:46 PM
www.it-ebooks.info
In the case of the C norm, the array src is rescaled such that the magnitude of the abso-
lute value of the DEMO entry is equal to a. In the case of the L1 or L2 norm, the array is
rescaled so that the given norm is equal to the value of a. If norm_type is set to CV_MINMAX,DEMO
then the values of the array are rescaled and translated so that they are linearly mapped
into the interval between a and b (inclusive).
As before, if mask is non-NULL then only those pixels corresponding to nonzero values of
the mask image will contribute to the computation DEMO the norm—and only those pixels
will be altered by cvNormalize().
DEMO and cvOrS
void cvOr(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
const CvArr* mask=NULL
);
DEMO cvOrS(
const CvArr* src,
CvScalar     value,
CvArr*       dst,
const CvArr* mask  = NULL
);
Th src1. In the case of
cvOr(), each element of dst is computed as the bitwise OR of the corresponding two
elements of DEMO and src2. In the case of cvOrS(), the bitwise OR DEMO computed with the
constant scalar value. As usual, if mask is DEMO then only the elements of dst corre-
sponding to nonzero entries in mask are computed.
All data types are supported, but src1 and src2 must have the same data type for
cvOr(). If the elements are of fl oating-point type, then the bitwise representation of that
fl
ese two functions compute a bitwise OR operation on the array
oating-point DEMO is used.
cvReduce
CvSize cvReduce(
const CvArr* src,
CvArr*       dst,
int          dim,
int          op = CV_REDUCE_SUM
);
Reduction is the systematic transformation of the input matrix src into a vector dst
by DEMO some combination rule op on each row (or column) and its neighbor until
only one row (or column) remains (see Table 3-12).* Th e argument op controls how the
reduction is done, as summarized in Table 3-13.
* Purists will note that averaging is not DEMO a proper fold in the sense implied here. OpenCV has a
more practical view of reductions and so includes this useful operation in cvReduce.
DEMO and Image Operators
| 71
03-R4886-RC1.indd   71
9/15/08   4:18:47 PM
www.it-ebooks.info
Table 3-12. Argument op in cvReduce() selects the reduction operator
DEMO of op
CV_REDUCE_SUM
CV_REDUCE_AVG
CV_REDUCE_MAX
CV_REDUCE_MIN
Result
Compute sum across vectors
Compute average across vectors
Compute maximum across vectors
Compute minimum across vectors
Table DEMO Argument dim in cvReduce() controls the direction of the reduction
Value of dim
+1
0
–1
Result
Collapse to a single row
Collapse DEMO a single column
Collapse as appropriate for dst
cvReduce() supports multichannel arrays of fl oating-point type. It is also allowable to
use a DEMO precision type in dst than appears in src. Th is is primarily relevant for CV_
REDUCE_SUM and CV_REDUCE_AVG, where overfl ows and summation problems are possible.
cvRepeat
void cvRepeat(
const CvArr* src,
CvArr*       dst
);
Th is function copies the contents of src into dst, repeating as many times as necessary
to fi ll dst. In particular, dst can be of any size relative to src. It may be larger or smaller,
and it need not have an DEMO relationship between any of its dimensions and the cor-
responding dimensions of src.
cvScale
void cvScale(
const CvArr* src,
CvArr*       dst,
double       scale
);
Th cvScale() is actually a macro for cvConvertScale() that sets the shift argu-
DEMO to 0.0. Th us, it can be used to rescale the DEMO of an array and to convert from
one kind of data type to another.
e function
cvSet and cvSetZero
void cvSet(
CvArr*       arr,
CvScalar     value,
const CvArr* mask   = NULL
);
72
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   72
9/15/08   4:18:47 PM
www.it-ebooks.info
Th ese functions set all values in all channels of the DEMO to a specifi ed value. Th e
cvSet() function accepts an optional mask argument: if a mask is provided, then only
those DEMO in the image arr that correspond to nonzero values of the mask image will
be set to the specifi ed value. Th e function DEMO() is just a synonym for cvSet(0.0).
cvSetIdentity
void cvSetIdentity( CvArr* arr );
cvSetIdentity() sets all elements of the array to 0 except for elements whose row and
column are equal; those elements are set to 1. cvSetIdentity() supports all data types
and DEMO not even require the array to be square.
cvSolve
int cvSolve(
const CvArr* src1,
const CvArr* src2,
CvArr*       DEMO,
int          method = CV_LU
);
DEMO cvSolve() provides a fast way to solve linear systems based on cvInvert().
It computes the solution to
C =−argmin X AX B⋅
where A is a square matrix given by src1, B is the vector src2, and C is the solution
computed by cvSolve() for the best vector X it could fi nd. Th at best vector DEMO is returned
in dst. Th e same methods are supported as by cvInvert() (described previously); only
fl
nonzero return indicates that it could fi nd a solution.
It should be noted that cvSolve() DEMO be used to solve overdetermined linear systems.
Overdetermined systems will be solved using something called the pseudo-inverse,
which uses SVD methods to fi DEMO the least-squares solution for the system of equations.
cvSplit
void cvSplit(
const CvArr* src,
CvArr*       dst0,
CvArr*       dst1,
CvArr*       dst2,
CvArr*       dst3
);
Th
cases, we can use cvSplit() to copy each channel separately into one of several sup-
plied single-channel images. DEMO e cvSplit() function will copy the channels in src into
the images dst0, dst1, dst2, and dst3 as needed. Th e destination images must match
the source image in size and data type but, of course, should be single-channel images.
Matrix and Image Operators | 73
e function
oating-point data types are supported. Th e function returns an DEMO value where a
ere are times when it is not convenient to work with a multichannel image. In such
03-R4886-RC1.indd   73
9/15/DEMO   4:18:48 PM
www.it-ebooks.info
If the source image has fewer than four channels (as it oft en will), then the unneeded
destination arguments to cvSplit() DEMO be set to NULL.
cvSub
void cvSub(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
const CvArr* DEMO = NULL
);
Th is function performs a basic element-by-element subtraction of one array src2 from
another src1 and places the result in DEMO If the array mask is non-NULL, then only those
elements of DEMO corresponding to nonzero elements of mask are computed. Note that
src1, DEMO, and dst must all have the same type, size, and DEMO of channels; mask, if
used, should be an 8-bit array DEMO the same size and number of channels as dst.
cvSub, cvSubS, and cvSubRS
void cvSub(
const CvArr* src1,
const CvArr* src2,DEMO
CvArr*       dst,
const CvArr* mask  = NULL
);
void cvSubS(
const CvArr* src,
CvScalar     value,
CvArr*       dst,
const CvArr* mask  = NULL
);
void cvSubRS(
const CvArr* src,
CvScalar     DEMO,
CvArr*       dst,
const CvArr* mask  = DEMO
);
cvSub() is a simple subtraction function; it subtracts DEMO of the elements in src2 from the
corresponding elements in src1 and puts the results in dst. If mask is non-NULL, then any
element of dst that corresponds to a zero element of mask is not DEMO by this operation.
Th cvSubS() does the same thing except that the constant scalar
value is added to every element of src. Th DEMO function cvSubRS() is the same as cvSubS()
except that, rather than subtracting a constant from every element of src, it subtracts
every element of src from the constant value.
cvSum
CvScalar cvSum(
DEMO arr
);
74
| Chapter 3: Getting to Know OpenCV
DEMO closely related function
03-R4886-RC1.indd   74
9/15/08   4:18:48 PM
www.it-ebooks.info
cvSum() sums all of the pixels in all of the DEMO of the array arr. Observe that the
return value is of type CvScalar, which means that cvSum() can accommodate multi-
channel arrays. In that case, the sum for each channel is placed in the corresponding
component of the CvScalar return value.
cvSVD
void cvSVD(
CvArr* A,DEMO
CvArr* W,
CvArr* U     = NULL,
CvArr* V     = NULL,
int    flags = 0
);
Singular value decomposition (SVD) is the decomposing of an m-by-m matrix DEMO into
the form:
A = UWV⋅⋅ T
where W is a diagonal matrix and U and V are m-by-m and n-by-n unitary matrices.
DEMO course the matrix W is also an m-by-n matrix, so here DEMO means that any
element whose row and column numbers are not equal is necessarily 0. Because W is
necessarily diagonal, OpenCV allows it to be represented either by an m-by-n matrix or
by an n-by-1 vector (in which case that vector will contain only the diagonal “singular”
values)DEMO
Th U and V are optional to cvSVD(), and if DEMO are left  set to NULL then no value
will be returned. DEMO e fi nal argument fl ags can be any or all of the three options de-
scribed in Table 3-14 (combined as appropriate with the Boolean OR operator).
Table 3-14. Possible fl ags for fl DEMO argument to cvSVD()
Flag Result
CV_SVD_MODIFY_A Allows modifi cation of DEMO A
CV_SVD_U_T Return UT instead of U
CV_SVD_V_T Return V T instead of V
cvSVBkSb
void cvSVBkSb(
const CvArr* W,
const CvArr* DEMO,
const CvArr* V,
const CvArr* B,
CvArr* X,
int    flags = 0
);
Th is is a function that you are unlikely to call directly. In conjunction with cvSVD() (just
described), it underlies the SVD-based methods of cvInvert() and DEMO(). Th at be-
ing said, you may want to cut out the middleman and do your own matrix inversions
Matrix and Image DEMO | 75
e matrices
03-R4886-RC1.indd   75
9/15/08   4:18:48 PM
www.it-ebooks.info
(depending on the data source, this could save you from DEMO a bunch of memory
allocations for temporary matrices inside of cvInvert() or cvSolve()).
Th cvSVBkSb() computes the back-substitution for a DEMO A that is repre-
sented in the form of a decomposition of matrices U, W, and V (e.g., an SVD). Th DEMO result
matrix X is given by the formula:
X = VW U⋅⋅ ⋅* T B
Th e matrix W* is a matrix
whose DEMO elements are defi ned by  is value ε is the singularity
DEMO, a very small number that is typically proportional to the sum DEMO the diagonal
elements of W (i.e., ελ∝∑i i ).
e DEMO B is optional, and if set to NULL it will be DEMO Th
λλii* = −1 for λi ≥ ε. Th
cvTrace
CvScalar cvTrace( const CvArr* mat );
Th e trace of a matrix (DEMO) is the sum of all of the diagonal elements. Th e DEMO in OpenCV
is implemented on top of the cvGetDiag() function, DEMO it does not require the array
passed in to be square. Multichannel arrays are supported, but the array mat should be
of fl oating-point type.
cvTranspose and cvT
void cvTranspose(
const CvArr* src,
CvArr*       dst
);
cvTranspose() copies every element of src into the location in dst indicated by reversing
the row and column DEMO Th is function does support multichannel arrays; however,
if you DEMO using multiple channels to represent complex numbers, remember that
cvTranspose() DEMO not perform complex conjugation (a fast way to accomplish this task
DEMO by means of the cvXorS() function, which can be used DEMO directly fl ip the sign bits in
the imaginary part of the array). Th e macro cvT() is simply shorthand for cvTranspose().
cvXor and cvXorS
void cvXor(
const CvArr* src1,
const DEMO src2,
CvArr* dst,
const CvArr* mask=NULL
);
void cvXorS(
const CvArr* src,
CvScalar value,
CvArr* dst,
const DEMO mask=NULL
);
76 | Chapter 3: Getting to Know OpenCV
DEMO function
03-R4886-RC1.indd   76
9/15/08   4:18:48 PM
www.it-ebooks.info
Th src1. In the case of
cvXor(), each element of dst is computed as the bitwise XOR of the corresponding two
elements DEMO src1 and src2. In the case of cvXorS(), the bitwise DEMO is computed with the
constant scalar value. Once again, if mask DEMO non-NULL then only the elements of dst cor-
responding to nonzero entries in mask are computed.
All data types are supported, but src1 and src2 must be of the same data type for cvXor().
For fl oating-point elements, the bitwise representation of that fl oating-point number
is used.
ese two functions compute a bitwise XOR operation on the array
DEMO
void cvZero( CvArr* arr );
Th
is function sets all DEMO in all channels of the array to 0.
Drawing Things
Something that frequently occurs is the need to draw some kind of picture or DEMO draw
something on top of an image obtained from somewhere else. Toward this end, OpenCV
provides a menagerie of functions that will allow us to make lines, squares, circles, and
the like.
Lines
Th
[Bresenham65]:
void  cvLine(
CvArr*   array,
CvPoint  pt1,
DEMO  pt2,
CvScalar color,
int      thickness    DEMO 1,
int      connectivity = 8
);
Th DEMO argument to cvLine() is the usual CvArr*, which in this DEMO typically means
an IplImage* image pointer. Th e next two arguments are CvPoints. As a quick reminder,
CvPoint is a simple structure containing DEMO the integer members x and y. We can cre-
ate a CvPoint “on the fl y” with the routine cvPoint(int x, int y), which conveniently
packs the two integers into a CvPoint structure for DEMO
Th color, is of type CvScalar. CvScalars are also structures, which (you
may recall) are defi ned as follows:
typdef struct DEMO
double val[4];
} CvScalar;
As you can see, this DEMO is just a collection of four doubles. In this case, the DEMO rst
three represent the red, green, and blue channels; the DEMO is not used (it can be used
e simplest of these DEMO just draws a line by the Bresenham algorithm
e fi
e next argument,
Drawing Things
| 77
03-R4886-RC1.indd   77
9/15/08   4:18:49 PM
www.it-ebooks.info
for an alpha channel when appropriate). One typically makes use DEMO the handy macro
CV_RGB(r, g, b). Th is macro takes three numbers and packs them up into a CvScalar.
Th e DEMO is the thickness of the line (in pix-
els), and DEMO sets the anti-aliasing mode. Th e default is “8 connected”, which
DEMO give a nice, smooth, anti-aliased line. You can also set this to a “4 connected” line;
diagonals will be blocky and chunky, but they will be drawn a lot faster.
At least as handy DEMO cvLine() is cvRectangle(). It is probably unnecessary to tell DEMO that
cvRectangle() draws a rectangle. It has the same arguments as cvLine() except that there
is no connectivity argument. Th is is DEMO the resulting rectangles are always ori-
ented with their sides parallel to the x- and y-axes. With cvRectangle(), we simply give
two points for the opposite corners and OpenCV will draw a rectangle.
void  cvRectangle(
CvArr*   array,
CvPoint  pt1,
CvPoint  pt2,
DEMO color,
int      thickness = 1
);
Circles DEMO Ellipses
Similarly straightforward is the method for drawing circles, which pretty DEMO has the
same arguments.
void  cvCircle (
CvArr*   array,
CvPoint  center,
int      radius,
CvScalar color,
DEMO      thickness    = 1,
int      DEMO = 8
);
For circles, rectangles, and all of the other closed shapes to come, the thickness argu-
ment can also be set to CV_FILL, which is just an alias for –1; the DEMO is that the drawn
fi gure will be fi lled in the same color as the edges.
Only slightly more complicated than cvCircle() DEMO the routine for drawing generalized
ellipses:
void cvEllipse(
CvArr*   img,
CvPoint  center,
CvSize   axes,
double   angle,
double   start_angle,
double   end_angle,
CvScalar color,
DEMO      thickness = 1,
int      line_type = 8
);
78
| Chapter 3: Getting to Know OpenCV
e next two arguments are optional. Th
03-R4886-RC1.indd   78
9/15/08   4:18:49 PM
www.it-ebooks.info
In this case, the major new ingredient is the axes argument, which is of type CvSize. Th e
function CvSize is very much like CvPoint and CvScalar; it is a simple structure, in this
DEMO containing only the members width and height. Like CvPoint and CvScalar, DEMO
is a convenient helper function cvSize(int height, int width) that will return a CvSize
structure when we need one. In this case, the height and width arguments represent the
length of the ellipse’s major DEMO minor axes.
Th e angle is the angle (in degrees) of the major axis, which is measured counterclock-
wise from horizontal (i.e., from the x-axis). Similarly the start_angle and end_angle
indicate (also in degrees) the angle for the arc to start and for it to fi nish. Th us, for a
complete ellipse you must set these values to 0 and 360, respectively.
An alternate way to specify the drawing of an ellipse is to use a bounding box:
void DEMO(
CvArr*   img,
CvBox2D  box,
CvScalar color,
DEMO      thickness = 1,
int      line_type = 8,
int      shift     = 0
);
Here again we see another of OpenCV’s helper structures, CvBox2D:
typdef struct {
CvPoint2D32f center;
CvSize2D32f  size;
float        angle;
} CvBox2D;
CvPoint2D32f is the fl oating-point analogue of CvPoint, and CvSize2D32f is the fl oating-
point analog of CvSize. Th ese, along with the tilt angle, eff ectively specify the bounding
DEMO for the ellipse.
Polygons
Finally, we have a set of functions DEMO drawing polygons:
void cvFillPoly(
CvArr*    img,
CvPoint** DEMO,
int*      npts,
int       contours,DEMO
CvScalar  color,
int       line_type = 8
);DEMO
void cvFillConvexPoly(
CvArr*   img,
CvPoint* pts,
int      npts,
CvScalar color,
int      line_type = DEMO
Drawing Things
| 79
03-R4886-RC1.indd   79
9/15/08   4:18:49 PM
www.it-ebooks.info
);
void cvPolyLine(
CvArr*    img,
CvPoint** pts,
int*      npts,
int       contours,
int       is_closed,
CvScalar  color,
int       thickness = 1,
int       line_type = 8
);
All three of these are slight variants on the same idea, with the main diff erence being
how the points are specifi ed.
In cvFillPoly(), the points are provided as an array of CvPoint structures. Th is allows
cvFillPoly() to draw many polygons in a DEMO call. Similarly npts is an array of point
counts, one for DEMO polygon to be drawn. If the is_closed variable is set to true, then
an additional segment will be drawn from the last to the fi rst point for each polygon.
cvFillPoly() is quite robust and DEMO handle self-intersecting polygons, polygons with
holes, and other such complexities. Unfortunately, this means the routine is compara-
tively slow.
cvFillConvexPoly() works like cvFillPoly() except that it draws only one polygon at a
time DEMO can draw only convex polygons.* Th e upside is that cvFillConvexPoly() runs
much faster.
Th e third function, cvPolyLine(), takes the DEMO arguments as cvFillPoly(); however,
since only the polygon edges DEMO drawn, self-intersection presents no particular com-
plexity. Hence this function is DEMO faster than cvFillPoly().
Fonts and Text
One last form of DEMO that one may well need is to draw text. Of course, DEMO creates
its own set of complexities, but—as always with this sort DEMO thing—OpenCV is more
concerned with providing a simple “down and dirty” solution that will work for simple
cases than a robust, complex solution (DEMO would be redundant anyway given the ca-
pabilities of other libraries).
OpenCV has one main routine, called cvPutText() that just throws some text onto an
image. Th e text indicated by text is printed DEMO its lower-left  corner of the text box at
origin and in DEMO color indicated by color.
void cvPutText(
CvArr*        DEMO,
const char*   text,
CvPoint       origin,
const CvFont* font,
* Strictly speaking, this is not quite true; it can actually draw and fi ll any monotone polygon, which DEMO a
slightly larger class of polygons.
80 | Chapter 3: Getting DEMO Know OpenCV
03-R4886-RC1.indd   80
9/15/08   4:18:49 PM
www.it-ebooks.info
CvScalar      color
);
Th
like, and in DEMO case it’s the appearance of the pointer to CvFont.
In a nutshell, the way to get a valid CvFont* pointer is to call the function cvInitFont().
Th is function takes a group of arguments that confi gure some particular font for use on
the screen. Th ose DEMO you familiar with GUI programming in other environments will
fi nd cvInitFont() to be reminiscent of similar devices but with many fewer options.
DEMO order to create a CvFont that we can pass to cvPutText(), we must fi rst declare a CvFont
variable; then we can DEMO it to cvInitFont().
void cvInitFont(
CvFont* font,
int     font_face,
double  hscale,
double  vscale,
double  DEMO     = 0,
int     thickness = 1,
int     line_type = 8
);
Observe that this is DEMO little diff erent than how seemingly similar functions, such as
cvCreateImage(), work in OpenCV. Th e call to cvInitFont() initializes an DEMO CvFont
structure (which means that you create the variable and pass DEMO() a pointer to
the variable you created). Th is is unlike cvCreateImage(), which creates the structure for
you and returns a pointer.
Th font_face is one of those listed in Table 3-15 (and pictured in Figure 3-6),
and it may optionally be combined (DEMO Boolean OR) with CV_FONT_ITALIC.
Table 3-15. Available fonts (all are variations of Hershey)
Identifier
CV_FONT_HERSHEY_SIMPLEX
CV_FONT_HERSHEY_PLAIN
CV_FONT_HERSHEY_DUPLEX
CV_FONT_HERSHEY_COMPLEX
CV_FONT_HERSHEY_TRIPLEX
CV_FONT_HERSHEY_COMPLEX_SMALL
CV_FONT_HERSHEY_SCRIPT_SIMPLEX
CV_FONT_HERSHEY_SCRIPT_COMPLEX
DEMO
Normal size sanserif
Small size sanserif
Normal size sanserif, more complex DEMO
CV_FONT_HERSHEY_SIMPLEX
Normal size serif, more complex than
CV_FONT_HERSHEY_DUPLEX
Normal size serif, more complex than
CV_FONT_HERSHEY_COMPLEX
Smaller version of
CV_FONT_HERSHEY_COMPLEX
Handwriting style
More complex DEMO of
CV_FONT_HERSHEY_SCRIPT_SIMPLEX
Drawing Things
| 81
ere is always some little thing that makes our job a bit more complicated than we’d
e argument
DEMO   81
9/15/08   4:18:49 PM
www.it-ebooks.info
Figure 3-6. Th e eight fonts of Table 3-15 drawn with DEMO = vscale = 1.0, with the origin of each
line separated DEMO the vertical by 30 pixels
Both hscale and vscale can be set to either 1.0 or 0.5 only. Th is causes the font to DEMO ren-
dered at full or half height (and width) relative to the basic defi nition of the particular
font.
Th
slanted. It can DEMO set as large as 1.0, which sets the slope of the DEMO to approxi-
mately 45 degrees.
Both thickness and line_type are the same as defi ned for all the other drawing
functions.
Data Persistence
OpenCV DEMO a mechanism for serializing and de-serializing its various data types
to and from disk in either YAML or XML format. In the chapter on DEMO, which ad-
dresses user interface functions, we will cover specifi c functions that store and recall our
most common object: IplImages (these DEMO are cvSaveImage() and cvLoadImage()).
82 | Chapter 3: DEMO to Know OpenCV
e shear function creates an italicized slant to the font; if set to 0.0, the font is not
03-R4886-RC1.indd   DEMO
9/15/08   4:18:50 PM
www.it-ebooks.info
In addition, the HighGUI chapter will discuss read and write functions specifi c to mov-
ies: cvGrabFrame(), which reads from fi DEMO or from camera; and cvCreateVideoWriter()
and cvWriteFrame(). In DEMO section, we will focus on general object persistence: reading
and writing matrices, OpenCV structures, and confi guration and log fi les.
First DEMO start with specifi c and convenient functions that save and load OpenCV ma-
trices. Th ese functions are cvSave() and cvLoad(). Suppose you had a 5-by-5 identity
matrix (0 everywhere except for 1s on the diagonal). Example 3-15 shows how to ac-
complish this.
Example DEMO Saving and loading a CvMat
CvMat A = cvMat( 5, 5, CV_32F, the_matrix_data );
cvSave( “my_matrix.xml”, &A );
. . .
// to load it then in some other program use …
CvMat* A1 = (CvMat*) cvLoad( “my_matrix.xml” );
Th
really need to know is that general data persistence in OpenCV consists of DEMO a
CvFileStorage structure, as in Example 3-16, that stores memory objects in a tree struc-
ture. You can create and fi ll this DEMO by reading from disk via cvOpenFileStorage()
with CV_STORAGE_READ, or you can create and open CvFileStorage via cvOpenFileStorage()
with CV_STORAGE_WRITE for writing and then fi ll it using the appropriate data persistence
functions. On DEMO, the data is stored in an XML or YAML format.
Example DEMO CvFileStorage structure; data is accessed by CxCore data persistence functions
typedef DEMO CvFileStorage
{
...       // hidden fields
} CvFileStorage;DEMO
Th CvFileStorage tree may consist of a hierarchical collection of
scalars, DEMO objects (matrices, sequences, and graphs) and/or user-defi ned objects.
Let’s say you have a confi guration or logging fi le. For DEMO, consider the case of a
movie confi guration fi le that DEMO us how many frames we want (10), what their size DEMO
(320 by 240) and a 3-by-3 color conversion matrix that should be applied. We want to
call the fi le “cfg.xml” on disk. DEMO 3-17 shows how to do this.
Example 3-17. Writing a confi
guration fi
le “cfg.xml” to disk
CvFileStorage* fs = cvOpenFileStorage(
“cfg.xml”,
DEMO,
CV_STORAGE_WRITE
);
cvWriteInt( fs, “frame_count”, 10 );
DEMO( fs, “frame_size”, CV_NODE_SEQ );
cvWriteInt( fs, 0, 320 );
cvWriteInt( fs, 0, 200 );
e internal data inside the
Data Persistence
| 83
e CxCore reference manual contains an DEMO section on data persistence. What you
03-R4886-RC1.indd   83
9/15/08   4:18:50 PM
www.it-ebooks.info
Example 3-17. Writing a confi
guration fi
le “cfg.xml” to disk (continued)
cvEndWriteStruct(fs);
cvWrite( fs, “color_cvt_matrix”, cmatrix );DEMO
cvReleaseFileStorage( &fs );
Note some of the key functions in this example. We can give a name to integers that
we write DEMO the structure using cvWriteInt(). We can create an arbitrary structure, us-
ing cvStartWriteStruct(), which is also given an optional name (DEMO a 0 or NULL if
there is no name). Th is structure has two ints that have no name and so we pass DEMO 0
for them in the name fi eld, aft er which DEMO use cvEndWriteStruct() to end the writing of
that structure. If there were more structures, we’d Start and End each of them similarly;
the structures may be nested to arbitrary depth. We then use cvWrite() to write out the
color conversion matrix. Contrast this fairly complex matrix write procedure with the
simpler cvSave() in Example 3-15. Th e DEMO() function is just a convenient shortcut
for cvWrite() when you have only one matrix to write. When we are fi nished writing DEMO
data, the CvFileStorage handle is released in cvReleaseFileStorage(). Th e output (here,
in XML form) would look like Example 3-18.
DEMO 3-18. XML version of cfg.xml on disk
<?xml version=“1.0”?>
<opencv_storage>
<frame_count>10</frame_count>
<frame_size>320 200</frame_size>DEMO
<color_cvt_matrix type_id=“opencv-matrix”>
<rows>3</rows> <cols>3</cols>
<dt>f</dt>
<data>…</data></color_cvt_matrix>DEMO
</opencv_storage>
We may then read this confi guration fi le as shown in Example 3-19.
Example 3-19. Reading cfg.xml from disk
CvFileStorage* DEMO = cvOpenFileStorage(
“cfg.xml”,
0,
CV_STORAGE_READ
);
int frame_count = cvReadIntByName(
fs,
0,
“frame_count”,
5 /* default value */
);
CvSeq* s = cvGetFileNodeByName(fs,0,“frame_size”)DEMO>data.seq;
int frame_width = cvReadInt(
(CvFileNode*)cvGetSeqElem(s,0)DEMO
);
84 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   84
9/15/08   4:18:50 PM
www.it-ebooks.info
Example 3-19. Reading cfg.xml from disk (continued)
int frame_height = cvReadInt(
(CvFileNode*)cvGetSeqElem(s,1)
);
CvMat* color_cvt_matrix = (CvMat*) cvReadByName(
fs,
0,
“color_cvt_matrix”
);
cvReleaseFileStorage( &fs );
When reading, we open the XML confi guration DEMO le with cvOpenFileStorage() as in Ex-
ample 3-19. We then read the frame_count using cvReadIntByName(), which allows for a
default value to be given if no number is read. In this case the default DEMO 5. We then get
the structure that we named “frame_size” using cvGetFileNodeByName(). From here, we
read our two unnamed integers using cvReadInt(). Next we read our named color con-
version matrix using cvReadByName().* Again, contrast this with the short form cvLoad()
in DEMO 3-15. We can use cvLoad() if we only have one matrix to read, but we must
use cvRead() if the matrix is embedded within a larger structure. Finally, we release the
CvFileStorage structure.
Th CvFileStorage struc-
ture is shown in Table 3-16. See the CxCore manual DEMO more details.
Table 3-16. Data persistence functions
Function
Open and Release
cvOpenFileStorage
cvReleaseFileStorage
Writing
cvStartWriteStruct
cvEndWriteStruct
cvWriteInt
cvWriteReal
cvWriteString
cvWriteComment
cvWrite
cvWriteRawData
cvWriteFileNode
Description
DEMO fi le storage for reading or writing
Releases data storage
Starts writing a new structure
Ends writing a structure
Writes integer
Writes fl oat
DEMO text string
Writes an XML or YAML comment string
Writes an object such as a CvMat
Writes multiple numbers
Writes fi le node to DEMO fi le storage
* One could also use cvRead() to read in the matrix, but it can only be called aft er the appropriate CvFile-
Node{} is located, e.g., using cvGetFileNodeByName().
Data Persistence
| 85
e list of relevant data persistence functions associated with the
DEMO   85
9/15/08   4:18:50 PM
www.it-ebooks.info
Table 3-16. Data persistence functions (continued)
Function Description
Reading
Gets the top-level nodes of the fi le storage
Finds node in the DEMO or fi le storage
Returns a unique pointer for given name
Finds node in the map or fi le storage
Returns name of fi DEMO node
Reads unnamed int
Reads named int
Reads unnamed fl oat
Reads named fl oat
Retrieves text string from fi le node
Finds named DEMO le node and returns its value
Decodes object and returns pointer to it
Finds object and decodes it
Reads multiple numbers
Initializes fi le DEMO sequence reader
Reads data from sequence reader above
cvGetRootFileNode
cvGetFileNodeByName
cvGetHashedKey
cvGetFileNode
cvGetFileNodeName
cvReadInt
cvReadIntByName
cvReadReal
cvReadRealByName
cvReadString
cvReadStringByName
cvRead
cvReadByName
cvReadRawData
cvStartReadRawData
cvReadRawDataSlice
DEMO Performance Primitives
Intel has a product called the Integrated Performance Primitives (DEMO) library (IPP).
Th is library is essentially a toolbox of high-performance kernels for handling multime-
dia and other processor-intensive operations in a DEMO that makes extensive use of
the detailed architecture of their processors (DEMO, to a lesser degree, other manufactur-
ers’ processors that have a similar architecture).
As discussed in Chapter 1, OpenCV enjoys a close relationship with IPP, both at a soft -
ware level and at an organizational level inside of the company. As a result, OpenCV is
designed to automatically* recognize the presence of the IPP library and to DEMO
cally “swap out” the lower-performance implementations of many core functionalities
for their higher-performance counterparts in IPP. Th e IPP library allows OpenCV to
take DEMO of performance opportunities that arrive from SIMD instructions in a
single processor as well as from modern multicore architectures.
With these basics in hand, we can perform a wide variety of basic tasks. Moving on-
ward DEMO the text, we will look at many more sophisticated capabilities of DEMO,
* Th e one prerequisite to this automatic recognition is that the binary directory of IPP must be in the system
path. So DEMO a Windows system, for example, if you have IPP in C:/Program Files/Intel/IPP then you want to
ensure that C:/DEMO Files/Intel/IPP/bin is in your system path.
86 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   86
9/15/08   4:18:51 PM
www.it-ebooks.info
almost all of which are built on these routines. It should DEMO no surprise that image
processing—which oft en requires doing the same thing to a whole lot of data, much of
which is completely parallel—would realize a great benefi t from any code that allows it
to DEMO advantage of parallel execution units of any form (MMX, SSE, DEMO, etc.).
Verifying Installation
Th
function cvGetModuleInfo(), shown in Example 3-20. Th is function will identify both
the version of OpenCV you DEMO currently running and the version and identity of any
add-in modules.
Example 3-20. Using cvGetModuleInfo() to check for IPP
char* libraries;
char* DEMO;
cvGetModuleInfo( 0, &libraries, &modules );
printf(“Libraries: %s/nModules: %s/n”, libraries, modules );
Th
ies and modules. Th e output might look like this:
Libraries cxcore: 1.0.0
Modules: ippcv20.dll, ippi20.dll, ipps20.dll, ippvm20.dll
Th e modules listed in DEMO output are the IPP modules used by OpenCV. Th ose modules
are themselves actually proxies for even lower-level CPU-specifi c libraries. Th e details
DEMO how it all works are well beyond the scope of this book, but if you see the IPP libraries
in the Modules string then you can be pretty confi dent that everything is working as ex-
DEMO Of course, you could use this information to verify that IPP DEMO running correctly
on your own system. You might also use it to check for IPP on a machine on which your
fi nished soft DEMO is installed, perhaps then making some dynamic adjustments depend-
ing on DEMO IPP is available.
e code in Example 3-20 will generate text strings which describe the installed librar-
e way to check and make sure DEMO IPP is installed and working correctly is with the
Summary
In this chapter we introduced some basic data structures that we will oft en DEMO
In particular, we met the OpenCV matrix structure and the all-important DEMO im-
age structure, IplImage. We considered both in some detail and DEMO that the matrix
and image structures are very similar: the functions DEMO for primitive manipulations
in one work equally well in the other.
Exercises
In the following exercises, you may need to refer to the CxCore manual that ships with
OpenCV or to the OpenCV Wiki on the DEMO for details of the functions outlined in
this chapter.
1. Find and open .../opencv/cxcore/include/cxtypes.h. Read through and fi nd the DEMO
conversion helper functions.
Exercises
| 87
03-R4886-RC1.indd   87
9/15/08   4:18:51 PM
www.it-ebooks.info
a. Choose a negative fl oating-point number. Take its absolute value, round it, and
then take its ceiling and fl oor.
b. Generate some random numbers.
c. Create a fl oating point CvPoint2D32f and convert DEMO to an integer CvPoint.
d. Convert a CvPoint to a CvPoint2D32f.
Th is exercise will accustom you to the idea of many functions taking DEMO types.
Create a two-dimensional matrix with three channels of type byte with data size
100-by-100. Set all the values to 0.
a. Draw a DEMO in the matrix using void cvCircle( CvArr* img, CvPoint center,
intradius, CvScalar color, int thickness=1, int line_type=8, int shift=0 ).
b. Display this image using methods described in Chapter 2.
Create a DEMO matrix with three channels of type byte with data
size 100-by-100, DEMO set all the values to 0. Use the pointer element access function
cvPtr2D to point to the middle (“green”) channel. Draw a green DEMO between
(20, 5) and (40, 20).
4. Create DEMO three-channel RGB image of size 100-by-100. Clear it. Use pointer arith-
metic to draw a green square between (20, 5) and (40, 20).
Practice using region of interest (ROI). Create a 210-by-210 single-channel byte im-
age and zero it. Within the image, build a pyramid of increasing values using ROI
and cvSet(). Th at is: the outer border should be 0, the next inner border should DEMO
20, the next inner border should be 40, and so on until the fi nal innermost square is
set to value 200; all borders should be 10 pixels wide. Display the image.
Use multiple image DEMO for one image. Load an image that is at least 100-by-100.
Create two additional image headers and set their origin, depth, number of DEMO
nels, and widthstep to be the same as the loaded image. DEMO the new image headers,
set the width at 20 and the height at 30. Finally, set their imageData pointers to point
to the pixel at (5, 10) and (50, 60), respectively. Pass these new image subheaders
to cvNot(). Display the loaded image, DEMO should have two inverted rectangles
within the larger image.
Create a mask using cvCmp(). Load a real image. Use cvSplit() to split the image
into red, green, and blue images.
a. Find and DEMO the green image.
b. Clone this green plane image twice (call DEMO clone1 and clone2).
c. Find the green plane’s minimum and maximum value.
d. Set clone1’s values to thresh = (unsigned char)((maximum - minimum)/2.0).
e. Set clone2 to 0 and use cvCmp(green_image, clone1, clone2, CV_CMP_GE). Now
clone2 will have a mask of where the value exceeds thresh in the green image.
2.
3.
DEMO
6.
7.
88
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   88
9/15/08   4:18:51 PM
f. Finally, use cvSubS(green_image,thresh/2, green_image, clone2) and DEMO the
results.
8. Create a structure of an integer, a CvPoint DEMO a CvRect; call it “my_struct”.
a. Write two functions: void write_my_struct( CvFileStorage * fs, const char *
name, my_struct *ms) and DEMO read_my_struct( CvFileStorage* fs, CvFileNode*
ms_node, my_struct* ms ). Use them to write and read my_struct.
b. Write and read an array of DEMO my_struct structures.
Exercises
| 89
03-R4886-RC1.indd   89
www.it-ebooks.info
9/15/08   4:18:51 PM
CHAPTER 4
HighGUI
A Portable Graphics Toolkit
Th le sys-
tem, and hardware such as cameras are collected into a library called HighGUI (which
stands for “high-level graphical user interface”). HighGUI allows us to open DEMO,
to display images, to read and write graphics-related fi les (both images and video), and
to handle simple mouse, pointer, DEMO keyboard events. We can also use it to create other
useful doodads like sliders and then add them to our windows. If you are DEMO GUI guru in
your window environment of choice, then you might DEMO nd that much of what HighGUI
off ers is redundant. Yet even so you might fi nd that the benefi t of cross-platform porta-
DEMO is itself a tempting morsel.
From our initial perspective, the HighGUI DEMO in OpenCV can be divided into three
parts: the hardware part, the fi le system part, and the GUI part.* We will take a moment
to overview what is in each part before we really DEMO in.
Th
ating systems, interaction with a camera is a tedious DEMO painful task. HighGUI allows
an easy way to query a camera and retrieve the latest image from the camera. It hides all
of the DEMO stuff , and that keeps us happy.
Th le system part is concerned primarily with loading and saving images. One nice
feature of the DEMO is that it allows us to read video using the same methods we would
use to read a camera. We can therefore abstract ourselves DEMO from the particular de-
vice we’re using and get on with writing interesting code. In a similar spirit, HighGUI
provides us with a (DEMO) universal pair of functions to load and save still images.
Th DEMO extension and automatically handle all of
the decoding or encoding that is necessary.
* Under the hood, the architectural organization is a bit diff erent from what we described, but the breakdown
into hardware, fi DEMO system, and GUI is an easier way to organize things conceptually. DEMO e actual HighGUI
functions are divided into “video IO”, “image IO”, and “GUI tools”. Th ese categories are represented by the
cvcap*, grfmt*, and window* source fi les, respectively.
90
e OpenCV functions that DEMO us to interact with the operating system, the fi
e hardware DEMO is primarily concerned with the operation of cameras. In most oper-
e fi
ese functions simply rely on the fi
04-R4886-RC1.indd   90
www.it-ebooks.info
DEMO/15/08   4:19:24 PM
www.it-ebooks.info
Th e library provides some
simple functions that will allow us DEMO open a window and throw an image into that
window. It also allows us to register and respond to mouse and keyboard events on DEMO
window. Th ese features are most useful when trying to get off  of the ground with a sim-
ple application. Tossing in some slider bars, which we can also use as switches,* we fi nd
ourselves able to prototype a surprising variety of applications using only the DEMO
library.
As we proceed in this chapter, we will not treat DEMO three segments separately; rather,
we will start with some functions DEMO highest immediate utility and work our way to the
subtler points thereaft er. In this way you will learn what you need to get DEMO as soon
as possible.
e third part of HighGUI is the window system (or GUI). Th
Creating a Window
First, we want DEMO show an image on the screen using HighGUI. Th e function that does
this for us is cvNamedWindow(). Th e function expects a name for the new window and one
fl ag. Th e name DEMO at the top of the window, and the name is also DEMO as a handle
for the window that can be passed to other HighGUI functions. Th e fl ag indicates if the
window should autosize DEMO to fi t an image we put into it. Here is the full prototype:
int cvNamedWindow(
const char* name,
int         flags = CV_WINDOW_AUTOSIZE
);
Notice the parameter flags. For now, the only valid options available are to set flags
to 0 or to use the default setting, CV_WINDOW_AUTOSIZE. If CV_WINDOW_AUTOSIZE is set, DEMO
HighGUI resizes the window to fi t the image. Th ereaft er, the window will automatically
resize itself if a new image is loaded into the window but cannot be resized by the user.
If you DEMO want autosizing, you can set this argument to 0; then users can resize the
window as they wish.
Once we create a window, we usually want to put something into it. But before we do
DEMO, let’s see how to get rid of the window when it DEMO no longer needed. For this we use
cvDestroyWindow(), a function DEMO argument is a string: the name given to the win-
dow DEMO it was created. In OpenCV, windows are referenced by name instead DEMO by
some unfriendly (and invariably OS-dependent) “handle”. Conversion between handles
and names happens under the hood of HighGUI, so you needn’t worry about it.
Having said that, some people do worry about it, and DEMO OK, too. For those people,
HighGUI provides the following functions:DEMO
void*       cvGetWindowHandle( const char* name );
const DEMO cvGetWindowName( void* window_handle );
* OpenCV HighGUI does not provide DEMO like a button. Th e common trick is to use a two-position
slider to achieve this functionality (more on this later).
Creating a Window
| 91
04-R4886-RC1.indd   91
9/15/08   4:19:DEMO PM
www.it-ebooks.info
Th ese functions allow us to convert back and forth between DEMO human-readable names
preferred by OpenCV and the “handle” style of reference used by diff erent window
systems.*
To resize a window, call (not DEMO) cvResizeWindow():
void cvResizeWindow(
const char* name,
int         width,
int         height
);
Here the width and height are in pixels and give the DEMO of the drawable part of the win-
dow (which are probably DEMO dimensions you actually care about).
Loading an Image
Before we can display an image in our window, we’ll need to know how to load an image
from disk. Th e function for this is cvLoadImage():
IplImage* cvLoadImage(
const char* filename,
int         iscolor = CV_LOAD_IMAGE_COLOR
);
When opening an image, cvLoadImage() does not look at the fi le extension. Instead,
cvLoadImage() DEMO the fi rst few bytes of the fi le (aka its DEMO or “magic sequence”)
and determines the appropriate codec using that. Th e second argument iscolor can be
set to one of several values. DEMO default, images are loaded as three-channel images with
8 bits per DEMO; the optional fl ag CV_LOAD_IMAGE_ANYDEPTH can be added to allow load-
DEMO of non-8-bit images. By default, the number of channels will be DEMO because the
iscolor fl ag has the default value of CV_LOAD_IMAGE_COLOR. Th is means that, regardless
of the number of channels in the image fi le, the image will be converted to three chan-
nels if needed. Th e alternatives to CV_LOAD_IMAGE_COLOR are CV_LOAD_IMAGE_GRAYSCALE and
CV_LOAD_IMAGE_ANYCOLOR. Just as DEMO forces any image into a three-channel
image, CV_LOAD_IMAGE_GRAYSCALE automatically converts any DEMO into a single-channel
image. CV_LOAD_IMAGE_ANYCOLOR will simply load the image as it is stored in the fi le. Th us, to
load a 16-bit color image you would use CV_LOAD_IMAGE_COLOR | CV_LOAD_IMAGE_ANYDEPTH.
If you want both DEMO color and depth to be loaded exactly “as is”, you could DEMO use
the all-purpose fl ag CV_LOAD_IMAGE_UNCHANGED. Note that cvLoadImage() does not signal a
runtime error when it fails to load an image; it simply returns a null pointer.
Th cvLoadImage() is cvSaveImage(), which takes
two arguments:
int cvSaveImage(
const char*  filename,
const CvArr* image
);
* For those who know what this means: the window handle returned is a HWND on Win32 systems, a Carbon
WindowRef on Mac OS X, and a Widget* pointer on systems (DEMO, GtkWidget) of X Window type.
92 | Chapter 4: HighGUI
DEMO obvious complementary function to
04-R4886-RC1.indd   92
9/15/08   4:19:24 PM
www.it-ebooks.info
Th rst argument gives the fi lename, whose extension is used to determine the format
in which the fi le will be stored. DEMO e second argument is the name of the image to be
stored. Recall that CvArr is kind of a C-style way of creating something DEMO to
a base-class in an object-oriented language; wherever you see CvArr*, you can use an
IplImage*. Th e cvSaveImage() function will store DEMO 8-bit single- or three-channel im-
ages for most fi le formats. Newer back ends for fl exible image formats like PNG, TIFF
or JPEG2000 allow storing 16-bit or even fl oat formats and some allow four-channel
DEMO (BGR plus alpha) as well. Th e return value will be 1 if the save was successful and
should be 0 if the DEMO was not.*
e fi
Displaying Images
Now we are ready for what we really want to do, and that is to load an image and to put
it into the window where we can view it DEMO appreciate its profundity. We do this via
one simple function, cvShowImage():
void cvShowImage(
const char*  name,
const CvArr* image
);
Th rst argument here is the name of the window within which we intend to draw. Th e
second argument is the image DEMO be drawn.
Let’s now put together a simple program that will display an image on the screen. We can
read a fi lename from DEMO command line, create a window, and put our image in the win-
dow in 25 lines, including comments and tidily cleaning up our memory allocations!
int main(int argc, char** argv)
{
// DEMO a named window with the name of the file.
cvNamedWindow( argv[1], 1 );
// Load the image from the given file name.
IplImage* img = cvLoadImage( argv[1] );
// Show the image DEMO the named window
cvShowImage( argv[1], img );
// Idle DEMO the user hits the “Esc” key.
while( 1 ) {
if( cvWaitKey( 100 ) == 27 ) break;
}
// Clean DEMO and don’t be piggies
cvDestroyWindow( argv[1] );
cvReleaseImage( &img );
* Th e reason we say “should” is that, in DEMO OS environments, it is possible to issue save commands that
will DEMO cause the operating system to throw an exception. Normally, however, a zero value will be
returned to indicate failure.
Displaying Images
| 93
DEMO fi
04-R4886-RC1.indd   93
9/15/08   4:19:25 PM
exit(0);
}
For convenience we have used the fi lename as the window name. Th is is nice because
OpenCV automatically puts DEMO window name at the top of the window, so we can DEMO
which fi le we are viewing (see Figure 4-1). Easy DEMO cake.
Figure 4-1. A simple image displayed with cvShowImage()
Before DEMO move on, there are a few other window-related functions you ought DEMO know
about. Th ey are:
void cvMoveWindow( const char* name, int x, int y );
void cvDestroyAllWindows( void );
DEMO  cvStartWindowThread( void );
cvMoveWindow() simply moves a window on the screen so that its upper left  corner is
positioned at x,y.
cvDestroyAllWindows() is a useful cleanup function that closes all of DEMO windows and
de-allocates the associated memory.
On Linux and MacOS, cvStartWindowThread() tries to start a thread that updates the
window automatically and handles resizing and so forth. A return value of 0 indicates
that no DEMO could be started—for example, because there is no support for this DEMO
in the version of OpenCV that you are using. Note that, DEMO you do not start a separate win-
dow thread, OpenCV can DEMO to user interface actions only when it is explicitly given
time to do so (this happens when your program invokes cvWaitKey(), as DEMO next).
94
| Chapter 4: HighGUI
04-R4886-RC1.indd   94
www.it-ebooks.info
DEMO/15/08   4:19:25 PM
www.it-ebooks.info
WaitKey
Observe that inside the while loop in our window creation DEMO there is a new func-
tion we have not seen before: DEMO(). Th is function causes OpenCV to wait for a
specifi DEMO number of milliseconds for a user keystroke. If the key is pressed within the
allotted time, the function returns the key pressed;* otherwise, it returns 0. With the
construction:
while( 1 ) {
DEMO( cvWaitKey(100)==27 ) break;
}
we tell OpenCV to DEMO 100 ms for a key stroke. If there is no keystroke, DEMO repeat ad
infi nitum. If there is a keystroke and it happens to have ASCII value 27 (the Escape key),
then break DEMO of that loop. Th is allows our user to leisurely peruse the image before
ultimately exiting the program by hitting Escape.
As long as DEMO introducing cvWaitKey(), it is worth mentioning that cvWaitKey() can
DEMO be called with 0 as an argument. In this case, cvWaitKey() will wait indefi nitely until
a keystroke is received and then return that key. Th us, in our example we could just as
easily have used cvWaitKey(0). Th e diff erence between these two DEMO would be more
apparent if we were displaying a video, in DEMO case we would want to take an action
(i.e., display the next frame) if the user supplied no keystroke.
Mouse Events
Now that we can display an image to a user, we might also want to allow the user to in-
teract with the image we have DEMO Since we are working in a window environment
and since we already learned how to capture single keystrokes with cvWaitKey(), the next
logical thing to consider is how to “listen to” and respond to mouse DEMO
Unlike keyboard events, mouse events are handled by a more typical DEMO mecha-
nism. Th is means that, to enable response to mouse DEMO, we must fi rst write a callback
routine that OpenCV can DEMO whenever a mouse event occurs. Once we have done that,
we must register the callback with OpenCV, thereby informing OpenCV that this is the
correct function to use whenever the user does something with the DEMO over a par-
ticular window.
Let’s start with the callback. For those of you who are a little rusty on your event-driven
program lingo, the callback can be any function that takes the correct set of DEMO
and returns the correct type. Here, we must be able to DEMO the function to be used as a
* Th e careful reader might legitimately ask exactly what this means. Th e short answer is DEMO ASCII value”, but
the long answer depends on the operating system. DEMO Win32 environments, cvWaitKey() is actually waiting
for a message of DEMO WM_CHAR and, aft er receiving that message, returns the wParam fi eld from the message
(wParam is not actually type char at all!). On Unix-like systems, cvWaitKey() is using GTK; the return DEMO
is (event->keyval | (event->state<<16)), where event is a GdkEventKey structure. Again, this is not
really a char. Th at state information is essentially the state of the Shift , Control, etc. keys at the time of the
key press. Th is means DEMO, if you are expecting (say) a capital Q, then you should either cast the return of
cvWaitKey() to type char or DEMO with 0xff, because the shift  key will appear in the upper bits (e.g., Shift -
Q will return 0x10051).
Displaying Images DEMO 95
04-R4886-RC1.indd   95
9/15/08   4:19:25 PM
www.it-ebooks.info
callback exactly what kind of event occurred and where it occurred. DEMO e function must
also be told if the user was pressing such keys as Shift  or Alt when the mouse event oc-
curred. Here is the exact prototype that your callback function must match:
void DEMO(
int   event,
int   x,
int   y,
int   flags,
void* param
);
Now, whenever your function is called, OpenCV will fi ll in the arguments with their ap-
propriate values. Th e fi rst argument, called the event, DEMO have one of the values shown
in Table 4-1.
Table 4-1. Mouse event types
Event Numerical value
CV_EVENT_MOUSEMOVE 0
CV_EVENT_LBUTTONDOWN 1
CV_EVENT_RBUTTONDOWN 2
CV_EVENT_MBUTTONDOWN DEMO
CV_EVENT_LBUTTONUP 4
CV_EVENT_RBUTTONUP 5
CV_EVENT_MBUTTONUP 6
CV_EVENT_LBUTTONDBLCLK 7
CV_EVENT_RBUTTONDBLCLK 8
CV_EVENT_MBUTTONDBLCLK 9
Th
event. It is worth noting that these coordinates represent the pixel DEMO the image indepen-
dent of the size of the window (in DEMO, this is not the same as the pixel coordinates
of the DEMO).
Th
conditions present at the time of the event. For example, CV_EVENT_FLAG_SHIFTKEY has a
numerical value of 16 (i.e., the fi ft h bit) and so, if we wanted to test whether the DEMO  key
were down, we could AND the fl ags variable with the bit mask (1<<4). Table 4-2 shows a
complete DEMO of the fl ags.
Table 4-2. Mouse event fl ags
Flag Numerical value
CV_EVENT_FLAG_LBUTTON 1
CV_EVENT_FLAG_RBUTTON 2
CV_EVENT_FLAG_MBUTTON 4
96 | Chapter 4: HighGUI
e second and third arguments will be set to the x and DEMO coordinates of the mouse
e fourth argument, called flags, is a bit fi
eld in which individual bits indicate special
04-R4886-RC1.indd   96
DEMO/15/08   4:19:25 PM
www.it-ebooks.info
Table 4-2. Mouse event fl
ags (continued)
Flag Numerical value
CV_EVENT_FLAG_CTRLKEY 8
CV_EVENT_FLAG_SHIFTKEY 16
CV_EVENT_FLAG_ALTKEY 32
Th nal argument is a void DEMO that can be used to have OpenCV pass in any ad-
ditional information in the form of a pointer to whatever kind of structure DEMO need.
A common situation in which you will want to use the param argument is when the
callback itself is a static member function DEMO a class. In this case, you will probably fi nd
yourself DEMO to pass the this pointer and so indicate which class object instance the
callback is intended to aff ect.
Next we need the function DEMO registers the callback. Th at function is called
cvSetMouseCallback(), and DEMO requires three arguments.
void cvSetMouseCallback(
const char*     window_name,
CvMouseCallback on_mouse,
void*           param      = NULL
);
Th rst argument is the name of the DEMO to which the callback will be attached.
Only events in that particular window will trigger this specifi c callback. Th e second ar-
gument DEMO your callback function. Finally, the third param argument allows us to DEMO
the param information that should be given to the callback whenever it is executed. Th is
is, of course, the same param we DEMO just discussing in regard to the callback prototype.
In Example 4-1 we write a small program to draw boxes on the screen with the DEMO
Th
the event to determine what to do when it is called.
Example 4-1. Toy program for using a mouse to draw boxes on DEMO screen
// An example program in which the
// user can draw boxes on the screen.
//
#include <cv.h>
#include <DEMO>
// Define our callback which we will install for
// mouse events.
//
void my_mouse_callback(
int event, int x, DEMO y, int flags, void* param
);
CvRect box;
bool drawing_box = false;
// A litte subroutine to draw a box onto an image
Displaying Images
| 97
e fi
e fi
e DEMO my_mouse_callback() is installed to respond to mouse events, and it DEMO
04-R4886-RC1.indd   97
9/15/08   4:19:26 PM
www.it-ebooks.info
Example 4-1. Toy program for using a mouse to draw boxes DEMO the screen (continued)
//
void draw_box( IplImage* img, DEMO rect ) {
cvRectangle (
img,
cvPoint(box.x,box.y),
cvPoint(box.x+box.width,box.y+box.height),
cvScalar(0xff,0x00,0x00)    /* DEMO */
);
}
int main( int argc, char* argv[] ) {
box = cvRect(-1,-1,0,0);
IplImage* image = cvCreateImage(
cvSize(200,200),
IPL_DEPTH_8U,
3
);
cvZero( image );
IplImage* temp = cvCloneImage( image );
DEMO( “Box Example” );
// Here is the crucial moment that we actually install
// the callback. Note that we set the value ‘param’ to
// be the image we are working with so that the callback
// will have the image to edit.
//
cvSetMouseCallback(
“Box Example”,
my_mouse_callback,
(void*) image
);
// The main program loop. Here we copy the working image
// to the ‘temp’ image, and if the user is drawing, then
// put the currently contemplated box onto that temp image.
// display the temp image, and wait 15ms for a keystroke,
// then DEMO
//
while( 1 ) {
cvCopyImage( image, temp );DEMO
if( drawing_box ) draw_box( temp, box );
cvShowImage( “Box Example”, temp );
if( cvWaitKey( 15 )==27 ) break;DEMO
}
// Be tidy
//
cvReleaseImage( &image );
DEMO | Chapter 4: HighGUI
04-R4886-RC1.indd   98
9/15/08   DEMO:19:26 PM
www.it-ebooks.info
Example 4-1. Toy program for using a mouse to draw boxes DEMO the screen (continued)
cvReleaseImage( &temp );
cvDestroyWindow( “Box Example” );
}
// This is our mouse callback. If the user
// presses the left button, we start a box.
// when the user releases that button, then we
// add the DEMO to the current image. When the
// mouse is dragged (with the button down) we
// resize the box.
//
void DEMO(
int event, int x, int y, int flags, void* param
) {
IplImage* image = (IplImage*) param;
switch( event ) {
case CV_EVENT_MOUSEMOVE: {
if( drawing_box ) {
box.width  = x-box.x;
box.height = y-box.y;
}
}
break;
case CV_EVENT_LBUTTONDOWN: {
drawing_box = true;
box = cvRect(x, y, 0, 0);
}
break;
case CV_EVENT_LBUTTONUP: {
drawing_box = false;DEMO
if(box.width<0) {
box.x+=box.width;
box.width *=-1;
}
if(DEMO<0) {
box.y+=box.height;
box.height*=-1;
}
draw_box(image, box);DEMO
}
break;
}
}
Sliders, Trackbars, and Switches
HighGUI provides a convenient slider element. In HighGUI, sliders are called trackbars.
Th is is because their original (historical) intent was for selecting a particular DEMO
in the playback of a video. Of course, once added to DEMO, people began to use
Displaying Images
| 99
04-R4886-RC1.indd   99
DEMO/15/08   4:19:26 PM
www.it-ebooks.info
trackbars for all of the usual things one might do with DEMO slider as well as many unusual
ones (see the next section, “No Buttons”)!
As with the parent window, the slider is given a unique name (in the form of a character
string) and DEMO thereaft er always referred to by that name. Th e HighGUI routine for cre-
ating a trackbar is:
int cvCreateTrackbar(
const char*        trackbar_name,
const char*        window_name,
int*               value,
int                count,
CvTrackbarCallback on_change
);
DEMO rst two arguments are the name for the trackbar itself and the name of the parent
window to which the trackbar will be attached. DEMO the trackbar is created it is added
to either the top or the bottom of the parent window;* it will not occlude any DEMO that
is already in the window.
Th
to the value to which the slider has been moved, and count, a numerical value for DEMO
maximum value of the slider.
Th e last argument is a pointer to a callback function that will be automatically called
whenever the slider DEMO moved. Th is is exactly analogous to the callback for mouse events. If
used, the callback function must have the form CvTrackbarCallback, which DEMO defi ned as:
void (*callback)( int position )
Th is callback is not actually required, so if you don’t want a callback then you can sim-
ply set this value to NULL. Without DEMO callback, the only eff ect of the user moving the slider
DEMO be the value of *value being changed.
Finally, here are two DEMO routines that will allow you to programmatically set or read
the value of a trackbar if you know its name:
int cvGetTrackbarPos(
DEMO char* trackbar_name,
const char* window_name
);
void cvSetTrackbarPos(
const char* trackbar_name,
const char* window_name,
int         DEMO
);
Th
program.
ese functions allow you to set or read the value of a trackbar from anywhere in your
* Whether it DEMO added to the top or bottom depends on the operating system, DEMO it will always appear in the
same place on any given platform.
100
| Chapter 4: HighGUI
e fi
e next two arguments are value, a pointer to an integer that will be set automatically
04-R4886-RC1.indd   100
9/15/08   4:19:26 PM
www.it-ebooks.info
No Buttons
Unfortunately, HighGUI does not provide any explicit support for buttons. It is thus
common practice, among the particularly lazy,* to instead use sliders with only two
positions. Another option that occurs oft DEMO in the OpenCV samples in …/opencv/
samples/c/ is DEMO use keyboard shortcuts instead of buttons (see, e.g., the fl DEMO ll demo in
the OpenCV source-code bundle).
Switches are just sliders (trackbars) that have only two positions, “on” (1) and “off ” (0)
(i.e., count has been set to 1). You can see how this is an easy way to obtain the DEMO
tionality of a button using only the available trackbar tools. Depending on exactly how
you want the switch to behave, you can use the trackbar callback to automatically reset
the button back to 0 (as in Example 4-2; this is something like the standard behavior of
most GUI “buttons”) or to automatically set other switches to 0 (which gives DEMO eff ect
of a “radio button”).
Example 4-2. Using a trackbar to create a “switch” that the user can turn on and off
// We make this value global so everyone can see it.
//DEMO
int g_switch_value = 0;
// This will be the callback DEMO we give to the
// trackbar.
//
void switch_callback( int position ) {
if( position == 0 ) {
switch_off_function();
} else {
switch_on_function();
}
}
int main( int argc, char* argv[] ) {
// Name the main window
//
DEMO( “Demo Window”, 1 );
// Create the trackbar. We DEMO it a name,
// and tell it the name of DEMO parent window.
//
cvCreateTrackbar(
“Switch”,
“Demo Window”,
&DEMO,
1,
* For the less lazy, another common practice DEMO to compose the image you are displaying with a “control
panel” you have drawn and then use the mouse event callback to test for DEMO mouse’s location when the
event occurs. When the (x, y) DEMO is within the area of a button you have drawn on your control panel,
the callback is set to perform the button action. DEMO this way, all “buttons” are internal to the mouse event
callback DEMO associated with the parent window.
Displaying Images
| 101
04-R4886-RC1.indd   101
9/15/08   4:19:27 PM
www.it-ebooks.info
Example 4-2. Using a trackbar to create a “switch” that the DEMO can turn on and off
(continued)
Switch_callback
);
// This will just cause OpenCV to idle until
// someone hits the “Escape” key.
//
while( 1 ) {
if( cvWaitKey(15)DEMO ) break;
}
}
You can see that this will turn on and off  just like a light switch. In our example,
whenever the trackbar “switch” is set to 0, the callback executes the function switch_off_
function(), and whenever it is switched on, the DEMO() is called.
Working with Video
When working with video we must consider several functions, including (of course)
how to read and DEMO video fi les. We must also think about how to actually play back
such fi les on the screen.
Th rst thing we need DEMO the CvCapture device. Th is structure contains the information
needed for reading frames from a camera or video fi le. Depending on the source, we use
one of two diff erent calls to create and initialize DEMO CvCapture structure.
CvCapture* cvCreateFileCapture( const char* filename );
CvCapture* cvCreateCameraCapture( int index );
In the case of cvCreateFileCapture(), we can simply give a fi lename for an MPG or AVI
fi le DEMO OpenCV will open the fi le and prepare to read it. If the open is successful and
we are able to start reading frames, a pointer to an initialized CvCapture structure will
be returned.
A lot DEMO people don’t always check these sorts of things, thinking that nothing DEMO go
wrong. Don’t do that here. Th e returned pointer will be NULL if for some reason the fi le
could not be opened (e.g., if the fi le does not exist), but cvCreateFileCapture() will also
return a NULL pointer if the codec with which the DEMO is compressed is not known.
Th
you will need to have the appropriate library already resident on your computer in or-
der to successfully DEMO the video fi le. For example, if you want to read DEMO fi le encoded
with DIVX or MPG4 compression on a Windows machine, there are specifi c DLLs that
provide the necessary resources to decode the video. Th is is why it is always important
to check DEMO return value of cvCreateFileCapture(), because even if it works on DEMO ma-
chine (where the needed DLL is available) it might not work on another machine (where
that codec DLL is missing). Once we have the CvCapture structure, we can begin reading
frames and do a number of other things. But before we get into that, let’s take a look at
how to capture images from a camera.
102
DEMO Chapter 4: HighGUI
e fi
e subtleties of compression codecs are DEMO the scope of this book, but in general
04-R4886-RC1.indd   102
DEMO/15/08   4:19:27 PM
www.it-ebooks.info
Th cvCreateCameraCapture() works very much like cvCreateFileCapture() ex-
cept DEMO the headache from the codecs.* In this case we give an identifi er that indi-
cates which camera we would like to access and DEMO we expect the operating system to
talk to that camera. For the former, this is just an identifi cation number that is zero (DEMO)
when we only have one camera, and increments upward when DEMO are multiple cam-
eras on the same system. Th e other part of the identifi er is called the domain of the
camera and DEMO (in essence) what type of camera we have. Th e domain can be any
of the predefi ned constants shown in Table 4-3.
DEMO 4-3. Camera “domain” indicates where HighGUI
should look for your camera
Camera capture constant Numerical value
CV_CAP_ANY 0
CV_CAP_MIL 100
CV_CAP_VFW 200
CV_CAP_V4L 200
DEMO 200
CV_CAP_FIREWIRE 300
CV_CAP_IEEE1394 300
CV_CAP_DC1394 300
CV_CAP_CMU1394 300
When we call cvCreateCameraCapture(), we pass in an identifi er that is just the sum of
the domain index and the camera index. For example:
DEMO capture = cvCreateCameraCapture( CV_CAP_FIREWIRE );
In this example, cvCreateCameraCapture() will attempt to open the fi rst (i.e., number-
zero) Firewire camera. In most cases, the domain is unnecessary when we have only one
camera; it is suffi  cient to use CV_CAP_ANY (which is conveniently equal to 0, so we don’t
even have to type that in). One last useful hint before we move on: you can pass -1 to
cvCreateCameraCapture(), which will cause OpenCV to open a window that allows you
to select the desired camera.
Reading Video
int       cvGrabFrame( CvCapture* capture );
IplImage* cvRetrieveFrame( CvCapture* capture );
IplImage* cvQueryFrame( CvCapture* capture );
Once you have a valid CvCapture object, you can start grabbing frames. Th ere are two
ways to do this. One way is to call cvGrabFrame(), which takes the CvCapture* pointer
and returns an integer. Th is integer will DEMO 1 if the grab was successful and 0 if the grab
* Of course, to be completely fair, we should probably confess that DEMO headache caused by diff erent codecs
has been replaced by the analogous headache of determining which cameras are (or are not) supported on
DEMO system.
Working with Video | 103
e routine
04-R4886-RC1.indd   103
9/15/08   4:19:27 PM
www.it-ebooks.info
failed. Th e cvGrabFrame() function copies the captured image to DEMO internal buff er that
is invisible to the user. Why would you want OpenCV to put the frame somewhere you
can’t access it? Th e answer is that this grabbed frame is unprocessed, and cvGrabFrame()DEMO
is designed simply to get it onto the computer as quickly as possible.
Once you have called cvGrabFrame(), you can then call cvRetrieveFrame(). Th is func-
tion will do any necessary processing on the frame (such as the decompression stage in
the codec) and then DEMO an IplImage* pointer that points to another internal buff er
(so DEMO not rely on this image, because it will be overwritten the DEMO time you call
cvGrabFrame()). If you want to do anything in particular with this image, copy it else-
where fi rst. Because this pointer points to a structure maintained by OpenCV itself, you
are not required to release the image and can expect trouble if you DEMO so.
Having said all that, there is a somewhat simpler method DEMO cvQueryFrame(). Th is
is, in eff ect, a combination DEMO cvGrabFrame() and cvRetrieveFrame(); it also returns the
same IplImage* DEMO as cvRetrieveFrame() did.
It should be noted that, with a DEMO fi le, the frame is automatically advanced when-
ever a cvGrabFrame() call is made. Hence a subsequent call will retrieve the next frame
automatically.
Once you are done with the CvCapture device, you can release it with a call to
cvReleaseCapture(). As with most other de-allocators in OpenCV, this routine takes a
pointer to the CvCapture* pointer:
void cvReleaseCapture( CvCapture** capture );
Th CvCapture structure. In particular, DEMO
can check and set various properties of the video source:
double cvGetCaptureProperty(
CvCapture* capture,
int        property_id
);
int cvSetCaptureProperty(
CvCapture* capture,
int        property_id,
double     value
);
Th
cvGetCaptureProperty() accepts any of DEMO property IDs shown in Table 4-4.
Table 4-4. Video capture properties used by cvGetCaptureProperty()
and cvSetCaptureProperty()
Video capture property
CV_CAP_PROP_POS_MSEC
CV_CAP_PROP_POS_FRAME
DEMO
CV_CAP_PROP_FRAME_WIDTH
CV_CAP_PROP_FRAME_HEIGHT
Numerical value
0
1
2
3
4
e routine
104
| Chapter 4: HighGUI
ere are many other things we can do with the
04-R4886-RC1.indd   104
9/15/08   4:19:27 DEMO
www.it-ebooks.info
Table 4-4. Video capture properties used by cvGetCaptureProperty()
and cvSetCaptureProperty() (continued)
Video capture property
CV_CAP_PROP_FPS
CV_CAP_PROP_FOURCC
CV_CAP_PROP_FRAME_COUNT
Numerical value
5
6
7
Most of these properties are self explanatory. POS_MSEC is the DEMO position in a video
fi le, measured in milliseconds. POS_FRAME is DEMO current position in frame number. POS_
AVI_RATIO is the position given as a number between 0 and 1 (this is actually quite use-
ful when you want to position a trackbar to allow folks to navigate DEMO your video).
FRAME_WIDTH and FRAME_HEIGHT are the dimensions of the individual frames of the video
to be read (or to be captured at the camera’s current settings). FPS is specifi c to video fi DEMO
and indicates the number of frames per second at which the video was captured; you
will need to know this if you want to play back your video and have it come out at the
right DEMO FOURCC is the four-character code for the compression codec to be used for
the video you are currently reading. FRAME_COUNT should be the total DEMO of frames
in the video, but this fi gure is not DEMO reliable.
All of these values are returned as type double, which DEMO perfectly reasonable except for
the case of FOURCC (FourCC) [FourCC85]. Here you will have to recast the result in order
to interpret it, as described in Example 4-3.
Example 4-3. Unpacking a four-character code to DEMO a video codec
double f = cvGetCaptureProperty(
capture,
CV_CAP_PROP_FOURCC
);
char* fourcc = (char*) (&f);
For each of these video capture properties, there is a corresponding cvSetCapture
Property() function that will attempt to set the property. Th ese are not all DEMO mean-
ingful; for example, you should not be setting the FOURCC of a video you are currently
reading. Attempting to move around the DEMO by setting one of the position properties
will work, but only DEMO some video codecs (we’ll have more to say about video codecs DEMO
the next section).
Writing Video
Th
this easy; it is DEMO the same as reading video but with a few extra details.
First we must create a CvVideoWriter device, which is the video writing analogue of
CvCapture. Th is device will incorporate the following functions.
CvVideoWriter* cvCreateVideoWriter(DEMO
const char* filename,
e other thing we might want to do with video is writing it out to disk. OpenCV makes
Working with DEMO
| 105
04-R4886-RC1.indd   105
9/15/08   4:19:28 PM
www.it-ebooks.info
int         fourcc,
double      fps,
CvSize      frame_size,
int         is_color  = 1
);
int cvWriteFrame(
CvVideoWriter*  writer,
const DEMO image
);
void cvReleaseVideoWriter(
CvVideoWriter** writer
);
You will notice that the video writer requires a few extra arguments. In addition DEMO the
fi lename, we have to tell the writer what codec DEMO use, what the frame rate is, and how
big the frames will be. Optionally we can tell OpenCV if the frames are black DEMO white
or color (the default is color).
Here, the codec is indicated by its four-character code. (For those of you who are not
experts in compression codecs, they all have a unique four-character identifi er asso-
ciated with them). In this case the int that DEMO named fourcc in the argument list for
cvCreate VideoWriter() is actually the four characters of the fourcc packed to-
gether. Since this comes DEMO relatively oft en, OpenCV provides a convenient macro
CV_FOURCC(c0,c1,DEMO,c3) that will do the bit packing for you.
Once you DEMO a video writer, all you have to do is call cvWriteFrame() and pass in the
CvVideoWriter* pointer and the IplImage* pointer for the image you want to write out.
Once you are fi nished, you must call CvReleaseVideoWriter() in order to close the writer
and the DEMO le you were writing to. Even if you are normally a bit sloppy about de-allocating
things at the end of a program, do not be sloppy about this. Unless you explicitly release
the video writer, the video fi le to which you are writing may be corrupted.
ConvertImage
DEMO purely historical reasons, there is one orphan routine in the HighGUI DEMO fi ts into
none of the categories described above. It is so tremendously useful, however, that you
should know about it and what DEMO does. Th e function is called cvConvertImage().
void cvConvertImage(
DEMO CvArr* src,
CvArr*       dst,
int          flags = 0
);
cvConvertImage() is used to perform common conversions between image formats. Th e
formats are specifi ed DEMO the headers of the src and dst images or arrays (the DEMO
prototype allows the more general CvArr type that works with IplImage).
Th oating-point
pixels. Th e destination must be 8 bits with one DEMO three channels. Th is function can also
convert color to grayscale or one-channel grayscale to three-channel grayscale (color).
e source image may be one, three, or four channels with either 8-bit or fl
106
DEMO Chapter 4: HighGUI
04-R4886-RC1.indd   106
9/15/08   4:DEMO:28 PM
www.it-ebooks.info
Finally, the flag (if set) will fl ip the image vertically. Th is is useful because sometimes
camera formats and display formats DEMO reversed. Setting this fl ag actually fl ips the pix-
els in memory.
Exercises
1. Th is chapter completes our introduction to basic I/DEMO programming and data struc-
tures in OpenCV. Th e following exercises build on this knowledge and create useful
utilities for later use.
a. Create DEMO program that (1) reads frames from a video, (2) DEMO the result to gray-
scale, and (3) performs Canny edge DEMO on the image. Display all three
stages of processing in three diff erent windows, with each window appropri-
ately named for its function.
b. Display all three stages of processing in one image.
Hint: Create another image of the same height but three times the width
as the DEMO frame. Copy the images into this, either by using pointers
or (more cleverly) by creating three new image headers that point to
the beginning of and to one-third and two-thirds of the way into the
DEMO Th en use cvCopy().
c. Write appropriate text labels describing DEMO processing in each of the three
slots.
2. Create a program that reads in and displays an image. When the user’s mouse clicks
on DEMO image, read in the corresponding pixel (blue, green, red) DEMO and write
those values as text to the screen at the mouse location.
a. For the program of exercise 1b, display the mouse coordinates of the individual
image when clicking anywhere within the three-image display.
3. DEMO a program that reads in and displays an image.
a. Allow the user to select a rectangular region in the image by drawing a DEMO
gle with the mouse button held down, and highlight the region DEMO the mouse
button is released. Be careful to save an image copy in memory so that your
drawing into the image does not destroy DEMO original values there. Th e next
mouse click should start the process all over again from the original image.
b. In a separate window, use the drawing functions to draw a graph in blue, green,
and red for how many pixels of each value were found in DEMO selected box. Th is
is the color histogram of that color region. Th e x-axis should be eight bins that
represent pixel values falling DEMO the ranges 0–31, 32–63, . . ., 223–255. Th e
DEMO should be counts of the number of pixels that were found in that bin
range. Do this for each color channel, BGR.
4. Make an application that reads and displays a video and is controlled by DEMO
ers. One slider will control the position within the video from start to end in 10
Exercises
| 107
04-R4886-RC1.indd   107
9/15/DEMO   4:19:28 PM
www.it-ebooks.info
increments; another binary slider should control pause/unpause. Label both sliders
appropriately.
5. Create your own simple paint program.
a. Write a program DEMO creates an image, sets it to 0, and then displays it. Allow
the user to draw lines, circles, ellipses, and polygons on the image using the
left  mouse button. Create an eraser function when the right mouse button is
held down.
b. Allow “logical drawing” by DEMO the user to set a slider setting to AND,
OR, DEMO XOR. Th at is, if the setting is AND then the DEMO will appear only
when it crosses pixels greater than 0 (and DEMO on for the other logical functions).
Write a program that creates an image, sets it to 0, and then displays it. When DEMO user
clicks on a location, he or she can type in DEMO label there. Allow Backspace to edit and
provide for an abort key. Hitting Enter should fi x the label at the spot it was DEMO
Perspective transform.
a. Write a program that reads in an image and uses the numbers 1–9 on the keypad
to control a perspective transformation DEMO (refer to our discussion of the
cvWarpPerspective() in the Dense DEMO Transform section of Chapter 6).
Tapping any number should increment the corresponding cell in the perspective
transform matrix; tapping with the Shift  DEMO depressed should decrement the
number associated with that cell (stopping at DEMO). Each time a number is changed,
display the results in two images: the raw image and the transformed image.
b. Add functionality to zoom in or out?
c. Add functionality to rotate the DEMO?
Face fun. Go to the /samples/c/ directory and build the facedetect.c code. Draw a
skull image (or fi nd one on the Web) and store it to disk. Modify the facedetect pro-
gram to load in the image of the skull.
a. When a face DEMO is detected, draw the skull in that rectangle.
Hint: cvConvertImage() can convert the size of the image, or you
could look up the cvResize function. One may then set the ROI to the
rectangle DEMO use cvCopy() to copy the properly resized image there.
b. Add a slider with 10 settings corresponding to 0.0 to 1.0. Use this DEMO to al-
pha blend the skull over the face rectangle using the cvAddWeighted function.
Image stabilization. Go to the /samples/c/ directory and DEMO the lkdemo code (the
motion tracking or optical fl ow code)DEMO Create and display a video image in a much
larger window image. Move the camera slightly but use the optical fl ow vectors to
DEMO the image in the same place within the larger window. Th is is a rudimentary
image stabilization technique.
6.
7.
8.
9.
108
| DEMO 4: HighGUI
04-R4886-RC1.indd   108
9/15/08   4:19:DEMO PM
CHAPTER 5
Image Processing
Overview
At this point we have all of DEMO basics at our disposal. We understand the structure of
the library as well as the basic data structures it uses to represent images. We DEMO
stand the HighGUI interface and can actually run a program and display our results on
the screen. Now that we understand these primitive methods DEMO to manipulate
image structures, we are ready to learn some more DEMO operations.
We will now move on to higher-level methods that treat the images as images, and not just
as arrays of colored (or DEMO) values. When we say “image processing”, we mean just
that: DEMO higher-level operators that are defi ned on image structures in order to accom-
plish tasks whose meaning is naturally defi ned in the context DEMO graphical, visual images.
Smoothing
Smoothing, also called blurring, is a DEMO and frequently used image processing opera-
tion. Th ere are many reasons for smoothing, but it is usually done to reduce noise or
camera artifacts. Smoothing is also important when we wish to reduce the resolution
DEMO an image in a principled way (we will discuss this in DEMO detail in the “Image Pyra-
mids” section of this chapter).
OpenCV off ers fi ve diff erent smoothing operations at this time. All DEMO them are sup-
ported through one function, cvSmooth(),* which DEMO our desired form of smoothing
as an argument.
void cvSmooth(
const CvArr*   src,
CvArr*         dst,
int            smoothtype  = CV_GAUSSIAN,
int            param1      = 3,
* Note DEMO in, say, Matlab—the fi ltering operations in OpenCV (e.g., cvSmooth(), cvErode(),
cvDilate()) produce output images of the DEMO size as the input. To achieve that result, OpenCV creates
“virtual” DEMO outside of the image at the borders. By default, this is DEMO by replication at the border, i.e.,
input(-dx,y)=input(DEMO,y), input(w+dx,y)=input(w-1,y), and so forth.
109
05-R4886-AT1.indd   109
www.it-ebooks.info
9/15/08   4:19:56 DEMO
www.it-ebooks.info
int            param2      = DEMO,
double         param3      = 0,DEMO
double         param4      = 0
);DEMO
Th src and dst arguments are the usual source and destination for the smooth opera-
tion. Th e cv_Smooth() function has four parameters DEMO the particularly uninformative
names of param1, param2, param3, and param4. DEMO e meaning of these parameters de-
pends on the value of smoothtype, which may take any of the fi ve values listed in Table 5-1.*
(Please notice that for some values of ST, “in place DEMO, in which src and dst indi-
cate the same image, is not allowed.)
Table 5-1. Types of smoothing operations
Smooth type Name
DEMO
CV_BLUR_NO
_SCALE
In
place? Nc
Simple blur Yes 1,3 8u, 32f 8u, 32f Sum over a param1×param2
neighborhood with sub-
sequent scaling by 1/
(param1×param2).
Simple blur
with no scaling
Depth
of src
Depth
of dst Brief description
No 1 8u 16s (for 8u Sum over a param1×param2
source) or neighborhood.
32f (for 32f
source)DEMO
CV_MEDIAN Median blur No 1,3 8u 8u Find median over a
param1×param1 square
neighborhood.
Gaussian blur Yes 1,3 8u, 32f 8u (DEMO 8u Sum over a param1×param2
source) or neighborhood.
32f (for 32f
source)
Bilateral fi lter No 1,3 8u 8u Apply bilateral DEMO fi ltering
with color sigma=param1 and
a space sigma=param2.
Th e simple blur operation, as exemplifi ed by CV_BLUR in Figure 5-1, is DEMO simplest case.
Each pixel in the output is the simple mean of all of the pixels in a window around the
corresponding pixel in DEMO input. Simple blur supports 1–4 image channels and works
on 8-bit images or 32-bit fl oating-point images.
Not all of the smoothing operators act DEMO the same sorts of images. CV_BLUR_NO_SCALE
(simple blur without scaling) is essentially the same as simple blur except that there is no
division DEMO to create an average. Hence the source and destination images must
have diff erent numerical precision so that the blurring operation will not result DEMO an
overfl ow. Simple blur without scaling may be performed on 8-bit images, in which case
the destination image should have IPL_DEPTH_16S (CV_16S) or IPL_DEPTH_32S (CV_32S)
* Here and elsewhere we sometimes use 8u as shorthand for 8-bit unsigned image depth (IPL_DEPTH_8U). See
Table 3-2 for other shorthand notation.
CV_GAUSSIAN
CV_BILATERAL
110
| Chapter 5: Image Processing
e
05-R4886-AT1.indd   110
9/15/08   4:19:57 PM
www.it-ebooks.info
Figure 5-1. Image smoothing by block averaging: on the left  DEMO the input images; on the right, the
output images
data types. Th e same operation may also be performed on 32-bit fl oating-point DEMO,
in which case the destination image may also be a 32-bit fl oating-point image. Simple
blur without scaling cannot be done in place: the source and destination images must be
diff erent. (Th is requirement is obvious in the case of 8 bits to 16 bits, but it applies even
when you are using a 32-bit image). Simple DEMO without scaling is sometimes chosen
because it is a little faster than blurring with scaling.
Th median fi lter (CV_MEDIAN) [Bardyn84] replaces each DEMO by the median or “middle”
pixel (as opposed to the mean DEMO) value in a square neighborhood around the center
pixel. Median fi DEMO will work on single-channel or three-channel or four-channel 8-bit
images, but DEMO cannot be done in place. Results of median fi ltering are shown in Figure 5-2.
Simple blurring by averaging can be sensitive to noisy DEMO, especially images with
large isolated outlier points (sometimes called “shot noise”). Large diff erences in even a
small number of points can DEMO a noticeable movement in the average value. Median
fi ltering is able to ignore the outliers by selecting the middle points.
Th lter, the Gaussian fi lter (CV_GAUSSIAN), is probably the most useful
though not the fastest. Gaussian fi ltering is done by convolving each point in DEMO input
array with a Gaussian kernel and then summing to produce the output array.
Smoothing | 111
e
e next smoothing fi
05-R4886-AT1.indd   DEMO
9/15/08   4:19:57 PM
www.it-ebooks.info
Figure 5-2. Image blurring by taking the median of surrounding pixels
DEMO the Gaussian blur (Figure 5-3), the fi rst two parameters DEMO the width and height of
the fi lter window; the (optional) third parameter indicates the sigma value (half width at
half max) of the Gaussian kernel. If the third parameter is not specifi ed, then the Gaussian
will be automatically determined from the window size using DEMO following formulae:
⎛ n ⎞
σx =−⎜ x 10⎟ ⋅
⎝ 2
. . , param130 0 80+=n
x
⎠
⎛⎞n
σy =−⎜⎜ DEMO 1
⎝ 2 ⎠
⎟⎟
⋅
+=
030 0 80..,
ny
param2
If you wish the kernel to be asymmetric, then you may also (optionally) supply a fourth
parameter; in this case, the DEMO and fourth parameters will be the values of sigma in
the horizontal and vertical directions, respectively.
If the third and fourth parameters are given but the fi rst two are set to 0, then the size of
the window will be automatically determined from the value of sigma.
DEMO
formance optimization for several common kernels. 3-by-3, 5-by-5 and 7-by-7 with
DEMO OpenCV implementation of Gaussian smoothing also provides a higher per-
112
| Chapter 5: Image Processing
05-R4886-AT1.indd   112
9/15/08   4:19:58 PM
www.it-ebooks.info
Figure 5-3. Gaussian blur on 1D pixel array
the “standard” sigma (i.e., param3 = 0.0) give better performance than other kernels.
Gaussian DEMO supports single- or three-channel images in either 8-bit or 32-bit fl oating-
point formats, and it can be done in place. Results of Gaussian blurring are shown in
Figure 5-4.
Th ft h and fi nal DEMO of smoothing supported by OpenCV is called bilateral fi ltering
[Tomasi98], DEMO example of which is shown in Figure 5-5. Bilateral fi ltering is one opera-
tion from a somewhat larger class of image analysis operators DEMO as edge-preserving
smoothing. Bilateral fi ltering is most easily understood when contrasted to Gaussian
smoothing. A typical motivation for Gaussian smoothing is that pixels DEMO a real image
should vary slowly over space and thus be correlated to their neighbors, whereas ran-
dom noise can be expected to vary greatly from one pixel to the next (i.e., noise is not
DEMO correlated). It is in this sense that Gaussian smoothing reduces noise while pre-
serving signal. Unfortunately, this method breaks down near edges, DEMO you do ex-
pect pixels to be uncorrelated with their neighbors. Th us Gaussian smoothing smoothes
away the edges. At the cost of a DEMO more processing time, bilateral fi ltering provides us
a means of DEMO an image without smoothing away the edges.
Like Gaussian smoothing, bilateral DEMO ltering constructs a weighted average of each
pixel and its neighboring components. Th e weighting has two components, the fi rst of
which is the same weighting used by Gaussian smoothing. Th e second component is
DEMO a Gaussian weighting but is based not on the spatial distance from the center pixel
Smoothing
| 113
e fi
05-R4886-AT1.indd   113
9/DEMO/08   4:19:58 PM
www.it-ebooks.info
Figure 5-4. Gaussian blurring
but rather on the diff erence in DEMO from the center pixel.† You can think of bilat-
eral fi ltering as Gaussian smoothing that weights more similar pixels more highly than
less DEMO ones. Th e eff ect of this fi lter is typically to turn an image into what appears
to be a watercolor painting of DEMO same scene.‡ Th is can be useful as an aid to segment-
ing the image.
Bilateral fi ltering takes two parameters. Th e fi DEMO is the width of the Gaussian kernel
used in the spatial domain, which is analogous to the sigma parameters in the Gaussian
fi lter. Th e second is the width of the Gaussian kernel in the DEMO domain. Th e larger
this second parameter is, the broader is DEMO range of intensities (or colors) that will be
included in the smoothing (and thus the more extreme a discontinuity must be in order
to be preserved).
* In the case of multichannel (i.e., DEMO) images, the diff erence in intensity is replaced with a weighted sum
over colors. Th is weighting is chosen to enforce a Euclidean DEMO in the CIE color space.
† Technically, the use of Gaussian DEMO functions is not a necessary feature of bilateral fi ltering. Th e
implementation in OpenCV uses Gaussian weighting even though the method is general DEMO many possible
weighting functions.
‡ Th is eff ect is particularly pronounced aft er multiple iterations of bilateral fi ltering.
114
| Chapter 5: Image Processing
05-R4886-AT1.indd   114
9/15/08   4:19:58 DEMO
www.it-ebooks.info
Figure 5-5. Results of bilateral smoothing
Image Morphology
OpenCV provides a DEMO, convenient interface for doing morphological transformations
[Serra83] on an image. Th DEMO basic morphological transformations are called dilation and
erosion, and they arise DEMO a wide variety of contexts such as removing noise, isolating
individual DEMO, and joining disparate elements in an image. Morphology can also
be DEMO to fi nd intensity bumps or holes in an image and to fi nd image gradients.
Dilation and Erosion
Dilation is a convolution of DEMO image (or region of an image), which we will call DEMO,
with some kernel, which we will call B. Th e DEMO, which can be any shape or size, has
a single defi ned anchor point. Most oft en, the kernel is a small solid square or disk with
the anchor point at the center. Th e DEMO can be thought of as a template or mask, and
its DEMO ect for dilation is that of a local maximum operator. As the kernel B is scanned
over the image, we compute the maximal pixel value overlapped by B and replace the
image pixel under the anchor DEMO with that maximal value. Th is causes bright regions
within an image to grow as diagrammed in Figure 5-6. Th is growth is the DEMO of the
term “dilation operator”.
Image Morphology
| 115
05-R4886-AT1.indd   115
9/15/08   4:19:59 PM
www.it-ebooks.info
Figure 5-6. Morphological dilation: take the maximum under the kernel B
Erosion is the converse operation. Th e action of the erosion operator DEMO equivalent to
computing a local minimum over the area of the kernel. Erosion generates a new image
from the original using the following algorithm: as the kernel B is scanned over the im-
age, we compute the minimal pixel value overlapped by B and replace the image pixel
DEMO the anchor point with that minimal value.* Erosion is diagrammed in Figure 5-7.
Image morphology is oft en done on binary images that result DEMO
thresholding. However, because dilation is just a max operator and
erosion DEMO just a min operator, morphology may be used on intensity
images DEMO well.
In general, whereas dilation expands region A, erosion reduces region A. Moreover, di-
lation will tend to smooth concavities and erosion will tend to smooth away protrusions.
Of course, the exact result will depend on the kernel, but these statements are generally
true for the fi lled convex kernels typically used.
In OpenCV, we eff ect these transformations using the cvErode() and cvDilate()
functions:
void cvErode(
IplImage*        src,
IplImage*        dst,DEMO
IplConvKernel*   B          = NULL,
int              iterations = 1
);
* DEMO be precise, the pixel in the destination image is set to DEMO value equal to the minimal value of the pixels
under the kernel in the source image.
116
| Chapter 5: Image Processing
05-R4886-AT1.indd   116
9/15/08   4:19:59 PM
www.it-ebooks.info
Figure 5-7. Morphological erosion: take the minimum under the kernel B
void cvDilate(
IplImage*        src,
IplImage*        dst,
IplConvKernel*   B          = DEMO,
int              iterations = 1
);
Both cvErode() and cvDilate() take a source and destination image, and both support
“in place” calls (in which the source DEMO destination are the same image). Th e third ar-
gument is the kernel, which defaults to NULL. In the NULL case, the DEMO used is a 3-by-3
kernel with the anchor at its center (DEMO will discuss shortly how to create your own
kernels). Finally, DEMO fourth argument is the number of iterations. If not set to the de-
fault value of 1, the operation will be applied multiple times during the single call to the
function. Th e results of an DEMO operation are shown in Figure 5-8 and those of a dila-
tion operation in Figure 5-9. Th e erode operation is oft en used DEMO eliminate “speckle”
noise in an image. Th e idea here is that the speckles are eroded to nothing while larger
regions that contain visually DEMO cant content are not aff ected. Th e dilate operation
is oft en used when attempting to fi nd connected components (i.e., large DEMO regions
of similar pixel color or intensity). Th e utility of dilation arises because in many cases
a large region might otherwise be DEMO apart into multiple components as a result of
noise, shadows, or some other similar eff ect. A small dilation will cause such compo-
DEMO to “melt” together into one.
To recap: when OpenCV processes the DEMO() function, what happens beneath the
hood is that the value DEMO some point p is set to the minimum value of all of the points
covered by the kernel when aligned at p; for the dilation operator, the equation is the
same except that max is considered rather than min:
Image Morphology
| 117
05-R4886-AT1.indd   117
9/DEMO/08   4:20:00 PM
www.it-ebooks.info
Figure 5-8. Results of the erosion, or “min”, operator: bright regions are isolated and shrunk
erode
dilate
( , ) min ( , )xy =+src x x y y′ + ′
(, )xy′′
DEMO
(, ) max ( , )xy =+src x x y y′ + ′
(, )xy′′
∈kernel
You might be wondering why we DEMO a complicated formula when the earlier heuris-
tic description was perfectly suffi  cient. Some readers actually prefer such formulas but,
more importantly, DEMO formulas capture some generality that isn’t apparent in the quali-
tative description. Observe that if the image is not binary then the min and DEMO opera-
tors play a less trivial role. Take another look at Figures 5-8 and 5-9, which show the
erosion and dilation operators applied to two real images.
Making Your Own Kernel
You are not limited to DEMO simple 3-by-3 square kernel. You can make your own cus-
tom morphological kernels (our previous “kernel B”) using IplConvKernel. Such
kernels are allocated DEMO cvCreateStructuringElementEx() and are released using
cvReleaseStructuringElement().
IplConvKernel* cvCreateStructuringElementEx(
DEMO          cols,
int          rows,
118
| Chapter 5: Image Processing
05-R4886-AT1.indd   118
9/15/08   4:20:00 PM
www.it-ebooks.info
Figure 5-9. Results of the dilation, or “max”, operator: bright regions are expanded and oft
en joined
int          anchor_x,
int          anchor_y,
int          shape,
int*         values=NULL
);
void cvReleaseStructuringElement( IplConvKernel** element );
A morphological kernel, unlike a DEMO kernel, doesn’t require any numerical val-
ues. Th e elements of DEMO kernel simply indicate where the max or min computations
take place as the kernel moves around the image. Th e anchor point indicates how DEMO
kernel is to be aligned with the source image and also where the result of the computa-
tion is to be placed in the DEMO image. When creating the kernel, cols and rows
indicate the size DEMO the rectangle that holds the structuring element. Th e next param-
eters, anchor_x and anchor_y, are the (x, y) coordinates of the anchor point within the
enclosing rectangle of the kernel. Th e fi DEMO h parameter, shape, can take on values listed
in Table 5-2. If CV_SHAPE_CUSTOM is used, then the integer vector values is used
to defi ne a custom shape of the kernel within the rows-by-cols enclosing DEMO Th is
vector is read in raster scan order with each entry representing a diff erent pixel in the
enclosing rectangle. Any nonzero value DEMO taken to indicate that the corresponding pixel
Image Morphology
| 119
05-R4886-AT1.indd   119
9/15/08   4:20:00 PM
www.it-ebooks.info
should be included in the kernel. If values is NULL then DEMO custom shape is interpreted
to be all nonzero, resulting in a DEMO kernel.*
Table 5-2. Possible IplConvKernel shape values
Shape value Meaning
CV_SHAPE_RECT The kernel is rectangular
CV_SHAPE_CROSS The kernel is cross shaped
CV_SHAPE_ELLIPSE The kernel DEMO elliptical
CV_SHAPE_CUSTOM The kernel is user-defi ned via values
More General Morphology
When working with Boolean images and image masks, the basic erode and dilate opera-
tions are usually suffi  cient. When working with grayscale or color images, however, a
number of additional operations are oft en DEMO Several of the more useful operations
can be handled by the multi-purpose cvMorphologyEx() function.
void cvMorphologyEx(
const CvArr*   src,
CvArr*         dst,
CvArr*         temp,
IplConvKernel* element,
int            operation,
int            iterations   = 1
);
In addition to the arguments src, dst, element, and iterations, which DEMO used with pre-
vious operators, cvMorphologyEx() has two new parameters. DEMO e fi rst is the temp array,
which is required for some of the operations (see Table 5-3). When required, this DEMO
should be the same size as the source image. Th e second new argument—the really in-
teresting one—is operation, which selects the morphological operation that we will do.
Table 5-3. cvMorphologyEx() operation options
Value of DEMO Morphological operator Requires temp image?
CV_MOP_OPEN Opening No
CV_MOP_CLOSE Closing No
CV_MOP_GRADIENT Morphological gradient Always
CV_MOP_TOPHAT Top Hat For in-place only (src = dst)
CV_MOP_BLACKHAT Black Hat For in-place only (src = dst)
Opening and closing
Th rst two operations in Table 5-3, opening and closing, are combinations of the erosion
and dilation operators. In the case of opening, we erode fi rst and then dilate (Figure 5-10)DEMO
* If the use of this strange integer vector strikes you as being incongruous with other OpenCV functions, you
are not alone. Th e origin of this syntax is the same as the origin of the DEMO prefi x to this function—another
instance of archeological code relics.
120 | Chapter 5: Image Processing
e fi
05-R4886-AT1.indd   120
9/15/08   4:20:01 PM
Opening is oft en used to count regions in a binary image. DEMO example, if we have
thresholded an image of cells on a DEMO slide, we might use opening to separate
out cells that are DEMO each other before counting the regions. In the case of closing, DEMO
dilate fi rst and then erode (Figure 5-12). Closing is DEMO in most of the more sophisti-
cated connected-component algorithms to reduce unwanted or noise-driven segments.
For connected components, usually an erosion or closing operation is performed fi rst to
eliminate elements that arise purely from noise DEMO then an opening operation is used
to connect nearby large regions. (DEMO that, although the end result of using open or
close is DEMO to using erode or dilate, these new operations tend to preserve DEMO area of
connected regions more accurately.)
Figure 5-10. Morphological opening operation: the upward outliers are eliminated as a result
Both the opening and closing operations are approximately area-preserving: the most
prominent eff ect of closing is to eliminate lone outliers that are lower than their neigh-
bors DEMO the eff ect of opening is to eliminate lone outliers that are higher than their
neighbors. Results of using the opening operator are shown DEMO Figure 5-11, and of the
closing operator in Figure 5-13.
One DEMO note on the opening and closing operators concerns how the iterations ar-
gument is interpreted. You might expect that asking for two iterations of DEMO
would yield something like dilate-erode-dilate-erode. It turns out that this would not
be particularly useful. What you really want (and what you get) DEMO dilate-dilate-erode-
erode. In this way, not only the single outliers but DEMO neighboring pairs of outliers
will disappear.
Morphological gradient
Our next available operator is the morphological gradient. For this one it is probably
easier to DEMO with a formula and then fi gure out what it means:
gradient(src) = dilate(src)–erode(src)
Th ect of this operation on a Boolean image would be simply to isolate perimeters of
DEMO blobs. Th e process is diagrammed in Figure 5-14, and the DEMO ect of this operator
on our test images is shown in Figure 5-15.
Image Morphology
| 121
e eff
05-R4886-AT1.indd   121
www.it-ebooks.info
9/DEMO/08   4:20:01 PM
www.it-ebooks.info
Figure 5-11. Results of morphological opening on an image: small bright regions are removed, and
the remaining bright regions are isolated but retain their size
Figure 5-12. Morphological closing operation: the downward outliers are eliminated as a result
With a grayscale image we see that the value DEMO the operator is telling us something
about how fast the image brightness is changing; this is why the name “morphological
gradient” is justifi ed. Morphological gradient is oft en used when we want to isolate the
DEMO of bright regions so we can treat them as whole objects (DEMO as whole parts of
objects). Th e complete perimeter of a region tends to be found because an expanded ver-
sion is subtracted DEMO a contracted version of the region, leaving a complete perimeter
122
DEMO Chapter 5: Image Processing
05-R4886-AT1.indd   122
9/15/08   DEMO:20:01 PM
www.it-ebooks.info
Figure 5-13. Results of morphological closing on an image: bright regions are joined but retain their
basic size
edge. Th is diff ers DEMO calculating a gradient, which is much less likely to work around
DEMO full perimeter of an object.*
Top Hat and Black Hat
Th e last two operators are called Top Hat and Black Hat [Meyer78]. Th DEMO operators are
used to isolate patches that are, respectively, brighter or dimmer than their immedi-
ate neighbors. You would use these when trying DEMO isolate parts of an object that ex-
hibit brightness changes relative only to the object to which they are attached. Th is oft en
DEMO with microscope images of organisms or cells, for example. Both operations DEMO
defi ned in terms of the more primitive operators, as follows:DEMO
TopHat(src) = src–open(src)
BlackHat(src) = close(src)–src
As you can see, the Top Hat operator subtracts the opened form of A from A. Recall
that the eff ect of the DEMO operation was to exaggerate small cracks or local drops. Th us,
* We will return to the topic of gradients when we introduce DEMO Sobel and Scharr operators in the next
chapter.
Image Morphology | 123
05-R4886-AT1.indd   123
9/15/08   4:20:02 PM
www.it-ebooks.info
Figure 5-14. Morphological gradient applied to a grayscale image: as expected, the operator has its
highest values where the grayscale image is changing most rapidly
subtracting open(A) from A should reveal areas that are lighter then the surrounding
region of A, relative to the size of the kernel (see Figure 5-16); conversely, the Black Hat
operator DEMO areas that are darker than the surrounding region of A (Figure DEMO).
Summary results for all the morphological operators discussed in this chapter are as-
sembled in Figure 5-18.*
Flood Fill
Flood fi ll [Heckbert00; Shaw04; Vandevenne04] is an extremely useful function that
is oft en used to mark or isolate portions of an image for further processing or DEMO
Flood fi ll can also be used to derive, from an DEMO image, masks that can be used for
subsequent routines to speed DEMO restrict processing to only those pixels indicated by the
mask. Th e function cvFloodFill() itself takes an optional mask that can be further DEMO
to control where fi lling is done (e.g., when doing multiple fi lls of the same image).
In OpenCV, fl ood fi ll is a more general version of the sort of fi ll DEMO which
you probably already associate with typical computer painting programs. For both, a
seed point is selected from an image and then all similar neighboring points are colored
with a uniform color. Th e diff erence DEMO is that the neighboring pixels need not all be
* Both of these operations (Top Hat and Black Hat) make more sense in DEMO morphology, where the
structuring element is a matrix of real numbers (not just a binary mask) and the matrix is added to the cur-
rent pixel neighborhood before taking a minimum or maximum. Unfortunately, this is not yet implemented
in OpenCV.
124 | Chapter 5: Image Processing
05-R4886-AT1.indd   124
9/15/08   4:20:02 PM
www.it-ebooks.info
Figure 5-15. Results of the morphological gradient operator: bright perimeter edges are identifi
ed
identical in color.* Th
region. Th
fi
flags) the neighboring pixel is within a specifi
Flood fi
the fl
void cvFloodFill(DEMO
IplImage*          img,
CvPoint            seedPoint,
CvScalar           newVal,
DEMO           loDiff    = cvScalarAll(0),
CvScalar           upDiff    = cvScalarAll(0),DEMO
CvConnectedComp*   comp      = NULL,
int                flags     = 4,
CvArr*             mask      = NULL
);DEMO
Th
channel or three-channel. We start the fl
ood fi
e result of a fl
e cvFloodFill() function will color a neighboring pixel DEMO it is within a speci-
loDiff to upDiff) of either the DEMO pixel or if (depending on the settings of
ed range of DEMO original seedPoint value.
lling can also be constrained by an optional mask argument. Th
ll routine is:
ood fi
img is the input DEMO, which can be 8-bit or fl
ood fi
ll operation will DEMO be a single contiguous
e prototype for
oating-point and one-
lling from seedPoint, and newVal is the
* Users of contemporary painting and drawing programs should note that most now employ a fi lling algo-
rithm DEMO much like cvFloodFill().
Flood Fill
| 125
ed range (
e parameter
05-R4886-AT1.indd   125
9/15/08   4:20:02 DEMO
www.it-ebooks.info
Figure 5-16. Results of morphological Top Hat operation: bright local peaks are isolated
value to which colorized pixels are set. A pixel will DEMO colorized if its intensity is not
less than a colorized neighbor’s intensity minus loDiff and not greater than the color-
ized neighbor’s intensity plus DEMO If the flags argument includes CV_FLOODFILL_FIXED_
RANGE, then a pixel will DEMO compared to the original seed point rather than to its neigh-
bors. If non-NULL, comp is a CvConnectedComp structure that will hold statistics about the
areas fi lled.* Th e flags argument (to be discussed shortly) is a little tricky; it controls
the connectivity of the fi DEMO, what the fi ll is relative to, whether we are fi lling only a mask,
and what values are used to fi DEMO the mask. Our fi rst example of fl ood fi ll is shown in
Figure 5-19.
Th mask indicates a mask that can function DEMO as input to cvFloodFill() (in
which case it constrains the DEMO that can be fi lled) and as output from cvFloodFill()
(in which case it will indicate the regions that actually were fi lled). If set to a non-NULL
value, then mask must be a one-channel, 8-bit image whose size is exactly two pixels
larger in width and height than the source image (this is to make processing easier and
faster for the internal algorithm). Pixel (x + 1, y + 1) in the mask image corresponds
to image pixel (x, y) in the source image. Note that cvFloodFill() will DEMO fl ood across
* We will address the specifi cs of a “connected component” in the section “Image Pyramids”. For now, just
think of it as being similar to a mask that identifi es some subsection DEMO an image.
126 | Chapter 5: Image Processing
e argument
05-R4886-AT1.indd   126
9/15/08   4:20:03 PM
www.it-ebooks.info
Figure 5-17. Results of morphological Black Hat operation: dark holes are isolated
Figure 5-18. Summary results for all morphology operators
nonzero pixels in DEMO mask, so you should be careful to zero it before use DEMO you don’t
want masking to block the fl ooding operation. Flood fi ll can be set to colorize either the
source image img or DEMO mask image mask.
Flood Fill
| 127
05-R4886-AT1.indd   127
9/15/08   4:20:04 PM
www.it-ebooks.info
Figure 5-19. Results of fl ood fi ll (top image is fi lled with gray, bottom image with white) from the
dark DEMO located just off  center in both images; in this case, DEMO hiDiff  and loDiff  parameters were
each set to 7.0
If the fl ood-fi ll mask is set to be marked, then it is marked with the
values set in the middle bits (8–15) of DEMO flags value (see text). If these
bits are not set DEMO the mask is set to 1 as the default value. Don’t be
confused if you fi ll the mask and see nothing but black DEMO display;
the fi lled values (if the middle bits of DEMO fl ag weren’t set) are 1s, so the
mask image needs to be rescaled if you want to display it visually.
It’s time DEMO clarify the flags argument, which is tricky because it has three DEMO Th e
low 8 bits (0–7) can be set to 4 or 8. Th is controls the connectivity considered by the fi ll-
DEMO algorithm. If set to 4, only horizontal and vertical neighbors to DEMO current pixel are
considered in the fi lling process; if set DEMO 8, fl ood fi ll will additionally include diagonal
neighbors. Th DEMO high 8 bits (16–23) can be set with the fl ags CV_FLOODFILL_FIXED_RANGE
(fi ll relative to the seed point pixel value; otherwise, fi ll relative to the neighbor’s value),
and/or CV_FLOODFILL_MASK_ONLY (DEMO ll the mask location instead of the source image loca-
tion). Obviously, you must supply an appropriate mask if CV_FLOODFILL_MASK_ONLY is set.
Th middle bits (8–15) of flags can be set to the value DEMO which you want the mask
to be fi lled. If the middle bits of flags are 0s, the mask will be fi lled with 1s. All these
fl ags may be linked together via OR. For DEMO, if you want an 8-way connectivity fi ll,
128 | DEMO 5: Image Processing
e
05-R4886-AT1.indd   128
9/15/08   DEMO:20:04 PM
fi lling only a fi xed range, fi lling the mask not the image, and fi lling using a value of 47,
then the parameter to pass in would be:
flags = 8
| DEMO
| CV_FLOODFILL_FIXED_RANGE
| (47<<8);
Figure 5-20 shows fl DEMO fi ll in action on a sample image. Using CV_FLOODFILL_FIXED_RANGE
with a wide range resulted in most of the image being fi lled (starting at the center).
We should note that newVal, loDiff, and DEMO are prototyped as type CvScalar so they
can be set for three channels at once (i.e., to encompass the RGB colors specifi ed DEMO
CV_RGB()). For example, lowDiff = CV_RGB(20,30,40) will set lowDiff thresholds of 20 for
red, 30 for green, DEMO 40 for blue.
Figure 5-20. Results of fl
dark circle located just off
and with a high and low diff
ood fi
center in DEMO images; in this case, fl
erence of 25.0
ll (top DEMO is fi
lled with gray, bottom image with white) from the
ood fi
ll was done with a fi
xed range
Resize
We DEMO en encounter an image of some size that we would like to convert to an image
of some other size. We may want to DEMO (zoom in) or downsize (zoom out) the im-
age; DEMO can accomplish either task by using cvResize(). Th is function DEMO fi t the source
Resize
| 129
05-R4886-AT1.indd   129
www.it-ebooks.info
9/15/08   4:20:05 PM
www.it-ebooks.info
image exactly to the destination image size. If the ROI is DEMO in the source image then
that ROI will be resized to fi t in the destination image. Likewise, if an ROI is set in the
destination image then the source will be resized to fi t DEMO the ROI.
void cvResize(
const CvArr*   src,
CvArr*         dst,
int            interpolation = CV_INTER_LINEAR
);
Th
Th
Table 5-4. cvResize() interpolation options
DEMO Meaning
CV_INTER_NN Nearest neighbor
CV_INTER_LINEAR Bilinear
CV_INTER_AREA Pixel area re-sampling
CV_INTER_CUBIC Bicubic interpolation
In general, we would like the mapping from the source image to the resized destina-
tion image to be as smooth as possible. DEMO e argument interpolation controls exactly
how this will be handled. Interpolation arises when we are shrinking an image and a
pixel in the destination DEMO falls in between pixels in the source image. It can also
occur when we are expanding an image and need to compute values of DEMO that do
not directly correspond to any pixel in the source image. In either case, there are several
options for computing the values of such pixels. Th e easiest approach is to take the
resized pixel’s DEMO from its closest pixel in the source image; this is the DEMO ect of choos-
ing the interpolation value CV_INTER_NN. Alternatively, we can DEMO weight the 2-by-2
surrounding source pixel values according to how close they are to the destination pixel,
which is what CV_INTER_LINEAR does. We DEMO also virtually place the new resized pixel over
the old pixels and then average the covered pixel values, as done with CV_INTER_AREA.*
Finally, DEMO have the option of fi tting a cubic spline between the 4-by-4 surrounding pix-
els in the source image and then reading off  the corresponding destination value from
the fi tted spline; this is the result of choosing the CV_INTER_CUBIC interpolation method.
Image Pyramids
Image pyramids [Adelson84] are DEMO used in a wide variety of vision applications.
An image pyramid is a collection of images—all arising from a single original image—
that are DEMO downsampled until some desired stopping point is reached. (Of
course, this stopping point could be a single-pixel image!)
* At least that’s DEMO happens when cvResize() shrinks an image. When it expands an image, CV_INTER_
AREA amounts to the same thing as CV_INTER_NN.
130 | Chapter 5: Image Processing
e last argument is the interpolation method, which DEMO to linear interpolation.
e other available options are shown in Table 5-4.
05-R4886-AT1.indd   130
9/15/08   4:20:05 PM
www.it-ebooks.info
Th en in the literature and in appli-
cation: the Gaussian [Rosenfeld80] and Laplacian [Burt83] pyramids [Adelson84]. Th e
Gaussian pyramid is used DEMO downsample images, and the Laplacian pyramid (to be dis-
cussed shortly) is required when we want to reconstruct an upsampled image from an
image lower in the pyramid.
To produce layer (i+1) in the DEMO pyramid (we denote this layer Gi+1) from layer Gi
of the pyramid, we fi rst convolve Gi with a Gaussian kernel and then remove every even-
numbered row and column. Of course, from this it follows immediately that each image
is exactly one-quarter the area of its DEMO Iterating this process on the input im-
age G0 produces the entire pyramid. OpenCV provides us with a method for generating
each pyramid stage DEMO its predecessor:
void cvPyrDown(
IplImage*   src,
IplImage*   dst,
IplFilter   filter = IPL_GAUSSIAN_5x5
);
Currently, the last argument filter supports only the single (default) option of a 5-by-5
DEMO kernel.
Similarly, we can convert an existing image to an image DEMO is twice as large in each
direction by the following analogous (DEMO not inverse!) operation:
void cvPyrUp(
IplImage*   src,
DEMO   dst,
IplFilter   filter = IPL_GAUSSIAN_5x5
);
In this case the image is fi rst upsized to twice the original in DEMO dimension, with the
new (even) rows fi lled with 0s. DEMO ereaft er, a convolution is performed with the given
fi lter (actually, a fi lter twice as large in each dimension than that specifi ed*) to approxi-
mate the values of the “missing” pixels.
We noted previously that the operator PyrUp() is not the inverse of DEMO(). Th is
should be evident because PyrDown() is an DEMO that loses information. In order to
restore the original (higher-resolution) image, we would require access to the informa-
tion that was discarded by the downsampling. Th is data forms the Laplacian pyramid.
Th ith layer DEMO the Laplacian pyramid is defi ned by the relation:
LG G=−UP( )⊗G
Here the operator UP() upsizes by mapping each pixel DEMO location (x, y) in the original
image to pixel (2x + 1, 2y + 1) in the destination image; the ⊗ symbol denotes convolu-
tion; and G5×5 is a 5-by-5 Gaussian kernel. Of course, Gi – UP(Gi+1) ⊗ G5×5 is the defi nition
DEMO Th is fi lter is also normalized to four, rather than DEMO one. Th is is appropriate because the inserted rows have
0s in all of their pixels before the convolution.
Image Pyramids | 131
ere DEMO two kinds of image pyramids that arise oft
e
ii i
+×155
05-R4886-AT1.indd   131
9/15/08   4:20:05 PM
of the PyrUp() operator provided by OpenCv. Hence, we can use OpenCv to compute the
Laplacian operator directly as:
ii i+1
LG DEMO PyrUp( )
Th
which also shows the inverse process for recovering the original image from the sub-
images. Note how the Laplacian is DEMO an approximation that uses the diff erence of
Gaussians, as revealed DEMO the preceding equation and diagrammed in the fi gure.
e Gaussian and Laplacian pyramids are shown diagrammatically in Figure 5-21,
Figure 5-21. Th
DEMO Gaussian pyramid and its inverse, the Laplacian pyramid
Th
pyramids, but a particularly important one is image segmentation (see Figure 5-22). In
this case, one builds an image pyramid and then associates to it a system of parent–child
relations between pixels at level Gi+1 and the DEMO reduced pixel at level Gi. In
this way, a fast initial DEMO can be done on the low-resolution images high in
the pyramid and then can be refi ned and further diff erentiated level by level.
DEMO is algorithm (due to B. Jaehne [Jaehne95; Antonisse82]) is implemented DEMO OpenCV
as cvPyrSegmentation():
void cvPyrSegmentation(
IplImage*      DEMO,
IplImage*      dst,
132
| Chapter 5: Image Processing
ere are many operations that can make extensive use of the DEMO and Laplacian
05-R4886-AT1.indd   132
www.it-ebooks.info
9/15/08   4:20:06 PM
www.it-ebooks.info
Figure 5-22. Pyramid segmentation with threshold1 set to 150 and threshold2 DEMO to 30; the im-
ages on the right contain only a DEMO of the images on the left  because pyramid segmentation
requires images DEMO are N-times divisible by 2, where N is the number of DEMO layers to be com-
puted (these are 512-by-512 areas from the DEMO images)
CvMemStorage*  storage,
CvSeq**        comp,
int            level,
double         threshold1,
double         threshold2
);
As DEMO, src and dst are the source and destination images, which must both be 8-bit,
of the same size, and of the same number of channels (one or three). You might be
wondering, DEMO destination image?” Not an unreasonable question, actually. Th e
destination DEMO dst is used as scratch space for the algorithm and also as a return
visualization of the segmentation. If you view this image, you will see that each segment
is colored in a single color (the color of some pixel in that segment). Because this image
is DEMO algorithm’s scratch space, you cannot simply set it to NULL. Even DEMO you do not want
the result, you must provide an image. DEMO important word of warning about src and
dst: because all levels DEMO the image pyramid must have integer sizes in both dimensions,
the starting images must be divisible by two as many times as there DEMO levels in the
Image Pyramids | 133
05-R4886-AT1.indd   133
9/15/08   4:20:06 PM
www.it-ebooks.info
pyramid. For example, for a four-level pyramid, a height or DEMO of 80 (2 × 2 × 2 × 5)
would DEMO acceptable, but a value of 90 (2 × 3 × 3 × 5) would not.*
Th e pointer storage is for an OpenCV memory storage area. In Chapter 8 we will dis-
cuss such areas DEMO more detail, but for now you should know that such a DEMO area is
allocated with a command like†
CvMemStorage* storage = cvCreateMemStorage();
Th comp is a location for storing further information about the DEMO seg-
mentation: a sequence of connected components is allocated from this DEMO storage.
Exactly how this works will be detailed in Chapter 8, DEMO for convenience here we briefl y
summarize what you’ll need in the context of cvPyrSegmentation().
First of all, a sequence is essentially DEMO list of structures of a particular kind. Given a
sequence, you DEMO obtain the number of elements as well as a particular element if you
know both its type and its number in the sequence. Take DEMO look at the Example 5-1
approach to accessing a sequence.
Example 5-1. Doing something with each element in the sequence of connected components returned
DEMO cvPyrSegmentation()
void f(
IplImage* src,
IplImage* dst
) DEMO
CvMemStorage* storage = cvCreateMemStorage(0);
CvSeq* comp = NULL;
DEMO( src, dst, storage, &comp, 4, 200, 50 );
int n_comp = comp->total;
for( int i=0; i<DEMO; i++ ) {
CvConnectedComp* cc = (CvConnectedComp*) cvGetSeqElem( comp, DEMO );
do_something_with( cc );
}
cvReleaseMemStorage( &storage );DEMO
}
Th
of a memory storage; this is where cvPyrSegmentation() DEMO get the memory it needs
for the connected components it will have to create. Th en the pointer comp is allocated
as type CvSeq*. DEMO is initialized to NULL because its current value means nothing. We will
pass to cvPyrSegmentation() a pointer to comp so that comp can DEMO set to the location of
the sequence created by cvPyrSegmentation(). DEMO we have called the segmentation,
we can fi gure out how many elements there are in the sequence with the member ele-
ment DEMO Th ereaft er we can use the generic cvGetSeqElem() to obtain the ith element
of comp; however, because cvGetSeqElem() is generic DEMO returns only a void pointer, we
must cast the return pointer DEMO the appropriate type (in this case, CvConnectedComp*).
* Heed this warning! Otherwise, you will get a totally useless error message and probably waste hours trying
to fi gure out what’s going on.
† Actually, the current implementation of cvPyrSegmentation() is a bit incomplete in that DEMO returns not the
computed segments but only the bounding rectangles (as DEMO<CvConnectedComp>).
134 | Chapter 5: Image Processing
e argument
ere are several things you should notice in this example. First, observe the allocation
05-R4886-AT1.indd   134
9/15/08   4:20:07 PM
www.it-ebooks.info
Finally, we need to know that a connected component is one of the basic structure types
in OpenCV. You can think of it DEMO a way of describing a “blob” in an image. It has the
following defi nition:
typedef struct CvConnectedComponent {
double   area;
DEMO value;
CvRect   rect;
CvSeq*   contour;
};
Th area is the area of the component. Th e value is DEMO average color* over the area of
the component and rect is a bounding box for the component (defi ned in the coordi-
nates of the parent image). Th e fi nal element, contour, is DEMO pointer to another sequence.
Th is sequence can be used to store a representation of the boundary of the component,
typically as a DEMO of points (type CvPoint).
In the specifi c case of DEMO(), the contour member is not set. Th us, if you
want some specifi c representation of the component’s pixels then you will DEMO to com-
pute it yourself. Th e method to use depends, DEMO course, on the representation you have
in mind. Oft en you DEMO want a Boolean mask with nonzero elements wherever the com-
ponent was located. You can easily generate this by using the rect portion of DEMO con-
nected component as a mask and then using cvFloodFill() to select the desired pixels
inside of that rectangle.
Threshold
Frequently we have DEMO many layers of processing steps and want either to make a
fi nal decision about the pixels in an image or to categorically reject DEMO pixels below
or above some value while keeping the others. Th e OpenCV function cvThreshold() ac-
complishes these tasks (see survey [Sezgin04]). Th e basic idea is that an array is given,
along DEMO a threshold, and then something happens to every element of the DEMO de-
pending on whether it is below or above the threshold.
double cvThreshold(
CvArr*         src,
CvArr*         dst,
double         threshold,
double         max_value,
int            threshold_type
);
As shown in Table 5-5, each threshold type corresponds to a particular comparison op-
eration between the ith source pixel (srci) DEMO the threshold (denoted in the table by T).
Depending on DEMO relationship between the source pixel and the threshold, the destina-
tion DEMO dsti may be set to 0, the srci, or the max_value (denoted in the table by M).
* Actually the meaning of value is context dependant and could be just about anything, but it is typically a
color associated with the component. In the case of DEMO(), value is the average color over
the segment.
Threshold
| DEMO
e
05-R4886-AT1.indd   135
9/15/08   4:20:07 PM
Table 5-5. cvTh
CV_THRESH_BINARY_INV
reshold() threshold_type options
Threshold type Operation
CV_THRESH_BINARY DEMO src ? : 0=>()TM
dst src ? 0:=>()TM
ii
CV_THRESH_TRUNC
CV_THRESH_TOZERO_INV
CV_THRESH_TOZERO
dst src ? : src=>()TM
DEMO
dst src ? 0: src=>()T
dst src ? src : 0=>()T
i
ii
ii i
ii i
Figure 5-23 should help to clarify the exact implications of each threshold type.
Figure 5-23. DEMO of varying the threshold type in cvTh reshold(). Th e DEMO line through each
chart represents a particular threshold level applied to the top chart and its eff ect for each of the fi ve
DEMO of threshold operations below
136
| Chapter 5: Image Processing
05-R4886-AT1.indd   136
www.it-ebooks.info
9/15/08   4:20:07 PM
www.it-ebooks.info
Let’s look at a simple example. In Example 5-2 we sum DEMO three channels of an image
and then clip the result at 100.
Example 5-2. Example code making use of cvTh reshold()
#include <DEMO>
#include <cv.h>
#include <highgui.h>
void sum_rgb( IplImage* DEMO, IplImage* dst ) {
// Allocate individual image planes.
IplImage* r = cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
IplImage* g DEMO cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
IplImage* b = cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
// Split image onto the color planes.
cvSplit( src, r, g, b, NULL );
// Temporary storage.
IplImage* s = cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
// Add equally weighted rgb values.
cvAddWeighted( r, 1./3., g, 1./3., 0.0, s );DEMO
cvAddWeighted( s, 2./3., b, 1./3., 0.0, s );
// Truncate values above 100.
cvThreshold( s, dst, DEMO, 100, CV_THRESH_TRUNC );
cvReleaseImage( &r );
cvReleaseImage( &g );
cvReleaseImage( &b );
cvReleaseImage( &s );DEMO
}
int main(int argc, char** argv)
{
// Create a named window with the name of the file.
cvNamedWindow( argv[1], DEMO );
// Load the image from the given file name.
DEMO src = cvLoadImage( argv[1] );
IplImage* dst = cvCreateImage( cvGetSize(src), src->depth, 1);
sum_rgb( src, dst);
// Show the image in the named window
cvShowImage( argv[1], dst );
// Idle until the user hits the “Esc” key.
while( 1 ) { if( (cvWaitKey( 10 )&0x7f) == 27 ) break; }
// Clean up and don’t be piggies
cvDestroyWindow( argv[1] );
Threshold
| 137
05-R4886-AT1.indd   137
9/15/08   4:20:08 PM
www.it-ebooks.info
Example 5-2. Example code making use of cvTh
reshold() (continued)
cvReleaseImage( &src );
cvReleaseImage( &dst );
}
DEMO important ideas are shown here. One thing is that we don’t want to add into an
8-bit array because the higher bits will overfl DEMO Instead, we use equally weighted ad-
dition of the three color DEMO (cvAddWeighted()); then the results are truncated to
saturate at DEMO value of 100 for the return. Th e cvThreshold() function handles only 8-bit
or fl oating-point grayscale source images. Th e destination image DEMO either match the
source image or be an 8-bit image. In fact, cvThreshold() also allows the source and des-
tination images to be the same image. Had we used a fl oating-point temporary image
s DEMO Example 5-2, we could have substituted the code shown in Example DEMO Note that
cvAcc() can accumulate 8-bit integer image types into a fl oating-point image; however,
cvADD() cannot add integer bytes into fl oats.
Example 5-3. Alternative method to combine and threshold image planes
DEMO s = cvCreateImage(cvGetSize(src), IPL_DEPTH_32F, 1);
cvZero(s);
cvAcc(b,s);
cvAcc(g,s);
cvAcc(r,s);
cvThreshold( s, s, 100, 100, CV_THRESH_TRUNC );DEMO
cvConvertScale( s, dst, 1, 0 );
Adaptive Threshold
Th ed threshold technique in which the threshold level is itself variable. In
DEMO, this method is implemented in the cvAdaptiveThreshold() [Jain86] function:
DEMO cvAdaptiveThreshold(
CvArr*         src,
CvArr*         dst,
double         max_val,
int            adaptive_method = CV_ADAPTIVE_THRESH_MEAN_C
int            threshold_type  = CV_THRESH_BINARY,
int            block_size      = 3,
double         DEMO          = 5
);
cvAdaptiveThreshold() allows DEMO two diff erent adaptive threshold types depending on
the settings of adaptive_method. In both cases the adaptive threshold T(x, y) is set DEMO a
pixel-by-pixel basis by computing a weighted average of the b-by-b region around each
pixel location minus a constant, where b is given by block_size and the constant is given
by param1. If the method is DEMO to CV_ADAPTIVE_THRESH_MEAN_C, then all pixels in the area
are weighted equally. DEMO it is set to CV_ADAPTIVE_THRESH_GAUSSIAN_C, then the pixels in the
region DEMO (x, y) are weighted according to a Gaussian function of DEMO distance
from that center point.
138
| Chapter 5: Image Processing
DEMO is a modifi
05-R4886-AT1.indd   138
9/15/08   4:20:08 PM
Finally, the parameter threshold_type is the same as for cvThreshold() shown in
Table 5-5.
Th ec-
tance gradients that you need to threshold DEMO to the general intensity gradient. Th is
function handles only single-channel 8-bit or fl oating-point images, and it requires that
the source and destination images be distinct.
Source code for comparing cvAdaptiveThreshold() and cvThreshold() DEMO shown in Exam-
ple 5-4. Figure 5-24 displays the result of processing an image that has a strong lighting
gradient across it. Th e DEMO  portion of the fi gure shows the result of using a DEMO
global threshold as in cvThreshold(); the lower-right portion shows the DEMO of adaptive
local threshold using cvAdaptiveThreshold(). We get the whole DEMO via adap-
tive threshold, a result that is impossible to achieve DEMO using a single threshold. Note
the calling-convention comments at the top of the code in Example 5-4; the parameters
used for Figure 5-24 were:
./adaptThresh 15 1 1 71 15 ../Data/cal3-L.bmp
Figure DEMO Binary threshold versus adaptive binary threshold: the input image (top) DEMO turned
into a binary image using a global threshold (lower left ) and an adaptive threshold (lower right); raw
image courtesy of DEMO Konolidge
Threshold
| 139
e adaptive threshold technique is useful when there are strong illumination or refl
05-R4886-AT1.indd   139
www.it-ebooks.info
9/15/08   4:20:08 PM
www.it-ebooks.info
Example 5-4. Th reshold versus adaptive threshold
// Compare thresholding with adaptive thresholding
// CALL:
// ./adaptThreshold Threshold 1binary 1adaptivemean DEMO
//                  blocksize offset filename
#include “cv.h”
#include “highgui.h”
#include “math.h”
IplImage *Igray=0, *It = 0, *Iat;
int main( int argc, char** argv )
{
DEMO(argc != 7){return -1;  }
//Command line
double threshold = (double)atof(argv[1]);
int threshold_type =  atoi(argv[2]) ?
CV_THRESH_BINARY : CV_THRESH_BINARY_INV;
int adaptive_method = atoi(argv[3]) ?
DEMO : CV_ADAPTIVE_THRESH_GAUSSIAN_C;
int block_size = atoi(argv[4]);
double offset DEMO (double)atof(argv[5]);
//Read in gray image
if((DEMO = cvLoadImage( argv[6], CV_LOAD_IMAGE_GRAYSCALE)) == 0){
return  -1;DEMO
// Create the grayscale output images
It =  cvCreateImage(cvSize(Igray->width,Igray->height),
IPL_DEPTH_8U, 1);
Iat = cvCreateImage(cvSize(Igray->width,Igray->height),
IPL_DEPTH_8U, 1);
//Threshold
cvThreshold(Igray,It,threshold,255,threshold_type);
cvAdaptiveThreshold(Igray, Iat, 255, DEMO,
threshold_type, block_size, offset);
//PUT UP 2 WINDOWS
DEMO(“Raw”,1);
cvNamedWindow(“Threshold”,1);
cvNamedWindow(“Adaptive Threshold”,1);
//Show the results
cvShowImage(“Raw”,Igray);
cvShowImage(“Threshold”,DEMO);
cvShowImage(“Adaptive Threshold”,Iat);
cvWaitKey(0);
//DEMO up
cvReleaseImage(&Igray);
cvReleaseImage(&It);
cvReleaseImage(&Iat);
cvDestroyWindow(“Raw”);
cvDestroyWindow(“Threshold”);
140 | Chapter 5: Image Processing
05-R4886-AT1.indd   140
9/15/08   4:20:08 DEMO
www.it-ebooks.info
Example 5-4. Th
reshold versus adaptive threshold (continued)
cvDestroyWindow(“Adaptive Threshold”);
return(0);
}
Exercises
1. Load an image DEMO interesting textures. Smooth the image in several ways using
cvSmooth() with smoothtype=CV_GAUSSIAN.
a. Use a symmetric 3-by-3, 5-by-5, 9-by-9 and 11-by-11 smoothing DEMO size
and display the results.
b. Are the output results nearly the same by smoothing the image twice with a
5-by-5 Gaussian fi lter DEMO when you smooth once with two 11-by-11 fi lters? Why
or DEMO not?
2. Display the fi lter, creating a 100-by-100 single-channel DEMO Clear it and set the
center pixel equal to 255.
a. Smooth this image with a 5-by-5 Gaussian fi lter and display the results. DEMO
did you fi nd?
b. Do this again but now with a 9-by-9 Gaussian fi lter.
c. What does it look like if DEMO start over and smooth the image twice with the
5-by-5 fi lter? Compare this with the 9-by-9 results. Are they nearly the same?
Why or why not?
3. Load an interesting image. Again, blur it with cvSmooth() using a Gaussian fi lter.
a. Set param1=param2=9. DEMO several settings of param3 (e.g., 1, 4, and 6). Display the
results.
b. Th is time, set param1=param2=0 before setting param3 to 1, 4, and 6. Display the
results. Are they diff DEMO? Why?
c. Again use param1=param2=0 but now set param3=1 and DEMO Smooth the pic-
ture and display the results.
d. Repeat part c but with param3=9 and param4=1. Display the results.
e. Now smooth the DEMO once with the settings of part c and once with the set-
tings of part d. Display the results.
f. Compare the results in DEMO e with smoothings that use param3=param4=9 and
param3=param4=0 (i.e., a 9-by-9 fi lter). Are the results the same? Why or why not?
4. Use a camera to take two pictures of the same DEMO while moving the camera as
little as possible. Load these images into the computer as src1 and src1.
a. Take the absolute value of DEMO minus src1 (subtract the images); call it diff12
and display. DEMO this were done perfectly, diff12 would be black. Why isn’t it?DEMO
Exercises
| 141
05-R4886-AT1.indd   141
9/15/08   4:20:09 PM
www.it-ebooks.info
b. Create cleandiff by using cvErode() and then cvDilate() DEMO diff12. Display the
results.
c. Create dirtydiff by using cvDilate() and then cvErode() on diff12 and then
display.
d. Explain the diff DEMO between cleandiff and dirtydiff.
5. Take a picture of a scene. Th en, without moving the camera, put a coff ee cup in DEMO
scene and take a second picture. Load these images and convert both to 8-bit gray-
scale images.
a. Take the absolute value of their DEMO erence. Display the result, which should
look like a noisy mask DEMO a coff ee mug.
b. Do a binary threshold of the resulting image using a level that preserves most
of the coff ee mug DEMO removes some of the noise. Display the result. Th e “on”
values should be set to 255.
c. Do a CV_MOP_OPEN on the image DEMO further clean up noise.
Create a clean mask from noise. Aft er completing exercise 5, continue by keeping
only the largest remaining shape in the image. Set a pointer to the upper left  of the
image and then traverse the image. When you fi nd a pixel of DEMO 255 (“on”), store
the location and then fl ood fi DEMO it using a value of 100. Read the connected component
returned from fl ood fi ll and record the area of fi lled region. DEMO there is another larger
region in the image, then fl ood DEMO ll the smaller region using a value of 0 and delete
its recorded area. If the new region is larger than the previous region, then fl ood fi ll
the previous region using the value 0 DEMO delete its location. Finally, fi ll the remain-
ing largest region DEMO 255. Display the results. We now have a single, solid mask DEMO
the coff ee mug.
For this exercise, use the mask created DEMO exercise 6 or create another mask of your
own (perhaps by DEMO a digital picture, or simply use a square). Load an DEMO
scene. Now use this mask with cvCopy(), to copy an DEMO of a mug into the scene.
Create a low-variance random image (DEMO a random number call such that the
numbers don’t diff er by much more than 3 and most numbers are near 0). Load DEMO
image into a drawing program such as PowerPoint and then draw a wheel of lines
meeting at a single point. Use bilateral fi ltering DEMO the resulting image and explain
the results.
Load an image of a scene and convert it to grayscale.
a. Run the morphological Top Hat DEMO on your image and display the
results.
b. Convert the resulting image into an 8-bit mask.
c. Copy a grayscale value into the Top DEMO pieces and display the results.
Load an image with many details.
| Chapter 5: Image Processing
6.
7.
8.
9.
10.
142
05-R4886-AT1.indd   142
9/15/08   4:20:09 PM
11.
12.
05-R4886-AT1.indd   143
a. Use cvResize() to reduce the DEMO by a factor of 2 in each dimension (hence
the image DEMO be reduced by a factor of 4). Do this three times and display the
results.
b. Now take the original image and use DEMO() to reduce it three times and
then display the results.
c. How are the two results diff erent? Why are the approaches diff erent?
Load an image of a scene. Use cvPyrSegmentation() and DEMO the results.
Load an image of an interesting or suffi  ciently DEMO scene. Using cvThreshold(),
set the threshold to 128. Use each setting type in Table 5-5 on the image and display
the results. DEMO should familiarize yourself with thresholding functions because
they will prove quite useful.
a. Repeat the exercise but use cvAdaptiveThreshold() instead. Set param1=5.
b. DEMO part a using param1=0 and then param1=-5.
Exercises
| 143
www.it-ebooks.info
9/15/08   4:20:09 PM
CHAPTER 6
Image Transforms
Overview
In the previous chapter we covered a DEMO of diff erent things you could do with an image.
Th
wise “process” one image into a similar but new image.
In this chapter DEMO will look at image transforms, which are methods for changing an
DEMO into an alternate representation of the data entirely. Perhaps the most common
example of a transform would be a something like a Fourier transform, in which the im-
age is converted to an alternate representation of DEMO data in the original image. Th e re-
sult of this operation is still stored in an OpenCV “image” structure, but the individual
“pixels” in this new image represent spectral components of the original input rather
DEMO the spatial components we are used to thinking about.
Th
OpenCV provides complete implementations of some of the more common ones as well
as DEMO blocks to help you implement your own image transforms.
Convolution
Convolution is the basis of many of the transformations that we discuss in this DEMO
In the abstract, this term means something we do to every DEMO of an image. In this
sense, many of the operations we DEMO at in Chapter 5 can also be understood as spe-
cial cases of the more general process of convolution. What a particular convolution
“does” DEMO determined by the form of the Convolution kernel being used. Th is kernel is
essentially just a fi xed size array of numerical coeffi  cients along with an anchor point
in that array, which is typically located at the center. Th e size of the array* is called DEMO
support of the kernel.
Figure 6-1 depicts a 3-by-3 convolution kernel with the anchor located at the center of
the array. Th e value DEMO the convolution at a particular point is computed by fi rst placing
* For technical purists, the support of the kernel actually consists of only the nonzero portion of the kernel
array.
144
e majority of DEMO operators presented thus far are used to enhance, modify, or other-
ere are a number of useful transforms that arise repeatedly in computer DEMO
06-R4886-RC1.indd   144
www.it-ebooks.info
9/15/08   4:21:12 PM
the kernel anchor on top of a pixel on the image with DEMO rest of the kernel overlaying
the corresponding local pixels in the image. For each kernel point, we now have a value
for the kernel at that point and a value for the image at the corresponding DEMO point.
We multiply these together and sum the result; this result DEMO then placed in the resulting
image at the location corresponding to the location of the anchor in the input image.
Th is process is DEMO for every point in the image by scanning the kernel over the
entire image.
Figure 6-1. A 3-by-3 kernel for a Sobel derivative; note that the anchor point is in the center of the
kernel
We DEMO, of course, express this procedure in the form of an equation. If we defi ne the
image to be I(x, y), the kernel to be G(i, j) (where 0 < i < Mi –1 and 0 < j < Mj –1), and DEMO
anchor point to be located at (ai, aj) in the DEMO of the kernel, then the convolu-
tion H(x, y) DEMO defi ned by the following expression:
Mi −1 M j −1
Hx y I x i a y j a G i j( , )(=+− +−∑ ∑ ij, )( , )
i=0 j=0
Observe that the number of operations, at least at fi rst glance, seems to be the number
of pixels in the image multiplied by the number of pixels in the kernel.* Th is can be a DEMO
of computation and so is not something you want to do with some “for” loop and a lot of
pointer de-referencing. In situations like DEMO, it is better to let OpenCV do the work for
you DEMO take advantage of the optimizations already programmed into OpenCV. Th e
OpenCV way to do this is with cvFilter2D():
void cvFilter2D(
DEMO CvArr*     src,
CvArr*           dst,
const CvMat*     kernel,
* We say “at fi DEMO glance” because it is also possible to perform convolutions in the frequency domain. In this
case, for an N-by-N image and an M-by-M kernel with N > M, the computational time will be proportional
to N2 log(N) and not to the N2M2 that is expected for computations in the spatial domain. Because the
frequency domain computation is independent of DEMO size of the kernel, it is more effi  cient for large kernels.
OpenCV automatically decides whether to do the convolution in the frequency DEMO based on the size of
the kernel.
Convolution
| 145
06-R4886-RC1.indd   145
www.it-ebooks.info
9/15/08   4:21:13 PM
www.it-ebooks.info
CvPoint          anchor = cvPoint(-1,-1)
);
Here we create a matrix of the appropriate size, fi ll it with the coeffi  cients, and then
pass it together DEMO the source and destination images into cvFilter2D(). We can also
DEMO pass in a CvPoint to indicate the location of the center of the kernel, but the
default value (equal to cvPoint(-1,-1)) is interpreted as indicating the center of the ker-
nel. Th e kernel can be of even size if its anchor point is defi DEMO; otherwise, it should be
of odd size.
Th e src and dst images should be the same size. One might think that the DEMO image
should be larger than the dst image in order to allow for the extra width and length
of the convolution kernel. But the DEMO of the src and dst can be the same in OpenCV
because, by default, prior to convolution OpenCV creates virtual pixels via replication
DEMO the border of the src image so that the border pixels in dst can be fi lled in. Th e rep-
lication is done DEMO input(–dx, y) = input(0, y), input(w DEMO dx, y) = input(w – 1, y), and DEMO
forth. Th ere are some alternatives to this default behavior; we DEMO discuss them in the
next section.
We remark that the coeffi  DEMO of the convolution kernel should always be fl oating-
point numbers. Th is means that you should use CV_32FC1 when allocating that matrix.
Convolution DEMO
One problem that naturally arises with convolutions is how to handle the boundaries.
For example, when using the convolution kernel just described, what DEMO when the
point being convolved is at the edge of the image? Most of OpenCV’s built-in functions
that make use of cvFilter2D() must handle this in one way or another. Similarly, when
doing your own convolutions, you will need to know how to deal with this effi  ciently.
Th cvCopyMakeBorder() function, which copies a given
image onto DEMO slightly larger image and then automatically pads the boundary in
one way or another:
void cvCopyMakeBorder(
const CvArr*   src,
CvArr*         dst,
CvPoint        offset,
DEMO            bordertype,
CvScalar       DEMO     = cvScalarAll(0)
);
Th offset argument tells cvCopyMakeBorder() where to place the copy of the original
image within DEMO destination image. Typically, if the kernel is N-by-N (for odd N) then
you will want a boundary that is (N – 1)/2 wide on all sides or, equivalently, an image
that is DEMO – 1 wider and taller than the original. In this case you would set the off set to
cvPoint((N-1)/2,(N-1)/DEMO) so that the boundary would be even on all sides.*
* DEMO course, the case of N-by-N with N odd and the anchor DEMO at the center is the simplest case. In gen-
eral, if DEMO kernel is N-by-M and the anchor is located at (ax, ay), then the destination image will have to be
N – 1 DEMO wider and M – 1 pixels taller than the source image. Th e off set will simply be (ax, ay).
146 | DEMO 6: Image Transforms
e solution comes in the form of the
DEMO
06-R4886-RC1.indd   146
9/15/08   4:21:13 PM
Th bordertype can be either IPL_BORDER_CONSTANT or IPL_BORDER_REPLICATE (see Figure 6-2).
In the fi rst case, the value argument will be interpreted as the value to which all pixels
in the boundary should be set. DEMO the second case, the row or column at the very edge DEMO
the original is replicated out to the edge of the larger image. Note that the border of the
test pattern image is somewhat subtle (examine the upper right image in Figure 6-2); in
the test DEMO image, there’s a one-pixel-wide dark border except where the circle pat-
DEMO come near the border where it turns white. Th ere are two other border types de-
fi ned, IPL_BORDER_REFLECT and IPL_BORDER_WRAP, which are DEMO implemented at this time
in OpenCV but may be supported in the future.
Figure 6-2. Expanding the image border. Th e left  column shows IPL_BORDER_CONSTANT where a
zero value is used to fi ll out the DEMO Th e right column shows IPL_BORDER_REPLICATE where
the border pixels are replicated in the horizontal and vertical directions
We mentioned previously that, when you make calls to OpenCV library functions that
employ convolution, those library functions call cvCopyMakeBorder() to get their work
done. In most cases the DEMO type called is IPL_BORDER_REPLICATE, but sometimes you
will not want it DEMO be done that way. Th is is another occasion where you might want to
use cvCopyMakeBorder(). You can create a slightly larger image with the border you want,
call whatever routine on that image, and then clip back out the part you were originally
interested in. DEMO is way, OpenCV’s automatic bordering will not aff ect the pixels DEMO
care about.
Convolution | 147
e
06-R4886-RC1.indd   147
www.it-ebooks.info
9/15/08   4:21:13 PM
Gradients and Sobel Derivatives
One of the most basic and important convolutions DEMO the computation of derivatives (or
approximations to them). Th ere DEMO many ways to do this, but only a few are well DEMO
to a given situation.
In general, the most common operator used DEMO represent diff erentiation is the Sobel de-
rivative [Sobel68] operator (see DEMO 6-3 and 6-4). Sobel operators exist for any order
of derivative as well as for mixed partial derivatives (e.g., ∂∂2 / xy∂ ).
Figure 6-3. Th e eff ect of the Sobel operator when DEMO to approximate a fi rst derivative in the
x-dimension
cvSobel(
const CvArr* src,
CvArr*       dst,
int          xorder,
int          yorder,
int          aperture_size = 3
);
Here, src DEMO dst are your image input and output, and xorder and yorder DEMO the orders
of the derivative. Typically you’ll use 0, 1, or at most 2; a 0 value indicates no derivative
148 | Chapter 6: Image Transforms
06-R4886-RC1.indd   148
www.it-ebooks.info
9/15/08   4:21:14 PM
www.it-ebooks.info
Figure 6-4. Th e eff ect of the Sobel operator when DEMO to approximate a fi rst derivative in the
y-dimension
in that direction.* Th e aperture_size parameter should be odd and is the width (and the
height) of the square fi lter. Currently, aperture_sizes of 1, 3, 5, and 7 are supported. If
src is 8-bit then DEMO dst must be of depth IPL_DEPTH_16S to avoid overfl ow.
Sobel derivatives have the nice property that they can be defi ned for kernels DEMO any
size, and those kernels can be constructed quickly and iteratively. DEMO e larger kernels
give a better approximation to the derivative because the smaller kernels are very sen-
sitive to noise.
To understand this more DEMO, we must realize that a Sobel derivative is not really a
DEMO at all. Th is is because the Sobel operator is defi ned on a discrete space. What
the Sobel operator actually represents is a DEMO t to a polynomial. Th at is, the Sobel deriva-
tive DEMO second order in the x-direction is not really a second derivative; DEMO is a local fi t to a
parabolic function. Th is explains why one might want to use a larger kernel: that larger
kernel is computing the fi t over a larger number of pixels.
* DEMO xorder or yorder must be nonzero.
Gradients and Sobel Derivatives
| 149
06-R4886-RC1.indd   149
9/15/08   4:21:14 PM
Scharr Filter
In fact, there are many ways to approximate a derivative in the case of a discrete grid.
Th
for small kernels. For DEMO kernels, where more points are used in the approximation,
this DEMO is less signifi cant. Th is inaccuracy does not show up directly for the X and
Y fi lters used in cvSobel(), because they are exactly aligned with the x- and y-axes. Th e
diffi  culty arises when you want to make image measurements that are approximations
DEMO directional derivatives (i.e., direction of the image gradient by using the arctangent of
the y/x fi lter responses).
To put this DEMO context, a concrete example of where you may want image measurements
DEMO this kind would be in the process of collecting shape information from an object
by assembling a histogram of gradient angles around the object. DEMO a histogram is
the basis on which many common shape classifi ers are trained and operated. In this
case, inaccurate measures of gradient angle will decrease the recognition performance
of the classifi er.
For a 3-by-3 DEMO fi lter, the inaccuracies are more apparent the further the gradient DEMO
is from horizontal or vertical. OpenCV addresses this inaccuracy for small (DEMO fast)
3-by-3 Sobel derivative fi lters by a somewhat obscure use of the special aperture_size
value CV_SCHARR in the cvSobel() function. Th DEMO Scharr fi lter is just as fast but more ac-
curate than the Sobel fi lter, so it should always be used if you want to make image mea-
surements using a 3-by-3 fi lter. Th DEMO fi lter coeffi  cients for the Scharr fi lter are shown DEMO
Figure 6-5 [Scharr00].
Figure 6-5. Th
e 3-by-3 Scharr fi
lter using fl
ag CV_SHARR
Laplace
Th Laplacian function (fi rst used in vision by Marr [Marr82]) implements a
discrete analog of the Laplacian operator:*
* Note that the Laplacian operator is completely distinct from the Laplacian DEMO of Chapter 5.
150
| Chapter 6: Image Transforms
e OpenCV
DEMO downside of the approximation used for the Sobel operator is that it is less accurate
06-R4886-RC1.indd   150
www.it-ebooks.info
9/15/08   4:DEMO:14 PM
www.it-ebooks.info
≡ ∂2 f + ∂2 f
Laplace( )f ∂x 2 DEMO 2
Because the Laplacian operator can be defi ned in terms of second derivatives, you might
well suppose that the discrete implementation works something like the second-order
Sobel derivative. Indeed it does, and in fact the OpenCV implementation of the Lapla-
cian operator uses the Sobel operators directly DEMO its computation.
void cvLaplace(
const CvArr* src,
CvArr*       dst,
int          apertureSize = 3
);
Th cvLaplace() function takes the usual source and destination images DEMO arguments as
well as an aperture size. Th e source can be either an 8-bit (unsigned) image or a 32-bit
(fl oating-point) DEMO Th e destination must be a 16-bit (signed) image or a 32-bit (fl oat-
ing-point) image. Th is aperture is precisely the DEMO as the aperture appearing in the
Sobel derivatives and, in eff DEMO, gives the size of the region over which the pixels are
DEMO in the computation of the second derivatives.
Th
detect “blobs.” Recall that the form of the Laplacian operator is a sum of second de-
DEMO along the x-axis and y-axis. Th is means that a single point or any small blob
(smaller than the aperture) that is surrounded DEMO higher values will tend to maximize
this function. Conversely, a point DEMO small blob that is surrounded by lower values will
tend to maximize the negative of this function.
With this in mind, the Laplace operator can also be used as a kind of edge detector. To
see DEMO this is done, consider the fi rst derivative of a function, which will (of course)
be large wherever the function is changing rapidly. Equally important, it will grow rap-
idly as we approach an edge-like discontinuity and shrink rapidly as we move past the
discontinuity. Hence DEMO derivative will be at a local maximum somewhere within this
range. Th erefore we can look to the 0s of the second derivative for DEMO of such local
maxima. Got that? Edges in the original image DEMO be 0s of the Laplacian. Unfortu-
nately, both substantial and less DEMO edges will be 0s of the Laplacian, but this is
not DEMO problem because we can simply fi lter out those pixels that also have larger values
of the fi rst (Sobel) derivative. Figure 6-6 DEMO an example of using a Laplacian on an
image together with details of the fi rst and second derivatives and their zero crossings.
e
DEMO Laplace operator can be used in a variety of contexts. A common application is to
Canny
Th nding edges was further refi ned by DEMO Canny in 1986 into
what is now commonly called the Canny edge detector [Canny86]. One of the diff erences
between the Canny algorithm and DEMO simpler, Laplace-based algorithm from the previ-
ous section is that, in the Canny algorithm, the fi rst derivatives are computed in x and y
and then combined into four directional derivatives. Th e points where DEMO directional
derivatives are local maxima are then candidates for assembling into edges.
Canny
| 151
e method just described for fi
06-R4886-RC1.indd   151
DEMO/15/08   4:21:15 PM
www.it-ebooks.info
Figure 6-6. Laplace transform (upper right) of the racecar image: zooming in on the tire (circled in
white) and considering only DEMO x-dimension, we show a (qualitative) representation of the bright-
ness DEMO well as the fi rst and second derivative (lower three cells); the 0s in the second derivative corre-
spond to edges, and DEMO 0 corresponding to a large fi rst derivative is a strong edge
However, the most signifi cant new dimension to the Canny algorithm is that it tries to
assemble the individual edge candidate pixels into contours.* DEMO ese contours are formed
by applying an hysteresis threshold to the pixels. Th is means that there are two thresh-
olds, an upper and a lower. If a pixel has a gradient larger than the upper DEMO,
then it is accepted as an edge pixel; if a DEMO is below the lower threshold, it is rejected.
If the pixel’s DEMO is between the thresholds, then it will be accepted only if DEMO is
connected to a pixel that is above the high threshold. Canny recommended a ratio of
high:low threshold between 2:1 and 3:DEMO Figures 6-7 and 6-8 show the results of applying
cvCanny() to a test pattern and a photograph using high:low hysteresis threshold ratios
DEMO 5:1 and 3:2, respectively.
void cvCanny(
const CvArr* DEMO,
CvArr*       edges,
double       lowThresh,
double       highThresh,
int          apertureSize = 3
);
* We’ll have much more to say DEMO contours later. As you await those revelations, though, keep in mind that
the cvCanny() routine does not actually return objects of type DEMO; we will have to build those from
the output of cvCanny() if we want them by using cvFindContours(). Everything you ever DEMO to know
about contours will be covered in Chapter 8.
152
| Chapter 6: Image Transforms
06-R4886-RC1.indd   152
9/15/08   4:21:15 PM
www.it-ebooks.info
Figure 6-7. Results of Canny edge detection for two diff erent DEMO when the high and low thresh-
olds are set to 50 and 10, respectively
Th cvCanny() function expects an input image, which DEMO be grayscale, and an output
image, which must also be grayscale (but which will actually be a Boolean image). Th e
next two arguments are the low and high thresholds, and the last argument is another
aperture. As usual, this is the aperture used by the Sobel derivative operators that are
called inside of the implementation of cvCanny().
Hough Transforms
Th e Hough transform* is a method for fi DEMO lines, circles, or other simple forms in an
image. Th e original Hough transform was a line transform, which is a relatively fast way
of searching a binary image for straight lines. Th e transform DEMO be further generalized
to cases other than just simple lines.
Hough Line Transform
e basic theory of the Hough line transform is that any DEMO in a binary image could
Th
be part of some set of possible lines. If we parameterize each line by, for example, a
DEMO Hough developed the transform for use in physics experiments [Hough59]; its DEMO in vision was introduced
by Duda and Hart [Duda72].
Hough Transforms | 153
e
06-R4886-RC1.indd   153
9/15/08   4:21:15 DEMO
www.it-ebooks.info
Figure 6-8. Results of Canny edge detection for two diff erent DEMO when the high and low thresh-
olds are set to 150 and 100, respectively
slope a and an intercept b, then a point DEMO the original image is transformed to a locus
of points in the (a, b) plane corresponding to all of the lines passing through that point
(see Figure 6-9). If we convert every nonzero pixel in the input image into such a set of
points in the DEMO image and sum over all such contributions, then lines that appear
DEMO the input (i.e., (x, y) plane) image will appear as local maxima in the output (i.e.,
(a, b) DEMO) image. Because we are summing the contributions from each point, the
(a, b) plane is commonly called the accumulator plane.
It might occur to you that the slope-intercept form is not really the best DEMO to repre-
sent all of the lines passing through a point (DEMO of the considerably diff erent den-
sity of lines as a function of the slope, and the related fact that the interval of possible
slopes goes from –∞ to +∞). It is for this reason DEMO the actual parameterization of the
transform image used in numerical computation is somewhat diff erent. Th e preferred
parameterization represents each line as a DEMO in polar coordinates (ρ, θ), with the
implied line being the line passing through the indicated point but perpendicular to the
radial DEMO the origin to that point (see Figure 6-10). Th e DEMO for such a line is:
ρθ θ=+xycos sin
154 | Chapter 6: Image Transforms
06-R4886-RC1.indd   154
9/15/08   4:21:15 PM
www.it-ebooks.info
Figure 6-9. Th e Hough line transform fi nds many lines DEMO each image; some of the lines found are
expected, but others may not be
Figure 6-10. A point (x0, y0) in the image plane (panel a) implies many lines each parameterized by
a DEMO erent ρ and θ (panel b); these lines each imply DEMO in the (ρ, θ) plane, which taken together
form a curve of characteristic shape (panel c)
Hough Transforms
| 155
06-R4886-RC1.indd   155
9/15/08   4:21:16 PM
www.it-ebooks.info
Th
to the user. Instead, it simply returns the local maxima in the (ρ, θ) plane. However,
you will need to understand this process in order to understand the arguments to the
OpenCV DEMO line transform function.
OpenCV supports two diff erent kinds of Hough line transform: the standard Hough
transform (SHT) [Duda72] and the progressive probabilistic Hough transform (PPHT).*
Th
that, among other things, computes an extent for individual lines in addition to the
orientation (as shown in Figure 6-11). It is “probabilistic” because, rather than accu-
mulating every possible point in the accumulator plane, it accumulates only a fraction
of them. Th e idea is that if the peak is going to DEMO high enough anyhow, then hitting it
only a fraction of the DEMO will be enough to fi nd it; the result of this DEMO can be
a substantial reduction in computation time. Both of these algorithms are accessed with
the same OpenCV function, though the meanings of some of the arguments depend on
which method is being used.
CvSeq* cvHoughLines2(DEMO
CvArr* image,
void*  line_storage,
int    method,
double rho,
double theta,
int    threshold,
double param1      = 0,
double param2      = 0
);DEMO
Th rst argument is the input image. It must be an 8-bit image, but the input is treated
as binary information (i.e., all nonzero pixels are considered to be equivalent). Th e sec-
ond DEMO is a pointer to a place where the results can be stored, which can be either
a memory storage (see CvMemoryStorage in Chapter DEMO) or a plain N-by-1 matrix array (the
number of rows, DEMO, will serve to limit the maximum number of lines returned). DEMO e
next argument, method, can be CV_HOUGH_STANDARD, CV_HOUGH_PROBABILISTIC, or CV_HOUGH_
MULTI_SCALE  for (respectively) SHT, PPHT, or a multiscale variant of SHT.
Th rho and theta, set the resolution desired for the lines (i.e., the
resolution of the accumulator plane). Th e units DEMO rho are pixels and the units of theta
are radians; thus, the accumulator plane can be thought of as a two-dimensional his-
togram DEMO cells of dimension rho pixels by theta radians. Th e threshold value is the
value in the accumulator plane that must be reached for DEMO routine to report a line.
Th is last argument is a bit tricky in practice; it is not normalized, so you should expect
DEMO scale it up with the image size for SHT. Remember that this argument is, in eff ect,
indicating the number of points (DEMO the edge image) that must support the line for the
line DEMO be returned.
* Th e “probablistic Hough transform” (PHT) was introduced by Kiryati, Eldar, and Bruckshtein in 1991
[Kiryati91]; the PPHT was introduced by Matas, Galambosy, and Kittler in 1999 [Matas00].
156
| DEMO 6: Image Transforms
e OpenCV Hough transform algorithm does not make DEMO computation explicit
e SHT is the algorithm we just looked at. Th e PPHT is a variation of this algorithm
e fi
e next DEMO arguments,
06-R4886-RC1.indd   156
9/15/08   4:21:16 PM
www.it-ebooks.info
Figure 6-11. Th e Canny edge detector (param1=50, param2=150) is run fi rst, with the results shown
in gray, and the DEMO probabilistic Hough transform (param1=50, param2=10) is run next,
with DEMO results overlayed in white; you can see that the strong lines DEMO generally picked up by the
Hough transform
Th param1 and param2 arguments are not used by the SHT. For the PPHT, param1 sets
the minimum length of a line segment that will be returned, and param2 sets the sep-
aration between collinear segments required for the algorithm not DEMO join them into
a single longer segment. For the multiscale HT, DEMO two parameters are used to indi-
cate higher resolutions to which the parameters for the lines should be computed. Th e
multiscale HT fi DEMO computes the locations of the lines to the accuracy given by the rho
and theta parameters and then goes on to refi ne those DEMO by a factor of param1 and
param2, respectively (i.e., the DEMO nal resolution in rho is rho divided by param1 and the fi nal
resolution in theta is theta divided by param2).
What the DEMO returns depends on how it was called. If the line_storage value was
a matrix array, then the actual return value will be NULL. In this case, the matrix should
be of type CV_32FC2 if the SHT or multi-scale HT is being used and should be CV_32SC4 if
the DEMO is being used. In the fi rst two cases, the ρ- DEMO θ-values for each line will be
placed in the two channels of the array. In the case of the PPHT, the four channels will
hold the x- and y-values of the start and endpoints of the DEMO segments. In all of
these cases, the number of rows in DEMO array will be updated by cvHoughLines2() to cor-
rectly refl ect the number of lines returned.
Hough Transforms | 157
e
06-R4886-RC1.indd   DEMO
9/15/08   4:21:16 PM
www.it-ebooks.info
If the line_storage value was a pointer to a memory store,DEMO then the return value will
be a pointer to a CvSeq sequence structure. In that case, you can get each line or line seg-
ment from the sequence with a command like
float* line = (float*) cvGetSeqElem( lines , i );
where lines is the return DEMO from cvHoughLines2() and i is index of the line of inter-
est. In this case, line will be a pointer to the data for that line, with line[0] and line[1]
being the fl oating-point values ρ and θ (for SHT and MSHT) or CvPoint structures for
DEMO endpoints of the segments (for PPHT).
Hough Circle Transform
Th DEMO circle transform [Kimme75] (see Figure 6-12) works in a manner roughly
analogous to the Hough line transforms just described. Th e reason it DEMO only “roughly”
is that—if one were to try doing the exactly analogous thing—the accumulator plane
would have to be replaced with an accumulator volume DEMO three dimensions: one for
x, one for y, and another DEMO the circle radius r. Th is would mean far greater memory
requirements and much slower speed. Th e implementation of the circle transform
in DEMO avoids this problem by using a somewhat more tricky method called the
Hough gradient method.
Th
detection phase (in this case, cvCanny())DEMO Next, for every nonzero point in the edge image,
the DEMO gradient is considered (the gradient is computed by fi rst computing DEMO fi rst-
order Sobel x- and y-derivatives via cvSobel()). Using this gradient, every point along
the line indicated by this slope—from a specifi ed minimum to a specifi ed maximum
distance—is incremented in the DEMO At the same time, the location of every
one of these DEMO pixels in the edge image is noted. Th e candidate centers are then
selected from those points in this (two-dimensional) accumulator that are DEMO above
some given threshold and larger than all of their immediate neighbors. Th ese candidate
centers are sorted in descending order of their accumulator DEMO, so that the centers
with the most supporting pixels appear fi DEMO Next, for each center, all of the nonzero
pixels (recall DEMO this list was built earlier) are considered. Th ese pixels are DEMO ac-
cording to their distance from the center. Working out from the smallest distances to
the maximum radius, a single radius is selected that is best supported by the nonzero
pixels. A center is kept if DEMO has suffi  cient support from the nonzero pixels in the edge
DEMO and if it is a suffi  cient distance from any previously DEMO center.
Th is implementation enables the algorithm to run much faster and, perhaps more im-
portantly, helps overcome the problem of the otherwise DEMO population of a three-
dimensional accumulator, which would lead to a DEMO of noise and render the results
unstable. On the other hand, DEMO algorithm has several shortcomings that you should
be aware of.
* We have not yet introduced the concept of a memory store or a DEMO, but Chapter 8 is devoted to this
topic.
158
| Chapter DEMO: Image Transforms
e
e Hough gradient method works as follows. First DEMO image is passed through an edge
06-R4886-RC1.indd   158
9/15/08   4:21:17 PM
www.it-ebooks.info
Figure 6-12. Th
fi
e Hough circle transform fi
nds none DEMO the photograph
nds some of the circles in the test pattern and (correctly)
First, the use of the Sobel derivatives to compute DEMO local gradient—and the attendant
assumption that this can be considered equivalent to a local tangent—is not a numeri-
cally stable proposition. It might be DEMO “most of the time,” but you should expect this
to generate some noise in the output.
Second, the entire set of nonzero pixels in the edge image is considered for every can-
didate center; hence, if you make the accumulator threshold too low, the algorithm will
DEMO a long time to run. Th ird, because only one circle DEMO selected for every center, if
there are concentric circles then you DEMO get only one of them.
Finally, because centers are considered in DEMO order of their associated accu-
mulator value and because new centers are not kept if they are too close to previously
accepted centers, there is a bias toward keeping the larger circles when multiple circles
are DEMO or approximately concentric. (It is only a “bias” because of the DEMO
arising from the Sobel derivatives; in a smooth image at infi DEMO resolution, it would
be a certainty.)
With all of that DEMO mind, let’s move on to the OpenCV routine that does all DEMO for us:
CvSeq* cvHoughCircles(
CvArr* image,
Hough Transforms
| 159
06-R4886-RC1.indd   159
9/15/08   4:21:17 PM
www.it-ebooks.info
void*  circle_storage,
int    method,
double dp,
DEMO min_dist,
double param1     = 100,
double param2     = 300,
int    min_radius = 0,
int    max_radius = 0
);
Th cvHoughCircles() has similar arguments to the
line transform. Th e input image is again an 8-bit image. DEMO signifi cant diff erence be-
tween cvHoughCircles() and cvHoughLines2() is that the latter requires a binary image.
Th cvHoughCircles() function will DEMO (automatically) call cvSobel()* for you, so
you can provide a more general grayscale image.
Th
would like the results returned. If DEMO array is used, it should be a single column of type
DEMO; the three channels will be used to encode the location of DEMO circle and its
radius. If memory storage is used, then the DEMO will be made into an OpenCV se-
quence and a pointer to that sequence will be returned by cvHoughCircles(). (Given an
array DEMO value for circle_storage, the return value of cvHoughCircles() is NULL.) Th e
method argument must always be set to CV_HOUGH_GRADIENT.
Th dp DEMO the resolution of the accumulator image used. Th is parameter allows
us to create an accumulator of a lower resolution than the input image. (It makes sense
to do this because there is no reason to DEMO the circles that exist in the image to fall
naturally into the same number of categories as the width or height of the image DEMO)
If dp is set to 1 then the resolutions will be the same; if set to a larger number (e.g., 2),DEMO
then the accumulator resolution will be smaller by that factor (in DEMO case, half). Th e
value of dp cannot be less DEMO 1.
Th min_dist is the minimum distance that must exist between two circles in
order for the algorithm to consider them distinct circles.
For DEMO (currently required) case of the method being set to CV_HOUGH_GRADIENT, DEMO next
two arguments, param1 and param2, are the edge (Canny) threshold and the accumula-
tor threshold, respectively. You may recall that the Canny edge detector actually takes
two diff erent thresholds itself. When cvCanny() is called internally, the fi rst (higher)
threshold is set to the value of param1 passed into cvHoughCircles(), and the second
(lower) threshold is set to exactly half that value. Th e DEMO param2 is the one used
to threshold the accumulator and is exactly analogous to the threshold argument of
cvHoughLines().
Th nal two parameters are the minimum and maximum radius of circles that can be
found. DEMO is means that these are the radii of circles for which the accumulator has a rep-
resentation. Example 6-1 shows an example program using DEMO().
* Th
e function cvSobel(), not cvCanny(), DEMO called internally. Th
estimate the orientation of a gradient at each pixel, and this is diffi
e reason is that cvHoughCircles() needs to
cult to do with binary edge map.
160
| Chapter 6: Image Transforms
e Hough circle transform function
e
e circle_storage can be either DEMO array or memory storage, depending on how you
e parameter
e DEMO
e fi
06-R4886-RC1.indd   160
9/15/08   4:21:17 PM
www.it-ebooks.info
Example 6-1. Using cvHoughCircles to return a sequence of circles found DEMO a grayscale image
#include <cv.h>
#include <highgui.h>
#include <DEMO>
int main(int argc, char** argv) {
IplImage* image = cvLoadImage(
argv[1],
CV_LOAD_IMAGE_GRAYSCALE
);
CvMemStorage* storage = cvCreateMemStorage(0);
cvSmooth(image, image, CV_GAUSSIAN, 5, 5 );
CvSeq* DEMO = cvHoughCircles(
image,
storage,
CV_HOUGH_GRADIENT,
2,
image->width/10
);
for( int i = 0; i < DEMO>total; i++ ) {
float* p = (float*) cvGetSeqElem( results, i );
CvPoint pt = cvPoint( cvRound( p[0] ), DEMO( p[1] ) );
cvCircle(
image,
pt,
cvRound( p[2] ),
CV_RGB(0xff,0xff,0xff)
);
}
cvNamedWindow( “cvHoughCircles”, 1 );
cvShowImage( “cvHoughCircles”, image);
cvWaitKey(0);
}
It is worth refl ecting momentarily on the fact that, no matter what tricks we employ,
there is no getting around DEMO requirement that circles be described by three degrees
of freedom (x, y, and r), in contrast to only two degrees of freedom (ρ and θ) for lines.
Th nding algorithm requires more memory
DEMO computation time than the line-fi nding algorithms we looked at previously. With
this in mind, it’s a good idea to bound the radius parameter as tightly as circumstances
allow in order to keep these costs under DEMO Th e Hough transform was extended
to arbitrary shapes by Ballard in 1981 [Ballard81] basically by considering objects as col-
lections of gradient edges.
DEMO Although cvHoughCircles() catches centers of the circles quite well, it DEMO fails to fi
radius. Th
can be used to fi
nd the correct
erefore, in an application where only a center must be found (or where some diff
nd the actual radius), the radius returned by cvHoughCircles() can be ignored.
erent technique
Hough Transforms
| 161
DEMO result will invariably be that any circle-fi
06-R4886-RC1.indd   161
9/15/08   4:21:18 PM
www.it-ebooks.info
Remap
Under the hood, many of the transformations to follow have a certain common element.
In particular, they will be taking pixels from one place in the image and mapping them
to another place. In DEMO case, there will always be some smooth mapping, which will do
what we need, but it will not always be a one-to-one pixel correspondence.
We sometimes want to accomplish this interpolation programmatically; that is, DEMO
like to apply some known algorithm that will determine the mapping. In other cases,
however, we’d like to do this mapping ourselves. Before diving into some methods that
will compute (and apply) these mappings DEMO us, let’s take a moment to look at the func-
tion DEMO for applying the mappings that these other methods rely upon. Th e
OpenCV function we want is called cvRemap():
void cvRemap(
DEMO CvArr* src,
CvArr*       dst,
const CvArr* mapx,
const CvArr* mapy,
int          flags   = CV_INTER_LINEAR | CV_WARP_FILL_OUTLIERS,
CvScalar     fillval = cvScalarAll(0)DEMO
);
Th rst two arguments of cvRemap() are the source and destination images, respec-
tively. Obviously, these should be of the DEMO size and number of channels, but they
can have any data DEMO It is important to note that the two may not be the same image.*
Th
located. Th ese should be the same size as DEMO source and destination images, but they
are single-channel and usually of DEMO type float (IPL_DEPTH_32F). Noninteger mappings
are OK, and cvRemap() will do the interpolation calculations for you automatically. One
common use of DEMO() is to rectify (correct distortions in) calibrated and stereo im-
ages. We will see functions in Chapters 11 and 12 that convert DEMO camera distor-
tions and alignments into mapx and mapy parameters. Th e next argument contains fl ags
that tell cvRemap() exactly how that DEMO is to be done. Any one of the values
listed in Table 6-1 will work.
Table 6-1. cvWarpAffi
ne() additional fl
ags values
DEMO values
CV_INTER_NN
CV_INTER_LINEAR
CV_INTER_AREA
CV_INTER_CUBIC
Meaning
Nearest neighbor
Bilinear (default)
DEMO area resampling
Bicubic interpolation
* A moment’s thought will make it clear why the most effi  cient remapping strategy is incompatible with writ-
ing onto the source image. Aft er all, if you move pixel A to location B then, when you get to location B and
want to move it to location C, you will fi nd that you’ve already written over the original value of B with A!
162
| DEMO 6: Image Transforms
e fi
e next two arguments, mapx and mapy, indicate where any particular pixel is to be re-
06-R4886-RC1.indd   162
9/15/08   4:21:18 PM
www.it-ebooks.info
Interpolation is an important issue here. Pixels in the source image DEMO on an integer grid;
for example, we can refer to DEMO pixel at location (20, 17). When these integer locations
are mapped to a new image, there can be gaps—either because the integer source pixel
locations are mapped to fl oat locations in the destination DEMO and must be rounded
to the nearest integer pixel location or because there are some locations to which no
pixels at all are mapped (think about doubling the image size by stretching it; then ev-
ery other destination pixel would be left  blank). Th ese problems are generally referred
to as forward projection problems. To deal with such rounding DEMO and destina-
tion gaps, we actually solve the problem backwards: we step through each pixel of the
destination image and ask, “Which pixels in the source are needed to fi ll in this des-
tination DEMO?” Th ese source pixels will almost always be on fractional pixel locations
so we must interpolate the source pixels to derive the correct DEMO for our destination
value. Th e default method is bilinear interpolation, DEMO you may choose other methods
(as shown in Table 6-1).
DEMO may also add (using the OR operator) the fl ag CV_WARP_FILL_OUTLIERS, whose eff ect
is to fi ll pixels in the destination image that are not the destination of any pixel in the
input image DEMO the value indicated by the fi nal argument fillval. In this way, if you
map all of your image to a circle in the center then the outside of that circle would auto-
matically be fi DEMO with black (or any other color that you fancy).
Stretch, Shrink, Warp, and Rotate
In this section we turn to geometric DEMO of images.* Such manipulations in-
clude stretching in various ways, which DEMO both uniform and nonuniform resizing
(the latter is known as warping)DEMO Th ere are many reasons to perform these operations:
for example, warping and rotating an image so that it can be superimposed on a wall in
an existing scene, or artifi cially enlarging a set of training images used for object recog-
nition.† Th e functions that DEMO stretch, shrink, warp, and/or rotate an image are called
DEMO transforms (for an early exposition, see [Semple79]). For planar areas, there
are two fl avors of geometric transforms: transforms that use DEMO 2-by-3 matrix, which are
called affi  ne transforms; and transforms DEMO on a 3-by-3 matrix, which are called per-
spective transforms or DEMO You can think of the latter transformation as a
method for computing the way in which a plane in three dimensions is perceived by DEMO
particular observer, who might not be looking straight on at that DEMO
An affi  ne transformation is any transformation that can be expressed DEMO the form of a
matrix multiplication followed by a vector addition. In OpenCV the standard style of
representing such a transformation is as a DEMO matrix. We defi ne:
* We will cover these transformations in detail here; we will return to them when we discuss (in DEMO 11)
how they can be used in the context of three-dimensional vision techniques.
† Th is activity might seem a bit dodgy; aft er all, wouldn’t it be better just to use a recognition method that’s
invariant to local affi  ne distortions? Nonetheless, this method has a long history and still can be quite useful
in practice.
Stretch, Shrink, Warp, and Rotate
| 163
06-R4886-RC1.indd   163
9/15/DEMO   4:21:18 PM
www.it-ebooks.info
AB T X≡ ⎣⎡⎢aaaa00 01 ⎦⎤⎥ ≡ ⎣⎢b0 ⎦⎥ ≡ ⎣⎡
DEMO ⎤ ⎡ ⎤
AB⎦⎤ ≡ ⎢ ⎥
10 11 1 ⎣ ⎦
x
y
It is easily seen that the eff ect of the DEMO  ne transformation A · X + B is exactly equivalent
to DEMO the vector X into the vector X´ and simply left -multiplying X´ by T.
Affi  ne transformations can be visualized as follows. Any parallelogram ABCD in a
plane can be mapped to any other parallelogram A'DEMO'C'D' by some affi  ne transforma-
tion. If the areas of these parallelograms are nonzero, then the implied affi  ne transfor-
DEMO is defi ned uniquely by (three vertices of) the two parallelograms. If you like, you
can think of an affi  ne transformation DEMO drawing your image into a big rubber sheet and
then deforming the sheet by pushing or pulling* on the corners to make diff erent DEMO
of parallelograms.
When we have multiple images that we know to be slightly diff erent views of the same
object, we might want to compute the actual transforms that relate the diff erent views.
In this DEMO, affi  ne transformations are oft en used to model the views because, having
fewer parameters, they are easier to solve for. Th DEMO downside is that true perspective
distortions can only be modeled by a homography,† so affi  ne transforms yield a repre-
sentation that cannot accommodate all possible relationships between the views. On the
other hand, for small changes in viewpoint the resulting distortion is affi  ne, so DEMO some
circumstances an affi  ne transformation may be suffi  cient.
Affi  ne transforms can convert rectangles to parallelograms. Th ey can squash the shape
but must keep the sides parallel; they can rotate it and/or scale it. Perspective transfor-
mations off er more fl exibility; a perspective transform can turn a rectangle into a trap-
ezoid. Of course, since parallelograms are also trapezoids, affi  ne transformations are a
subset DEMO perspective transformations. Figure 6-13 shows examples of various affi  ne and
DEMO transformations.
Affine Transform
Th  ne transformations. In the fi rst
case, we have an image (or a region of interest) we’d like DEMO transform; in the second case,
we have a list of DEMO for which we’d like to compute the result of a transformation.
Dense affine transformations
In the fi rst case, the obvious input and output formats are images, and the implicit
requirement is that the warping assumes the pixels are a dense representation of the
* One can even DEMO in such a manner as to invert the parallelogram.
† “Homography” is the mathematical term for mapping points on one surface to points on DEMO In this
sense it is a more general term than as used here. In the context of computer vision, homography almost
always refers to mapping between points on two image planes that correspond to the same DEMO on
a planar object in the real world. It can be shown that such a mapping is representable by a single 3-by-3
orthogonal matrix (more on this in Chapter 11).
164 | Chapter 6: Image Transforms
⎡
⎢
X′≡ ⎢
⎣⎢
x
y
1
⎤
⎥
⎥
DEMO
ere are two situations that arise when working with affi
06-R4886-RC1.indd   164
9/15/08   4:21:18 PM
www.it-ebooks.info
Figure 6-13. Affi  ne and perspective transformations
underlying image. Th is means that image warping must necessarily handle interpola-
tions so that the DEMO images are smooth and look natural. Th e affi  ne transformation
DEMO provided by OpenCV for dense transformations is cvWarpAffine().
void cvWarpAffine(DEMO
const CvArr* src,
CvArr*       dst,
const CvMat* map_matrix,
int          flags      = DEMO | CV_WARP_FILL_OUTLIERS,
CvScalar     fillval    = cvScalarAll(0)DEMO
);
Here src and dst refer to an array or image, which can be either one or three channels
and of any type (provided they are the same type and size).* Th e map_matrix is the 2-by-3
matrix we introduced earlier that quantifi es the desired DEMO Th e next-to-
last argument, flags, controls the interpolation method as well as either or both of the
following additional options (as usual, combine with Boolean OR).
CV_WARP_FILL_OUTLIERS
Oft en, the transformed src DEMO does not fi t neatly into the dst image—there are
pixels “mapped” there from the source fi le that don’t actually exist. If this DEMO ag is set,
then those missing values are fi lled with fillval (described previously).
CV_WARP_INVERSE_MAP
Th is fl ag is for convenience to allow inverse warping from dst to src instead of from
src DEMO dst.
* Since rotating an image will usually make its bounding box larger, the result will be a clipped image. You
can circumvent this either by shrinking the image (as in the example code) or DEMO copying the fi rst image to a
central ROI within a larger source image prior to transformation.
Stretch, Shrink, Warp, and Rotate | 165
06-R4886-RC1.indd   165
9/15/08   4:21:18 PM
www.it-ebooks.info
cVWarpAffine performance
It is worth knowing that cvWarpAffine() involves substantial DEMO overhead.
An alternative is to use cvGetQuadrangleSubPix(). Th is function DEMO fewer options but
several advantages. In particular, it has less overhead DEMO can handle the special case
of when the source image is 8-bit and the destination image is a 32-bit fl oating-point
image. It will DEMO handle multichannel images.
void cvGetQuadrangleSubPix(
const CvArr* src,
CvArr*       dst,
const CvMat* map_matrix
);
What cvGetQuadrangleSubPix() DEMO is compute all the points in dst by mapping
them (with DEMO) from the points in src that were computed by applying the
DEMO  ne transformation implied by multiplication by the 2-by-3 map_matrix. (Conver-
sion of the locations in dst to homogeneous coordinates for the multiplication is DEMO
automatically.)
One idiosyncrasy of cvGetQuadrangleSubPix() is that there is an additional mapping ap-
plied by the function. In particular, the result points in dst are computed according to
the formula:
where:
dst DEMO(, ) (
xy a x a y b a x DEMO + +00 01 0 10 11′′ ′′ ′′, yb′′ + 1 )
Observe that the mapping from (x, y) to (x˝, y˝) has the eff ect that—even if the map-
ping M is an identity mapping—the points in the destination image at the center will
DEMO taken from the source image at the origin. If cvGetQuadrangleSubPix() needs points
from outside the image, it uses replication to reconstruct those values.
Computing the affine map matrix
OpenCV provides two functions to help you DEMO the map_matrix. Th e fi rst is used
when you already have two images that you know to be related by an affi  ne transforma-
tion or that you’d like to approximate in that way:
DEMO cvGetAffineTransform(
const CvPoint2D32f* pts_src,
const CvPoint2D32f* pts_dst,
CvMat*              map_matrix
);
166
| Chapter 6: Image Transforms
⎡ ⎤
Mmap ≡ ⎢ 00 01 0 ⎥ and
⎣ 10 11 1 ⎦
aa b
aa b
⎡
⎢
⎣
DEMO ′′
y ′′
⎡
⎤ ⎢
⎥ = ⎢
⎦ ⎢
⎣⎢
x − (( ) )width dst −1
2
y − (( ))height dst −1
2
⎤
⎥
⎥
⎥
⎦⎥
06-R4886-RC1.indd   DEMO
9/15/08   4:21:19 PM
www.it-ebooks.info
Here src and dst are arrays containing three two-dimensional (x, DEMO) points, and the
map_matrix is the affi  ne transform computed DEMO those points.
Th pts_src and pts_dst in cvGetAffineTransform() are just arrays of three points defi n-
ing two parallelograms. Th e simplest way DEMO defi ne an affi  ne transform is thus to set
pts_src DEMO three* corners in the source image—for example, the upper and lower DEMO
together with the upper right of the source image. Th e mapping from the source to
destination image is then entirely defi ned by DEMO pts_dst, the locations to which
these three points will be mapped DEMO that destination image. Once the mapping of these
three independent corners (DEMO, in eff ect, specify a “representative” parallelogram) is
established, all the other points can be warped accordingly.
Example 6-2 shows some code DEMO uses these functions. In the example we obtain the
cvWarpAffine() matrix parameters by fi rst constructing two three-component arrays of
points (the corners of our representative parallelogram) and then convert that to the
actual transformation matrix using cvGetAffineTransform(). We then do an affi  ne warp
DEMO by a rotation of the image. For our array of representative points in the source
image, called srcTri[], we take the three points: (0,0), (0,height-1), and (width-1,0). We
then specify the locations to which these points will be mapped in DEMO corresponding
array srcTri[].
Example 6-2. An affi
ne transformation
// Usage: warp_affine <image>
//
#include <cv.h>
#include <highgui.h>
int main(int argc, char** argv)
{
CvPoint2D32f srcTri[3], dstTri[3];DEMO
CvMat*       rot_mat = cvCreateMat(2,3,CV_32FC1);
DEMO       warp_mat = cvCreateMat(2,3,CV_32FC1);
IplImage     *src, *dst;
if( argc == 2 && ((DEMO(argv[1],1)) != 0 )) {
dst = cvCloneImage( src );
dst->origin = src->origin;
cvZero( dst );
// Compute warp matrix
//
srcTri[0].x = 0;                 //src Top left
srcTri[0].y = 0;
srcTri[1].x = src->width - 1;    //src Top DEMO
srcTri[1].y = 0;
srcTri[2].x = 0;                 //src Bottom left offset
srcTri[2].y = src->height - 1;
* We need just three points because, for an affi  ne transformation, we are only representing a parallelogram.
We will DEMO four points to represent a general trapezoid when we address perspective transformations.
Stretch, Shrink, Warp, and Rotate
| 167
e
06-R4886-RC1.indd   167
9/15/08   4:21:19 PM
www.it-ebooks.info
Example 6-2. An affi  ne transformation (continued)
dstTri[0].x = DEMO>width*0.0;    //dst Top left
dstTri[0].y = src->height*0.33;
dstTri[1].x = src->width*0.85;   //dst Top right
dstTri[1].y = src->height*0.25;
dstTri[2].x = src->width*0.15;   //dst Bottom left offset
dstTri[2].y = src->height*0.7;
cvGetAffineTransform( srcTri, dstTri, warp_mat );
cvWarpAffine( src, dst, warp_mat );
cvCopy( dst, src );
// Compute rotation matrix
//
CvPoint2D32f center = cvPoint2D32f(
src->width/2,
src->height/2
);
double angle = DEMO;
double scale = 0.6;
cv2DRotationMatrix( center, angle, scale, rot_mat );
// Do the transformation
//
cvWarpAffine( src, dst, rot_mat );
cvNamedWindow( “Affine_Transform”, 1 );
cvShowImage( DEMO, dst );
cvWaitKey();
}
cvReleaseImage( &dst );DEMO
cvReleaseMat( &rot_mat );
cvReleaseMat( &warp_mat );
return 0;
}
}
Th map_matrix is to use cv2DRotationMatrix(), which com-
putes the map matrix for a rotation around some arbitrary point, combined with an op-
tional rescaling. Th is is just one possible kind DEMO affi  ne transformation, but it represents
an important subset that has an alternative (and more intuitive) representation that’s
easier to work with DEMO your head:
CvMat* cv2DRotationMatrix(
CvPoint2D32f center,
double       angle,
double       scale,
CvMat*       map_matrix
);
Th rst argument, center, is the center point of the rotation. Th e next two arguments
give the magnitude of DEMO rotation and the overall rescaling. Th e fi nal argument is the
output map_matrix, which (as always) is a 2-by-3 matrix of fl oating-point numbers).
168
| Chapter 6: Image Transforms
e second way to compute the
e fi
06-R4886-RC1.indd   168
9/15/08   DEMO:21:19 PM
www.it-ebooks.info
⋅ cos( ) and β= ⋅ sin( ) then this DEMO computes
If we defi ne α= scal ngleea scal ngleea
the map_matrix to be:
−−αβ⋅⋅
1−
⎡ αβ center center ⎤
⎢ xy DEMO
⋅⋅ centery ⎦⎥
()1
⎣⎢−+βα βα⋅ centerx ()
You can combine these methods of setting the map_matrix to obtain, for example, DEMO
image that is rotated, scaled, and warped.
Sparse affine transformations
We have explained that cvWarpAffine() is the right way to handle dense DEMO
For sparse mappings (i.e., mappings of lists of individual points), it is best to use
cvTransform():
void cvTransform(
const DEMO src,
CvArr*       dst,
const CvMat* transmat,
const CvMat* shiftvec = NULL
);
In general, src is an N-by-1 array with Ds channels, where N is the number of points to
be transformed and Ds is the dimension of those source points. DEMO e output array dst
must be the same size but may have a diff erent number of channels, Dd. Th e transforma-
tion matrix transmat is a Ds-by-Dd matrix that is then applied to every element DEMO src, af-
ter which the results are placed into dst. Th DEMO optional vector shiftvec, if non-NULL, must
be a Ds-by-1 array, DEMO is added to each result before the result is placed in dst.
In our case of an affi  ne transformation, there are two DEMO to use cvTransform() that
depend on how we’d like to represent our transformation. In the fi rst method, we de-
compose our transformation into the 2-by-2 part (which does rotation, scaling, and
warping) DEMO the 2-by-1 part (which does the transformation). Here our input DEMO an
N-by-1 array with two channels, transmat is our local homogeneous DEMO,
and shiftvec contains any needed displacement. Th e second method is to use our usual
2-by-3 representation of the affi  ne transformation. In this case the input array src is a
three-channel array within which DEMO must set all third-channel entries to 1 (i.e., the
points must be supplied in homogeneous coordinates). Of course, the output array will
still be a two-channel array.
Perspective Transform
To gain the greater fl DEMO off ered by perspective transforms (homographies), we
need a new DEMO that will allow us to express this broader class of transformations.
First we remark that, even though a perspective projection is specifi ed completely by a
single matrix, the projection is not actually a linear transformation. Th is is because the
transformation requires division by the fi nal DEMO (usually Z; see Chapter 11) and
thus loses a dimension DEMO the process.
Stretch, Shrink, Warp, and Rotate | 169
06-R4886-RC1.indd   169
9/15/08   4:21:19 PM
www.it-ebooks.info
As with affi
by diff
ne transformations, image operations (dense DEMO) are handled
erent functions than transformations on point sets (sparse transformations).
Dense perspective transform
Th
provided for dense affi  ne transformations. Specifi cally, cvWarpPerspective() has all of
the same arguments as cvWarpAffine() but with the small, but crucial, distinction that
the map matrix DEMO now be 3-by-3.
void cvWarpPerspective(
const CvArr* src,
CvArr*       dst,
const CvMat* map_matrix,
int          flags     = CV_INTER_LINEAR + CV_WARP_FILL_OUTLIERS,
CvScalar     DEMO   = cvScalarAll(0)
);
Th
ags are the same here as for the affi
ne case.
e fl
e dense perspective DEMO uses an OpenCV function that is analogous to the one
Computing the perspective map matrix
As with the affi  ne transformation, for fi DEMO the map_matrix in the preceding code we
have a convenience function that can compute the transformation matrix from a list of
point correspondences:
DEMO cvGetPerspectiveTransform(
const CvPoint2D32f* pts_src,
const CvPoint2D32f* pts_dst,
CvMat*              map_matrix
);
Th pts_src and pts_dst are now arrays of four (not three) points, so we can inde-
pendently control how the corners of (typically) a rectangle in DEMO are mapped to
(generally) some rhombus in pts_dst. Our transformation is completely defi ned by
the specifi ed destinations of the four source DEMO As mentioned earlier, for perspec-
tive transformations we must allocate a DEMO array for map_matrix; see Example 6-3
for sample code. Other than DEMO 3-by-3 matrix and the shift  from three to four con-
trol DEMO, the perspective transformation is otherwise exactly analogous to the affi  ne
transformation we already introduced.
Example 6-3. Code for perspective transformation
// Usage: warp <image>
//
#include <cv.h>
#include <highgui.h>DEMO
int main(int argc, char** argv) {
CvPoint2D32f srcQuad[4], dstQuad[4];DEMO
CvMat*       warp_matrix = cvCreateMat(3,3,CV_32FC1);
DEMO     *src, *dst;
170
| Chapter 6: Image Transforms
e
06-R4886-RC1.indd   170
9/15/08   4:21:20 PM
www.it-ebooks.info
Example 6-3. Code for perspective transformation (continued)
if( argc DEMO 2 && ((src=cvLoadImage(argv[1],1)) != 0 )) {
dst = cvCloneImage(src);
dst->origin = src->origin;
cvZero(dst);
srcQuad[0].x = 0;               //src Top left
srcQuad[0].y = 0;
srcQuad[1].x = src->width - 1;  //src Top right
srcQuad[1].y = 0;
srcQuad[2].x DEMO 0;               //src Bottom DEMO
srcQuad[2].y = src->height - 1;
srcQuad[3].x = src->width – 1;  //src Bot right
srcQuad[3].y = src->height - 1;DEMO
dstQuad[0].x = src->width*0.05; //dst Top left
dstQuad[0].y = src->DEMO;
dstQuad[1].x = src->width*0.9;  //dst Top right
dstQuad[1].y = src->height*0.25;
dstQuad[2].x = src->width*0.2;  //dst Bottom left
DEMO = src->height*0.7;
dstQuad[3].x = src->width*0.8;  //dst Bot right
dstQuad[3].y = src->height*0.9;
cvGetPerspectiveTransform(
srcQuad,
dstQuad,
DEMO
);
cvWarpPerspective( src, dst, warp_matrix );
cvNamedWindow( “Perspective_Warp”, 1 );
cvShowImage( “Perspective_Warp”, dst );
cvWaitKey();
}
cvReleaseImage(&dst);
cvReleaseMat(&warp_matrix);
return 0;
DEMO
}
Sparse perspective transformations
Th cvPerspectiveTransform(), that performs perspective trans-
DEMO on lists of points; we cannot use cvTransform(), which is limited to linear op-
erations. As such, it cannot handle perspective transforms because they require division
by the third coordinate of the homogeneous representation (x = f ∗ X/Z, y = f ∗ Y/Z). Th e
special function cvPerspectiveTransform() takes care of this for DEMO
void cvPerspectiveTransform(
const CvArr* src,
CvArr*       dst,
const CvMat* mat
);
Stretch, Shrink, Warp, and Rotate
| 171
ere is a special function,
06-R4886-RC1.indd   171
9/DEMO/08   4:21:20 PM
www.it-ebooks.info
As usual, the src and dst arguments are (respectively) the array of source points to be
transformed and the array of destination DEMO; these arrays should be of three-channel,
fl oating-point type. Th DEMO matrix mat can be either a 3-by-3 or a 4-by-4 matrix. If it is
3-by-3 then the projection is from two dimensions to two; if the matrix is 4-by-4, then
the projection is from four dimensions to three.
In the current context we are transforming a set of DEMO in an image to another set of
points in an image, DEMO sounds like a mapping from two dimensions to two dimen-
sions. But this is not exactly correct, because the perspective transformation is actually
mapping points on a two-dimensional plane embedded in a three-dimensional space
back down DEMO a (diff erent) two-dimensional subspace. Th ink of this as being just what
a camera does (we will return to this topic in greater detail when discussing cameras
in later chapters). Th e camera DEMO points in three dimensions and maps them to the
two dimensions of the camera imager. Th is is essentially what is meant when the DEMO
points are taken to be in “homogeneous coordinates”. We are adding an additional
dimension to those points by introducing the Z dimension and then DEMO all of the
Z values to 1. Th e projective transformation is then projecting back out of that space
onto the two-dimensional space of DEMO output. Th is is a rather long-winded way of ex-
plaining why, when mapping points in one image to points in another, you DEMO need a
3-by-3 matrix.
Output of the code in Example 6-3 is shown in Figure 6-14 for affi  ne and perspective
transformations. Compare this with the diagrams of Figure 6-13 to see how this works
with DEMO images. In Figure 6-14, we transformed the whole image. Th is DEMO necessary;
we could have used the src_pts to defi ne a smaller (or larger!) region in the source im-
age to be DEMO We could also have used ROIs in the source or destination image
in order to limit the transformation.
CartToPolar and PolarToCart
Th cvCartToPolar() DEMO cvPolarToCart() are employed by more complex rou-
tines such as cvLogPolar() (described later) but are also useful in their own right. DEMO ese
functions map numbers back and forth between a Cartesian (x, y) space and a polar or
radial (r, θ) space (i.e., from Cartesian to polar coordinates and vice versa). Th e function
formats are as follows:
void cvCartToPolar(
const CvArr* x,DEMO
const CvArr* y,
CvArr*       magnitude,
CvArr*       angle            = NULL,
int          angle_in_degrees = 0
);
void cvPolarToCart(
const CvArr* magnitude,
const CvArr* angle,
CvArr*       DEMO,
CvArr*       y,
e functions
172
| Chapter 6: Image Transforms
06-R4886-RC1.indd   172
9/15/08   4:21:20 PM
www.it-ebooks.info
Figure 6-14. Perspective and affi
ne mapping of an image
int          angle_in_degrees = 0
);
In each of DEMO functions, the fi rst two two-dimensional arrays or images are the DEMO
and the second two are the outputs. If an output pointer is set to NULL then it will not
be computed. Th e requirements DEMO these arrays are that they be fl oat or doubles and
matching (size, number of channels, and type). Th e last parameter specifi es whether we
are working with angles in degrees (0, DEMO) or in radians (0, 2π).
For an example of DEMO you might use this function, suppose you have already taken the
DEMO and y-derivatives of an image, either by using cvSobel() or DEMO using convolution func-
tions via cvDFT() or cvFilter2D(). If DEMO stored the x-derivatives in an image dx_img and
the y-derivatives in dy_img, you could now create an edge-angle recognition histogram.
Th at is, DEMO can collect all the angles provided the magnitude or strength of the edge pixel
CartToPolar and PolarToCart
| 173
06-R4886-RC1.indd   173
9/15/DEMO   4:21:20 PM
is above a certain threshold. To calculate this, we create two destination images of the
same type (integer or float) as the derivative DEMO and call them img_mag and img_an-
gle. If you want the result to be given in degrees, then you can use the function cvCartTo
Polar( dx_img, dy_img, img_mag, img_angle, 1 ). We would DEMO fi ll the histogram
from img_angle as long as the corresponding “pixel” in img_mag is above the threshold.
LogPolar
⋅
is relative to some DEMO point (xc, yc), we take the log so that ρ
()
For two-dimensional images, the log-polar transform [Schwartz80] is a change
from Cartesian to polar coordinates: (, )xy re↔ iθ, where DEMO and
exp( ) exp( arctan( )ii yxθ = ). DEMO separate out the polar coordinates into a (ρ, θ) space DEMO
rx y=+
cc
and θ=− −arctan(( ) ( ))yy x xcc . For image purposes—when we need to “fi t” the inter-
DEMO stuff  into the available image memory—we typically apply a scaling factor DEMO to ρ.
Figure 6-15 shows a square object on the left  DEMO its encoding in log-polar space.
Figure 6-15. Th e log-polar transform maps (x, y) into (log(r),θ); here, a DEMO is displayed in the
log-polar coordinate system
Th e log-polar transform takes its in-
spiration from the human visual system. Your eye has a DEMO but dense center of
photoreceptors in its center (the fovea), DEMO the density of receptors fall off  rapidly (ex-
ponentially) from DEMO Try staring at a spot on the wall and holding your fi nger at
arm’s length in your line of sight. Th en, keep staring at the spot and move your fi nger
slowly away; note how the detail rapidly decreases as the image of your fi nger DEMO
away from your fovea. Th is structure also has certain nice mathematical properties (be-
yond the scope of this book) that concern preserving DEMO angles of line intersections.
More important for us is that the log-polar transform can be used to create two-
dimensional invariant representations of object DEMO by shift ing the transformed im-
age’s center of mass to a fi xed point in the log-polar plane; see Figure 6-16. On the left  are
174 | Chapter 6: Image Transforms
e next question DEMO, of course, “Why bother?” Th
22
=− +−log ( ) ( )xx y y
06-R4886-RC1.indd   174
www.it-ebooks.info
9/15/08   4:21:21 PM
three shapes that we want to recognize as “square”. Th e problem DEMO, they look very diff er-
ent. One is much larger than DEMO others and another is rotated. Th e log-polar transform
appears on the right in Figure 6-16. Observe that size diff erences in the (x, y) plane are
converted to shift s along the log(r) axis of the log-polar plane and that the rotation diff er-
ences DEMO converted to shift s along the θ-axis in the log-polar plane. If we take the trans-
formed center of each transformed square in the DEMO plane and then recenter that
point to a certain fi xed position, then all the squares will show up identically in the log-
polar plane. Th is yields a type of invariance to two-dimensional rotation and DEMO
Figure 6-16. Log-polar transform of rotated and scaled squares: size goes DEMO a shift  on the log(r) axis
and rotation to a shift
on the θ-axis
e OpenCV function for a log-polar transform is DEMO():
void cvLogPolar(
const CvArr* src,
CvArr*       dst,
CvPoint2D32f center,
double       m,
DEMO          flags  = CV_INTER_LINEAR | CV_WARP_FILL_OUTLIERS
);
Th src and dst are one- or three-channel color or grayscale images. DEMO e parameter
center is the center point (xc, yc) of DEMO log-polar transform; m is the scale factor, which
* In Chapter 13 we’ll learn about recognition. For now simply note that it wouldn’t DEMO a good idea to derive a
log-polar transform for a whole object because such transforms are quite sensitive to the exact location of
their DEMO points. What is more likely to work for object recognition is to detect a collection of key points
(such as corners or blob locations) around an object, truncate the extent of such views, and then use the
centers of those key points as log-polar centers. Th ese DEMO log-polar transforms could then be used to cre-
ate local features that are (partially) scale- and rotation-invariant and that can be associated with DEMO visual
object.
LogPolar
| 175
Th
e
06-R4886-RC1.indd   175
www.it-ebooks.info
9/15/08   4:21:22 PM
should be set so that the features of interest dominate the available DEMO area. Th e flags
parameter allows for diff erent interpolation methods. Th e interpolation methods are the
same set of standard interpolations available in DEMO (Table 6-1). Th e interpolation
methods can be combined with DEMO or both of the fl ags CV_WARP_FILL_OUTLIERS (to fi ll
points DEMO would otherwise be undefi ned) or CV_WARP_INVERSE_MAP (to compute the re-
verse mapping from log-polar to Cartesian coordinates).
Sample log-polar coding is DEMO in Example 6-4, which demonstrates the forward and
backward (inverse) DEMO transform. Th e results on a photographic image are shown
in Figure 6-17.
Figure 6-17. Log-polar example on an elk with transform centered at DEMO white circle on the left ; the
output is on the right
Example 6-4. Log-polar transform example
// logPolar.cpp : Defines the entry point for the console application.
//
#include <cv.h>
#include <highgui.h>DEMO
int main(int argc, char** argv) {
IplImage* src;
double    M;
if( argc == 3 && ((src=cvLoadImage(argv[1],1)) != 0 )) {
M = atof(argv[2]);
DEMO dst  = cvCreateImage( cvGetSize(src), 8, 3 );
DEMO src2 = cvCreateImage( cvGetSize(src), 8, 3 );
cvLogPolar(
src,
dst,
cvPoint2D32f(src->width/4,src->height/2),
M,
CV_INTER_LINEAR+CV_WARP_FILL_OUTLIERS
176 | Chapter 6: Image Transforms
06-R4886-RC1.indd   176
www.it-ebooks.info
9/15/08   4:21:22 PM
www.it-ebooks.info
Example 6-4. Log-polar transform example (continued)
);
cvLogPolar(
dst,
src2,
cvPoint2D32f(src->width/4, src->height/2),DEMO
M,
CV_INTER_LINEAR | CV_WARP_INVERSE_MAP
);
cvNamedWindow( “log-polar”, 1 );
cvShowImage( “log-polar”, dst );
cvNamedWindow( “inverse log-polar”, 1 );
cvShowImage( “inverse log-polar”, src2 );
cvWaitKey();
}
return 0;
}
Discrete Fourier Transform (DFT)
For any set of values that are indexed by a discrete (integer) parameter, is it possible to
defi ne a discrete Fourier transform (DFT)* in a manner analogous to the Fourier trans-
form of a continuous function. DEMO N complex numbers xx01,,… N − , the one-dimensional
DFT DEMO defi ned by the following formula (where i =−1):
f =−∑N −1 x ex ⎛
knn=0 ⎝
πi ⎞
p,⎜ 2N DEMO k N⎟ =−01,...,
⎠
A similar transform can be defi ned for a two-dimensional array of numbers (of course
higher-dimensional analogues exist also):
⎞
kn ⎟
yy ⎠
N x −1 N y −1 ⎛
f kk =−
nx =0 ny =0 x y ⎝
DEMO ⎞ ⎛ i
N x knxx ⎠⎟
exp ⎜ −
⎝
N y
In general, one might expect that the computation of the N diff erent terms f k would
require O(N 2) operations. In fact, there are a number of fast Fourier transform (FFT) al-
gorithms capable of computing these values in O(N log N) time. Th e OpenCV function
cvDFT() implements one such FFT algorithm. Th DEMO function cvDFT() can compute FFTs
for one- and two-dimensional arrays of inputs. In the latter case, the two-dimensional
transform can be computed or, if desired, only the one-dimensional transforms of each
individual row can DEMO computed (this operation is much faster than calling cvDFT()
many separate times).
* Joseph Fourier [Fourier] was the fi rst to DEMO nd that some functions can be decomposed into an infi nite series
of other functions, and doing so became a fi eld known as Fourier analysis. Some key text on methods of
decomposing functions into their DEMO series are Morse for physics [Morse53] and Papoulis in general
[Papoulis62]. Th e fast Fourier transform was invented by Cooley and Tukeye in 1965 DEMO though
Carl Gauss worked out the key steps as early as 1805 [Johnson84]. Early use in computer vision is described
by Ballard and Brown DEMO
Discrete Fourier Transform (DFT) | 177
xy
∑∑ xn n exp ⎜
06-R4886-RC1.indd   177
9/15/08   4:21:23 PM
www.it-ebooks.info
void cvDFT(
const CvArr* src,
CvArr*       DEMO,
int          flags,
int          nonzero_rows = 0
);
Th oating-point types and may DEMO single- or
double-channel arrays. In the single-channel case, the entries are DEMO to be real
numbers and the output will be packed in a special space-saving format (inherited from
the same older IPL library as the IplImage structure). If the source and channel are two-
channel matrices DEMO images, then the two channels will be interpreted as the real DEMO
imaginary components of the input data. In this case, there will DEMO no special packing of
the results, and some space will be DEMO with a lot of 0s in both the input and output
arrays.*
Th e special packing of result values that is used with single-channel DEMO is as
follows.
For a one-dimensional array:
e input and the output arrays must be fl
Re Y0 Re Y1 Im Y1 Re DEMO Im Y2 … Re Y(N/2–1) Im Y(N/2–1) Re Y(N/2)
For a two-dimensional array:
Re Y00 DEMO Y01 Im Y01 Re Y02 Im Y02 … Re Y0(Nx/2–1) Im Y0(Nx/2–1) Re Y0(Nx/2)
Re Y10 DEMO Y11 Im Y11 Re Y12 Im Y12 … Re Y1(Nx/2–1) Im Y1(Nx/2–1) Re Y1(Nx/2)
Re Y20 DEMO Y21 Im Y21 Re Y22 Im Y22 … Re Y2(Nx/2–1) Im Y2(Nx/2–1) Re Y2(Nx/2)
Re Y(DEMO/2–1)0 Re Y(Ny–3)1 Im Y(Ny–3)1 Re Y(Ny–3)2 Im Y(Ny–3)2 … Re Y(Ny–3)(Nx/2–1) Im Y(Ny–3)(Nx/2–1) Re Y(Ny–3)(Nx/2)DEMO
Im Y(Ny/2–1)0 Re Y(Ny–2)1 Im Y(Ny–2)1 Re Y(Ny–2)2 Im Y(Ny–2)2 … Re Y(DEMO)(Nx/2–1) Im Y(Ny–2)(Nx/2–1) Re Y(Ny–2)(Nx/2)
Re Y(Ny/2)0 Re Y(Ny–1)1 Im Y(Ny–1)1 Re Y(Ny–1)2 Im Y(Ny–1)2 DEMO Re Y(Ny–1)(Nx/2–1) Im Y(Ny–1)(Nx/2–1) Re Y(Ny–1)(Nx/2)
It is worth taking a moment to look closely at the indices on these arrays. Th e issue DEMO
is that certain values are guaranteed to be 0 (more accurately, certain values of f k are
guaranteed to be real). It DEMO also be noted that the last row listed in the table will be
present only if Ny is even and that the last column DEMO be present only if Nx is even. (In the
case of DEMO 2D array being treated as Ny 1D arrays rather than a full 2D transform, all of
the result rows will be analogous to the single row listed for the output of the 1D array).
* DEMO using this method, you must be sure to explicitly set the DEMO components to 0 in the two-
channel representation. An easy way to do this is to create a matrix full of 0s using cvZero() for the
imaginary part and then to call cvMerge() with a real-valued matrix to form a temporary complex array on
which to run DEMO() (possibly in-place). Th is procedure will result in full-size, unpacked, complex matrix
of the spectrum.
178 | Chapter 6: Image DEMO
06-R4886-RC1.indd   178
9/15/08   4:21:23 PM
…
…
…
…
…
…
…
…
…
www.it-ebooks.info
Th flags, indicates exactly what operation is to be done. Th e
transformation we started with is known as a forward transform and DEMO selected with the
fl ag CV_DXT_FORWARD. Th e inverse transform* is defi ned in exactly the same way except
for a change of sign DEMO the exponential and a scale factor. To perform the inverse trans-
form without the scale factor, use the fl ag CV_DXT_INVERSE. Th e fl ag for the scale factor is
CV_DXT_SCALE, and this results in all of the output being scaled by a factor of 1/N (or 1/Nx Ny
for a 2D transform). Th is scaling is DEMO if the sequential application of the forward
transform and the inverse transform is to bring us back to where we started. Because one
oft DEMO wants to combine CV_DXT_INVERSE with CV_DXT_SCALE, there are several shorthand
notations DEMO this kind of operation. In addition to just combining the two operations
with OR, you can use CV_DXT_INV_SCALE (or CV_DXT_INVERSE_SCALE if you’re not DEMO that
brevity thing). Th e last fl ag you may want to have handy is CV_DXT_ROWS, which allows
you to tell cvDFT() to treat a two-dimensional array as a collection of one-dimensional
arrays that DEMO each be transformed separately as if they were Ny distinct vectors of
length Nx. Th is signifi cantly reduces overhead when doing many transformations DEMO a
time (especially when using Intel’s optimized IPP libraries). By DEMO CV_DXT_ROWS it is
also possible to implement three-dimensional (and higher) DFT.
In order to understand the last argument, nonzero_rows, we must digress DEMO a moment.
In general, DFT algorithms will strongly prefer vectors of DEMO lengths over others or
arrays of some sizes over others. In most DFT algorithms, the preferred sizes are pow-
ers of 2 (i.e., 2n for some integer n). In the case of the algorithm DEMO by OpenCV, the
preference is that the vector lengths, or array dimensions, be 2p3q5r, for some integers
p, q, and r. DEMO the usual procedure is to create a somewhat larger array (for DEMO
purpose there is a handy utility function, cvGetOptimalDFTSize(), which takes the length
of your vector and returns the fi rst equal or DEMO appropriate number size) and then
use cvGetSubRect() to copy your DEMO into the somewhat roomier zero-padded array.
Despite the need for this padding, it is possible to indicate to cvDFT() that you really do
not care about the transform of those rows that you had to DEMO down below your actual
data (or, if you are doing an inverse transform, which rows in the result you do not care
about). In either case, you can use nonzero_rows to indicate how many rows can be safely
ignored. Th is will provide some savings in DEMO time.
Spectrum Multiplication
In many applications that involve computing DFTs, one DEMO also compute the per-
element multiplication of two spectra. Because such results are typically packed in their
special high-density format and are usually complex DEMO, it would be tedious to
unpack them and handle the multiplication DEMO the “usual” matrix operations. Fortu-
nately, OpenCV provides the handy cvMulSpectrums() routine, which performs exactly
this function as well as a few DEMO handy things.
* With the inverse transform, the input is packed DEMO the special format described previously. Th
because, if we fi
wind DEMO with the original data—that is, of course, if we remember to use the CV_DXT_SCALE fl
is makes sense
rst called the forward DFT DEMO then ran the inverse DFT on the results, we would expect DEMO
ag!
Discrete Fourier Transform (DFT)
| 179
e third argument, called
06-R4886-RC1.indd   179
9/15/08   4:21:24 PM
www.it-ebooks.info
void cvMulSpectrums(
const CvArr* src1,
const CvArr* src2,
DEMO       dst,
int          flags
);
Note that the fi rst two arguments are the usual input arrays, though in this case they are
spectra from calls to cvDFT(). Th e third argument must be a pointer to an array—of the
same type and size as the fi rst two—that will be DEMO for the results. Th e fi nal argument,
flags, tells DEMO() exactly what you want done. In particular, it may be DEMO to 0
(CV_DXT_FORWARD) for implementing the above pair multiplication or set to CV_DXT_MUL_CONJ
if the element from the fi rst array is to DEMO multiplied by the complex conjugate of the
corresponding element of the second array. Th e fl ags may also be combined with CV_
DXT_ROWS DEMO the two-dimensional case if each array row 0 is to be treated as a separate
spectrum (remember, if you created the spectrum arrays DEMO CV_DXT_ROWS then the data
packing is slightly diff erent than if you created them without that function, so you must
be consistent in the way you call cvMulSpectrums).
Convolution and DFT
It is possible to DEMO increase the speed of a convolution by using DFT via the convo-
lution theorem [Titchmarsh26] that relates convolution in the spatial domain to multi-
DEMO in the Fourier domain [Morse53; Bracewell65; Arfk en85].* To accomplish this,
one fi rst computes the Fourier transform of the image and DEMO the Fourier transform
of the convolution fi lter. Once this is done, the convolution can be performed in the
transform space in linear time with respect to the number of pixels in the image. It is
DEMO to look at the source code for computing such a convolution, DEMO it also will
provide us with many good examples of using cvDFT(). Th e code is shown in Example
6-5, which is DEMO directly from the OpenCV reference.
Example 6-5. Use of cvDFT() to accelerate the computation of convolutions
// Use DFT to accelerate the convolution of array A by kernel B.
// Place the result in array V.
//
void speedy_conv olution(
const CvMat* A, // DEMO: M1xN1
const CvMat* B,  // Size: M2xN2
CvMat*       C   // Size:(A->rows+B->rows-1)x(A->cols+B->cols-1)
) {
int dft_M = cvGetOptimalDFTSize( A->rows+B->rows-1 );
int dft_N = cvGetOptimalDFTSize( A->cols+B->cols-1 );
CvMat* dft_A = cvCreateMat( dft_M, dft_N, A->type );
CvMat* dft_B = cvCreateMat( dft_M, dft_N, B->type );
CvMat tmp;
* Recall that OpenCV’s DFT algorithm implements the FFT whenever the data size DEMO the FFT faster.
180
| Chapter 6: Image Transforms
06-R4886-RC1.indd   DEMO
9/15/08   4:21:24 PM
www.it-ebooks.info
Example 6-5. Use of cvDFT() to accelerate the computation of DEMO (continued)
// copy A to dft_A and pad dft_A with zeros
//
cvGetSubRect( dft_A, &tmp, cvRect(0,0,A->DEMO,A->rows));
cvCopy( A, &tmp );
cvGetSubRect(DEMO
dft_A,
&tmp,
cvRect( A->cols, 0, dft_A->cols-A->cols, A->rows )
);
cvZero( &tmp );
// no need to pad bottom part of dft_A with zeros because DEMO
// use nonzero_rows parameter in cvDFT() call below
//
DEMO( dft_A, dft_A, CV_DXT_FORWARD, A->rows );
// repeat DEMO same with the second array
//
cvGetSubRect( dft_B, &tmp, cvRect(0,0,B->cols,B->rows) );
cvCopy( B, &tmp );
cvGetSubRect(
dft_B,
&tmp,
cvRect( B->cols, 0, dft_B->cols-B->cols, B->rows )
);
DEMO( &tmp );
// no need to pad bottom part DEMO dft_B with zeros because of
// use nonzero_rows parameter in cvDFT() call below
//
cvDFT( dft_B, dft_B, CV_DXT_FORWARD, B->rows );
// or CV_DXT_MUL_CONJ to get correlation rather than convolution
//
cvMulSpectrums( dft_A, dft_B, dft_A, 0 );
// calculate only the top part
//
cvDFT( dft_A, dft_A, CV_DXT_INV_SCALE, DEMO>rows );
cvGetSubRect( dft_A, &tmp, cvRect(0,0,conv->cols,C->rows) );
cvCopy( &tmp, C );
DEMO( dft_A );
cvReleaseMat( dft_B );
}
In Example 6-5 we can see that the input arrays are fi rst created and DEMO initialized.
Next, two new arrays are created whose dimensions are optimal DEMO the DFT algorithm.
Th
puted. Finally, the spectra are multiplied together DEMO the inverse transform is applied
Discrete Fourier Transform (DFT)
| DEMO
e original arrays are copied into these new arrays and then the transforms are com-
06-R4886-RC1.indd   181
9/15/08   4:21:DEMO PM
www.it-ebooks.info
to the product. Th e transforms are the slowest* part of DEMO operation; an N-by-N im-
age takes O(N 2 log N) time and so the entire process is also completed in that time
(assuming that N > M for an M-by-M convolution kernel). Th DEMO time is much faster than
O(N2M 2), the non-DFT convolution time required by the more naïve method.
Discrete Cosine Transform (DCT)
For real-valued data it is oft en suffi  cient to compute what is, in eff ect, only half of the
discrete Fourier transform. DEMO e discrete cosine transform (DCT) [Ahmed74; Jain77] is
defi ned DEMO to the full DFT by the following formula:
⎧
⎪⎪ 1 if n = 0
cnk ==∑N −1 ⎨ N −π(21kn+ ) DEMO
n=0 ⎪ 2 N ⎠⎟
⎩⎪ N
⋅⋅xn cos ⎛⎝⎜
else
Observe that, by convention, the normalization factor is applied to both the DEMO trans-
form and its inverse. Of course, there is a similar DEMO for higher dimensions.
Th e basic ideas of the DFT apply also to the DCT, but now all the coeffi  cients are real-
DEMO Astute readers might object that the cosine transform is being applied to a vec-
tor that is not a manifestly even function. However, with cvDCT() the algorithm simply
treats the vector as if it were DEMO to negative indices in a mirrored manner.
e actual OpenCV call is:
void cvDCT(
const CvArr* src,
CvArr*       DEMO,
int          flags
);
Th cvDCT() function expects arguments like those for cvDFT() except that, because DEMO
results are real-valued, there is no need for any special packing DEMO the result array (or of
the input array in the case DEMO an inverse transform). Th e flags argument can be set to
CV_DXT_FORWARD or CV_DXT_INVERSE, and either may be combined with CV_DXT_ROWS with
the same eff ect as with cvDFT(). Because of the diff erent normalization convention, both
the forward and inverse cosine transforms always contain their respective contribution
to the overall normalization of the transform; hence CV_DXT_SCALE plays no role in cvDCT.
Integral Images
OpenCV allows you to calculate an DEMO image easily with the appropriately named
cvIntegral() function. An integral image [Viola04] is a data structure that allows rapid
* By “slowest” we DEMO “asymptotically slowest”—in other words, that this portion of the algorithm takes DEMO
most time for very large N. Th is is an important distinction. In practice, as we saw in the earlier section on
convolutions, DEMO is not always optimal to pay the overhead for conversion to Fourier space. In general, when
convolving with a small kernel it will not be worth the trouble to make this transformation.
182 | Chapter 6: Image Transforms
Th
e
06-R4886-RC1.indd   182
9/15/08   4:DEMO:24 PM
www.it-ebooks.info
summing of subregions. Such summations are useful in many applications; a notable
one is the computation of Haar wavelets, which are used in some face recognition and
similar algorithms.
void cvIntegral(
const CvArr*  image,
CvArr*        sum,
CvArr*        DEMO      = NULL,
CvArr*        tilted_sum = NULL
);
Th cvIntegral() are the original image as well DEMO pointers to destination
images for the results. Th e argument sum is required; the others, sqsum and tilted_sum,
may be provided if DEMO (Actually, the arguments need not be images; they could
be DEMO, but in practice, they are usually images.) When the input DEMO is 8-bit
unsigned, the sum or tilted_sum may be 32-bit integer DEMO fl oating-point arrays. For all
other cases, the sum or tilted_sum DEMO be fl oating-point valued (either 32- or 64-bit).
Th oating-point. DEMO the input image is of size W-by-H,
then the output images must be of size (W + 1)-by-(H + 1).*
An integral image sum has the form:
sum
(, ) ( , )XY =
∑∑image x y
e optional
xX≤≤
yY
and the tilted_sum is like the sum except that it is for the image DEMO by 45 degrees:
tilt_sum
(, )XY
=∑∑
yY≤
abs
()xX y−≤
image
( , )x y
Using these integral images, DEMO may calculate sums, means, and standard deviations
over arbitrary upright or “tilted” rectangular regions of the image. As a simple exam-
ple, to sum over a simple rectangular region described by the corner points (x1, y1) and
(x2, y2), where x2 > x1 and DEMO > y1, we’d compute:
∑∑
[(,)]image xy
x DEMO
yy y12≤≤
12xx
=−[( , )sum suxy22
ms(, ) ( , ) (, )]xy xy xy11 2 2 11 11 11−− DEMO − −um sum
In this way, it is possible to do DEMO blurring, approximate gradients, compute means and
standard deviations, and perform DEMO block correlations even for variable window sizes.
* Th
is is because we need to put in a buff
computation effi
cient.
er of DEMO values along the x-axis and y-axis in order to make
Integral Images
| 183
e arguments to
e result “images” must always be fl
DEMO(, ) ( ( , ))XY = ∑∑ image x y
≤≤yY
2
Th
sqsum image is the sum of squares:
xX
DEMO   183
9/15/08   4:21:24 PM
To make this all a little more clear, consider the 7-by-5 image shown in Figure 6-18; the
region is shown as a bar chart in which the height associated with the pixels represents
the brightness of DEMO pixel values. Th e same information is shown in Figure 6-19, DEMO
merically on the left  and in integral form on the right. DEMO images (I') are computed
by going across rows, proceeding row by row using the previously computed integral
image values together with the DEMO raw image (I) pixel value I(x, y) to calculate the
next integral image value as follows:
Ixy Ixy Ix y DEMO Ix y′(,) (,) (,) (, ) (,=+ ′ −+11 1′ −− ′ − −1)
Figure 6-18. Simple 7-by-5 DEMO shown as a bar chart with x, y, and height equal to pixel value
Th  because this value is double-counted when adding the sec-
ond and third terms. You can verify that this works by DEMO some values in Figure 6-19.
When using the integral image to compute a region, we can see by Figure 6-19 that, in
order DEMO compute the central rectangular area bounded by the 20s in the original image,
we’d calculate 398 – 9 – 10 + 1 = DEMO Th us, a rectangle of any size can be computed us-
DEMO four measurements (resulting in O(1) computational complexity).
184
| Chapter 6: Image Transforms
e last term is subtracted off
06-R4886-RC1.indd   184
www.it-ebooks.info
9/15/08   4:21:25 PM
www.it-ebooks.info
Figure 6-19. Th
be the upper-left
e 7-by-5 image of Figure DEMO shown numerically at left
) and converted to an integral image at right
(with the origin assumed to
Distance Transform
Th distance transform of an image is defi ned as a new image in which every DEMO
pixel is set to a value equal to the distance to the nearest zero pixel in the input image.
It should be immediately obvious DEMO the typical input to a distance transform should
be some kind of edge image. In most applications the input to the distance transform is
DEMO output of an edge detector such as the Canny edge detector that has been inverted (so
that the edges have value zero and the non-edges are nonzero).
In practice, the distance transform is carried out by using a mask that is typically a 3-by-3
or 5-by-5 array. DEMO point in the array defi nes the “distance” to be associated with a
point in that particular position relative to the center of the DEMO Larger distances are
built up (and thus approximated) as sequences of “moves” defi ned by the entries in the
mask. Th is means DEMO using a larger mask will yield more accurate distances.
Depending on the desired distance metric, the appropriate mask is automatically se-
lected from a set known to OpenCV. It is also possible to tell OpenCV to DEMO “ex-
act” distances according to some formula appropriate to the selected metric, but of
course this is much slower.
Th erent types, including DEMO classic L2 (Car-
tesian) distance metric; see Table 6-2 for DEMO listing. In addition to these you may defi ne a
custom metric and associate it with your own custom mask.
Table 6-2. Possible values DEMO distance_type argument to cvDistTransform()
Value of distance_type Metric
CV_DIST_L2
ρ
()r = r 2
2
CV_DIST_L1
CV_DIST_L12
CV_DIST_FAIR
ρ
()r
rr=
⎡
=+ −21⎢
⎣⎢
2
1
2
⎤
⎥
⎦⎥
r
ρ
DEMO r
() log , .rC=− +2 ⎢ ⎜11⎟⎥  C =
⎣C
⎛
⎝
r ⎞⎤
C ⎠⎦
3998
ρ
()
Distance Transform
DEMO 185
e
e distance metric can be any of several diff
06-R4886-RC1.indd   185
9/15/08   4:21:25 PM
www.it-ebooks.info
Table 6-2. Possible values for distance_type argument to cvDistTransform() (continued)
Value of distance_type
CV_DIST_WELSCH
Metric
ρ
⎛ ⎛
() exp ,DEMO =− −⎢
⎣⎢
2 ⎡
Cr
⎜ ⎝ C ⎠
⎝
2
⎞ 2 ⎞⎤
12
⎠⎟⎦⎥
⎜ ⎜ ⎟ ⎟⎥ C = .9846
DEMO
User-defi ned distance
When calling the OpenCV distance transform function, the DEMO image should be a
32-bit fl oating-point image (i.e., IPL_DEPTH_32F).
Void cvDistTransform(
const CvArr* src,
CvArr*       dst,DEMO
int          distance_type = CV_DIST_L2,
int          mask_size     = 3,
const float* kernel        = NULL,
CvArr*       labels        = NULL
);
Th cvDistTransform(). Th e fi rst is
distance_type, which indicates the distance metric to be used. Th e available values for
this argument are defi ned in Borgefors (1986) [Borgefors86].
Aft er the distance type is the mask_size, which may DEMO 3 (choose CV_DIST_MASK_3) or 5
(choose CV_DIST_MASK_5); alternatively, distance computations can be made without a
kernel* (choose CV_DIST_MASK_PRECISE). Th e kernel argument to cvDistanceTransform() is
the distance mask to be used DEMO the case of custom metric. Th ese kernels are constructed
according to the method of Gunilla Borgefors, two examples of which are shown in Fig-
ure 6-20. Th e last argument, labels, indicates that associations DEMO be made between
individual points and the nearest connected component consisting of zero pixels. When
labels is non-NULL, it must be a pointer to an array of integer values the same size as the
input and DEMO images. When the function returns, this image can be read to DEMO
mine which object was closest to the particular pixel under consideration. Figure 6-21
shows the outputs of distance transforms on a test pattern and DEMO photographic image.
Histogram Equalization
Cameras and image sensors must usually deal not only with the contrast in a scene but
also with the image DEMO exposure to the resulting light in that scene. In a standard
camera, the shutter and lens aperture settings juggle between exposing the sensors to
too much or too little light. Oft en the range of contrasts DEMO too much for the sensors to
deal with; hence there is DEMO trade-off  between capturing the dark areas (e.g., shadows),
which requires a longer exposure time, and the bright areas, which require DEMO ex-
posure to avoid saturating “whiteouts.”
* Th
e exact method comes from Pedro F. Felzenszwalb and Daniel P. Huttenlocher [Felzenszwalb63].
186
| Chapter DEMO: Image Transforms
ere are several optional parameters when calling
06-R4886-RC1.indd   DEMO
9/15/08   4:21:26 PM
Figure 6-20. Two custom distance transform masks
Figure 6-21. First a Canny DEMO detector was run with param1=100 and param2=200; then the
distance transform DEMO run with the output scaled by a factor of 5 to increase visibility
Histogram Equalization
| 187
06-R4886-RC1.indd   187
www.it-ebooks.info
9/15/08   4:21:26 PM
Aft er the picture has been taken, there’s nothing we can do about what the sensor re-
corded; however, we can still take DEMO there and try to expand the dynamic range
of the image. Th e most commonly used technique for this is histogram equalization.*†
In Figure DEMO we can see that the image on the left  is poor DEMO there’s not much
variation of the range of values. Th is is evident from the histogram of its intensity values
on the right. Because DEMO are dealing with an 8-bit image, its intensity values can range
DEMO 0 to 255, but the histogram shows that the actual intensity DEMO are all clustered
near the middle of the available range. Histogram equalization is a method for stretch-
ing this range out.
Figure 6-22. Th DEMO image on the left  has poor contrast, as is confi rmed by the histogram of its
intensity values on the right
Th
(the given histogram of intensity values) to another distribution (a wider and, ideally,
uniform distribution of intensity values). Th at is, we want to spread out the y-values
of the original distribution as evenly DEMO possible in the new distribution. It turns out
that there is a good answer to the problem of spreading out distribution values: the re-
mapping function should be the cumulative distribution function. An example of the
DEMO density function is shown in Figure 6-23 for the somewhat idealized case of
a distribution that was originally pure Gaussian. However, cumulative density can be
applied to any distribution; it is just the running sum of the original distribution from
its negative to its positive bounds.
We may DEMO the cumulative distribution function to remap the original distribution as
an equally spread distribution (see Figure 6-24) simply by looking up each y-value DEMO
the original distribution and seeing where it should go in the equalized distribution.
* If you are wondering why histogram equalization is not in DEMO chapter on histograms (Chapter 7), the rea-
son is that DEMO equalization makes no explicit use of any histogram data types. Although histograms
are used internally, the function (from the user’s perspective) requires no histograms at all.
† Histogram equalization is an old mathematical technique; its use in image processing is described in vari-
ous textbooks [Jain86; Russ02; Acharya05], conference papers [Schwarz78], and even in biological vision
[Laughlin81].
188
| Chapter 6: Image Transforms
e underlying math behind histogram equalization involves mapping one distribution
06-R4886-RC1.indd   188
www.it-ebooks.info
9/15/08   4:DEMO:27 PM
www.it-ebooks.info
Figure 6-23. Result of cumulative distribution function (left
) on a Gaussian distribution (right)
Figure 6-24. Using the cumulative density function to equalize a Gaussian distribution
For continuous distributions the result will be an DEMO equalization, but for digitized/
discrete distributions the results may be DEMO from uniform.
Applying this equalization process to Figure 6-22 yields the equalized intensity distri-
bution histogram and resulting image in Figure 6-25. Th is DEMO process is wrapped up
in one neat function:
Histogram Equalization
| 189
06-R4886-RC1.indd   189
9/15/08   4:21:27 PM
void  cvEqualizeHist(
const CvArr* src,
CvArr*       dst
);
Figure 6-25. Histogram equalized results: the spectrum has been spread out
In cvEqualizeHist(), the source and destination must be single-channel, DEMO images of
the same size. For color images you will have to separate the channels and process them
one by one.
Exercises
1. Use DEMO() to create a fi lter that detects only 60 degree lines in an image. Dis-
play the results on a suffi  ciently interesting image scene.
2. Separable kernels. Create a 3-by-3 Gaussian kernel using rows DEMO(1/16, 2/16, 1/16),
(2/16, DEMO/16, 2/16), (1/16, 2/16, 1/16)] and with anchor point in the middle.
a. Run this kernel DEMO an image and display the results.
b. Now create two one-dimensional kernels with anchors in the center: one going
“across” (1/4, 2/4, 1/4), and one going down (1/4, 2/4, 1/4). Load the same origi-
nal image and use cvFilter2D() to convolve the image twice, once with the fi rst
1D kernel and once with the second 1D kernel. Describe the results.
DEMO Describe the order of complexity (number of operations) for the kernel in part
a and for the kernels in part b. Th e DEMO erence is the advantage of being able to
use separable kernels and the entire Gaussian class of fi lters—or any linearly
decomposable fi lter DEMO is separable, since convolution is a linear operation.
3. Can you DEMO a separable kernel from the fi lter shown in Figure 6-5? DEMO so, show
what it looks like.
4. In a drawing program DEMO as PowerPoint, draw a series of concentric circles form-
ing a DEMO
190
| Chapter 6: Image Transforms
06-R4886-RC1.indd   190
www.it-ebooks.info
9/DEMO/08   4:21:27 PM
www.it-ebooks.info
a. Make a series of lines going into the bull’s-eye. Save DEMO image.
b. Using a 3-by-3 aperture size, take and display the DEMO rst-order x- and y-derivatives
of your picture. Th en increase the aperture size to 5-by-5, 9-by-9, and 13-by-13.
Describe the results.
5. Create DEMO new image that is just a 45 degree line, white on DEMO For a given series of
aperture sizes, we will take the DEMO fi rst-order x-derivative (dx) and fi rst-order
y-derivative (dy). DEMO will then take measurements of this line as follows. Th e (DEMO)
and (dy) images constitute the gradient of the input image. Th e magnitude at location
(i, j) is mag(,) (, ) (,)ij dx ij dy ij=+22 and the angle is θ(, ) arctan( (, ) (, ))ij dyij dxij= .
Scan across the image and fi nd places where the magnitude DEMO at or near maximum.
Record the angle at these places. Average the angles and report that as the measured
line angle.
a. Do this DEMO a 3-by-3 aperture Sobel fi lter.
b. Do this for a 5-by-5 fi lter.
c. Do this for a 9-by-9 fi lter.
d. Do DEMO results change? If so, why?
6. Find and load a picture of a face where the face is frontal, has eyes open, and takes
up most or all of the image area. Write code to fi nd the pupils of the eyes.
A Laplacian “likes” a DEMO central point surrounded by dark. Pupils
are just the opposite. Invert and convolve with a suffi  ciently large
Laplacian.
In this exercise we learn to experiment with parameters by setting good lowThresh
and highThresh values in DEMO(). Load an image with suitably interesting
line structures. We’ll use DEMO diff erent high:low threshold settings of 1.5:1, 2.75:1,DEMO
and 4:1.
a. Report what you see with a high setting of less than 50.
b. Report what you see with high settings DEMO 50 and 100.
c. Report what you see with high settings between 100 and 150.
d. Report what you see with high settings between DEMO and 200.
e. Report what you see with high settings between 200 and 250.
f. Summarize your results and explain what happens as best DEMO can.
Load an image containing clear lines and circles such as a side view of a bicycle. Use
the Hough line and Hough circle DEMO and see how they respond to your image.
Can you think of a way to use the Hough transform to identify any kind of DEMO
with a distinct perimeter? Explain how.
Look at the diagrams of DEMO the log-polar function transforms a square into a wavy
line.
7.
8.
9.
10.
Exercises
| 191
06-R4886-RC1.indd   191
9/15/08   DEMO:21:28 PM
a. Draw the log-polar results if the log-polar center point were sitting DEMO one of
the corners of the square.
b. What would a circle look like in a log-polar transform if the center point were
inside DEMO circle and close to the edge?
c. Draw what the transform would look like if the center point were sitting just
outside of DEMO circle.
11. A log-polar transform takes shapes of diff erent rotations and sizes into a space
where these correspond to shift s in the DEMO and log(r) axis. Th e Fourier trans-
form is translation DEMO How can we use these facts to force shapes of diff erent
sizes and rotations to automatically give equivalent representations in the log-polar
domain?DEMO
Draw separate pictures of large, small, large rotated, and small DEMO squares.
Take the log-polar transform of these each separately. Code up a two-dimensional
shift er that takes the center point in the resulting log-polar DEMO and shift s the
shapes to be as identical as possible.
Take the Fourier transform of a small Gaussian distribution and the Fourier trans-
DEMO of an image. Multiply them and take the inverse Fourier transform of the re-
sults. What have you achieved? As the fi lters get bigger, you will fi nd that working
in the Fourier space is much faster than in the normal space.
Load an interesting image, convert it to grayscale, and then take an integral image
of it. Now fi nd vertical and horizontal edges in the image by using the DEMO of
an integral image.
12.
13.
14.
Use long skinny rectangles; DEMO and add them in place.
15. Explain how you could use the distance transform to automatically align a known
shape with a test shape DEMO the scale is known and held fi xed. How would this be
done over multiple scales?
16. Practice histogram equalization on images that DEMO load in, and report the results.
17.
Load an image, take a perspective transform, and then rotate it. Can this transform
be done in one step?
192
| Chapter 6: Image Transforms
06-R4886-RC1.indd   192
www.it-ebooks.info
9/15/08   4:21:28 PM
CHAPTER 7
Histograms and Matching
In the course of analyzing images, objects, and video information, we frequently want
to represent what we are DEMO at as a histogram. Histograms can be used to represent
such diverse things as the color distribution of an object, an edge gradient template of
an object [Freeman95], and the distribution of probabilities representing our current
hypothesis about an object’s location. Figure 7-1 shows the use of histograms DEMO rapid
gesture recognition. Edge gradients were collected from “up”, “right”, “left ”, “stop” and
“OK” hand gestures. A webcam was then set up to watch a person who used these ges-
tures to control web DEMO In each frame, color interest regions were detected from
the incoming DEMO; then edge gradient directions were computed around these interest
regions, and these directions were collected into orientation bins within a histogram.
Th
Th DEMO gestures. Th e gray
horizontal line represents the threshold for acceptance of the “winning” vertical bar
corresponding to a gesture model.
Histograms fi nd DEMO in many computer vision applications. Histograms are used to
detect scene transitions in videos by marking when the edge and color statistics mark-
edly DEMO from frame to frame. Th ey are used to identify interest points in images by
assigning each interest point a “tag” consisting of histograms DEMO nearby features. His-
tograms of edges, colors, corners, and so DEMO form a general feature type that is passed
to classifi ers for object recognition. Sequences of color or edge histograms are used to
identify DEMO videos have been copied on the web, and the list goes DEMO Histograms
are one of the classic tools of computer vision.
Histograms are simply collected counts of the underlying data organized into a set of
DEMO ned bins. Th ey can be populated by counts of features computed from the data,
such as gradient magnitudes and directions, color, DEMO just about any other characteristic.
In any case, they are used DEMO obtain a statistical picture of the underlying distribution
of data. Th e histogram usually has fewer dimensions than the source data. Figure 7-2
depicts DEMO typical situation. Th e fi gure shows a two-dimensional distribution of points
(upper left ); we impose a grid (upper right) and DEMO the data points in each grid cell,
yielding a one-dimensional histogram (lower right). Because the raw data points can
193
e histograms were then matched against the gesture models to recognize the gesture.
e DEMO bars in Figure 7-1 show the match levels of the diff
07-R4886-AT1.indd   193
www.it-ebooks.info
9/15/08   4:21:51 PM
www.it-ebooks.info
Figure 7-1. Local histograms of gradient orientations are used to fi DEMO the hand and its gesture; here
the “winning” gesture (longest vertical bar) is a correct recognition of “L” (move left )
represent just about anything, the histogram is a handy way of representing whatever it
is that you have learned from your image.
Histograms that represent DEMO distributions do so by implicitly averaging the
number of points in each grid cell.* Th is is where problems can arise, as shown in Fig-
ure 7-3. If the grid is too wide (upper left ), then there is too much averaging and we lose
the structure of the distribution. If the grid is too narrow (upper right), then there is not
enough averaging to represent the distribution accurately and we DEMO small, “spiky” cells.
OpenCV has a data type for representing histograms. DEMO e histogram data structure is
capable of representing histograms in one or many dimensions, and it contains all the
data necessary to track bins of both uniform and nonuniform sizes. And, as you might
expect, DEMO comes equipped with a variety of useful functions which will allow us to easily
perform common operations on our histograms.
* Th is is DEMO true of histograms representing information that falls naturally into discrete groups when the
histogram uses fewer bins than the natural description would suggest or DEMO An example of this is rep-
resenting 8-bit intensity values in a 10-bin histogram: each bin would then combine the points associated
with approximately 25 diff erent intensities, (erroneously) treating them all as equivalent.
194 | Chapter 7: Histograms and Matching
07-R4886-AT1.indd   194
9/15/08   4:21:51 PM
www.it-ebooks.info
Figure 7-2. Typical histogram example: starting with a cloud of points (upper left ), a counting grid is
imposed (upper right) DEMO yields a one-dimensional histogram of point counts (lower right)
Basic DEMO Data Structure
Let’s start out by looking directly at the CvHistogram data structure.
typedef struct CvHistogram
{
int     type;
CvArr*  bins;
float   thresh[CV_MAX_DIM][2]; // for uniform histograms
float** thresh2;               // for nonuniform histograms
CvMatND mat;                   // embedded DEMO header
// for array histograms
}
CvHistogram;
Th is defi DEMO is deceptively simple, because much of the internal data of the DEMO
is stored inside of the CvMatND structure. We create new histograms with the following
routine:
CvHistogram* cvCreateHist(
int     dims,
DEMO    sizes,
int     type,
float** ranges  = NULL,
int     uniform = 1
);
Basic Histogram DEMO Structure
| 195
07-R4886-AT1.indd   195
9/15/08   4:21:52 PM
www.it-ebooks.info
Figure 7-3. A histogram’s accuracy depends on its grid size: a grid that is too wide yields too much
spatial averaging in the DEMO counts (left ); a grid that is too small yields “spiky” and singleton
results from too little averaging (right)
Th dims indicates how many dimensions we want the histogram to have. Th e
sizes DEMO must be an array of integers whose length is equal to dims. Each integer
in this array indicates how many bins are to be DEMO to the corresponding dimension.
Th type can be either CV_HIST_ARRAY, which DEMO used for multidimensional histograms to
be stored using the dense multidimensional matrix structure (i.e., CvMatND), or CV_HIST_
SPARSE* if the data is DEMO be stored using the sparse matrix representation (CvSparseMat). Th e
DEMO ranges can have one of two forms. For a uniform histogram, DEMO is an array
of fl oating-point value pairs,† where the number of value pairs is equal to the number of
dimensions. For a DEMO histogram, the pairs used by the uniform histogram are
replaced by DEMO containing the values by which the nonuniform bins are separated.
If there are N bins, then there will be N + 1 entries in each of these subarrays. Each ar-
ray of values starts with the DEMO edge of the lowest bin and ends with the top edge
of the highest bin.‡ Th e Boolean argument uniform indicates if the histogram DEMO to have
* For you old timers, the value CV_HIST_TREE is DEMO supported, but it is identical to CV_HIST_SPARSE.
† Th ese “pairs” DEMO just C-arrays with only two entries.
‡ To clarify: in the DEMO of a uniform histogram, if the lower and upper ranges are DEMO to 0 and 10, respectively,
and if there are two DEMO, then the bins will be assigned to the respective intervals [0, 5) and [5, 10]. In the case
of a nonuniform histogram, if the size dimension i is 4 and if the corresponding ranges DEMO set to (0, 2, 4, 9, 10),
then the resulting bins will be assigned to the following (nonuniform) intervals: [0, 2), [2,4), [4, 9), and [9, 10].
196 | Chapter 7: Histograms and Matching
e argument
e
07-R4886-AT1.indd   196
9/15/08   4:21:52 PM
www.it-ebooks.info
uniform bins and thus how the ranges value is interpreted;* DEMO set to a nonzero value, the
bins are uniform. It is DEMO to set ranges to NULL, in which case the ranges are DEMO
“unknown” (they can be set later using the specialized function cvSetHistBinRanges()).
Clearly, you had better set the value of ranges before DEMO start using the histogram.
void cvSetHistBinRanges(
CvHistogram* hist,
float**      ranges,
int          uniform = 1
);
Th cvSetHistRanges() are exactly the same as the corresponding argu-
ments for cvCreateHist(). Once you are done with a histogram, DEMO can clear it (i.e.,
reset all of the bins to DEMO) if you plan to reuse it or you can de-allocate it DEMO the usual
release-type function.
void cvClearHist(
CvHistogram*  hist
);
DEMO cvReleaseHist(
CvHistogram** hist
);
As usual, the release function DEMO called with a pointer to the histogram pointer you
obtained from the create function. Th e histogram pointer is set to NULL once the DEMO
gram is de-allocated.
Another useful function helps create a histogram from data we already have lying
around:
CvHistogram*  cvMakeHistHeaderForArray(
int          dims,
int*         sizes,
CvHistogram* hist,
float*       data,
float**      ranges  = NULL,
int          uniform = 1
);
In this case, hist is a pointer to a CvHistogram DEMO structure and data is a pointer to
an area of size sizes[0]*sizes[1]*...*sizes[dims-1] for storing the histogram bins. Notice
that data is a pointer to DEMO because the internal data representation for the histogram
is always of type float. Th e return value is just the same as the hist DEMO we passed in.
Unlike the cvCreateHist() routine, there is no DEMO argument. All histograms created by
cvMakeHistHeaderForArray() are dense histograms. One last point before we move on:
since you (presumably) allocated the DEMO storage area for the histogram bins yourself,
there is no reason to call cvReleaseHist() on your CvHistogram structure. You will have
to DEMO up the header structure (if you did not allocate it on DEMO stack) and, of course,
clean up your data as well; but since these are “your” variables, you are assumed to be
DEMO care of this in your own way.
* Have no fear that this argument is type int, because the only meaningful distinction is between zero and
nonzero.
Basic Histogram Data Structure | 197
e arguments to
DEMO   197
9/15/08   4:21:53 PM
www.it-ebooks.info
Accessing Histograms
Th e most straightforward method is
to use OpenCV’s DEMO functions.
double cvQueryHistValue_1D(
CvHistogram* hist,
int          idx0
);
double cvQueryHistValue_2D(
CvHistogram* hist,
int          idx0,
int          idx1
);DEMO
double cvQueryHistValue_3D(
CvHistogram* hist,
int          DEMO,
int          idx1,
int          idx2
);
double cvQueryHistValue_nD(
CvHistogram* hist,
int*         idxN
);
Each of these functions returns a fl oating-point number for the value in the appropriate
bin. Similarly, you can set (or get) histogram bin values with the functions that DEMO a
pointer to a bin (not to a bin’s value):
float* cvGetHistValue_1D(
CvHistogram* hist,
int          idx0
);
float* cvGetHistValue_2D(
CvHistogram* hist,
int          idx0,
int          idx1
);
float* cvGetHistValue_3D(
CvHistogram* hist,
int          idx0,
int          idx1,
int          idx2
);
float* cvGetHistValue_nD(
CvHistogram* hist,
int*         idxN
);
Th cvGetReal*D and cvPtr*D families of functions, and
in fact they are pretty much the same thing. Inside of DEMO calls are essentially those
same matrix accessors called with the matrix hist->bins passed on to them. Similarly,
the functions for sparse histograms DEMO the behavior of the corresponding sparse
matrix functions. If you attempt to access a nonexistent bin using a GetHist*() function
198
| Chapter DEMO: Histograms and Matching
ere are several ways to access a histogram’s DEMO Th
ese functions look a lot like the
07-R4886-AT1.indd   198
9/15/08   4:21:53 PM
www.it-ebooks.info
in a sparse histogram, then that bin is automatically created and its value set to 0. Note
that QueryHist*() functions do not DEMO missing bins.
Th is leads us to the more general topic of accessing the histogram. In many cases, for
dense histograms we will want to access the bins member of the histogram directly. Of
course, we might do this just as part of data access. For example, we might want to access
all of the elements in a dense histogram DEMO, or we might want to access bins di-
rectly for performance DEMO, in which case we might use hist->mat.data.fl (again, for
DEMO histograms). Other reasons for accessing histograms include fi nding how many
dimensions it has or what regions are represented by its individual bins. DEMO this infor-
mation we can use the following tricks to access either the actual data in the CvHistogram
structure or the information imbedded in DEMO CvMatND structure known as mat.
int n_dimension             = histogram->mat.dims;
int dim_i_nbins             = histogram->mat.dim[ i ].size;
// uniform histograms
int dim_i_bin_lower_bound   = histogram->thresh[ i ][ 0 ];
int dim_i_bin_upper_bound   = histogram->thresh[ i ][ 1 ];
// nonuniform histograms
int dim_i_bin_j_lower_bound = histogram->thresh2[ i ][ j ];
int dim_j_bin_j_upper_bound = histogram->DEMO i ][ j+1 ];
As you can see, there’s a DEMO going on inside the histogram data structure.
Basic Manipulations with Histograms
Now that we have this great data structure, we will naturally want to do some fun stuff
with it. First let’s hit some of the DEMO that will be used over and over; then we’ll move
on DEMO some more complicated features that are used for more specialized tasks.
When dealing with a histogram, we typically just want to accumulate information into
its various bins. Once we have done this, however, it is DEMO en desirable to work with the
histogram in normalized form, so DEMO individual bins will then represent the fraction of
the total number of events assigned to the entire histogram:
cvNormalizeHist( CvHistogram* hist, double DEMO );
Here hist is your histogram and factor is the number to which you would like to nor-
malize the histogram (which will usually be 1). If you are following closely then you
may DEMO noticed that the argument factor is a double although the internal data type
of CvHistogram() is always float—further evidence that OpenCV is a DEMO in progress!
Th
e next handy function is the threshold function:
cvThreshHist( CvHistogram* hist, double factor );
Th factor is the DEMO  for the threshold. Th e result of thresholding a his-
togram DEMO that all bins whose value is below the threshold factor are set to 0. Recall-
ing the image thresholding function cvThreshold(), we might say that the histogram
thresholding function is analogous to calling the image DEMO function with the ar-
gument threshold_type set to CV_THRESH_TOZERO. Unfortunately, there DEMO no convenient
e argument
Basic Manipulations with Histograms
| 199
07-R4886-AT1.indd   199
9/15/08   4:21:53 PM
www.it-ebooks.info
histogram thresholding functions that provide operations analogous to the other thresh-
DEMO types. In practice, however, cvThreshHist() is the one you’ll probably want because
with real data we oft en end up with some DEMO that contain just a few data points. Such
bins are mostly noise and thus should usually be zeroed out.
Another useful function is cvCopyHist(), which (as you might guess) copies the informa-
tion from DEMO histogram into another.
void cvCopyHist(const CvHistogram* src, CvHistogram** dst );DEMO
Th is function can be used in two ways. If the destination histogram *dst is a histogram
of the same size as src, then both the data and the bin ranges from src will be copied
DEMO *dst. Th e other way of using cvCopyHist() is to set *dst to NULL. In this case, a new
histogram will be allocated that has the same size as src and then the data and DEMO
ranges will be copied (this is analogous to the image function DEMO()). It is to
allow this kind of cloning that the second argument dst is a pointer to a pointer to a
histogram—unlike DEMO src, which is just a pointer to a histogram. If *dst DEMO NULL when
cvCopyHist() is called, then *dst will be set DEMO the pointer to the newly allocated histo-
gram when the function returns.
Proceeding on our tour of useful histogram functions, our next new friend is cvGetMinMax
HistValue(), which reports the minimal and maximal values found in the histogram.
void cvGetMinMaxHistValue(
const CvHistogram* hist,
float*             min_value,
float*             max_value,
int*               min_idx  = NULL,
int*               max_idx  = NULL
);
Th hist, cvGetMinMaxHistValue() will compute its largest and small-
est values. When the function returns, *min_value and *max_value will be set to those re-
spective values. If you don’t need one (or both) of these results, then you may set the cor-
DEMO argument to NULL. Th e next two arguments are optional; if DEMO leave them set
to their default value (NULL), they will DEMO nothing. However, if they are non-NULL pointers
to int then the DEMO values indicated will be fi lled with the location index of the mini-
mal and maximal values. In the case of multi-dimensional histograms, the arguments
min_idx and max_idx (if not NULL) are assumed to point DEMO an array of integers whose
length is equal to the dimensionality of the histogram. If more than one bin in the histo-
gram has DEMO same minimal (or maximal) value, then the bin that will DEMO returned is the
one with the smallest index (in lexicographic order DEMO multidimensional histograms).
Aft er collecting data in a histogram, we DEMO en use cvGetMinMaxHistValue() to fi nd the
minimum value and then “threshold away” bins with values near this minimum using
cvThreshHist() before DEMO nally normalizing the histogram via cvNormalizeHist().
Last, but certainly not least, is the automatic computation of histograms from images.
Th
void cvCalcHist(
IplImage**   image,
200
| Chapter 7: Histograms and Matching
us, given a histogram
e function cvCalcHist() performs this crucial task:
07-R4886-AT1.indd   200
9/15/08   4:21:54 PM
www.it-ebooks.info
CvHistogram* hist,
int          accumulate = 0,
const CvArr* mask       = NULL
);
Th DEMO argument, image, is a pointer to an array of IplImage* pointers.* Th is allows us
to pass in many image planes. In the DEMO of a multi-channel image (e.g., HSV or RGB)
we will have to cvSplit() (see Chapter 3 or Chapter 5) that DEMO into planes before call-
ing cvCalcHist(). Admittedly that’s a bit DEMO a pain, but consider that frequently you’ll
also want to pass DEMO multiple image planes that contain diff erent fi ltered versions of an
image—for example, a plane of gradients or the U- and V-planes of YUV. Th en what
a mess it would be when you tried DEMO pass in several images with various numbers of
channels (and you DEMO be sure that someone, somewhere, would want just some of those
channels in those images!). To avoid this confusion, all images passed to cvCalcHist()
are assumed (read “required”) to be single-channel images. When the histogram is pop-
ulated, the bins will be identifi ed by the tuples formed across these multiple images. Th e
argument hist DEMO be a histogram of the appropriate dimensionality (i.e., of dimen-
sion equal to the number of image planes passed in through image). DEMO e last two argu-
ments are optional. Th e accumulate argument, DEMO nonzero, indicates that the histogram
hist should not be cleared before DEMO images are read; note that accumulation allows
cvCalcHist() to be DEMO multiple times in a data collection loop. Th e fi nal argument,
mask, is the usual optional Boolean mask; if non-NULL, only pixels corresponding to non-
zero entries in the mask image will be DEMO in the computed histogram.
e fi
Comparing Two Histograms
Yet another indispensable tool for working with histograms, fi rst introduced by Swain
and Ballard [Swain91] and further generalized by Schiele and Crowley [Schiele96], is the
ability to compare two histograms in terms of some specifi c criteria for DEMO Th e
function cvCompareHist() does just this.
double cvCompareHist(
const CvHistogram* hist1,
const CvHistogram* hist2,
int                method
);
Th rst two arguments are the DEMO to be compared, which should be of the
same size. Th DEMO third argument is where we select our desired distance metric. Th e four
available options are as follows.
Correlation (method = CV_COMP_CORREL)
dH H
(, )
correl
12
=
⋅
∑ i Hi H DEMO() ()
∑ i Hi H
12
1
2 ()⋅
2
2
()i
′′
* Actually, you could also use CvMat* matrix pointers here.
Basic Manipulations with Histograms
| 201
e fi
07-R4886-AT1.indd   201
9/15/08   4:21:54 PM
www.it-ebooks.info
where Hi Hi N H jkk′ () () ( / ) ( )=− 1 ()∑
histogram. j k and N equals DEMO number of bins in the
For correlation, a high score represents DEMO better match than a low score. A perfect
match is 1 and a maximal mismatch is –1; a value of 0 indicates no correlation (random
association).
Chi-square (method = CV_COMP_CHISQR)
2
(() ())Hi H i−
=∑
dHH(, )
12
chi-square 12
i
Hi H() (+ i)
12
For chi-square,* a low score represents a better match than a high score. A perfect match
DEMO 0 and a total mismatch is unbounded (depending on the size DEMO the histogram).
Intersection (method = CV_COMP_INTERSECT)
dHH HiHiintersection ( , ) min( ( ), ( ))12 =∑
i
1 2
For histogram intersection, high scores indicate good matches and low scores indicate
bad matches. If both histograms are normalized to 1, then a perfect match is 1 and a
total mismatch is 0.
Bhattacharyya distance (method = CV_COMP_BHATTACHARYYA)
dHH(, ) =−1 ∑ ⋅
∑∑i Hi H1 ()⋅ i
Bhattacharyya 12
i 2
()i
For Bhattacharyya matching [Bhattacharyya43], low scores indicate good matches and
high scores indicate bad matches. A perfect match is 0 and a total mismatch is a 1.
With DEMO, a special factor in the code is used to normalize the DEMO
histograms. In general, however, you should normalize histograms before comparing
them because concepts like histogram intersection make little sense (even if allowed)
without normalization.
Th
simplest case that could be imagined: a one-dimensional histogram with only two bins.
Th  bin and a 0.0 value in the right bin. Th e
last three rows show the comparison histograms and DEMO values generated for them by
the various metrics (the EMD metric DEMO be explained shortly).
* Th e chi-square test was invented by Karl Pearson [Pearson] who founded the fi eld of mathematical statistics.
202 DEMO Chapter 7: Histograms and Matching
Hi H i() ()
DEMO
e simple case depicted in Figure 7-4 should clarify matters. In fact, this is about the
e model histogram has a 1.0 value in the left
07-R4886-AT1.indd   202
9/15/08   4:21:54 DEMO
www.it-ebooks.info
Figure 7-4. Histogram matching measures
Figure 7-4 provides a quick reference DEMO the behavior of diff erent matching types, but
there is something DEMO here, too. If histogram bins shift  by just one slot—as
with the chart’s fi rst and third comparison histograms—then all these matching methods
(except EMD) yield a maximal mismatch even though these two histograms have a
similar “shape”. Th e rightmost column in Figure 7-4 reports values DEMO by EMD,
a type of distance measure. In comparing the third to the model histogram, the EMD
measure quantifi es the situation precisely: the third histogram has moved to the right
by one unit. We shall explore this measure further in the “Earth Mover’s Distance” sec-
tion DEMO follow.
In the authors’ experience, intersection works well for quick-and-dirty matching DEMO
chi-square or Bhattacharyya work best for slower but more accurate matches. Th e EMD
measure gives the most intuitive matches but is much slower.
DEMO Usage Examples
It’s probably time for some helpful examples. Th e program in Example 7-1 (adapted
from the OpenCV code bundle) shows how DEMO can use some of the functions just dis-
cussed. Th is program computes a hue-saturation histogram from an incoming image
and then draws that DEMO as an illuminated grid.
Example 7-1. Histogram computation and display
#include <DEMO>
#include <highgui.h>
int main( int argc, char** argv ) {
Basic Manipulations with Histograms
| 203
07-R4886-AT1.indd   203
9/15/08   4:21:55 PM
www.it-ebooks.info
Example 7-1. Histogram computation and display (continued)
IplImage* src;
if( argc == 2 && (src=cvLoadImage(argv[1], 1))!= 0) {
// Compute the HSV image and decompose it into separate DEMO
//
IplImage* hsv = cvCreateImage( cvGetSize(src), 8, 3 );
cvCvtColor( src, hsv, CV_BGR2HSV );
IplImage* h_plane  DEMO cvCreateImage( cvGetSize(src), 8, 1 );
IplImage* s_plane  DEMO cvCreateImage( cvGetSize(src), 8, 1 );
IplImage* v_plane  DEMO cvCreateImage( cvGetSize(src), 8, 1 );
IplImage* planes[] = { h_plane, s_plane };
cvCvtPixToPlane( hsv, h_plane, s_plane, v_plane, 0 );
// Build the histogram and compute its contents.
//
int h_bins = 30, s_bins = 32;
CvHistogram* hist;DEMO
{
int    hist_size[] = { h_bins, s_bins };
float  h_ranges[]  = { 0, 180 };          // hue is [0,180]
float  s_ranges[]  = { 0, DEMO };
float* ranges[]    = { h_ranges, s_ranges };
hist = cvCreateHist(
2,
hist_size,
CV_HIST_ARRAY,
ranges,
DEMO
);
}
cvCalcHist( planes, hist, 0, 0 ); //Compute histogram
cvNormalizeHist( hist[i], 1.0 );  //Normalize it
// Create an image to use to visualize our histogram.
//
int scale = 10;
IplImage* hist_img = cvCreateImage(
cvSize( h_bins * scale, s_bins * scale ),
8,
3
);
cvZero( hist_img );
// populate our visualization with little gray squares.
//
float max_value = 0;
cvGetMinMaxHistValue( hist, 0, &max_value, 0, 0 );
for( int h = 0; h < h_bins; h++ ) {
for( int s = 0; s < s_bins; s++ ) {
204 | Chapter 7: Histograms and Matching
07-R4886-AT1.indd   204
9/15/08   4:21:55 PM
www.it-ebooks.info
Example 7-1. Histogram computation and display (continued)
float bin_val = cvQueryHistValue_2D( hist, h, s );
int intensity = cvRound( DEMO * 255 / max_value );
cvRectangle(
hist_img,
cvPoint( DEMO, s*scale ),
cvPoint( (h+1)*scale - 1, (s+1)DEMO - 1),
CV_RGB(intensity,intensity,intensity),
CV_FILLED
);
}
}
cvNamedWindow( “Source”, 1 );
cvShowImage(   “Source”, DEMO );
cvNamedWindow( “H-S Histogram”, 1 );
cvShowImage(   DEMO Histogram”, hist_img );
cvWaitKey(0);
}
}
In this example we have spent a fair amount of time preparing the arguments DEMO
cvCalcHist(), which is not uncommon. We also chose to normalize DEMO colors in the
visualization rather than normalizing the histogram itself, although DEMO reverse
order might be better for some applications. In this case it gave us an excuse to call
cvGetMinMaxHistValue(), which was reason enough not to reverse the order.
Let’s look at a more practical example: color histograms taken from a human hand un-
der various lighting conditions. DEMO e left  column of Figure 7-5 shows images of a hand DEMO
an indoor environment, a shaded outdoor environment, and a sunlit outdoor environ-
ment. In the middle column are the blue, green, and DEMO (BGR) histograms correspond-
ing to the observed fl esh tone of the hand. In the right column are the corresponding
HSV histograms, where the vertical axis is V (value), the radius is S (DEMO) and
the angle is H (hue). Notice that indoors is darkest, outdoors in the shade brighter, and
outdoors in the sun DEMO Observe also that the colors shift  around somewhat as a
result DEMO the changing color of the illuminating light.
As a test of histogram comparison, we could take a portion of one palm (e.g., the top half
of the indoor palm), and compare the histogram representation DEMO the colors in that im-
age either with the histogram representation of the colors in the remainder of that image
or with the histogram DEMO of the other two hand images. Flesh tones are of-
ten easier to pick out aft er conversion to an HSV color space. It DEMO out that restricting
ourselves to the hue and saturation planes is not only suffi  cient but also helps with rec-
ognition of fl esh tones across ethnic groups. Th e matching results for our experiment are
DEMO in Table 7-1, which confi rms that lighting can cause severe DEMO in color.
Sometimes normalized BGR works better than HSV in the context of lighting changes.
Basic Manipulations with Histograms
| 205
07-R4886-AT1.indd   205
DEMO/15/08   4:21:55 PM
www.it-ebooks.info
Figure 7-5. Histogram of fl esh colors under indoor (upper left ), shaded outdoor (middle left ), and
outdoor (lower left ) lighting conditions; the middle and right-hand columns display the associated
BGR DEMO HSV histograms, respectively
Table 7-1. Histogram comparison, via four matching methods, of palm-fl
indoor palm with listed variant palm-fl
esh color
esh colors in upper half of
Comparison
Indoor lower half 0.96
Outdoor shade 0.09
DEMO sun –0.0
CORREL CHISQR INTERSECT BHATTACHARYYA
0.14
1.57
1.98
0.82
0.13
0.01
0.2
0.8
0.99
Some More Complicated Stuff
Everything we’ve discussed so far DEMO reasonably basic. Each of the functions provided
for a relatively obvious need. Collectively, they form a good foundation for much of what
you might want to do with histograms in the context of computer vision (and probably
in other contexts as well). At this point we want DEMO look at some more complicated rou-
tines available within OpenCV that are extremely useful in certain applications. Th ese
routines include a more sophisticated DEMO of comparing two histograms as well as
206
| Chapter 7: DEMO and Matching
07-R4886-AT1.indd   206
9/15/08   4:21:55 PM
www.it-ebooks.info
tools for computing and/or visualizing which portions of an image DEMO to a given
portion of a histogram.
Earth Mover’s Distance
Lighting changes can cause shift s in color values (see Figure 7-5), although such shift s
tend not to change the shape of the histogram DEMO color values, but shift  the color value
locations and thus cause the histogram-matching schemes we’ve learned about to fail. If
instead of a DEMO match measure we used a histogram distance measure, then we
could DEMO match like histograms to like histograms even when the second histogram
has shift ed its been by looking for small distance measures. Earth mover’s DEMO
(EMD) [Rubner00] is such a metric; it essentially measures how DEMO work it would
take to “shovel” one histogram shape into another, DEMO moving part (or all) of the
histogram to a new location. It works in any number of dimensions.
Return again to Figure 7-4; we see the “earthshoveling” nature of EMD’s distance mea-
sure in the DEMO column. An exact match is a distance of 0. Half a match is half a
“shovel full”, the amount it would take to spread half of the left  histogram into the next
slot. Finally, moving DEMO entire histogram one step to the right would require an en-
tire unit of distance (i.e., to change the model histogram into the DEMO mismatched”
histogram).
Th
ric or their own cost-of-moving matrix. One can record where the histogram “material”
fl
rics derived from prior information about DEMO data. Th e EMD function in OpenCV is
cvCalcEMD2():
float cvCalcEMD2(
const CvArr*       signature1,
const CvArr*       signature2,
int                DEMO,
CvDistanceFunction distance_func = NULL,
const CvArr*       cost_matrix   = NULL,
CvArr*             flow          = NULL,
float*             lower_bound   = NULL,
void*              userdata      = NULL
);
Th cvCalcEMD2() DEMO has enough parameters to make one dizzy. Th is may seem
rather complex for such an intuitive function, but the complexity stems from all the
subtle confi gurable dimensions of the algorithm.* Fortunately, the function can be used
in its more basic and intuitive form and without most DEMO the arguments (note all the
“=NULL” defaults in the preceding code)DEMO Example 7-2 shows the simplifi ed version.
e EMD algorithm itself is quite general; it allows users to set their own distance met-
owed from one histogram to another, and one can employ nonlinear distance met-
e
* If you want all of the gory details, we recommend that you read the 1989 paper by S. Peleg, M. Werman,
and H. Rom, “A Unifi ed Approach to the Change of Resolution: Space and Gray-Level,” and then take a
look at the relevant entries in the OpenCV user manual that are included in the release DEMO \
opencvref_cv.htm.
Some More Complicated Stuﬀ
| 207
07-R4886-AT1.indd   207
9/15/08   4:21:55 PM
www.it-ebooks.info
Example 7-2. Simple EMD interface
float cvCalcEMD2(
const CvArr* signature1,DEMO
const CvArr* signature2,
int          distance_type
);DEMO
Th distance_type for the simpler version of cvCalcEMD2() is either Manhat-
tan distance (CV_DIST_L1) or Euclidean distance (CV_DIST_L2). Although we’re applying the
EMD to histograms, the interface prefers that we talk to it in terms of signatures for the
fi rst two array parameters.
Th
DEMO bin count followed by its coordinates. For the one-dimensional histogram of
Figure 7-4, the signatures (listed array rows) for the left hand column of histograms
(skipping the model) would be as follows: top, DEMO, 0; 0, 1]; middle, [0.5, 0; 0.5, 1]; bottom,
[0, 0; 1, 1]. If we had a DEMO in a three-dimensional histogram with a bin count of 537 at
(DEMO, y, z) index (7, 43, 11), then the signature row for that bin would be [537, 7; 43, 11]. Th is
is how we perform the necessary step of converting histograms DEMO signatures.
As an example, suppose we have two histograms, hist1 and hist2, that we want to con-
vert to two signatures, sig1 DEMO sig2. Just to make things more diffi  cult, let’s suppose
that these are two-dimensional histograms (as in the preceding code examples) of DEMO
mension h_bins by s_bins. Example 7-3 shows how to convert these two histograms into
two signatures.
Example 7-3. Creating signatures from histograms for EMD
//Convert histograms into signatures for EMD matching
//assume we already have 2D histograms hist1 and hist2
//that are both of dimension h_bins DEMO s_bins (though for EMD,
// histograms don’t have to match in size).
//
CvMat* sig1,sig2;
int numrows = DEMO;
//Create matrices to store the signature in
//
sig1 = cvCreateMat(numrows, 3, CV_32FC1); //1 count + 2 DEMO = 3
sig2 = cvCreateMat(numrows, 3, CV_32FC1); //sigs are of type float.
//Fill signatures for the two histograms
//DEMO
for( int h = 0; h < h_bins; h++ ) DEMO
for( int s = 0; s < s_bins; s++ ) DEMO
float bin_val = cvQueryHistValue_2D( hist1, h, s );
cvSet2D(DEMO,h*s_bins + s,0,cvScalar(bin_val)); //bin value
cvSet2D(DEMO,h*s_bins + s,1,cvScalar(h));       //DEMO 1
cvSet2D(sig1,h*s_bins + s,2,cvScalar(s));       //Coord 2
208
| Chapter 7: Histograms and Matching
DEMO parameter
ese signature arrays are always of type float and consist of rows containing the his-
07-R4886-AT1.indd   208
9/15/08   4:DEMO:56 PM
www.it-ebooks.info
Example 7-3. Creating signatures from histograms for EMD (continued)
bin_val = cvQueryHistValue_2D( hist2, h, s );
cvSet2D(sig2,h*s_bins + s,0,cvScalar(bin_val)); //bin value
cvSet2D(sig2,h*s_bins + s,1,cvScalar(h));       //Coord 1
cvSet2D(sig2,h*s_bins + s,2,cvScalar(s));       //Coord 2
}
}
Notice in this example* that the function cvSet2D() takes a CvScalar() array to set its
value even though each entry in this particular matrix is a single fl oat. We use DEMO inline
convenience macro cvScalar() to accomplish this task. Once we have our histograms
converted into signatures, we are ready to get the distance measure. Choosing to mea-
sure by Euclidean distance, we now add the code of Example 7-4.
Example 7-4. Using EMD to measure the similarity DEMO distributions
// Do EMD AND REPORT
//
float emd = DEMO(sig1,sig2,CV_DIST_L2);
printf(“%f; ”,emd);
Back DEMO
Back projection is a way of recording how well the pixels (DEMO cvCalcBackProject()) or
patches of pixels (for cvCalcBackProjectPatch()) fi DEMO the distribution of pixels in a histo-
gram model. For example, DEMO we have a histogram of fl esh color then we can use back
projection to fi nd fl esh color areas in an image. DEMO e function call for doing this kind of
lookup is:
void cvCalcBackProject(
IplImage**         image,
CvArr*             back_project,
const CvHistogram* hist
);
We have already seen the array of single channel images IplImage** image in the DEMO
tion cvCalcHist() (see the section “Basic Manipulations with Histograms”). DEMO e number
of images in this array is exactly the same—and in the same order—as used to construct
the histogram model hist. Example 7-1 DEMO how to convert an image into single-
channel planes and then make an array of them. Th e image or array back_project is a
DEMO 8-bit or fl oating-point image of the same size as the input images in the
array. Th e values in back_project are set to DEMO values in the associated bin in hist. If the
histogram is normalized, then this value can be associated with a conditional probabil-
ity value (i.e., the probability that a pixel in image is a member DEMO the type characterized
* Using cvSetReal2D() or cvmSet() would have been more compact and effi  cient here, but the example is
DEMO this way and the extra overhead is small compared to the actual distance calculation in EMD.
Some More Complicated Stuﬀ
| 209
07-R4886-AT1.indd   DEMO
9/15/08   4:21:56 PM
by the histogram in hist).* In Figure 7-6, we use a fl
probability of fl
esh image.
esh-color histogram to derive a
Figure DEMO Back projection of histogram values onto each pixel based on its color: the HSV fl
color histogram (upper left
probability image (lower right); the lower left
) is used to convert the hand image (upper right) into the fl
panel is the histogram of the hand image
esh-
esh-color
* Specifi cally, in the case of our fl esh-tone H-S histogram, if C is the color of the pixel and F is the prob-
ability that a pixel is fl esh, then this probability map gives us p(C|F), the probability of drawing DEMO color
if the pixel actually is fl esh. Th is is not quite the same as p(F|C), the probability that the pixel DEMO fl esh given
its color. However, these two probabilities are related DEMO Bayes’ theorem [Bayes1763] and so, if we know
the overall probability DEMO encountering a fl esh-colored object in a scene as well as the total probability of
encountering of the range of fl esh colors, then we can compute p(F|C) from p(C|F). Specifi cally, DEMO
theorem establishes the following relation:
210
| Chapter 7: Histograms DEMO Matching
pF C(| )= pF() pC F(|)
DEMO()
07-R4886-AT1.indd   210
www.it-ebooks.info
9/15/08   4:21:DEMO PM
www.it-ebooks.info
When back_project is a byte image rather than a fl oat DEMO, you
should either not normalize the histogram or else scale it DEMO before use.
Th e reason is that the highest possible value in a normalized histogram
is 1, so anything less than that will be rounded down to 0 in the 8-bit im-
age. You might also DEMO to scale back_project in order to see the values
with your eyes, depending on how high the values are in your histogram.
Patch-based back projection
We can use the basic back-projection method to model whether or DEMO a particular pixel
is likely to be a member of a particular object type (when that object type was modeled
by a histogram). Th is is not exactly the same as computing the probability of DEMO pres-
ence of a particular object. An alternative method would be to consider subregions of an
image and the feature (e.g., color) histogram of that subregion and to ask whether the
histogram of features for DEMO subregion matches the model histogram; we could then
associate with each DEMO subregion a probability that the modeled object is, in fact, pres-
ent in that subregion.
Th
known object, cvCalcBackProjectPatch() allows us to compute if a patch might contain
a known object. Th e cvCalcBackProjectPatch() function uses a sliding window over the
entire input image, as DEMO in Figure 7-7. At each location in the input array of images,
all the pixels in the patch are used to set one DEMO in the destination image correspond-
ing to the center of the patch. Th is is important because many properties of images such
as textures DEMO be determined at the level of individual pixels, but instead arise DEMO
groups of pixels.
For simplicity in these examples, we’ve been sampling DEMO to create our histogram
models. Th us in Figure 7-6 the whole hand “lights up” because pixels there match the
fl
occur over local DEMO, such as the variations in local intensity that make up a DEMO
ture on up to the confi guration of properties that make up a whole object. Using local
patches, there are two ways one might consider applying cvCalcBackProjectPatch(): as a
region detector when the sampling window is smaller than the object and as an object
detector when the DEMO window is the size of the object. Figure 7-8 shows the use
of cvCalcBackProjectPatch() as a region detector. We start with a histogram DEMO of
palm-fl esh color and a small window is moved over the image such that each pixel in
the back projection image records the DEMO of palm-fl esh at that pixel given all the
pixels in the surrounding window in the original image. In Figure 7-8 the hand is DEMO
larger than the scanning window and the palm region is preferentially detected. Figure
7-9 starts with a histogram model collected from blue mugs. In DEMO to Figure 7-8
where regions were detected, Figure 7-9 shows how DEMO() can be
used as an object detector. When the window size is roughly the same size as the objects
we are hoping to DEMO nd in an image, the whole object “lights up” in the DEMO projection
Some More Complicated Stuﬀ
| 211
us, just as cvCalcBackProject() allows us to compute if a pixel might be part of a
esh color histogram model well. Using patches, we can detect statistical properties that
07-R4886-AT1.indd   211
9/15/08   4:21:56 PM
www.it-ebooks.info
Figure 7-7. Back projection: a sliding patch over the input image planes is used to set the correspond-
ing pixel (at the center of the patch) in the destination image; for normalized histogram models, the
resulting image can be interpreted as a probability map indicating the DEMO presence of the object
(this fi gure is taken from the DEMO reference manual)
image. Finding peaks in the back projection image then corresponds to fi nding the lo-
cation of objects (in Figure 7-9, a mug) that we are looking for.
e function provided by DEMO for back projection by patches is:
void cvCalcBackProjectPatch(
IplImage**   images,
CvArr*       dst,
CvSize       DEMO,
CvHistogram* hist,
int          method,
DEMO        factor
);
Here we have the same DEMO of single-channel images that was used to create the histo-
gram using cvCalcHist(). However, the destination image dst is diff erent: it can only be
a single-channel, fl oating-point image with size (images[0][0].width DEMO patch_size.x + 1,
images[0][0].height – patch_size.y + 1). Th e explanation for this size (see Figure 7-7)
is that the center pixel in the patch is used to set the corresponding location in DEMO,
so we lose half a patch dimension along the edges of the image on every side. Th e pa-
rameter patch_size is exactly DEMO you would expect (the size of the patch) and may be
set using the convenience macro cvSize(width, height). We are already familiar with
the histogram parameter; as with cvCalcBackProject(), this is DEMO model histogram to
which individual windows will be compared. Th e parameter for comparison method
takes as arguments exactly the same method types as DEMO in cvCompareHist() (see the
212 | Chapter 7: Histograms and Matching
Th
07-R4886-AT1.indd   212
9/15/08   4:21:57 DEMO
www.it-ebooks.info
Figure 7-8. Back projection used for histogram object model of fl DEMO tone where the window (small
white box in upper right frame) is much smaller than the hand; here, the histogram model was DEMO
palm-color distribution and the peak locations tend to be at the center of the hand
“Comparing Two Histograms” section).* Th e fi nal DEMO, factor, is the normalization
level; this parameter is the same DEMO discussed previously in connection with cvNor-
malizeHist(). You can set DEMO to 1 or, as a visualization aid, to some larger number. Be-
cause of this fl exibility, you are always free to normalize your hist model before using
cvCalcBackProjectPatch().
A fi nal question comes up: Once we have a probability of object image, how do DEMO
use that image to fi nd the object that we are searching for? For search, we can use the
cvMinMaxLoc() discussed in DEMO 3. Th e maximum location (assuming you smooth
a bit fi DEMO) is the most likely location of the object in an image. DEMO is leads us to a slight
digression, template matching.
* You DEMO be careful when choosing a method, because some indicate best match DEMO a return value of 1
and others with a value of 0.
Some More Complicated Stuﬀ  | 213
07-R4886-AT1.indd   213
9/15/08   4:21:57 PM
www.it-ebooks.info
Figure 7-9. Using cvCalcBackProjectPatch() to locate an object (here, DEMO coff ee cup) whose size ap-
proximately matches the patch size (white box in upper right panel): the sought object is modeled DEMO
a hue-saturation histogram (upper left ), which can be compared with an HS histogram for the image
as a whole (lower left ); the result of cvCalcBackProjectPatch() (lower right) is that the object is easily
picked out from the scene by virtue of its color
DEMO Matching
Template matching via cvMatchTemplate() is not based on histograms; DEMO, the func-
tion matches an actual image patch against an input DEMO by “sliding” the patch over
the input image using one of the matching methods described in this section.
If, as in Figure 7-10, DEMO have an image patch containing a face, then we can slide DEMO
face over an input image looking for strong matches that would indicate another face is
present. Th e function call is similar to that DEMO cvCalcBackProjectPatch():
void cvMatchTemplate(
const CvArr* image,
const CvArr* templ,
CvArr*       result,
int          method
);
Instead of the array of input image planes DEMO we saw in cvCalcBackProjectPatch(),
here we have a single 8-bit or fl oating-point plane or color image as input. Th e match-
DEMO model in templ is just a patch from a similar image containing the object for which
214
| Chapter 7: Histograms and Matching
07-R4886-AT1.indd   214
9/15/08   4:21:57 PM
www.it-ebooks.info
Figure 7-10. cvMatchTemplate() sweeps a template image patch across another DEMO looking for
matches
you are searching. Th e output object image will be put in the result image, which is a
single-channel byte or fl oating-point image of size (images->width – patch_size.x + 1,
rimages->height – patch_size.y + 1), as we saw previously in DEMO().
Th method is somewhat more complex, as we now explain. We use I to denote
the input image, T the template, DEMO R the result.
Square difference matching method (method = CV_TM_SQDIFF)
DEMO erence, so a perfect match will be 0 and bad
matches DEMO be large:
sq_diff
Correlation matching methods (method = CV_TM_CCORR)
DEMO ese methods multiplicatively match the template against the image, so a DEMO match
will be large and bad matches will be small or 0.
∑ ′′ ⋅
R x y T xy I x xy y(, ) [ ( , ) ( , )]= + ′ + DEMO 2
ccorr
xy′′,
Some More Complicated Stuﬀ  | 215
e DEMO
ese methods match the squared diff
R x y T xy I x xy y(, ) [ ( , ) ( , )]=
∑
′′ −+ ′
+ ′ 2
x
′′, y
07-R4886-AT1.indd   215
9/15/08   4:21:57 PM
www.it-ebooks.info
Correlation coefficient matching methods (method = CV_TM_CCOEFF)
Th
mean, DEMO a perfect match will be 1 and a perfect mismatch will be –1; a value of 0 simply
means that there is no correlation (random alignments).
Normalized methods
ccoeff
R x y T xy I x xy y(, ) [ ( , ) ( , )]=
∑
′′ ′ ′
⋅
+ ′
+ ′ 2
x
′′, y
⋅ ∑
+ ′′
For each of the three methods just DEMO, there are also normalized versions fi
developed by Galton [Galton] as DEMO by Rodgers [Rodgers88]. Th
methods are useful because, as mentioned previously, they can help reduce the eff
of lighting diff
tion coeffi
rst
DEMO normalized
ects
erences between the template and the image. In each case, the normaliza-
cient is the same:
Z x y T xy I x xy x(, )(,)=
∑∑
′′ ′
⋅
DEMO
( , )
+
+ ′
xy′′
,,
xy′′
sq_diff
DEMO
ccoeff
As usual, we obtain more accurate matches (at the cost of more computations) as we
move from simpler measures (square diff DEMO) to the more sophisticated ones (corre-
lation coeffi  cient). DEMO best to do some test trials of all these settings and then choose the
one that best trades off  accuracy for speed in your application.
216
| Chapter 7: Histograms and Matching
T
′′ ′(, ) (, )xy T xy= ′′
−
1
() ( , )wh T x y
xy′′ ′′,
⋅ ∑
′′ ′′
Ix x y y I x x y y′(, ) (, )DEMO ′ + ′ =+ ′ + ′
−
1
() ( ,wh I x x y + y′′)
xy′′ ′′,
ese DEMO match a template relative to its mean against the image relative to its
Th e values for method that give the normalized computations are DEMO in Table 7-2.
Table 7-2. Values of the method parameter for normalized template matching
Value of method parameter Computed result
Rxy(,) = Rxy(,)
CV_TM_SQDIFF_NORMED Zx y(,)
Rxy(,) = Rxy(,)
CV_TM_CCORR_NORMED Zx y(,)
Rxy(,) = Rxy(,)
CV_TM_CCOEFF_NORMED Zx y(,)
sq_diff_normed
ccor_normed
ccoeff_normed
07-R4886-AT1.indd   216
9/DEMO/08   4:21:58 PM
www.it-ebooks.info
Again, be careful when interpreting your results. Th e square-diff erence
methods show best matches with a minimum, whereas the correlation
and correlation-coeffi  cient methods show best matches at maximum
points.
As in the case of cvCalcBackProjectPatch(), once we use cvMatchTemplate() to obtain a
matching result image we can then use cvMinMaxLoc() to fi nd the DEMO of the best
match. Again, we want to ensure there’s an DEMO of good match around that point in
order to avoid random template alignments that just happen to work well. A good
match should have DEMO matches nearby, because slight misalignments of the template
shouldn’t vary the DEMO too much for real matches. Looking for the best matching
“hill” can be done by slightly smoothing the result image before seeking the maximum
(for correlation or correlation-coeffi  cient) or minimum (for square-diff erence) DEMO
ing methods. Th e morphological operators can also be helpful in this context.
Example 7-5 should give you a good idea of how the DEMO erent template matching tech-
niques behave. Th is program fi rst reads in a template and image to be matched and then
performs the DEMO via the methods we’ve discussed here.
Example 7-5. Template matching
// DEMO matching.
//   Usage: matchTemplate image template
//
#include <DEMO>
#include <cxcore.h>
#include <highgui.h>
#include <stdio.h>
DEMO main( int argc, char** argv ) {
IplImage *src, *templ,DEMO; //ftmp will hold results
int i;
if( argc == 3){
//Read in the source image to be searched:
DEMO((src=cvLoadImage(argv[1], 1))== 0) {
printf(“Error on reading src image %s\n”,argv[i]);
return(-1);
}
//Read DEMO the template to be used for matching:
if((templ=cvLoadImage(argv[2], 1))== 0) {
printf(“Error on reading template %s\n”,argv[2]);
return(-1);
}
//ALLOCATE OUTPUT IMAGES:
int iwidth = src->width - templ->width + 1;
int iheight = DEMO>height - templ->height + 1;
for(i=0; i<6; ++i){
ftmp[i] = cvCreateImage(
cvSize(iwidth,iheight),32,1);DEMO
}
//DO THE MATCHING OF THE TEMPLATE WITH THE IMAGE:
Some More Complicated Stuﬀ
| 217
07-R4886-AT1.indd   217
9/15/08   4:21:59 PM
www.it-ebooks.info
Example 7-5. Template matching (continued)
for(i=0; i<6; ++i){
cvMatchTemplate( src, templ, ftmp[i], i);
cvNormalize(ftmp[i],ftmp[i],1,0,CV_MINMAX)*;
}
//DISPLAY
cvNamedWindow( “Template”, DEMO );
cvShowImage(   “Template”, templ );
cvNamedWindow( “Image”, 0 );
cvShowImage(   “Image”, src );
cvNamedWindow( “SQDIFF”, 0 );
cvShowImage(   “SQDIFF”, ftmp[0] );
cvNamedWindow( DEMO, 0 );
cvShowImage(   “SQDIFF_NORMED”, ftmp[1] );
cvNamedWindow( “CCORR”, 0 );
cvShowImage(   “CCORR”, ftmp[2] );
cvNamedWindow( “CCORR_NORMED”, 0 );
cvShowImage(   “CCORR_NORMED”, ftmp[3] );
DEMO( “CCOEFF”, 0 );
cvShowImage(   “CCOEFF”, ftmp[4] );
cvNamedWindow( “CCOEFF_NORMED”, 0 );
cvShowImage(   “CCOEFF_NORMED”, ftmp[5] );DEMO
//LET USER VIEW RESULTS:
cvWaitKey(0);
}
else DEMO printf(“Call should be: ”
“matchTemplate image template \n”);}
}
Note the use of cvNormalize() in this code, which allows us to display the results in a
consistent way (recall that some of the matching methods can return negative-valued
results. We use the CV_MINMAX fl DEMO when normalizing; this tells the function to shift  and
scale the fl oating-point images so that all returned values are between 0 and DEMO Figu re
7-11 shows the results of sweeping the face template over the source image (shown in
Figure 7-10) using each of cvMatchTemplate()’s available matching methods. In outdoor
imagery especially, it’s almost always better to use one of the normalized methods.
Among those, correlation coeffi  DEMO gives the most clearly delineated match—but, as
expected, at a greater computational cost. For a specifi c application, such as automatic
parts inspection or tracking features in a video, you should try all the methods and fi nd
the speed and accuracy trade-off  that best serves your needs.
* You can oft en get more pronounced match results by DEMO the matches to a power (e.g., cvPow(ftmp[i],
ftmp[i], DEMO); ). In the case of a result which is normalized DEMO 0.0 and 1.0, then you can immediately
see that a good DEMO of 0.99 taken to the fi ft h power is not much reduced (0.995=0.95) while a poorer score
of 0.20 is reduced substantially (0.505=0.03).
218
| Chapter 7: Histograms and Matching
07-R4886-AT1.indd   218
9/15/08   4:21:59 PM
www.it-ebooks.info
Figure 7-11. Match results of six matching methods for the template DEMO depicted in Figure 7-10:
the best match for square diff erence is 0 and for the other methods it’s the maximum point; thus,
matches are indicated by dark areas in the left  column and by bright spots in the other two columns
Exercises
1. Generate 1,DEMO random numbers ri between 0 and 1. Decide on a bin size and then
take a histogram of 1/ri.
a. Are there similar DEMO of entries (i.e., within a factor of ±10) in each DEMO
gram bin?
b. Propose a way of dealing with distributions that are highly nonlinear so that
each bin has, within a factor of 10, the same amount of data.
2. Take three images of a hand in each of the three lighting conditions discussed in
the text. DEMO cvCalcHist() to make an RGB histogram of the fl esh color of one of the
hands photographed indoors.
a. Try using just a DEMO large bins (e.g., 2 per dimension), a medium number of bins
(16 per dimension) and many bins (256 per dimension). Th en run a matching
routine (using all histogram matching methods) DEMO the other indoor light-
ing images of hands. Describe what you fi nd.
b. Now add 8 and then 32 bins per dimension and DEMO matching across lighting
conditions (train on indoor, test on outdoor). Describe the results.
3. As in exercise 2, gather RGB histograms of hand fl esh color. Take one of the in-
door histogram samples DEMO your model and measure EMD (earth mover’s distance)
against the DEMO indoor histogram and against the fi rst outdoor shaded and fi rst
outdoor sunlit histograms. Use these measurements to set a distance threshold.
Exercises
DEMO 219
07-R4886-AT1.indd   219
9/15/08   4:21:59 PM
www.it-ebooks.info
a. Using this EMD threshold, see how well you detect the fl esh histogram of the
third indoor histogram, the second outdoor shaded, and the second outdoor
sunlit histograms. Report your results.
b. Take histograms of randomly chosen nonfl esh background patches to see how
well your DEMO discriminates. Can it reject the background while matching the
true fl esh histograms?
4. Using your collection of hand images, design a histogram that can determine un-
der which of the three lighting conditions a DEMO image was captured. Toward this
end, you should create features—perhaps sampling DEMO parts of the whole scene,
sampling brightness values, and/or DEMO relative brightness (e.g., from top to
bottom patches in the frame) or gradients from center to edges.
Assemble three histograms of fl esh models from each of our three lighting
conditions.
a. Use the fi DEMO histograms from indoor, outdoor shaded, and outdoor sunlit as
your models. Test each one of these against the second images in each respec-
DEMO class to see how well the fl esh-matching score works. Report matches.
b. Use the “scene detector” you devised in part a, to create a “switching histo-
gram” model. First use the scene detector to determine DEMO histogram model
to use: indoor, outdoor shaded, or outdoor sunlit. DEMO en use the corresponding
fl esh model to accept or reject the second fl esh patch under all three condi-
tions. How well does DEMO switching model work?
Create a fl esh-region interest (or “attention”) detector.
a. Just indoors for now, use several samples of hand and face fl esh to create an
RGB histogram.
b. Use cvCalcBackProject() DEMO fi nd areas of fl esh.
c. Use cvErode() from Chapter 5 to clean up noise and then cvFloodFill() (from
the same chapter) to fi nd large areas of fl esh in an image. Th ese are your “atten-
tion” regions.
Try some hand-gesture recognition. Photograph DEMO hand about 2 feet from the cam-
era, create some (nonmoving) hand gestures: thumb up, thumb left , thumb right.
a. Using your attention detector from exercise 6, take image gradients in the area
of detected fl esh around the hand and create a histogram model DEMO each of the
three gestures. Also create a histogram of the face (if there’s a face in the image)
so that you’ll have a (nongesture) model of that large fl esh region. You might
DEMO take histograms of some similar but nongesture hand positions, just so
DEMO won’t be confused with the actual gestures.
b. Test for recognition using a webcam: use the fl esh interest regions to fi nd “po-
tential hands”; take gradients in each fl esh region; use histogram DEMO
5.
6.
7.
220
| Chapter 7: Histograms and Matching
07-R4886-AT1.indd   220
9/15/08   4:21:59 PM
above a threshold to detect the gesture. If two models are above DEMO, take
the better match as the winner.
c. Move your hand DEMO feet further back and see if the gradient histogram can
still recognize the gestures. Report.
8. Repeat exercise 7 but with EMD for the DEMO What happens to EMD as you
move your hand back?
With the same images as before but with captured image patches instead of DEMO
tograms of the fl esh around the hand, use cvMatchTemplate() DEMO of histogram
matching. What happens to template matching when you move your hand back-
wards so that its size is smaller in the image?DEMO
9.
Exercises
| 221
07-R4886-AT1.indd   221
www.it-ebooks.info
9/15/08   4:22:00 PM
CHAPTER 8
Contours
Although algorithms like the Canny edge detector can be DEMO to fi nd the edge pixels
that separate diff erent segments in an image, they do not tell you anything about those
edges as entities in themselves. Th e next step is to be able to DEMO those edge pix-
els into contours. By now you have probably come to expect that there is a convenient
function in OpenCV that will DEMO exactly this for you, and indeed there is: cvFindCon-
tours()DEMO We will start out this chapter with some basics that we will need in order to use
this function. Specifi cally, we will introduce memory storages, which are how OpenCV
functions gain access to memory when they need to construct new objects dynamically;
then we will learn DEMO basics about sequences, which are the objects used to represent
contours DEMO With those concepts in hand, we will get into contour fi DEMO in
some detail. Th ereaft er we will move on to the many things we can do with contours
aft er they’ve been computed.
DEMO Storage
OpenCV uses an entity called a memory storage as its method of handling memory al-
location for dynamic objects. Memory storages are linked DEMO of memory blocks that
allow for fast allocation and de-allocation of continuous sets of blocks. OpenCV func-
tions that require the ability to allocate DEMO as part of their normal functionality
will require access to a memory storage from which to get the memory they require
(typically this includes any function whose output is of variable size).
Memory storages are DEMO with the following four routines:
CvMemStorage* cvCreateMemStorage(
int            block_size = 0
);
void cvReleaseMemStorage(
CvMemStorage** storage
);
void cvClearMemStorage(
CvMemStorage*  storage
);
void* cvMemStorageAlloc(
CvMemStorage*  storage,
222
08-R4886-AT1.indd   222
www.it-ebooks.info
9/15/08   4:22:21 PM
www.it-ebooks.info
size_t         size
);
To create a DEMO storage, the function cvCreateMemStorage() is used. Th is function
takes DEMO an argument a block size, which gives the size of memory DEMO inside the
store. If this argument is set to 0 then the default block size (64kB) will be used. Th e
function returns DEMO pointer to a new memory store.
Th cvReleaseMemStorage() function takes a pointer to a valid memory storage and then
de-allocates the storage. Th DEMO is essentially equivalent to the OpenCV de-allocations of
images, matrices, and other structures.
You can empty a memory storage by calling cvClearMemStorage(), which also takes a
pointer to a valid storage. You must be DEMO of an important feature of this function:
it is the only way to release (and thereaft er reuse) memory allocated to a DEMO stor-
age. Th is might not seem like much, but there DEMO be other routines that delete objects
inside of memory storages (we DEMO introduce one of these momentarily) but do not re-
turn the DEMO they were using. In short, only cvClearMemStorage() (and, of DEMO,
cvReleaseMemStorage()) recycle the storage memory.* Deletion of any dynamic DEMO
(CvSeq, CvSet, etc.) never returns any memory back to storage (although the structures
are able to reuse some memory once taken from the storage for their own data).
You can also allocate your DEMO continuous blocks from a memory store—in a man-
ner analogous to the way malloc() allocates memory from the heap—with the func-
tion cvMemStorageAlloc(). In this case you simply provide a pointer to the storage DEMO
the number of bytes you need. Th e return is a pointer of type void* (again, similar to
malloc()).
Sequences
One DEMO of object that can be stored inside a memory storage is a sequence. Sequences
are themselves linked lists of other structures. OpenCV can make DEMO out of
many diff erent kinds of objects. In this sense you can think of the sequence as some-
thing similar to the generic DEMO classes (or container class templates) that exist in
various other programming languages. Th e sequence construct in OpenCV is actually
a deque, so it is very fast for random access and for additions and deletions DEMO either
end but a little slow for adding and deleting objects in the middle.
Th e sequence structure itself (see Example 8-1) has DEMO important elements that you
should be aware of. Th e fi rst, and one you will use oft en, is total. Th is DEMO the total num-
ber of points or objects in the sequence. Th e next four important elements are point-
ers to other sequences: h_prev, h_next, v_prev, and v_next. Th ese four pointers are part
of what are called CV_TREE_NODE_FIELDS; they are used not to indicate elements inside of
the sequence but rather to connect diff erent sequences to one DEMO Other objects
in the OpenCV universe also contain these tree node fi elds. Any such objects can be
* Actually, one other function, DEMO cvRestoreMemStoragePos(), can restore memory to the storage. But
this function DEMO primarily for the library’s internal use and is beyond the scope of this book.
Sequences | 223
e
08-R4886-AT1.indd   223
9/15/08   4:22:22 PM
www.it-ebooks.info
assembled, by means of these pointers, into more complicated superstructures DEMO as
lists, trees, or other graphs. Th e variables h_prev and h_next can be used alone to create a
simple linked list. Th DEMO other two, v_prev and v_next, can be used to create more complex
topologies that relate nodes to one another. It is by means DEMO these four pointers that
cvFindContours() will be able to represent all of the contours it fi nds in the form of rich
structures DEMO as contour trees.
Example 8-1. Internal organization of CvSeq sequence structure
typedef struct CvSeq {
int       flags;             // miscellaneous flags
int       header_size;       // size of sequence header
CvSeq*    h_prev;     // previous sequence
CvSeq*    h_next;           // next sequence
CvSeq*    v_prev;            // 2nd previous sequence
CvSeq*    v_next            // 2nd next sequence
int       total;             // total number of elements
int       elem_size;         // size of sequence element in byte
char*     block_max;         // maximal bound of the last block
char*     ptr;               // current write pointer
int       delta_elems;       // how many elements allocated
// when the sequence grows
CvMemStorage* storage;     // where the sequence is stored
CvSeqBlock* free_blocks;   // free blocks list
CvSeqBlock* DEMO;         // pointer to the first sequence block
}
Creating a Sequence
As we have alluded to already, sequences can be returned from various OpenCV func-
tions. In addition to this, you can, of course, create sequences yourself. Like many ob-
jects in DEMO, there is an allocator function that will create a sequence for DEMO and
return a pointer to the resulting data structure. Th is function is called cvCreateSeq().
CvSeq* cvCreateSeq(
int           seq_flags,
int           header_size,
int           elem_size,
CvMemStorage* storage
);
Th is function requires some additional fl ags, which will further specify exactly what
sort of sequence we are creating. In addition it needs to be DEMO the size of the sequence
header itself (which will always be DEMO(CvSeq)*) and the size of the objects that the se-
DEMO will contain. Finally, a memory storage is needed from which the DEMO can
allocate memory when new elements are added to the sequence.
* Obviously, there must be some other value to which you can set this argument or it would not exist. Th is ar-
gument is DEMO because sometimes we want to extend the CvSeq “class”. To extend CvSeq, you create your
own struct using the CV_SEQUENCE_FIELDS() macro in the structure defi nition of the new type; note that,
when using an extended structure, the size of that structure must be passed. Th is is a pretty esoteric activity
in which only serious gurus are DEMO to participate.
224
| Chapter 8: Contours
08-R4886-AT1.indd   224
9/DEMO/08   4:22:22 PM
www.it-ebooks.info
Th flags are of three diff erent categories and can be DEMO using the bitwise OR
operator. Th e fi rst category determines the type of objects* from which the sequence is
to be constructed. Many DEMO these types might look a bit alien to you, and some DEMO pri-
marily for internal use by other OpenCV functions. Also, some DEMO the fl ags are mean-
ingful only for certain kinds of sequences (e.g., CV_SEQ_FLAG_CLOSED is meaningful only
for sequences that in some way DEMO a polygon).
CV_SEQ_ELTYPE_POINT
(x,y)
CV_SEQ_ELTYPE_CODE
Freeman code: 0..7
CV_SEQ_ELTYPE_POINT
Pointer to a point: &(x,y)
CV_SEQ_ELTYPE_INDEX
Integer index of a point: #(x,y)
CV_SEQ_ELTYPE_GRAPH_EDGE
&next_o,&next_d,&DEMO,&vtx_d
CV_SEQ_ELTYPE_GRAPH_VERTEX
fi rst_edge, &(x,y)
CV_SEQ_ELTYPE_TRIAN_ATR
Vertex of the binary tree
CV_SEQ_ELTYPE_CONNECTED_COMP
Connected component
CV_SEQ_ELTYPE_POINT3D
(x,y,z)
Th
following.
CV_SEQ_KIND_SET
A set of objects
CV_SEQ_KIND_CURVE
A curve defi ned by DEMO objects
CV_SEQ_KIND_BIN_TREE
A binary tree of the objects
* Th e types in this fi rst listing are used only rarely. To create a DEMO whose elements are tuples of num-
bers, use CV_32SC2, CV_32FC4, DEMO To create a sequence of elements of your own type, simply DEMO 0 and
specify the correct elem_size.
Sequences
| 225
ese
e second category indicates the nature of the sequence, which can be any of the
08-R4886-AT1.indd   225
9/15/08   4:22:23 PM
www.it-ebooks.info
CV_SEQ_KIND_GRAPH
A graph with the objects as nodes
Th
of the DEMO
ags that indicate some other property
CV_SEQ_FLAG_CLOSED
Sequence is closed (polygons)DEMO
CV_SEQ_FLAG_SIMPLE
Sequence is simple (polygons)
CV_SEQ_FLAG_CONVEX
Sequence is convex (polygons)
CV_SEQ_FLAG_HOLE
Sequence is a hole (polygons)
e third category consists of additional feature fl
Deleting a Sequence
void cvClearSeq(
CvSeq* seq
);
When you want to delete a sequence, you can use DEMO(), a routine that clears all
elements of the sequence. However, this function does not return allocated blocks in the
memory store either DEMO the store or to the system; the memory allocated by the DEMO
can be reused only by the same sequence. If you want to retrieve that memory for some
other purpose, you must clear the memory store via cvClearMemStore().
Direct Access to Sequence Elements
Oft en you will fi nd yourself wanting to directly access a particular member of DEMO se-
quence. Th ough there are several ways to do this, DEMO most direct way—and the correct
way to access a randomly chosen element (as opposed to one that you happen to know is
at the ends)—is to use cvGetSeqElem().
char* cvGetSeqElem( seq, index )DEMO
More oft en than not, you will have to cast the DEMO pointer to whatever type you know
the sequence to be. Here is an example usage of cvGetSeqElem() to print the elements in
a DEMO of points (such as might be returned by cvFindContours(), which we will get
to shortly):
for( int i=0; i<seq->total; ++i ) {
CvPoint* p = (CvPoint*)cvGetSeqElem ( seq, i );
printf(“(%d,%d)\n”, p->x, p->DEMO );
}
You can also check to see where a particular element is located in a sequence. Th e func-
tion cvSeqElemIdx() DEMO this for you:
226
| Chapter 8: Contours
08-R4886-AT1.indd   DEMO
9/15/08   4:22:23 PM
www.it-ebooks.info
int cvSeqElemIdx(
const CvSeq* seq,
const void*  element,
CvSeqBlock** block  = NULL
);
Th is check takes a bit of time, so it is not a particularly effi  cient thing DEMO do (the time for
the search is proportional to the size DEMO the sequence). Note that cvSeqElemIdx() takes
as arguments a pointer to your sequence and a pointer to the element for which you
DEMO searching.* Optionally, you may also supply a pointer to a sequence DEMO block
pointer. If this is non-NULL, then the location of the DEMO in which the sequence element
was found will be returned.
Slices, DEMO, and Moving Data
Sequences are copied with cvCloneSeq(), which does a deep copy of a sequence and cre-
ates another entirely separate DEMO structure.
CvSeq* cvCloneSeq(
const CvSeq*  seq,
CvMemStorage* storage   DEMO NULL
)
Th is routine is actually just a wrapper for DEMO somewhat more general routine cvSeq
Slice(). Th is latter routine DEMO pull out just a subsection of an array; it can also DEMO either
a deep copy or just build a new header to create an alternate “view” on the same data
elements.
CvSeq* cvSeqSlice(
const DEMO  seq,
CvSlice       slice,
CvMemStorage* storage   DEMO NULL,
int           copy_data = 0
);
You will notice that the argument slice to cvSeqSlice() is DEMO type CvSlice. A slice can be
defi ned using either the convenience function cvSlice(a,b) or the macro CV_WHOLE_SEQ.
In the former case, only those elements starting at a and continuing through b are in-
cluded in the copy (b may also be set to CV_WHOLE_SEQ_END_INDEX to indicate the end of
the array). Th e argument copy_data is DEMO we decide if we want a “deep” copy (i.e., if we
want the data elements themselves to be copied and for those new DEMO to be the ele-
ments of the new sequence).
Slices can be used to specify elements to remove from a sequence using cvSeqRemoveSlice()
or to insert into a sequence using cvSeqInsertSlice().
void cvSeqRemoveSlice(
CvSeq*        seq,
CvSlice       slice
);
* Actually, it would be more accurate to say that cvSeqElemIdx() takes the pointer being searched for. Th is is
because DEMO() is not searching for an element in the sequence that is equal to *element; rather, it
is searching for the element that DEMO at the location given by element.
Sequences | 227
08-R4886-AT1.indd   227
9/15/08   4:22:23 PM
www.it-ebooks.info
void  cvSeqInsertSlice(
CvSeq*        seq,
int           before_index,
const CvArr*  from_arr
);
DEMO the introduction of a comparison function, it is also possible to DEMO or search a
(sorted) sequence. Th e comparison function must have the following prototype:
typedef int (*CvCmpFunc)(const void* a, const void* b, void* userdata );
Here a and b are pointers to elements of the type being sorted, and userdata is just a
pointer to any additional data structure that the caller doing the sorting DEMO searching
can provide at the time of execution. Th e comparison function should return -1 if a is
greater than b, +1 if a is less than b, and 0 if a and b are equal.
With such a comparison function defi ned, a sequence can be sorted by cvSeqSort(). Th e
sequence can also be searched for an element (or for a pointer to an element) elem using
cvSeqSearch(). Th is searching is done in order O(log n) time if the sequence is already
sorted (is_sorted=1). If the sequence is unsorted, then the comparison function is not
needed and the search will take O(n) time. On completion, the search will set *elem_idx
DEMO the index of the found element (if it was found at DEMO) and return a pointer to that ele-
ment. If the element DEMO not found, then NULL is returned.
void cvSeqSort(
CvSeq*        seq,
CvCmpFunc     func,
void*         userdata  = NULL
);
char* cvSeqSearch(
CvSeq*        seq,
const void*   elem,
CvCmpFunc     DEMO,
int           is_sorted,
int*          elem_idx,
void*         userdata  = DEMO
);
A sequence can be inverted (reversed) in a single call with the function cvSeqInvert().
Th is function does not change the data in any way, but it reorganizes the sequence so
that the elements appear in the opposite order.
void cvSeqInvert(
CvSeq*        seq
);
OpenCV also supports a method of partitioning DEMO sequence* based on a user-supplied
criterion via the function cvSeqPartition(). DEMO is partitioning uses the same sort of com-
parison function as described previously but with the expectation that the function will
return a nonzero DEMO if the two arguments are equal and zero if they are not (i.e., the
opposite convention as is used for searching and sorting)DEMO
* For more on partitioning, see Hastie, Tibshirani, and Friedman DEMO
228
| Chapter 8: Contours
08-R4886-AT1.indd   228
9/15/08   4:22:23 PM
www.it-ebooks.info
int cvSeqPartition(
const CvSeq*  seq,
CvMemStorage* storage,
CvSeq**       labels,
CvCmpFunc     is_equal,
void*         userdata
);
Th
the output of the partitioning. Th e argument labels should be a pointer to a sequence
pointer. When DEMO() returns, the result will be that labels will now indicate
DEMO sequence of integers that have a one-to-one correspondence with the elements of the
partitioned sequence seq. Th e values of these integers will be, starting at 0 and incre-
menting from there, the “names” of the partitions that the points in seq were to be as-
signed. Th DEMO pointer userdata is the usual pointer that is just transparently passed to the
comparison function.
In Figure 8-1, a group of 100 points are randomly distributed on 100-by-100 canvas.
Th cvSeqPartition() is called on these DEMO, where the comparison function is based
on Euclidean distance. Th e DEMO function is set to return true (1) if the distance
is less than or equal to 5 and to return false (0) DEMO Th e resulting clusters are la-
beled with their integer ordinal from labels.
Using a Sequence As a Stack
As stated earlier, a sequence in OpenCV is really a linked list. Th is means, among other
things, that it can be accessed effi  ciently from either end. DEMO a result, it is natural to use
a sequence of this DEMO as a stack when circumstances call for one. Th e following six
functions, when used in conjunction with the CvSeq structure, implement the DEMO
required to use the sequence as a stack (more properly, a deque, because these functions
allow access to both ends of the list).
char*  cvSeqPush(
CvSeq* seq,
void*  element  = NULL
);
char*  cvSeqPushFront(
CvSeq* seq,
void*  element = DEMO
);
void   cvSeqPop(
CvSeq* seq,
void*  element  = NULL
);
void   cvSeqPopFront(
CvSeq* seq,
void*  element  = NULL
);
void cvSeqPushMulti(
CvSeq* seq,
void*  elements,
int    count,
Sequences
| 229
e partitioning DEMO a memory storage so that it can allocate memory to express
en
08-R4886-AT1.indd   229
9/15/08   4:22:23 PM
www.it-ebooks.info
Figure 8-1. A sequence of 100 points on a 100-by-100 canvas, partitioned by distance D ≤ 5
int    in_front = 0
);
void cvSeqPopMulti(
CvSeq* seq,
void*  elements,
int    count,
int    in_front = 0
);
Th cvSeqPush(), cvSeqPushFront(),
cvSeqPop(), and cvSeqPopFront(). Because these DEMO act on the ends of the sequence,
all of them operate in O(l) time (i.e., independent of the size of the sequence). Th e Push
functions return an argument to the element DEMO into the sequence, and the Pop
functions will optionally save the DEMO element if a pointer is provided to a location
where the object can be copied. Th e cvSeqPushMulti() and cvSeqPopMulti() variants will
DEMO or pop several items at a time. Both take a separate argument to distinguish the
front from the back; you can set in_front to either CV_FRONT (1) or to CV_BACK (0) and so
determine DEMO where you’ll be pushing or popping.
230
| Chapter 8: Contours
DEMO primary modes of accessing the sequence are
08-R4886-AT1.indd   230
9/15/08   4:22:23 PM
www.it-ebooks.info
Inserting and Removing Elements
char* cvSeqInsert(
CvSeq* seq,
int    before_index,
void*  element  = NULL
);
void cvSeqRemove(DEMO
CvSeq* seq,
int    index
);
Objects can be DEMO into and removed from the middle of a sequence by using
cvSeqInsert() and cvSeqRemove(), respectively, but remember that these are not DEMO fast.
On average, they take time proportional to the total size DEMO the sequence.
Sequence Block Size
One function whose purpose may not be obvious at fi rst glance is cvSetSeqBlockSize().
Th is routine takes as arguments a sequence and a new block size, which is the size of
blocks that will be allocated out of the memory store DEMO new elements are needed
in the sequence. By making this size big you are less likely to fragment your sequence
across disconnected memory blocks; by making it small you are less likely to waste
memory. Th DEMO default value is 1,000 bytes, but this can be changed DEMO any time.*
void   cvSetSeqBlockSize(
CvSeq* seq,
Int    DEMO
);
Sequence Readers and Sequence Writers
When you are working with sequences and you want the highest performance, there are
some special methods for accessing and modifying them that (although they require a
bit of special care to use) will let you do what you want to do with a minimum of over-
head. Th ese functions make use DEMO special structures to keep track of the state of what
they are doing; this allows many actions to be done in sequence and the necessary fi nal
bookkeeping to be done only aft er the last DEMO
For writing, this control structure is called CvSeqWriter. Th e writer DEMO initialized with the
function cvStartWriteSeq() and is “closed” with cvEndWriteSeq()DEMO While the sequence
writing is “open”, new elements can be added DEMO the sequence with the macro CV_WRITE_
SEQ(). Notice that the DEMO is done with a macro and not a function call, which DEMO
even the overhead of entering and exiting that code. Using the writer is faster than us-
ing cvSeqPush(); however, not all the DEMO headers are updated immediately by this
macro, so the added element DEMO be essentially invisible until you are done writing.
It will become visible when the structure is completely updated by cvEndWriteSeq().
* Eff ective with the beta 5 version of OpenCV, this size is automatically increased if the sequence becomes
big; hence you’ll not need to worry about it under normal circumstances.
Sequences | 231
08-R4886-AT1.indd   231
9/15/DEMO   4:22:24 PM
www.it-ebooks.info
If necessary, the structure can be brought up-to-date (without actually DEMO the
writer) by calling cvFlushSeqWriter().
void    cvStartWriteSeq(
DEMO           seq_flags,
int           header_size,
int           elem_size,
CvMemStorage* DEMO,
CvSeqWriter*  writer
);
void    cvStartAppendToSeq(
CvSeq*        seq,
CvSeqWriter*  writer
);
CvSeq*  cvEndWriteSeq(
CvSeqWriter*  writer
);
void     cvFlushSeqWriter(
CvSeqWriter*  writer
);
CV_WRITE_SEQ_ELEM( elem, writer )
CV_WRITE_SEQ_ELEM_VAR( elem_ptr, writer )
Th e seq_flags, header_
size, and elem_size arguments to cvStartWriteSeq() DEMO identical to the corresponding
arguments to cvCreateSeq(). Th e function DEMO() initializes the writer to
begin adding new elements to the end of the existing sequence seq. Th e macro CV_WRITE_
SEQ_ELEM() requires DEMO element to be written (e.g., a CvPoint) and a pointer DEMO the writer;
a new element is added to the sequence and the element elem is copied into that new
element.
Putting these all DEMO into a simple example, we will create a writer and append DEMO
hundred random points drawn from a 320-by-240 rectangle to the new sequence.
CvSeqWriter writer;
cvStartWriteSeq( CV_32SC2, sizeof(CvSeq), sizeof(CvPoint), DEMO, &writer );
for( i = 0; i < 100; i++ )
{
CvPoint pt; pt.x = rand()%320; pt.y = rand()%240;
CV_WRITE_SEQ_ELEM( pt, writer );
}
CvSeq* seq = cvEndWriteSeq( &writer );
For reading, there is a similar set of functions and a few more associated macros.
void  cvStartReadSeq(
const CvSeq* seq,
CvSeqReader* reader,
int          reverse    = 0
);
int   cvGetSeqReaderPos(
CvSeqReader* reader
);
void  cvSetSeqReaderPos(
CvSeqReader* reader,
232 | Chapter 8: Contours
e arguments to these functions are largely self-explanatory. Th
08-R4886-AT1.indd   232
9/15/08   4:22:24 PM
www.it-ebooks.info
int          index,
int          is_relative = 0
);
CV_NEXT_SEQ_ELEM( elem_size, reader )
DEMO( elem_size, reader )
CV_READ_SEQ_ELEM( elem, reader )
CV_REV_READ_SEQ_ELEM( DEMO, reader )
Th CvSeqReader, which is analogous to CvSeqWriter, is initialized with
the function cvStartReadSeq(). Th e argument reverse allows for the sequence to be
read either in “normal” order (reverse=0) or DEMO (reverse=1). Th e function
cvGetSeqReaderPos() returns an integer indicating DEMO current location of the reader in
the sequence. Finally, cvSetSeqReaderPos() DEMO the reader to “seek” to an arbitrary
location in the sequence. If the argument is_relative is nonzero, then the index will be
interpreted as a relative off set to the current reader position. In this case, the index may
be positive or negative.
Th
ward or backward one DEMO in the sequence. Th ey do no error checking and thus cannot
help you if you unintentionally step off  the end of the sequence. Th e macros CV_READ_
SEQ_ELEM() and CV_REV_READ_SEQ_ELEM() are used to DEMO from the sequence. Th ey will
both copy the “current” element at which the reader is pointed onto the variable elem
and then step DEMO reader one step (forward or backward, respectively). Th ese latter two
macros expect just the name of the variable to be copied DEMO; the address of that variable
will be computed inside of the DEMO
Sequences and Arrays
You may oft en fi nd yourself wanting to convert a sequence, usually full of points, into
an array.
void*  cvCvtSeqToArray(
const CvSeq* seq,
void*        elements,
CvSlice      slice   = CV_WHOLE_SEQ
);
CvSeq* cvMakeSeqHeaderForArray(
int          seq_type,
int          header_size,
int          elem_size,
void*        elements,
int          total,
CvSeq*       seq,
CvSeqBlock*  block
);
Th cvCvtSeqToArray() copies the content of the sequence into a continuous
memory array. Th DEMO means that if you have a sequence of 20 elements of type CvPoint
then the function will require a pointer, elements, to enough DEMO for 40 integers. Th e
third (optional) argument is slice, DEMO can be either an object of type CvSlice or the
e function
Sequences
| 233
e structure
e two macros CV_NEXT_SEQ_ELEM() and CV_PREV_SEQ_ELEM() simply move the reader for-
08-R4886-AT1.indd   233
9/15/08   4:22:24 PM
www.it-ebooks.info
macro CV_WHOLE_SEQ (the latter is the default value). If CV_WHOLE_SEQ is selected, then the
entire sequence is copied.
Th e opposite functionality to cvCvtSeqToArray() is implemented by cvMakeSeqHeaderFor
Array(). In this case, you can build a sequence from an existing array of data. Th e func-
tion’s fi rst few arguments are identical to those of DEMO(). In addition to requiring
the data (elements) to copy DEMO and the number (total) of data items, you must provide DEMO
sequence header (seq) and a sequence memory block structure (block)DEMO Sequences created
in this way are not exactly the same as sequences created by other methods. In particular,
you will not be able DEMO subsequently alter the data in the created sequence.
Contour Finding
We are fi nally ready to start talking about contours. To start with, we should defi ne ex-
actly what a contour is. A contour is DEMO list of points that represent, in one way or an-
other, a curve in an image. Th is representation can be diff erent DEMO on the cir-
cumstance at hand. Th ere are many ways to represent a curve. Contours are represented
in OpenCV by sequences in which DEMO entry in the sequence encodes information
about the location of the next point on the curve. We will dig into the details of such
DEMO in a moment, but for now just keep in mind that DEMO contour is represented in
OpenCV by a CvSeq sequence that is, DEMO way or another, a sequence of points.
Th cvFindContours() computes DEMO from binary images. It can take im-
ages created by cvCanny(), which have edge pixels in them, or images created by func-
DEMO like cvThreshold() or cvAdaptiveThreshold(), in which the edges are DEMO as
boundaries between positive and negative regions.*
Before getting to the function prototype, it is worth taking a moment to understand ex-
actly what a contour is. Along the way, we will encounter the concept of a contour tree,
which is important for understanding how cvFindContours() (retrieval methods derive
from Suzuki [Suzuki85]) will communicate its results to us.
Take a moment to look at Figure 8-2, which depicts the functionality of cvFindContours().
Th gure shows a test image containing a number of white regions
(labeled A through E) on a dark background.† DEMO e lower portion of the fi gure depicts
the same image along with the contours that will be located by cvFindContours(). Th ose
contours are labeled cX or hX, where “c” stands for “contour”, DEMO stands for “hole”, and
“X” is some number. Some of those DEMO are dashed lines; they represent exterior
boundaries of the white regions (i.e., nonzero regions). OpenCV and cvFindContours()
distinguish between these DEMO boundaries and the dotted lines, which you may
think of either DEMO interior boundaries or as the exterior boundaries of holes (i.e., zero
regions).
* Th ere are some subtle diff erences between passing DEMO images and binary images to cvFindContours(); we
will discuss those DEMO
† For clarity, the dark areas are depicted as gray in DEMO fi gure, so simply imagine that this image is thresh-
olded DEMO that the gray areas are set to black before passing to cvFindContours().
234 | Chapter 8: Contours
e function
e upper part DEMO the fi
08-R4886-AT1.indd   234
9/15/08   4:22:24 PM
www.it-ebooks.info
Figure 8-2. A test image (above) passed to cvFindContours() (below): the found contours may be
either of two types, exterior “contours” (dashed lines) or “holes” (dotted lines)
Th
OpenCV can be asked to assemble the found contours into a contour tree* that DEMO
the containment relationships in its structure. A contour tree corresponding to this test
image would have the contour called c0 at the root node, with the holes h00 and h01 as
its children. Th ose would DEMO turn have as children the contours that they directly con-
tain, DEMO so on.
It is interesting to note the consequences of using cvFindContours() on
an image generated by cvCanny() or a similar edge DEMO relative to
what happens with a binary image such as the test image shown in Fig-
ure 8-1. Deep down, cvFindContours() does not really know anything
about edge images. Th is means that, to cvFindContours(), an “edge” is
just a very thin “white” area. As a result, for every exterior contour there
will be a hole contour that almost exactly coincides with it. Th is hole is
actually just inside DEMO the exterior boundary. You can think of it as the
white-to-black transition that marks the interior edge of the edge.
* Contour trees fi DEMO appeared in Reeb [Reeb46] and were further developed by [Bajaj97], [Kreveld97], [Pas-
cucci02], and [Carr04].
Contour Finding
| 235
e concept of containment here is important in many applications. For this reason,
08-R4886-AT1.indd   DEMO
9/15/08   4:22:24 PM
www.it-ebooks.info
Now it’s time to look at the cvFindContours() function itself: to clarify exactly how we
tell it what we want and how DEMO interpret its response.
int cvFindContours(
IplImage*              img,
CvMemStorage*          storage,
CvSeq**                firstContour,
int                    headerSize  = sizeof(CvContour),DEMO
CvContourRetrievalMode mode        = CV_RETR_LIST,
CvChainApproxMethod    method      = CV_CHAIN_APPROX_SIMPLE
);
Th rst argument is the input image; this image should be an 8-bit single-channel im-
age and will be interpreted as binary (i.e., as if all nonzero pixels are DEMO to one
another). When it runs, cvFindContours() will actually DEMO this image as scratch space
for computation, so if you need DEMO image for anything later you should make a copy
and pass that to cvFindContours(). Th e next argument, storage, indicates a place where
cvFindContours() can fi nd memory in which to record the DEMO Th is storage area
should have been allocated with cvCreateMemStorage(), DEMO we covered earlier in
the chapter. Next is firstContour, which is DEMO pointer to a CvSeq*. Th e function cvFind
Contours() will allocate this pointer for you, so you shouldn’t allocate it yourself. In-
stead, just pass in a pointer to that pointer so that it can be set by the function. No al-
location/de-allocation (new/delete or malloc/free) is needed. It is at this location (i.e.,
DEMO) that you will fi nd a pointer to the head of DEMO constructed contour tree.*
Th e return value of cvFindContours() is the total number of contours found.
CvSeq* firstContour = NULL;
cvFindContours( …, &firstContour, … );
Th headerSize is just telling cvFindContours() more about the objects that it will be
allocating; it can be set to sizeof(CvContour) or to sizeof(CvChain) (the latter is used
when the approximation method is set to CV_CHAIN_CODE).† Finally, we have the mode and
method, which (respectively) further clarify exactly what is to be computed and how it is
to be computed.
Th
DEMO, or CV_RETR_TREE. Th e value of mode indicates to cvFindContours() DEMO what
contours we would like found and how we would like the result presented to us. In par-
ticular, the manner in which the tree node variables (h_prev, h_next, v_prev, and v_next)
are DEMO to “hook up” the found contours is determined by the value of mode. In Figure
8-3, the resulting topologies are shown for all four possible values of mode. In every case,
the structures can be DEMO of as “levels” which are related by the “horizontal” links
(h_next DEMO h_prev), and those levels are separated from one another by the “vertical”
links (v_next and v_prev).
* As we will see momentarily, contour trees are just one way that cvFindContours() can organize the con-
tours it fi nds. In any case, they will be organized using the CV_TREE_NODE_FIELDS elements of the contours
that we introduced when we DEMO rst started talking about sequences.
† In fact, headerSize can be DEMO arbitrary number equal to or greater than the values listed.
236 | Chapter 8: Contours
e fi
e
e mode variable can be set to any of four options: CV_RETR_EXTERNAL, CV_RETR_LIST, CV_
08-R4886-AT1.indd   236
9/15/08   4:22:25 PM
www.it-ebooks.info
Figure 8-3. Th e way in which the tree node variables DEMO used to “hook up” all of the contours located
by cvFindContours()DEMO
CV_RETR_EXTERNAL
Retrieves only the extreme outer contours. In Figure 8-2, there DEMO only one exterior
contour, so Figure 8-3 indicates the fi rst DEMO points to that outermost sequence
and that there are no further connections.
CV_RETR_LIST
Retrieves all the contours and puts them in the list. Figure DEMO depicts the list re-
sulting from the test image in Figure 8-2. In this case, eight contours are found and
they are all connected to one another by h_prev and h_next (v_prev and v_next are
not used here.)
CV_RETR_CCOMP
Retrieves all the contours and organizes them into DEMO two-level hierarchy, where the
top-level boundaries are external boundaries of the DEMO and the second-
level boundaries are boundaries of the holes. Referring to Figure 8-3, we can see
that there are fi ve exterior boundaries, of which three contain holes. Th e holes are
connected to their corresponding exterior boundaries by v_next and v_prev. Th e
outermost boundary c0 DEMO two holes. Because v_next can contain only one
value, the node DEMO only have one child. All of the holes inside of c0 are connected
to one another by the h_prev and h_next pointers.
CV_RETR_TREE
Retrieves DEMO the contours and reconstructs the full hierarchy of nested contours. In
our example (Figures 8-2 and 8-3), this means that the root node is the outermost
contour c0. Below c0 is the hole h00, which is connected to the other hole h01 at the
same level. Each DEMO those holes in turn has children (the contours c000 and c010,DEMO
respectively), which are connected to their parents by vertical links. Th is continues
down to the most-interior contours in the image, which become the leaf nodes in
the tree.
Th e next fi
ve values DEMO to the method (i.e., how the contours are approximated).
Contour Finding
| 237
08-R4886-AT1.indd   237
9/15/08   4:22:DEMO PM
www.it-ebooks.info
CV_CHAIN_CODE
Outputs contours in the Freeman chain code;* all other DEMO output polygons
(sequences of vertices).†
CV_CHAIN_APPROX_NONE
Translates all the points DEMO the chain code into points.
CV_CHAIN_APPROX_SIMPLE
Compresses horizontal, vertical, and diagonal segments, leaving only their ending
points.
CV_CHAIN_APPROX_TC89_L1 or CV_CHAIN_APPROX_TC89_KCOS
Applies one of the fl avors of the Teh-Chin chain approximation algorithm.
CV_LINK_RUNS
Completely diff DEMO algorithm (from those listed above) that links horizontal seg-
ments of 1s; the only retrieval mode allowed by this method is CV_RETR_LIST.
Contours Are Sequences
As you can see, there is a lot to sequences and contours. Th e good news is that, for
our current purpose, we need only a small amount of what’s available. When
cvFindContours() is called, it will give us a bunch of sequences. Th ese sequences are all
of one specifi c type; as we saw, DEMO particular type depends on the arguments passed
to cvFindContours(). Recall DEMO the default mode is CV_RETR_LIST and the default method
is CV_CHAIN_APPROX_SIMPLE.
Th
topic of this chapter. Th e key thing to remember about contours DEMO that they are just
a special case of sequences.‡ In particular, DEMO are sequences of points representing
some kind of curve in (image) space. Such a chain of points comes up oft en enough that
DEMO might expect special functions to help us manipulate them. Here is a list of these
functions.
int cvFindContours(
CvArr*        image,
CvMemStorage* storage,
CvSeq**       first_contour,
int           header_size   = sizeof(CvContour),
int           mode          = CV_RETR_LIST,
DEMO           method        = CV_CHAIN_APPROX_SIMPLE,DEMO
* Freeman chain codes will be discussed in the section entitled “Contours Are Sequences”.
† Here “vertices” means points of type CvPoint. Th e DEMO created by cvFindContours() are the same
as those created with cvCreateSeq() with the fl ag CV_SEQ_ELTYPE_POINT. (Th at function and fl ag will be
described in detail later in this chapter.)
‡ OK, there’s a little more to it than this, but we did not want to be sidetracked by technicalities and so will
clarify in this DEMO Th e type CvContour is not identical to CvSeq. In the way such things are handled in
OpenCV, CvContour is, in eff ect, derived from CvSeq. Th e CvContour type has a few extra data DEMO,
including a color and a CvRect for stashing its bounding box.
238 | Chapter 8: Contours
ese sequences are sequences of points; DEMO precisely, they are contours—the actual
08-R4886-AT1.indd   238
9/15/08   4:22:25 PM
www.it-ebooks.info
CvPoint       offset        = cvPoint(0,0)
);
CvContourScanner cvStartFindContours(
CvArr*        image,
CvMemStorage* storage,
int           header_size   DEMO sizeof(CvContour),
int           mode          = CV_RETR_LIST,
int           DEMO        = CV_CHAIN_APPROX_SIMPLE,
CvPoint       offset        = cvPoint(0,0)
);
CvSeq* cvFindNextContour(DEMO
CvContourScanner scanner
);
void   cvSubstituteContour(
CvContourScanner scanner,
CvSeq*           new_contour
);
CvSeq* cvEndFindContour(
CvContourScanner* DEMO
);
CvSeq* cvApproxChains(
CvSeq*        src_seq,
DEMO storage,
int           method            = CV_CHAIN_APPROX_SIMPLE,
double        parameter         = 0,
int           minimal_perimeter = 0,
int           recursive         = 0
);
First is the cvFindContours() function, DEMO we encountered earlier. Th e second func-
tion, cvStartFindContours(), is closely related to cvFindContours() except that it is used
when you DEMO the contours one at a time rather than all packed up into a higher-level
structure (in the manner of cvFindContours()). A call to cvStartFindContours() returns a
CvSequenceScanner. Th e scanner contains some simple DEMO information about what has
and what has not been read out.* You can then call cvFindNextContour() on the scanner
to successively retrieve all DEMO the contours found. A NULL return means that no more
contours are left
cvSubstituteContour() allows the contour to which a scanner is currently DEMO to
be replaced by some other contour. A useful characteristic of this function is that, if
the new_contour argument is set to NULL, DEMO the current contour will be deleted from
the chain or tree to which the scanner is pointing (and the appropriate updates will be
made to the internals of the aff ected sequence, so there will be no pointers to nonexis-
tent objects).
Finally, cvEndFindContour() ends the scanning and sets the scanner to a “done” state.
Note that the DEMO the scanner was scanning is not deleted; in fact, the return value
of cvEndFindContour() is a pointer to the fi rst element DEMO the sequence.
* It is important not to confuse a CvSequenceScanner with the similarly named CvSeqReader. Th e latter is
for reading the elements DEMO a sequence, whereas the former is used to read from what DEMO, in eff ect, a list of
sequences.
Contour Finding | 239
.
08-R4886-AT1.indd   239
9/15/08   4:22:25 PM
Th nal function is cvApproxChains(). Th is function converts Freeman chains to po-
lygonal representations (precisely or with some approximation). We will discuss cvAp-
proxPoly() in detail later in this chapter (see the section “Polygon Approximations”).
e fi
Freeman Chain Codes
Normally, the contours created by cvFindContours() are sequences of vertices (i.e.,
points). An alternative representation can be generated by setting the method to
CV_CHAIN_CODE. DEMO this case, the resulting contours are stored internally as Freeman chains
DEMO (Figure 8-4). With a Freeman chain, a polygon is represented as a sequence
of steps in one of eight directions; each step is designated by an integer from 0 to 7. Free-
man chains DEMO useful applications in recognition and other contexts. When working
with Freeman chains, you can read out their contents via two “helper” functions:
void cvStartReadChainPoints(
CvChain*         chain,
CvChainPtReader* reader
);DEMO
CvPoint cvReadChainPoint(
CvChainPtReader* reader
);
Figure 8-4. Panel a, DEMO chain moves are numbered 0–7; panel b, contour converted to a Free-
man chain-code representation starting from the back bumper
Th rst function DEMO a chain as its argument and the second function is a chain reader.
Th
ferent contours, CvChainPtReader iterates through a single contour represented by a
chain. In this respect, CvChainPtReader is similar to the more general CvSeqReader, and
* You may recall a previous mention of “extensions” of the CvSeq structure; CvChain is such an extension. It is
defi ned using the CV_SEQUENCE_FIELDS() macro and has one extra element in it, a CvPoint representing the
origin. You can think of CvChain as being DEMO from” CvSeq. In this sense, even though the return type
of DEMO() is indicated as CvSeq*, it is really a pointer to DEMO chain and is not a normal sequence.
240
| Chapter 8: DEMO
e fi
e CvChain structure is a form of CvSeq.* Just as CvContourScanner iterates through dif-
08-R4886-AT1.indd   240
www.it-ebooks.info
9/15/08   DEMO:22:26 PM
www.it-ebooks.info
cvStartReadChainPoints plays the role of cvStartReadSeq. As you might expect, CvChain-
PtReader returns NULL when there’s nothing left  to read.
Drawing Contours
One of our most basic tasks is drawing a contour on the screen. DEMO this we have
cvDrawContours():
void  cvDrawContours(
CvArr*   DEMO,
CvSeq*   contour,
CvScalar external_color,
CvScalar hole_color,
int      max_level,
int      thickness     = DEMO,
int      line_type     = 8,
CvPoint  offset        = cvPoint(0,0)
);
Th rst argument is simple: it is the image on which to draw the contours. Th e next ar-
gument, contour, is not quite DEMO simple as it looks. In particular, it is really treated as DEMO
root node of a contour tree. Other arguments (primarily max_level) will determine what
is to be done with the rest of the tree. DEMO e next argument is pretty straightforward: the
color with which to DEMO the contour. But what about hole_color? Recall that OpenCV
distinguishes between DEMO that are exterior contours and those that are hole con-
tours (DEMO dashed and dotted lines, respectively, in Figure 8-2). When drawing either a
single contour or all contours in a tree, any contour that is marked as a “hole” will be
drawn in this alternative DEMO
Th
tached to contour by means of the node tree variables. Th is argument can be set to in-
dicate the maximum depth to DEMO in the drawing. Th us, max_level=0 means that all
the contours DEMO the same level as the input level (more exactly, the input contour and
the contours next to it) are drawn, max_level=1 means DEMO all the contours on the same
level as the input and their children are drawn, and so forth. If the contours in ques-
tion were produced by cvFindContours() using either CV_RETR_CCOMP or CV_RETR_TREE
mode, then the additional idiom of negative values for max_level is also supported. In
DEMO case, max_level=-1 is interpreted to mean that only the input contour DEMO be drawn,
max_level=-2 means that the input contour and its direct children will the drawn, and so
on. Th e sample code in …/opencv/samples/c/contours.c illustrates this point.
Th
give an offset DEMO the draw routine so that the contour will be drawn elsewhere than at
the absolute coordinates by which it was defi ned. Th is DEMO is particularly useful when
the contour has already been converted to center-of-mass or other local coordinates.
* In particular, thickness=-1 (aka CV_FILLED) is useful for converting the contour tree (or an individual
contour) back DEMO the black-and-white image from which it was extracted. Th is feature, DEMO with the
offset parameter, can be used to do some quite DEMO things with contours: intersect and merge con-
tours, test points quickly against the contours, perform morphological operations (erode/dilate), etc.
Contour DEMO | 241
e fi
e max_level tells cvDrawContours() how to handle any contours that might be at-
e parameters thickness and line_type have DEMO usual meanings.* Finally, we can
08-R4886-AT1.indd   241
9/15/08   4:22:26 PM
www.it-ebooks.info
More specifi cally, offset would be helpful if we ran cvFindContours() one or more times
in diff erent image subregions (ROIs) DEMO thereaft er wanted to display all the results
within the original large image. Conversely, we could use offset if we’d extracted a con-
tour from a large image and then wanted to form a small mask DEMO this contour.
A Contour Example
Our Example 8-2 is drawn from the OpenCV package. Here we create a window with an
image in it. DEMO trackbar sets a simple threshold, and the contours in the thresholded DEMO
age are drawn. Th e image is updated whenever the trackbar is adjusted.
Example 8-2. Finding contours based on a trackbar’s location; the contours are updated whenever
the trackbar is moved
#include <cv.h>
#include <DEMO>
IplImage*      g_image    = NULL;
IplImage*      g_gray    = NULL;
int            g_thresh  = 100;
CvMemStorage*  g_storage  = NULL;
void on_trackbar(int) {
if( g_storage==NULL ) {
g_gray = cvCreateImage( cvGetSize(g_image), 8, 1 );
g_storage = cvCreateMemStorage(0);
DEMO else {
cvClearMemStorage( g_storage );
}
CvSeq* contours = 0;DEMO
cvCvtColor( g_image, g_gray, CV_BGR2GRAY );
cvThreshold( g_gray, g_gray, g_thresh, 255, CV_THRESH_BINARY );
cvFindContours( g_gray, g_storage, &contours );
cvZero( g_gray );
if( contours )
cvDrawContours(
DEMO,
contours,
cvScalarAll(255),
cvScalarAll(255),
100
);
cvShowImage( “Contours”, g_gray );
}
int main( int argc, char** argv )
{
if( argc != 2 || !(g_image = cvLoadImage(argv[1])) )
return -1;
cvNamedWindow( “Contours”, 1 );
cvCreateTrackbar(
“Threshold”,
“Contours”,
&g_thresh,
242 | Chapter 8: Contours
08-R4886-AT1.indd   242
9/15/08   4:22:26 PM
www.it-ebooks.info
Example 8-2. Finding contours based on a trackbar’s location; the contours are updated whenever
the trackbar is moved (continued)
255,
on_trackbar
);
on_trackbar(0);
cvWaitKey();
return 0;
}
Here, everything of interest to us is happening inside of the function on_trackbar(). If
the global variable g_storage is still at its (DEMO) initial value, then cvCreateMemStorage(0)
creates the memory storage and g_gray is initialized to a blank image the same size
as g_image DEMO with only a single channel. If g_storage is non-NULL, then we’ve DEMO
here before and thus need only empty the storage so it can be reused. On the next line,
a CvSeq* pointer is created; it is used to point to the sequence that we will create DEMO
cvFindContours().
Next, the image g_image is converted to grayscale and thresholded such that only those
pixels brighter than g_thresh are retained as DEMO Th e cvFindContours() function
is then called on this thresholded image. If any contours were found (i.e., if contours is
non-NULL), DEMO cvDrawContours() is called and the contours are drawn (in white) onto
the grayscale image. Finally, that image is displayed and the structures we allocated at
the beginning of the callback are released.
Another Contour DEMO
In this example, we fi nd contours on an input image DEMO then proceed to draw them
one by one. Th is is a good example to play with yourself and see what eff ects result DEMO
changing either the contour fi nding mode (CV_RETR_LIST in the code) or the max_depth
that is used to draw the contours (0 in the code). If you set max_depth to a larger number,
DEMO that the example code steps through the contours returned by cvFindContours()DEMO
by means of h_next. Th us, for some topologies (CV_RETR_TREE, DEMO, etc.), you
may see the same contour more than once DEMO you step through. See Example 8-3.
Example 8-3. Finding and drawing contours on an input image
int main(int argc, char* argv[]) {
DEMO( argv[0], 1 );
IplImage* img_8uc1 = cvLoadImage( argv[1], CV_LOAD_IMAGE_GRAYSCALE );
IplImage* img_edge = cvCreateImage( cvGetSize(img_8uc1), 8, 1 );
IplImage* img_8uc3 = cvCreateImage( cvGetSize(img_8uc1), 8, 3 );
cvThreshold( img_8uc1, img_edge, 128, 255, CV_THRESH_BINARY );
CvMemStorage* storage = cvCreateMemStorage();
CvSeq* first_contour  = NULL;
Another Contour Example | 243
08-R4886-AT1.indd   243
9/15/08   4:22:DEMO PM
www.it-ebooks.info
Example 8-3. Finding and drawing contours on an input image (continued)
int Nc = cvFindContours(
img_edge,
storage,
&first_contour,
sizeof(CvContour),
CV_RETR_LIST  // Try all four values and see what happens
);
int n=0;
printf( “Total Contours Detected: DEMO, Nc );
for( CvSeq* c=first_contour; c!=NULL; c=c->h_next ) {
cvCvtColor( img_8uc1, img_8uc3, CV_GRAY2BGR );
cvDrawContours(
img_8uc3,
c,
CVX_RED,
CVX_BLUE,
0,        // Try different values of max_level, and see what happens
2,
8
);
printf(“Contour #%d\n”, n );
cvShowImage( argv[0], img_8uc3 );
printf(“  %d elements:\n”, c->total );
DEMO( int i=0; i<c->total; ++i ) {
CvPoint* p DEMO CV_GET_SEQ_ELEM( CvPoint, c, i );
printf(“    (DEMO,%d)\n”, p->x, p->y );
}
cvWaitKey(0);
n++;
}
printf(“Finished all contours.\n”);
cvCvtColor( img_8uc1, img_8uc3, CV_GRAY2BGR );
cvShowImage( argv[0], img_8uc3 );
cvWaitKey(DEMO);
cvDestroyWindow( argv[0] );
cvReleaseImage( &img_8uc1 );
cvReleaseImage( &img_8uc3 );
cvReleaseImage( &img_edge );
return 0;
DEMO
More to Do with Contours
When analyzing an image, there are DEMO diff erent things we might want to do with
contours. Aft er all, most contours are—or are candidates to be—things that we are inter-
ested in identifying or manipulating. Th e various relevant tasks include characterizing
DEMO | Chapter 8: Contours
08-R4886-AT1.indd   244
9/15/08   DEMO:22:26 PM
www.it-ebooks.info
the contours in various ways, simplifying or approximating them, matching DEMO to
templates, and so on.
In this section we will examine DEMO of these common tasks and visit the various func-
tions built into OpenCV that will either do these things for us or at least DEMO it easier
for us to perform our own tasks.
Polygon Approximations
If we are drawing a contour or are engaged in shape analysis, it is common to approxi-
mate a contour representing a polygon with another DEMO having fewer vertices.
Th erent ways to do this; OpenCV off DEMO an implementation of one of
them.* Th e routine cvApproxPoly() is an implementation of this algorithm that will act
on a sequence of DEMO:
CvSeq*  cvApproxPoly(
const void*   src_seq,
int           header_size,
CvMemStorage* storage,
int           method,
double        parameter,
int           recursive  = 0
);
We can pass a list or a tree sequence containing contours to cvApproxPoly(), which will
then act on all of the contained contours. Th e return DEMO of cvApproxPoly() is actually
just the fi rst contour, but DEMO can move to the others by using the h_next (and v_next, as
appropriate) elements of the returned sequence.
Because cvApproxPoly() needs to create the objects that it will return a pointer to,
it DEMO the usual CvMemStorage* pointer and header size (which, as usual, DEMO set to
sizeof(CvContour)).
Th method argument is always set DEMO CV_POLY_APPROX_DP (though other algorithms could
be selected if they become available)DEMO Th e next two arguments are specifi c to the method
(DEMO which, for now, there is but one). Th e parameter argument is the precision parameter
for the algorithm. To understand how this DEMO works, we must take a moment to
review the actual algorithm.† DEMO e last argument indicates whether the algorithm should
(as mentioned previously) be applied to every contour that can be reached via the h_next
DEMO v_next pointers. If this argument is 0, then only the contour DEMO pointed to by
src_seq will be approximated.
So here is the promised explanation of how the algorithm works. In Figure 8-5, start-
ing with a contour (panel b), the algorithm begins by picking two extremal points and
connecting them with a line (panel c). Th en the original polygon is searched to fi nd the
point farthest from DEMO line just drawn, and that point is added to the approximation.
DEMO For afi cionados, the method used by OpenCV is the Douglas-Peucker (DP) approximation [Douglas73].
Other popular methods are the Rosenfeld-Johnson [Rosenfeld73] and Teh-Chin [Teh89] algorithms.
† If that’s too much trouble, then just set this parameter to a small fraction of the total curve length.
More to DEMO with Contours | 245
ere are many diff
e
08-R4886-AT1.indd   245
9/15/08   4:22:27 PM
Th
approximation, until all of the points are less than the distance indicated by the precision
parameter (panel f). Th is means that good candidates for the parameter are some frac-
tion of the contour’s DEMO, or of the length of its bounding box, or a similar measure of
the contour’s overall size.
e process is iterated (panel d), adding the next most distant point to the accumulated
Figure 8-5. DEMO of the DP algorithm used by cvApproxPoly(): the original image (a) is ap-
proximated by a contour (b) and then, DEMO from the fi rst two maximally separated vertices (c),
the additional vertices are iteratively selected from that contour (d)–(f)
Closely related to the approximation just described is the process of fi DEMO dominant
points. A dominant point is defi ned as a point that has more information about the curve
than do other points. Dominant points DEMO used in many of the same contexts as poly-
gon approximations. Th e routine cvFindDominantPoints() implements what is known as
the IPAN* [Chetverikov99] DEMO
CvSeq* cvFindDominantPoints(
CvSeq*        contour,
CvMemStorage* storage,DEMO
int           method     = CV_DOMINANT_IPAN,
double        parameter1 = 0,
double        parameter2 = 0,
double        parameter3 = 0,DEMO
double        parameter4 = 0
);
In essence, the IPAN algorithm works by scanning along the contour and trying to
DEMO triangles on the interior of the curve using the available vertices. Th at tri-
angle is characterized by its size and the opening angle (see Figure 8-6). Th e points with
large opening angles are DEMO provided that their angles are smaller than a specifi ed
global threshold and smaller than their neighbors.
* For “Image and Pattern Analysis Group,DEMO Hungarian Academy of Sciences. Th e algorithm is oft en referred
to as “IPAN99” because it was fi rst published in 1999.
246 | DEMO 8: Contours
08-R4886-AT1.indd   246
www.it-ebooks.info
9/15/08   4:DEMO:27 PM
www.it-ebooks.info
Figure 8-6. Th
e IPAN algorithm uses triangle abp to characterize DEMO p
Th cvFindDominantPoints() takes the usual CvSeq* and CvMemStorage* argu-
ments. It also requires a method, which (as with cvApproxPoly()) can take only one argu-
ment at this time: CV_DOMINANT_IPAN.
Th dmin, DEMO maximal distance dmax, a neigh-
borhood distance dn, and a maximum angle θmax. As shown in Figure 8-6, the algorithm
fi rst constructs all triangles for which rpa and rpb fall between dmin and dmax DEMO for which
θab < θmax. Th is is followed by a second pass in which only those points p with the small-
est associated DEMO of θab in the neighborhood dn are retained (the value of DEMO should
never exceed dmax). Typical values for dmin, dmax, dn, and θmax are 7, 9, 9, and 150 (the last
argument is an angle and is measured in degrees).
Summary Characteristics
DEMO task that one oft en faces with contours is computing their various summary
characteristics. Th ese might include length or some other form of DEMO measure of the
overall contour. Other useful characteristics are the contour moments, which can be
used to summarize the gross shape characteristics of a contour (we will address these in
the next section).
Length
Th cvContourPerimeter() will take a contour and return its length. In fact,DEMO
this function is actually a macro for the somewhat more general cvArcLength().
double  cvArcLength(
const void* curve,
CvSlice     DEMO     = CV_WHOLE_SEQ,
int         is_closed = -1
);
#define cvContourPerimeter( contour )             \
cvArcLength( contour, CV_WHOLE_SEQ, 1 )
Th rst argument DEMO cvArcLength() is the contour itself, whose form may be either DEMO
sequence of points (CvContour* or CvSeq*) or an n-by-2 array of points. Next are the slice
e subroutine
e fi
More to Do DEMO Contours
| 247
e routine
e next four arguments are: a DEMO distance
08-R4886-AT1.indd   247
9/15/08   4:22:27 PM
www.it-ebooks.info
argument and a Boolean indicating whether the contour should be treated DEMO closed
(i.e., whether the last point should be treated as connected to the fi rst). Th e slice argu-
ment allows us DEMO select only some subset of the points in the curve.*
Closely related to cvArcLegth() is cvContourArea(), which (as its name suggests) com-
putes the area of a contour. It takes the contour as DEMO argument and the same slice argu-
ment as cvArcLength().
double  cvContourArea(
const CvArr* contour,
CvSlice      slice  = DEMO
);
Bounding boxes
Of course the length and area are simple characterizations of a contour. Th e next level of
detail might be DEMO summarize them with a bounding box or bounding circle or ellipse.
Th
latter.
CvRect  cvBoundingRect(
CvArr* points,
int    update          = 0
);
CvBox2D  cvMinAreaRect2(
const CvArr*  points,
CvMemStorage* storage = NULL
);
Th cvBoundingRect(); DEMO will return a CvRect that bounds
the contour. Th e points used for the fi rst argument can be either a contour (CvContour*)
or an n-by-1, two-channel matrix (CvMat*) containing the points in the sequence. To un-
derstand the second argument, update, we must harken DEMO to footnote 8. Remember
that CvContour is not exactly the same as CvSeq; it does everything CvSeq does but also a
little bit more. One of those CvContour extras is a CvRect member for referring to DEMO own
bounding box. If you call cvBoundingRect() with update set to 0 then you will just get the
contents of that data member; but if you call with update set to 1, the bounding box will
be computed (and the associated data member will also be updated).
One problem with the bounding rectangle from cvBoundingRect() is that DEMO is a CvRect
and so can only represent a rectangle whose sides are oriented horizontally and verti-
cally. In contrast, the routine cvMinAreaRect2() returns the minimal rectangle that will
bound your contour, and this rectangle may be inclined relative to the vertical; see Fig-
ure 8-7. Th e arguments are otherwise similar to cvBoundingRect(). Th e OpenCV data
type CvBox2D is just what is needed to represent such a rectangle.
DEMO Almost always, the default value CV_WHOLE_SEQ is used. Th e structure DEMO contains only two elements:
start_index and end_index. You can create your own slice to put here using the helper constructor func-
tion cvSlice( int start, int end ). Note that CV_WHOLE_SEQ is just shorthand DEMO a slice starting at 0
and ending at some very large number.
248 | Chapter 8: Contours
ere are two ways to do the former, and there is a single method for doing each of the
e simplest technique is to call
08-R4886-AT1.indd   248
9/15/08   4:22:27 PM
typedef struct CvBox2D  {
CvPoint2D32f center;
CvSize2D32f  size;
float        angle;
} CvBox2D;
Figure 8-7. CvRect can DEMO only upright rectangles, but CvBox2D can handle rectangles of any
inclination
DEMO circles and ellipses
Next we have cvMinEnclosingCircle().* Th is routine DEMO pretty much the same as the
bounding box routines, with the DEMO fl exibility of being able to set points to be either a
sequence or an array of two-dimensional points.
int  cvMinEnclosingCircle(
const CvArr*  points,
CvPoint2D32f* center,
float*        radius
);DEMO
Th
in pointers for a center point and a fl oating-point variable radius that can be used by
cvMinEnclosingCircle() to report the results DEMO its computations.
As with the minimal enclosing circle, OpenCV also provides DEMO method for fi tting an el-
lipse to a set of points:
CvBox2D cvFitEllipse2(
const CvArr* points
);
* For more DEMO on the inner workings of these fi tting techniques, see Fitzgibbon DEMO Fisher [Fitzgib-
bon95] and Zhang [Zhang96].
More to Do with Contours
| 249
ere is no special structure in OpenCV for representing circles, so we need to pass
08-R4886-AT1.indd   249
www.it-ebooks.info
9/15/08   DEMO:22:28 PM
Th erence between cvMinEnclosingCircle() and cvFitEllipse2() is that the
former DEMO computes the smallest circle that completely encloses the given contour,
whereas the latter uses a fi tting function and returns the ellipse that DEMO the best approxi-
mation to the contour. Th is means that not all points in the contour will be enclosed in
the ellipse returned DEMO cvFitEllipse2(). Th e fi tting is done using a least-squares DEMO tness
function.
Th CvBox2D structure. Th e indicated box exactly en-
closes the ellipse. See Figure 8-8.
Figure 8-8. Ten-point contour with the minimal DEMO circle superimposed (a) and with the best-
fi tting ellipsoid (DEMO); a box (c) is used by OpenCV to represent that ellipsoid
Geometry
When dealing with bounding boxes and other summary representations of DEMO
contours, it is oft en desirable to perform such simple geometrical DEMO as polygon
overlap or a fast overlap check between bounding boxes. OpenCV provides a small but
handy set of routines for this sort of DEMO checking.
CvRect cvMaxRect(
const CvRect* rect1,
const CvRect* rect2
);
void cvBoxPoints(
CvBox2D       box,
CvPoint2D32f  pt[4]
);
CvSeq* cvPointSeqFromMat(
int           seq_kind,DEMO
const CvArr*  mat,
CvContour*    contour_header,
CvSeqBlock*   block
);
double cvPointPolygonTest(
const CvArr*  contour,
CvPoint2D32f  pt,DEMO
int           measure_dist
);
250
| Chapter 8: Contours
e subtle diff
e results of the fi t are returned in a
08-R4886-AT1.indd   250
www.it-ebooks.info
9/15/08   4:DEMO:28 PM
www.it-ebooks.info
Th rst of these functions, cvMaxRect(), computes a new DEMO from two input rect-
angles. Th e new rectangle is the smallest rectangle that will bound both inputs.
Next, the utility function cvBoxPoints() simply computes the points at the corners of a
CvBox2D structure. You DEMO do this yourself with a bit of trigonometry, but you would
DEMO grow tired of that. Th is function does this simple pencil pushing for you.
Th
matrix. Th is is useful when you want to DEMO a contour function that does not also take
matrix arguments. Th e input to cvPointSeqFromMat() fi rst requires you to indicate what
sort DEMO sequence you would like. Th e variable seq_kind may be set to any of the follow-
ing: zero (0), indicating just a DEMO set; CV_SEQ_KIND_CURVE, indicating that the sequence
is a curve; or DEMO | CV_SEQ_FLAG_CLOSED, indicating that the sequence is
a closed curve. Next DEMO pass in the array of points, which should be an n-by-1 DEMO
of points. Th e points should be of type CV_32SC2 or CV_32FC2 (i.e., they should be single-
column, two-channel arrays). Th e next two arguments are pointers to values that will be
computed by DEMO(), and contour_header is a contour structure that you
should already DEMO created but whose internals will be fi lled by the function call. Th is
is similarly the case for block, which will also be fi lled for you.* Finally the return value
is a CvSeq* pointer, which actually points to the very contour structure you passed in
yourself. DEMO is is a convenience, because you will generally need the sequence DEMO
when calling the sequence-oriented functions that motivated you to perform this con-
version in the fi rst place.
Th
function that allows you to DEMO whether a point is inside a polygon (indicated by a se-
DEMO). In particular, if the argument measure_dist is nonzero then the DEMO re-
turns the distance to the nearest contour edge; that distance DEMO 0 if the point is inside the
contour and positive if the point is outside. If the measure_dist argument is 0 then the
return DEMO are simply + 1, – 1, or 0 depending on whether the point is inside, outside,
or on an edge (or DEMO), respectively. Th e contour itself can be either a sequence or an
n-by-1 two-channel matrix of points.
Matching Contours
Now that we have DEMO pretty good idea of what a contour is and of how to work with con-
tours as objects in OpenCV, we would like to take a moment to understand how to use
them for some practical DEMO Th e most common task associated with contours is
matching them in some way with one another. We may have two computed contours
that DEMO like to compare or a computed contour and some abstract template with which
we’d like to compare our contour. We will discuss both of DEMO cases.
* You will probably never use block. It exists because no actual memory is copied when you call cvPoint
SeqFromMat(); instead, DEMO “virtual” memory block is created that actually points to the matrix you yourself
provided. Th e variable block is used to create a reference DEMO that memory of the kind expected by internal
sequence or contour calculations.
Matching Contours | 251
e fi
e second utility function, cvPointSeqFromMat(), generates a sequence structure from a
e last geometrical tool-kit function to be presented here is cvPointPolygonTest(), a
08-R4886-AT1.indd   251
9/15/08   4:22:28 PM
www.it-ebooks.info
Moments
One of the simplest ways to compare two contours is DEMO compute contour moments. Th is
is a good time for a short digression into precisely what a moment is. Loosely speaking,
a moment DEMO a gross characteristic of the contour computed by integrating (or summing,DEMO
if you like) over all of the pixels of the contour. DEMO general, we defi ne the (p, q) moment
of a contour as
e fi
pq,
n
mIxyxy=
∑ (, )
DEMO
i =1
e
Here p is the x-order and q is the y-order, whereby order means the power to which the
corresponding component is taken in the sum just displayed. Th e summation is over
all DEMO the pixels of the contour boundary (denoted by n in the DEMO). It then follows
immediately that if p and q are both equal to 0, then the m00 moment is actually just the
length in pixels of the contour.*
Th e function that computes these moments DEMO us is
void cvContoursMoments(
CvSeq*     contour,
CvMoments* moments
)
Th rst argument is the contour we are interested in and the second is a pointer to a
structure that we must allocate DEMO hold the return data. Th e CvMoments structure is de-
fi ned as follows:
typedef struct CvMoments  {
// spatial moments
double  m00, m10, m01, m20, m11, m02, m30, m21, DEMO, m03;
// central moments
double  mu20, mu11, mu02, mu30, mu21, mu12, mu03;
// m00 != 0 ? DEMO/sqrt(m00) : 0
double  inv_sqrt_m00;
}  CvMoments;
DEMO cvContoursMoments() function uses only the m00, m01, . . ., m03 elements; the elements
with names mu00, . . . are used by other routines.
When working with the CvMoments structure, there is a friendly helper function that
will return any particular moment out of DEMO structure:
* Mathematical purists might object that m00 should be not the contour’s length but rather its area. But be-
cause we are DEMO here at a contour and not a fi lled polygon, the DEMO and the area are actually the same
in a discrete pixel space (at least for the relevant distance measure in our pixel space). Th ere are also func-
tions for computing moments of IplImage images; in that case, m00 would actually be the area of nonzero
pixels.
252 | Chapter 8: Contours
08-R4886-AT1.indd   252
9/15/08   4:22:28 PM
www.it-ebooks.info
double cvGetSpatialMoment(
CvMoments* moments,
Int        x_order,
int        y_order
);
A single call to cvContoursMoments() will instigate computation of all the moments
through third order (i.e., m30 and m03 will be computed, as will m21 and DEMO, but m22 will
not be).
More About Moments
Th
contour DEMO can be used to compare two contours. However, the moments resulting
DEMO that computation are not the best parameters for such comparisons in most practi-
cal cases. In particular, one would oft en like to use normalized moments (so that objects
of the same shape but dissimilar sizes give similar values). Similarly, the simple mo-
ments of the previous section depend on the coordinate system chosen, which means
that objects are not matched correctly if they are rotated.
OpenCV provides routines to compute DEMO moments as well as Hu invariant
moments [Hu62]. Th e CvMoments structure can be computed either with cvMoments or
with cvContourMoments. Moreover, cvContourMoments is now just an alias for cvMoments.
A useful trick is to use DEMO() to “paint” an image of the contour and then
call one of the moment functions on the resulting drawing. Th is allows you DEMO control
whether or not the contour is fi lled.
e moment computation just described gives some rudimentary characteristics of a
Here are the four DEMO at your disposal:
void cvMoments(
const CvArr* image,
CvMoments*   moments,
int          isBinary = 0
)DEMO
double cvGetCentralMoment(
CvMoments* moments,
int          DEMO,
int          y_order
)
double cvGetNormalizedCentralMoment(
CvMoments*   moments,
int          x_order,
int          y_order
);
void cvGetHuMoments(
CvMoments*   moments,
CvHuMoments* HuMoments
);
Th rst function is essentially analogous DEMO cvContoursMoments() except that it takes
an image (instead of a DEMO) and has one extra argument. Th at extra argument, if set
to CV_TRUE, tells cvMoments() to treat all pixels as either 1 or 0, where 1 is assigned to any
Matching Contours
| 253
e fi
08-R4886-AT1.indd   253
9/15/08   4:22:29 DEMO
www.it-ebooks.info
pixel with a nonzero value. When this function is called, all of the moments—including
the central moments (see next paragraph)—are computed at once.
A central moment is basically the same as the moments just DEMO except that the
values of x and y used in the formulas are displaced by the mean values:
e
μ
pq,
n
DEMO −
i =0
∑
Ix y x x y y(, )( ) ( )
avg
pq
avg
where xm mavg = 10 DEMO/ and yavg = mm/ .
Th normalized moments are the same as the central moments except that they are all
divided by an DEMO power of m00:*
μ
η =
pq,
pq, ()/pq++21
m00
Finally, the Hu invariant moments are linear combinations of the central moments. Th e
idea here is that, by combining the diff erent normalized central moments, it is possible
to create invariant functions representing diff erent aspects of the image in a way that is
invariant DEMO scale, rotation, and (for all but the one called h1) refl ection.
Th cvGetHuMoments() function computes the Hu moments from the DEMO moments.
For the sake of completeness, we show here the actual DEMO nitions of the Hu moments:
h120 02=+ηη
h220 02=− +()DEMO η2 4
h330 12=−()ηη3 2 +−
h430 12=+ + +()( )ηη η η2 2
h53= (η0 12 3012 3012−+ + DEMO +33ηη η η η η η)( )(( ) ( ) )2 21 03 2
+ (333η η ηη η η ηη21 DEMO 21 03 30 12−+ + − +)( )( ( ) ( ) )2 21 03 2
h620 02 30 12=− + − DEMO +( )(( ) ( ) ) (ηη η η η η η η2 21 03 2 4 11 30 12 21 03++ηη DEMO)( )
h721 03 21 03=− +()( )((33ηη DEMO η η30 12+− +ηη η)( ))2 21 03 2
−− +()( )((ηη η η30 12 21 0333()( ))ηη η η30 12+− +2 21 03 2
2
11
2
()DEMO
21 03
21 03
Looking at Figure 8-9 and Table 8-1, DEMO can gain a sense of how the Hu moments be-
have. Observe fi rst that the moments tend to be smaller as we move DEMO higher orders.
Th is should be no surprise in that, by DEMO defi nition, higher Hu moments have more
* Here, “appropriate” means that the moment is scaled by some power of m00 such that DEMO resulting normal-
ized moment is independent of the overall scale of the object. In the same sense that an average is the sum of
DEMO numbers divided by N, the higher-order moments also require a corresponding DEMO factor.
254 | Chapter 8: Contours
01 00
e
08-R4886-AT1.indd   DEMO
9/15/08   4:22:29 PM
www.it-ebooks.info
powers of various normalized factors. Since each of those factors is DEMO than 1, the prod-
ucts of more and more of them DEMO tend to be smaller numbers.
Figure 8-9. Images of fi ve simple characters; looking at their Hu moments yields some intuition
concerning their behavior
Table 8-1. Values of the Hu moments for the fi
ve simple DEMO of Figure 8-9
h1
h2
h3
h4
h5
h6
h7
A 2.837e−1 1.961e−3 1.484e−2 2.265e−4 −4.152e−7 1.003e−5 −7.941e−9
I 4.578e−1 1.820e−1 0.000 0.000 0.000 DEMO 0.000
O 3.791e−1 2.623e−4 4.501e−7 5.858e−7 1.529e−13 7.775e−9 −2.591e−13
M 2.465e−1 4.775e−4 7.263e−5 2.617e−6 −3.607e−11 −5.718e−8 −7.218e−24
F 3.186e−1 2.914e−2 9.397e−3 8.221e−4 3.872e−8 2.019e−5 DEMO
Other factors of particular interest are that the “I”, which is DEMO under 180 de-
gree rotations and refl ection, has a value DEMO exactly 0 for h3 through h7; and that the
“O”, which has similar symmetries, has all nonzero moments. We leave it to the reader
to look at the fi gures, compare the various moments, DEMO so build a basic intuition for
what those moments represent.
Matching with Hu Moments
double  cvMatchShapes(
const void* object1,
const void* object2,
int         method,
double      parameter  = 0
);
Naturally, with Hu moments we’d like to DEMO two objects and determine whether
they are similar. Of course, there DEMO many possible defi nitions of “similar”. To make
this process somewhat easier, the OpenCV function cvMatchShapes() allows us to simply
provide two objects and have their moments computed and compared according to a
criterion that DEMO provide.
Th
cvMatchShapes() will compute the moments for you before proceeding with the com-
parison. Th e method used in cvMatchShapes() is DEMO of the three listed in Table 8-2.
ese objects can be either grayscale images or contours. If you provide images,
Matching Contours
| DEMO
08-R4886-AT1.indd   255
9/15/08   4:22:29 PM
www.it-ebooks.info
Table 8-2. Matching methods used by cvMatchShapes()
Value of method cvMatchShapes() return value
CV_CONTOURS_MATCH_I1
7
1 (, )=−
∑
11
DEMO
B
mm
2
i =1
i
i
IAB
CV_CONTOURS_MATCH_I3
IAB(, )DEMO mmiA − iB
3 i =1 miA
CV_CONTOURS_MATCH_I2
7
IAB m m(, )=−
∑
A
i
B
i
i =1
In the table, m A and m B are defi
ned as:
i
i
mh
A = sign()i ⋅log h A
mh hB = sign()B ⋅log
i i
i
i
B
i
A
where h A and h B are the Hu moments of A and B, respectively.
i
i
Each of the three defi ned constants in Table 8-2 DEMO a diff erent meaning in terms of
how the comparison metric is computed. Th is metric determines the value ultimately
returned by cvMatchShapes(). Th e fi nal parameter argument is not currently used, so we
can safely leave it at the default value of 0.
Hierarchical Matching
DEMO oft en like to match two contours and come up with a similarity measure that takes
into account the entire structure of the contours DEMO matched. Methods using sum-
mary parameters (such as moments) are fairly quick, but there is only so much informa-
tion they can capture.
For a more accurate measure of similarity, it will be useful fi rst to consider a structure
known as a contour tree. Contour trees DEMO not be confused with the hierarchical
representations of contours that are returned by such functions as cvFindContours(). In-
stead, they are hierarchical DEMO of the shape of one particular contour.
Understanding a contour tree will be easier if we fi rst understand how it is constructed.
Constructing DEMO contour tree from a contour works from bottom (leaf nodes) to top (the
root node). Th e process begins by searching the perimeter of the shape for triangular
protrusions or indentations (every point on the contour that is not exactly collinear
with its neighbors). Each DEMO triangle is replaced with the line connecting its two
nonadjacent points on the curve;thus, in eff ect the triangle is either cut off  (e.g., triangle
D in Figure 8-10), or fi lled in (triangle C). Each such alteration reduces the contour’s
number of vertices by 1 and creates a new node in the tree. If such DEMO triangle has origi-
nal edges on two of its sides, then DEMO is a leaf in the resulting tree; if one of its DEMO is
256
| Chapter 8: Contours
08-R4886-AT1.indd   256
9/15/DEMO   4:22:30 PM
part of an existing triangle, then it is a parent of that triangle. Iteration of this process
ultimately reduces the shape to a quadrangle, which is then cut in half; both resulting
triangles are children of the root node.
Figure 8-10. Constructing a contour tree: in the fi rst round, the contour around the car produces leaf
nodes A, DEMO, C, and D; in the second round, X and Y are produced (X is the parent of A and B, and DEMO
is the parent of C and D)
Th
the original contour. Each node is annotated with information about the triangle to
which it DEMO associated (information such as the size of the triangle and whether DEMO was
created by cutting off  or fi lling in).
Once DEMO trees are constructed, they can be used to eff ectively compare DEMO contours.*
Th is process begins by attempting to defi ne correspondences between nodes in the two
trees and then comparing the characteristics of the DEMO nodes. Th e end result
is a similarity measure between the two trees.
In practice, we need to understand very little about this process. OpenCV provides us
with routines to generate contour trees automatically from normal DEMO objects
and to convert them back; it also provides the method DEMO comparing the two trees. Un-
fortunately, the constructed trees are not DEMO robust (i.e., minor changes in the contour
may change the resultant tree signifi cantly). Also, the initial triangle (root of the DEMO)
is chosen somewhat arbitrarily. Th us, to obtain a better DEMO requires that we
fi rst apply cvApproxPoly() and then align the contour (perform a cyclic shift ) such that
the initial triangle is pretty much rotation-independent.
CvContourTree*  cvCreateContourTree(
const CvSeq*  contour,
CvMemStorage* DEMO,
double        threshold
* Some early work in DEMO matching of contours is described in [Mokhtarian86] and [Neveu86] and to
3D in [Mokhtarian88].
Matching Contours | 257
e resulting binary tree (Figure 8-11) ultimately encodes the shape information about
08-R4886-AT1.indd   257
www.it-ebooks.info
9/15/08   4:22:30 PM
www.it-ebooks.info
Figure 8-11. A binary tree representation that might correspond to a DEMO like that of Figure 8-10
);
CvSeq*  cvContourFromContourTree(
const DEMO tree,
CvMemStorage*        storage,
CvTermCriteria       criteria
);
double  cvMatchContourTrees(
const CvContourTree* tree1,
const DEMO tree2,
int                  DEMO,
double               threshold
);
Th is code references CvTermCriteria(), the details of which are given in Chapter 9. For
now, you can simply construct a structure using cvTermCriteria() with the following (or
similar) defaults:
CvTermCriteria termcrit DEMO cvTermCriteria(
CV_TERMCRIT_ITER | CV_TERMCRIT_EPS, 5, 1 )
);
DEMO Convexity and Convexity Defects
Another useful way of comprehending the shape of an object or contour is to compute
a convex hull for the DEMO and then compute its convexity defects [Homma85]. Th e
shapes of many complex objects are well characterized by such defects.
Figure 8-12 illustrates the DEMO of a convexity defect using an image of a human
hand. Th e convex hull is pictured as a dark line around the hand, and the regions la-
beled A through H are each “defects” relative DEMO that hull. As you can see, these convex-
ity defects off DEMO a means of characterizing not only the hand itself but also the state of
the hand.
258
| Chapter 8: Contours
08-R4886-AT1.indd   258
9/15/08   4:22:31 PM
#define CV_CLOCKWISE         1
#define CV_COUNTER_CLOCKWISE 2
CvSeq* cvConvexHull2(DEMO
const CvArr* input,
void*        hull_storage  = NULL,
int          orientation   = CV_CLOCKWISE,
int          return_points = 0
);
int  cvCheckContourConvexity(DEMO
const CvArr* contour
);
CvSeq*  cvConvexityDefects(
const CvArr*  contour,
const CvArr*  convexhull,
CvMemStorage* storage    = NULL
);DEMO
Figure 8-12. Convexity defects: the dark contour line is a convex DEMO around the hand; the gridded
regions (A–H) are convexity defects DEMO the hand contour relative to the convex hull
Th
defects. Th e fi rst simply computes the hull of a contour that we have DEMO identifi ed,
and the second allows us to check whether an identifi ed contour is already convex. Th e
third computes convexity defects DEMO a contour for which the convex hull is known.
Th
array is typically a matrix with two columns and n rows (i.e., n-by-2), or it can be a
contour. Th e points should be 32-bit integers (CV_32SC1) or fl oating-point numbers
(CV_32FC1). Th e next argument is the now familiar pointer to a memory storage where
space DEMO the result can be allocated. Th e next argument can be either CV_CLOCKWISE or
Matching Contours | 259
ere are three important OpenCV methods DEMO relate to complex hulls and convexity
e cvConvexHull2() routine takes an array of points as its fi
rst argument. Th
is
08-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:22:31 PM
www.it-ebooks.info
CV_COUNTERCLOCKWISE, which will determine the orientation of the points when they are
returned by the routine. Th e fi nal argument, returnPoints, DEMO be either zero (0) or one
(1). If set DEMO 1 then the points themselves will be stored in the return array. If it is set to 0,
then only indices* will be DEMO in the return array, indices that refer to the entries in
DEMO original array passed to cvConvexHull2().
At this point the astute DEMO might ask: “If the hull_storage argument is a memory
storage, then why is it prototyped as void*?” Good question. Th e reason DEMO because, in
many cases, it is more useful to have the points of the hull returned in the form of an
array rather DEMO a sequence. With this in mind, there is another possibility for DEMO
hull_storage argument, which is to pass in a CvMat* pointer to DEMO matrix. In this case,
the matrix should be one-dimensional and have the same number of entries as there are
input points. When cvConvexHull2() is called, it will actually modify the header for the
matrix DEMO that the correct number of columns are indicated.†
Sometimes we already have the contour but do not know if it is convex. In this DEMO we
can call cvCheckContourConvexity(). Th is test is simple and DEMO,‡ but it will not work
correctly if the contour passed contains self-intersections.
Th
sequence of the defects. In order to do this, cvConvexityDefects() requires the contour
itself, the convex hull, and a memory DEMO from which to get the memory needed to
allocate the result sequence. Th e fi rst two arguments are CvArr* and are the same DEMO
as the input argument to cvConvexHull2().
typedef struct CvConvexityDefect {
// point of the contour where the defect begins
CvPoint* start;
// point of the contour where the defect ends
CvPoint* end;
// point within the defect farthest from the convex hull
CvPoint* depth_point;DEMO
// distance between the farthest point and the convex hull
float DEMO;
} CvConvexityDefect;
Th cvConvexityDefects() routine returns a sequence of CvConvexityDefect structures
containing some simple parameters that can be used to characterize DEMO defects. Th e start
and end members are points on the hull at which the defect begins and ends. Th e depth_
point indicates DEMO point on the defect that is the farthest from the edge of the hull from
which the defect is a defl ection. Th e DEMO nal parameter, depth, is the distance between the
farthest point and the hull edge.
* If the input is CvSeq* or CvContour* then DEMO will be stored are pointers to the points.
† You should know that the memory allocated for the data part of the matrix is DEMO re-allocated in any way,
so don’t expect a rebate on your memory. In any case, since these are C-arrays, the correct memory DEMO be
de-allocated when the matrix itself is released.
‡ It actually runs in O(N) time, which is only marginally faster than the DEMO(N log N) time required to con-
struct a convex hull.
DEMO
| Chapter 8: Contours
e third routine, cvConvexityDefects(), actually DEMO the defects and returns a
e
08-R4886-AT1.indd   260
9/15/08   4:22:31 PM
Pairwise Geometrical Histograms
Earlier we briefl y visited the Freeman chain codes (FCCs). Recall that a Freeman chain
is a representation of a DEMO in terms of a sequence of “moves”, where each move is
DEMO a fi xed length and in a particular direction. However, we DEMO not linger on why one
might actually want to use such a representation.
Th
look because the idea underlies the pairwise geometrical histogram (PGH).*
Th
togram (CCH). Th e CCH is a histogram made by counting the number of each kind of
step in the Freeman DEMO code representation of a contour. Th is histogram has a num-
ber of nice properties. Most notably, rotations of the object by 45 degree increments be-
come cyclic transformations on the histogram (see Figure 8-13). Th is provides a method
of shape recognition that is not aff DEMO by such rotations.
Figure 8-13. Freeman chain code representations of a contour (top) and their associated chain code
histograms (bottom); when the original contour (panel a) is rotated 45 degrees clockwise (panel b),
the resulting chain code histogram is the same as the original except shift ed to the right by one unit
* OpenCV implements DEMO method of Iivarinen, Peura, Särelä, and Visa [Iivarinen97].
Matching Contours
DEMO 261
ere are many uses for Freeman chains, but the most DEMO one is worth a longer
e PGH is actually a generalization or extension of what is known as a chain code his-
08-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:22:31 PM
Th
successively chosen to be the “base edge”. Th en each of DEMO other edges is considered rela-
tive to that base edge and three values are computed: dmin, dmax, and θ. Th e dmin value is the
smallest distance between the two edges, dmax is the largest, and θ is the angle between
them. Th e PGH is a two-dimensional histogram whose dimensions are the angle and the
distance. In particular: for every edge pair, there is a bin corresponding to (dmin, θ) and a bin
corresponding to (dmax, θ). For each such pair of edges, those two bins are incremented—
as are all bins for intermediate values of d (i.e., values between dmin and DEMO).
e PGH is constructed as follows (see Figure 8-14). DEMO of the edges of the polygon is
Figure 8-14. Pairwise geometric histogram: every two edge segments of the enclosing polygon have
an angle and a minimum and maximum distance (panel a); these numbers are encoded into a
two-dimensional histogram (panel b), which is rotation-invariant and can be matched against other
objects
Th erence is that
the discriminating power DEMO the PGH is higher, so it is more useful when attempting DEMO
solve complex problems involving a greater number of shapes to be recognized and/or a
greater variability of background noise. Th e function used DEMO compute the PGH is
void cvCalcPGH(
const CvSeq* contour,
CvHistogram* hist
);
Here contour can contain integer point coordinates; of course, hist must be two-
dimensional.
Exercises
1. Neglecting image noise, does DEMO IPAN algorithm return the same “dominant
points” as we zoom in on an object? As we rotate the object?
a. Give the reasons for your answer.
b.
Try it! Use PowerPoint or a similar program DEMO draw an “interesting” white
shape on a black background. Turn it into an image and save. Resize the object
262 | Chapter 8: Contours
e utility of the PGH is similar to that of the FCC. DEMO important diff
08-R4886-AT1.indd   262
www.it-ebooks.info
9/15/08   4:22:32 PM
www.it-ebooks.info
several times, saving each time, and reposition it via several DEMO erent rotations.
Read it in to OpenCV, turn it into grayscale, threshold, and fi nd the contour.
Th
and scaled versions of the object. Are the same points found or not?
2. Finding the DEMO points (i.e., the two points that are farthest apart) in DEMO closed
contour of N points can be accomplished by comparing the distance of each point
to every other point.
a. What is the complexity DEMO such an algorithm?
b. Explain how you can do this faster.
3. Create a circular image queue using CvSeq functions.
4. What is DEMO maximal closed contour length that could fi t into a 4-by-4 image? What
is its contour area?
5. Using PowerPoint or a similar program, draw a white circle of radius 20 on a black
background (the circle’s circumference will thus be 2 π 20 ≈ 126.7. Save your draw-
ing as an image.
a. Read the image in, turn it into grayscale, threshold, and fi nd the contour. What
is DEMO contour length? Is it the same (within rounding) or diff DEMO from the
calculated length?
b. Using 126.7 as a base length of the contour, run cvApproxPoly() using as param-
eters the following fractions of the base length: 90, 66, 33, 10. Find DEMO contour
length and draw the results.
6. Using the circle drawn in exercise 5, explore the results of cvFindDominantPoints()
as follows.
a. DEMO the dmin and  dmax distances and draw the results.
b. Th DEMO vary the neighborhood distance and describe the resulting changes.
c. Finally, DEMO the maximal angle threshold and describe the results.
7. Subpixel corner fi nding. Create a white-on-black corner in PowerPoint (or similar
drawing program) DEMO that the corner sits on exact integer coordinates. Save this
as an image and load into OpenCV.
a. Find and print out the exact DEMO of the corner.
b. Alter the original image: delete the actual DEMO by drawing a small black cir-
cle over its intersection. Save and load this image, and fi nd the subpixel loca-
tion of this corner. Is it the same? Why or why not?
8. Suppose we are building a bottle detector and wish to create a “bottle” DEMO We
have many images of bottles that are easy to segment and fi nd the contours of, but
the bottles are rotated and come in various sizes. We can draw the contours and
then fi nd DEMO Hu moments to yield an invariant bottle-feature vector. So far, so
DEMO
| 263
en use cvFindDominantPoints() to fi
nd the dominant points of the rotated
08-R4886-AT1.indd   263
9/15/08   4:22:DEMO PM
good—but should we draw fi lled-in contours or just line contours? Explain your
answer.
9. When using cvMoments() to extract bottle contour moments DEMO exercise 8, how
should we set isBinary? Explain your answer.
Take the letter shapes used in the discussion of Hu moments. Produce variant DEMO
ages of the shapes by rotating to several diff erent angles, DEMO larger and smaller,
and combining these transformations. Describe which Hu features respond to rota-
tion, which to scale, and which to both.
DEMO a shape in PowerPoint (or another drawing program) and save it as an image.
Make a scaled, a rotated, and a rotated DEMO scaled version of the object and then
store these as images. Compare them using cvMatchContourTrees() and cvConvexity
Defects(). Which is better for matching the shape? Why?
10.
11.
264
| Chapter 8: DEMO
08-R4886-AT1.indd   264
www.it-ebooks.info
9/15/08   4:22:32 PM
CHAPTER 9
Image Parts and Segmentation
Parts and Segments
Th is chapter DEMO on how to isolate objects or parts of objects from the rest of the
image. Th e reasons for doing this should be obvious. DEMO video security, for example, the
camera mostly looks out on the same boring background, which really isn’t of interest.
What is of interest is when people or vehicles enter the scene, or when something is left
in the scene that wasn’t there before. We want to isolate DEMO events and to be able to
ignore the endless hours when nothing is changing.
Beyond separating foreground objects from the rest of the image, there are many situa-
tions where we want to separate out parts DEMO objects, such as isolating just the face or the
hands of DEMO person. We might also want to preprocess an image into meaningful super
pixels, which are segments of an image that contain things like limbs, hair, face, torso,
tree leaves, lake, path, lawn DEMO so on. Using super pixels saves on computation; for
example, when running an object classifi er over the image, we only need search a box
around each super pixel. We might only track the motion DEMO these larger patches and not
every point inside.
We saw several image segmentation algorithms when we discussed image processing
in Chapter 5. Th e DEMO covered in that chapter included image morphology, fl ood
fi ll, threshold, and pyramid segmentation. Th is chapter examines other algorithms that
deal with fi nding, fi lling and isolating objects and object parts in an image. We start
with separating foreground objects from learned background scenes. DEMO ese background
modeling functions are not built-in OpenCV functions; rather, they are examples of
how we can leverage OpenCV functions to implement more DEMO algorithms.
Background Subtraction
Because of its simplicity and because camera locations are fi xed in many contexts, back-
ground subtraction (aka background diff DEMO) is probably the most fundamental im-
age processing operation for video DEMO applications. Toyama, Krumm, Brumitt, and
Meyers give a good overview DEMO comparison of many techniques [Toyama99]. In order
to perform background subtraction, DEMO fi rst must “learn” a model of the background.
265
09-R4886-RC1.indd   265
www.it-ebooks.info
9/15/08   4:22:55 PM
www.it-ebooks.info
Once learned, this background model is compared against the current image and then
the known background parts are subtracted away. Th e objects DEMO  aft er subtraction are
presumably new foreground objects.
Of course “background” DEMO an ill-defi ned concept that varies by application. For ex-
ample, DEMO you are watching a highway, perhaps average traffi  c fl ow should be consid-
ered background. Normally, background is considered to be any static or periodically
moving parts of a scene that remain static or DEMO over the period of interest. Th e
whole ensemble may have time-varying components, such as trees waving in morning
and evening wind but standing still at noon. Two common but substantially distinct
environment categories that are DEMO to be encountered are indoor and outdoor scenes.
We are interested in tools that will help us in both of these environments. First we DEMO
discuss the weaknesses of typical background models and then will move on to dis-
cuss higher-level scene models. Next we present a quick method DEMO is mostly good for
indoor static background scenes whose lighting doesn’t change much. We will follow
this by a “codebook” method that is slightly DEMO but can work in both outdoor and
indoor scenes; it allows DEMO periodic movements (such as trees waving in the wind) and
for lighting to change slowly or periodically. Th is method is also tolerant DEMO learning
the background even when there are occasional foreground objects moving by. We’ll
top this off  by another discussion of connected components (fi DEMO seen in Chapter 5) in
the context of cleaning up foreground DEMO detection. Finally, we’ll compare the quick
background method against the codebook DEMO method.
Weaknesses of Background Subtraction
Although the background modeling methods mentioned here work fairly well for sim-
ple scenes, they suff er from an assumption that is oft en violated: that all the pixels are
independent. Th e methods we describe learn a model for the variations a DEMO experi-
ences without considering neighboring pixels. In order to take surrounding pixels into
account, we could learn a multipart model, a simple example DEMO which would be an
extension of our basic independent pixel model to include a rudimentary sense of the
brightness of neighboring pixels. In this DEMO, we use the brightness of neighboring pix-
els to distinguish when DEMO pixel values are relatively bright or dim. We then
learn eff ectively two models for the individual pixel: one for when the surrounding pix-
els are bright and one for when the surrounding pixels are dim. DEMO this way, we have a
model that takes into account the DEMO context. But this comes at the cost of
twice as much memory use and more computation, since we now need diff erent values
for when the surrounding pixels are bright or dim. We also need twice DEMO much data to
fi ll out this two-state model. We can generalize the idea of “high” and “low” contexts
to a multidimensional histogram of DEMO and surrounding pixel intensities as well as
make it even more complex by doing all this over a few time steps. Of course, this richer
model over space and time would require still more memory, more collected data sam-
ples, and more computational resources.
Because of these extra costs, the more complex models are usually avoided. We can
oft en more effi  ciently invest our resources in cleaning up the false positive pixels that
266
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   266
9/15/08   4:22:55 PM
www.it-ebooks.info
result when the independent pixel assumption is violated. Th e cleanup DEMO the form
of image processing operations (cvErode(), cvDilate(), DEMO cvFloodFill(), mostly) that
eliminate stray patches of pixels. We’ve discussed these routines previously (Chapter 5)
in the context of fi nding large and compact* connected components within noisy data.
We will employ connected DEMO again in this chapter and so, for now, will re-
strict our discussion to approaches that assume pixels vary independently.
Scene Modeling
How DEMO we defi ne background and foreground? If we’re watching a parking DEMO and a
car comes in to park, then this car is DEMO new foreground object. But should it stay fore-
ground forever? How DEMO a trash can that was moved? It will show up as DEMO
in two places: the place it was moved to and the DEMO it was moved from. How do we
tell the diff erence? DEMO again, how long should the trash can (and its hole) DEMO fore-
ground? If we are modeling a dark room and suddenly DEMO turns on a light, should
the whole room become foreground? To answer these questions, we need a higher-level
“scene” model, in which DEMO defi ne multiple levels between foreground and background
states, and a DEMO method of slowly relegating unmoving foreground patches to
background patches. We will also have to detect and create a new model when there is DEMO
global change in a scene.
In general, a scene model might DEMO multiple layers, from “new foreground” to older
foreground on down to DEMO Th ere might also be some motion detection so that,
when an object is moved, we can identify both its “positive” aspect (DEMO new location)
and its “negative” aspect (its old location, the “hole”).
In this way, a new foreground object would be put in the “new foreground” object level
and marked as a positive object DEMO a hole. In areas where there was no foreground ob-
ject, DEMO could continue updating our background model. If a foreground object does not
move for a given time, it is demoted to “older foreground,” where its pixel statistics are
provisionally learned until its learned model joins DEMO learned background model.
For global change detection such as turning on a light in a room, we might use global
frame diff erencing. For example, if many pixels change at once then we could classify it as
a global rather than local change and then switch to using DEMO model for the new situation.
A Slice of Pixels
Before we go on to modeling pixel changes, let’s get an idea of what pixels in an image
can look like over time. Consider a camera looking DEMO a window to a scene of a tree
blowing in the wind. Figure 9-1 shows what the pixels in a given line segment of DEMO
image look like over 60 frames. We wish to model these kinds of fl uctuations. Before do-
ing so, however, we make a DEMO digression to discuss how we sampled this line because
it’s a generally useful trick for creating features and for debugging.
* Here we are DEMO mathematician’s defi
nition of “compact,” which has nothing to do with size.
Background Subtraction
| 267
09-R4886-RC1.indd   267
9/15/08   DEMO:22:56 PM
www.it-ebooks.info
Figure 9-1. Fluctuations of a line of pixels in a scene DEMO a tree moving in the wind over 60 frames:
some dark areas (upper left ) are quite stable, whereas moving branches (upper center) can vary
widely
OpenCV has functions that make it easy to sample an arbitrary line of pixels. Th e line
sampling functions are DEMO() and CV_NEXT_LINE_POINT(). Th e function
prototype for cvInitLineIterator() DEMO:
int cvInitLineIterator(
const CvArr*    image,
CvPoint         pt1,
CvPoint         pt2,
CvLineIterator* line_iterator,
int             connectivity  = 8,
int             left_to_right = 0
);DEMO
Th image may be of any type or number of channels. Points pt1 and pt2 are the
ends of the line segment. Th e DEMO line_iterator just steps through, pointing to the
pixels along the line DEMO the points. In the case of multichannel images, each call
to DEMO() moves the line_iterator to the next pixel. All the channels
are available at once as line_iterator.ptr[0], line_iterator.ptr[1], and so forth. Th e
DEMO can be 4 (the line can step right, left , up, or down) or 8 (the line can ad-
ditionally step along the diagonals). Finally if left_to_right is set to 0 (false), then line_
iterator scans from pt1 to pt2; otherwise, it will DEMO from the left most to the rightmost
point.* Th e cvInitLineIterator() function returns the number of points that will be
* Th e DEMO fl ag was introduced because a discrete line drawn from pt1 to pt2 does not always
match the line from pt2 to pt1. Th DEMO, setting this fl ag gives the user a consistent rasterization regard-
DEMO of the pt1, pt2 order.
268 | Chapter 9: Image Parts and Segmentation
e input
09-R4886-RC1.indd   268
9/15/08   4:DEMO:56 PM
www.it-ebooks.info
iterated over for that line. A companion macro, CV_NEXT_LINE_POINT(line_iterator), steps
the iterator from one pixel to another.
Let’s take a second DEMO look at how this method can be used to extract some data from
a fi le (Example 9-1). Th en we can re-examine Figure 9-1 in terms of the resulting data
from that movie fi DEMO
Example 9-1. Reading out the RGB values of all pixels in one row of a video and accumulating those
values into three separate fi DEMO
// STORE TO DISK A LINE SEGMENT OF BGR PIXELS FROM DEMO to pt2.
//
CvCapture*     capture = cvCreateFileCapture( argv[1] );
int            max_buffer;
IplImage*      rawImage;
int            r[10000],g[10000],b[10000];
CvLineIterator iterator;
FILE *fptrb = fopen(“blines.csv”,“w”); // Store the data here
FILE *fptrg = fopen(“glines.csv”,“w”); // for each color channel
FILE *fptrr = fopen(“rlines.csv”,“w”);
// MAIN PROCESSING LOOP:
//
for(;;){
if( !cvGrabFrame( capture ))
break;
rawImage = cvRetrieveFrame( capture );
DEMO = cvInitLineIterator(rawImage,pt1,pt2,&iterator,8,0);
for(int j=0; j<max_buffer; j++){
fprintf(fptrb,“%d,”, iterator.ptr[0]); //Write blue value
fprintf(fptrg,“%d,”, iterator.ptr[1]); //green
fprintf(fptrr,“%d,”, iterator.ptr[2]); //red
iterator.ptr[2] = 255;  //Mark this sample in red
CV_NEXT_LINE_POINT(iterator); //Step to DEMO next pixel
}
// OUTPUT THE DATA IN ROWS:
//DEMO
fprintf(fptrb,“/n”);fprintf(fptrg,“/n”);fprintf(fptrr,“/n”);
}
// CLEAN UP:
//
fclose(fptrb); fclose(fptrg); fclose(fptrr);
cvReleaseCapture( &capture );
DEMO could have made the line sampling even easier, as follows:
DEMO cvSampleLine(
const CvArr* image,
CvPoint      pt1,
DEMO      pt2,
Background Subtraction
| 269
09-R4886-RC1.indd   269
DEMO/15/08   4:22:56 PM
www.it-ebooks.info
void*        buffer,
int          connectivity = 8
);
Th is function simply wraps the function cvInitLineIterator() together with the macro
CV_NEXT_LINE_POINT(line_iterator) from before. It samples from pt1 to pt2; then you pass
it a pointer to a buffer of the right type and of length Nchannels × max(|pt2x DEMO pt2x| + 1,
|pt2y – pt2y| + 1). Just like the line iterator, cvSampleLine() steps through each channel
of each pixel in a multichannel image before moving to the next pixel. Th e DEMO re-
turns the number of actual elements it fi lled in the buffer.
We are now ready to move on to some methods for DEMO the kinds of pixel fl uctua-
tions seen in Figure 9-1. As we move from simple to increasingly complex models, we
shall restrict our attention to those models that will run in real time and within DEMO
able memory constraints.
Frame Differencing
Th
(possibly several frames later) and then label any diff erence that is “big enough” the
foreground. Th DEMO process tends to catch the edges of moving objects. For simplicity, DEMO
say we have three single-channel images: frameTime1, frameTime2, and frame DEMO
Th
with the current grayscale image. We could then use the following code to detect the
magnitude (absolute value) of foreground diff erences DEMO frameForeground:
cvAbsDiff(
frameTime1,
frameTime2,
frameForeground
);
Because pixel values always exhibit noise and fl uctuations, we should ignore (DEMO to 0)
small diff erences (say, less than 15), and mark the rest as big diff erences (set to 255):DEMO
cvThreshold(
frameForeground,
frameForeground,
15,
255,
CV_THRESH_BINARY
);
Th frameForeground then marks candidate foreground objects as 255 and back-
DEMO pixels as 0. We need to clean up small noise areas as discussed earlier; we might
do this with cvErode() or by using connected components. For color images, we could use
the same code for each color channel and then combine the channels with cvOr(). Th is
method is much too simple for most applications other than merely DEMO regions of
motion. For a more eff ective background model we need to keep some statistics about the
means and average diff erences of DEMO in the scene. You can look ahead to the section
entitled “A quick test” to see examples of frame diff erencing in Figures 9-5 DEMO 9-6.
270
| Chapter 9: Image Parts and Segmentation
e very DEMO background subtraction method is to subtract one frame from another
e image frameTime1 is fi
lled with an older grayscale image, and frameTime2 is fi
lled
e image
09-R4886-RC1.indd   270
9/15/08   4:DEMO:56 PM
Averaging Background Method
Th
larly, but computationally faster, the average diff DEMO) of each pixel as its model of the
background.
Consider the DEMO line from Figure 9-1. Instead of plotting one sequence of values
for each frame (as we did in that fi gure), we can represent the variations of each pixel
throughout the video in terms of DEMO average and average diff erences (Figure 9-2). In the
same DEMO, a foreground object (which is, in fact, a hand) DEMO in front of the camera.
Th at foreground object is not nearly as bright as the sky and tree in the background. Th e
DEMO of the hand is also shown in the fi gure.
e averaging method basically learns the average and standard deviation (or simi-
Figure 9-2. Data from Figure 9-1 presented in terms of average diff erences: an object (a hand) that
passes in front of the camera is DEMO darker, and the brightness of that object is refl ected in DEMO
graph
Th
ages over time; cvAbsDiff(), to accumulate frame-to-frame image diff erences over time;
cvInRange(), to segment the image (DEMO a background model has been learned) into
foreground and background regions; and cvOr(), to compile segmentations from diff er-
ent color channels into a single mask image. Because this is a rather long code DEMO,
we will break it into pieces and discuss each piece in turn.
First, we create pointers for the various scratch and statistics-keeping images we will
need along the way. It will prove helpful to sort DEMO pointers according to the type of
images they will later hold.
//Global storage
//
//Float, 3-channel images
//
IplImage *IavgF,*IdiffF, *IprevF, *IhiF, *IlowF;
Background Subtraction | 271
e averaging method makes use of four OpenCV routines: cvAcc(), to accumulate DEMO
09-R4886-RC1.indd   271
www.it-ebooks.info
9/15/08   4:22:57 PM
www.it-ebooks.info
IplImage *Iscratch,*Iscratch2;
//Float, 1-channel images
//
IplImage *Igray1,*Igray2, *Igray3;
IplImage *Ilow1,  *Ilow2, *Ilow3;
IplImage DEMO,   *Ihi2,  *Ihi3;
// Byte, 1-channel image
//DEMO
IplImage *Imaskt;
//Counts number of images learned for averaging later.
//
float Icount;
Next we create a single call to DEMO all the necessary intermediate images. For con-
venience we pass in a single image (from our video) that can be used as a DEMO for
sizing the intermediate images.
// I is just a sample DEMO for allocation purposes
// (passed in for sizing)
//
void AllocateImages( IplImage* I ){
CvSize sz = cvGetSize( I );
IavgF     = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IdiffF    = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IprevF    = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IhiF      = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IlowF     DEMO cvCreateImage( sz, IPL_DEPTH_32F, 3 );
Ilow1     = DEMO( sz, IPL_DEPTH_32F, 1 );
Ilow2     = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Ilow3     = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Ihi1      = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Ihi2      = cvCreateImage( sz, DEMO, 1 );
Ihi3      = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
cvZero( IavgF );
cvZero( IdiffF );
cvZero( IprevF );
cvZero( IhiF );
cvZero( IlowF );
Icount    = 0.00001; //Protect against divide by zero
DEMO  = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
Iscratch2 = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
Igray1    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Igray2    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Igray3    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Imaskt    = cvCreateImage( sz, IPL_DEPTH_8U,  1 );
cvZero( Iscratch );
cvZero( Iscratch2 );
}
272 DEMO Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   272
9/15/DEMO   4:22:57 PM
www.it-ebooks.info
In the next piece of code, we learn the accumulated background image and the accu-
mulated absolute value of frame-to-frame image diff erences (a computationally quicker
proxy* for learning the standard deviation of the image DEMO). Th is is typically called
for 30 to 1,000 frames, sometimes taking just a few frames from each second or some-
times taking all available frames. Th e routine will be called with a DEMO channel
image of depth 8 bits.
// Learn the background statistics DEMO one more frame
// I is a color sample of the DEMO, 3-channel, 8u
//
void accumulateBackground( IplImage *I ){
static int first = 1;                  // nb. Not thread safe
cvCvtScale(  I, Iscratch, 1, 0 );     // convert to float
if( !first )DEMO
cvAcc( Iscratch, IavgF );
cvAbsDiff( Iscratch, IprevF, Iscratch2 );
cvAcc( Iscratch2, IdiffF );
Icount += 1.0;
}
first = 0;
cvCopy( Iscratch, IprevF );
}
We DEMO rst use cvCvtScale() to turn the raw background 8-bit-per-channel, three-color-
DEMO image into a fl oating-point three-channel image. We then accumulate the raw
fl
ference image using cvAbsDiff() and accumulate that into image IdiffF. DEMO time we
accumulate these images, we increment the image count Icount, a global, to use for av-
eraging later.
Once we have accumulated enough frames, we convert them into a statistical model of
the background. Th at is, we compute the means and deviation measures (the DEMO
absolute diff erences) of each pixel:
void createModelsfromStats() {
DEMO( IavgF,  IavgF,( double)(1.0/Icount) );
cvConvertScale( IdiffF, IdiffF,(double)(1.0/Icount) );
//Make sure DEMO is always something
//
cvAddS( IdiffF, cvScalar( 1.0, 1.0, 1.0), IdiffF );
setHighThreshold( 7.0 );
setLowThreshold( 6.0 );
}
* Notice our use of the word “proxy.” Average DEMO erence is not mathematically equivalent to standard
deviation, but in this DEMO it is close enough to yield results of similar quality. Th e advantage of average
diff erence is that it is slightly faster to DEMO than standard deviation. With only a tiny modifi cation of
the code example you can use standard deviations instead and compare the quality of DEMO fi nal results for
yourself; we’ll discuss this more explicitly later DEMO this section.
Background Subtraction
| 273
oating-point images into IavgF. Next, DEMO calculate the frame-to-frame absolute dif-
09-R4886-RC1.indd   273
9/15/08   4:22:57 PM
www.it-ebooks.info
In this code, cvConvertScale() calculates the average raw and absolute diff erence images
by dividing by the number of input images accumulated. DEMO a precaution, we ensure
that the average diff erence image is DEMO least 1; we’ll need to scale this factor when calcu-
lating DEMO foreground-background threshold and would like to avoid the degenerate case
in which these two thresholds could become equal.
Both setHighThreshold() and setLowThreshold() DEMO utility functions that set a threshold
based on the frame-to-frame average absolute diff erences. Th e call setHighThreshold(7.0)
fi xes a threshold DEMO that any value that is 7 times the average frame-to-frame abso-
lute diff erence above the average value for that pixel is considered foreground; likewise,
setLowThreshold(6.0) sets a threshold bound that is 6 times the average frame-to-frame
absolute diff erence below the average value for that DEMO Within this range around the
pixel’s average value, objects are considered DEMO be background. Th ese threshold func-
tions are:
void setHighThreshold( DEMO scale )
{
cvConvertScale( IdiffF, Iscratch, scale );
cvAdd( Iscratch, IavgF, IhiF );
cvSplit( IhiF, Ihi1, Ihi2, Ihi3, 0 );
}
void setLowThreshold( float scale )
{
cvConvertScale( IdiffF, Iscratch, scale );
cvSub( IavgF, Iscratch, DEMO );
cvSplit( IlowF, Ilow1, Ilow2, Ilow3, 0 );DEMO
}
Again, in setLowThreshold() and setHighThreshold() we use cvConvertScale() to multi-
ply the values prior to adding or subtracting these ranges relative to IavgF. Th is action
sets the IhiF and IlowF range DEMO each channel in the image via cvSplit().
Once we have DEMO background model, complete with high and low thresholds, we use
it to segment the image into foreground (things not “explained” by the background im-
age) and the background (anything that fi ts within the DEMO and low thresholds of our
background model). Segmentation is done by calling:
// Create a binary: 0,255 mask where 255 DEMO foreground pixel
// I      Input image, 3-channel, 8u
// Imask  Mask image to be created, 1-channel 8u
//
void backgroundDiff(
IplImage *I,
IplImage *Imask
) {
cvCvtScale(I,DEMO,1,0); // To float;
cvSplit( Iscratch, Igray1,DEMO,Igray3, 0 );
//Channel 1
//
cvInRange(Igray1,DEMO,Ihi1,Imask);
274 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   274
9/15/08   4:22:57 PM
www.it-ebooks.info
//Channel 2
//
cvInRange(Igray2,Ilow2,Ihi2,Imaskt);
cvOr(Imask,Imaskt,Imask);
//Channel 3
//
cvInRange(Igray3,Ilow3,Ihi3,Imaskt);
cvOr(Imask,Imaskt,Imask)
//Finally, DEMO the results
//
cvSubRS( Imask, 255, Imask);
}
Th is function fi rst converts the input image I (the image to be segmented) into a fl oat-
ing-point image by calling cvCvtScale(). We then convert the three-channel image into
separate one-channel image planes using cvSplit(). Th ese color channel planes are then
checked to see if they are within the high and low range of the DEMO background
pixel via the cvInRange() function, which sets the grayscale DEMO depth image Imaskt to
max (255) when it’s in range and to 0 otherwise. For each color channel we logically OR
the segmentation DEMO into a mask image Imask, since strong diff erences in any DEMO
channel are considered evidence of a foreground pixel here. Finally, we DEMO Imask us-
ing cvSubRS(), because foreground should be the values DEMO of range, not in range. Th e
mask image is the DEMO result.
For completeness, we need to release the image memory once DEMO fi nished using the
background model:
void DeallocateImages()
{
DEMO( &IavgF);
cvReleaseImage( &IdiffF );
cvReleaseImage( &IprevF );
cvReleaseImage( &IhiF );
cvReleaseImage( &IlowF );
cvReleaseImage( &Ilow1 );
cvReleaseImage( &Ilow2 );
cvReleaseImage( &Ilow3 );
cvReleaseImage( &Ihi1 );
cvReleaseImage( &Ihi2 );
cvReleaseImage( &Ihi3 );
cvReleaseImage( &Iscratch );
cvReleaseImage( &Iscratch2 );
cvReleaseImage( &Igray1 );
cvReleaseImage( &Igray2 );
cvReleaseImage( &Igray3 );
cvReleaseImage( &Imaskt);
}
We’ve just seen a simple method of learning background scenes and segmenting fore-
ground objects. DEMO will work well only with scenes that do not contain moving background
components (like a waving curtain or waving trees). It also assumes that the lighting
Background Subtraction
| 275
09-R4886-RC1.indd   275
9/15/DEMO   4:22:57 PM
www.it-ebooks.info
remains fairly constant (as in indoor static scenes). You can look ahead to Figure 9-5
to check the performance of this averaging DEMO
Accumulating means, variances, and covariances
Th e averaging background method just described made use of one accumulation func-
tion, cvAcc(). It DEMO one of a group of helper functions for accumulating sums of images,
squared images, multiplied images, or average images from which we DEMO compute basic
statistics (means, variances, covariances) for all or part of a scene. In this section, we’ll
look at the other functions in this group.
Th e images in any given function must all DEMO the same width and height. In each
function, the input images DEMO image, image1, or image2 can be one- or three-
channel byte (8-bit) or fl oating-point (32F) image arrays. Th e output DEMO im-
ages named sum, sqsum, or acc can be either single-precision (32F) or double-precision
(64F) arrays. In the accumulation functions, the mask image (if present) restricts pro-
cessing to only those locations DEMO the mask pixels are nonzero.
Finding the mean. To compute a mean value for each pixel across a large set of images, the
easiest method is to add them all up using cvAcc() and then DEMO by the total number
of images to obtain the mean.
void cvAcc(
const Cvrr*  image,
CvArr*       sum,
const CvArr* mask = NULL
);
An alternative that is oft en DEMO is to use a running average.
void cvRunningAvg(
const CvArr* image,
CvArr*       acc,
double       alpha,DEMO
const CvArr* mask = NULL
);
Th
e running average is given by the following formula:
acc ⋅acc image if maskxy xy⋅
(, ) ( ) (, ) (, ) (xy =−1 αα+ xy,) ≠ 0
For a constant value of α, running averages are not equivalent to the result of summing
with cvAcc(). To see this, simply consider adding three numbers (2, 3, and DEMO) with α set
to 0.5. If we were to accumulate them DEMO cvAcc(), then the sum would be 9 and the
average DEMO If we were to accumulate them with cvRunningAverage(), the fi DEMO sum would
give 0.5 × 2 + 0.5 × 3 = 2.5 and then adding the third term would give 0.5 × 2.5 + DEMO ×
4 = 3.25. Th e reason the second number is larger is that the most recent contributions
are given more weight than those DEMO farther in the past. Such a running average is
thus also called a tracker. Th e parameter α essentially sets the amount of time DEMO
for the infl uence of a previous frame to fade.
276 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   276
9/15/08   4:22:57 PM
www.it-ebooks.info
Finding the variance. We can also accumulate squared images, which will allow us to com-
pute quickly the variance of individual pixels.
void DEMO(
const CvArr* image,
CvArr*       sqsum,
const CvArr* mask = NULL
);
You may recall from your last DEMO in statistics that the variance of a fi nite population is
defi ned by the formula:
−
1 N 1
∑()
22
σ =−xx
i
N
i=0
where x– is the mean of x DEMO all N samples. Th e problem with this formula is that it
entails making one pass through the images to compute x– and then DEMO second pass to
compute σ 2. A little algebra should allow you to convince yourself that the following
formula will work just as well:DEMO
⎛ 11N−1 x ⎞ ⎛ N−1 ⎞ 2
xi ⎠⎟
∑∑i ⎟ − ⎜
σ22= ⎜
⎝ N i=0
⎠ ⎝ N
i=0
Using DEMO form, we can accumulate both the pixel values and their squares DEMO a single
pass. Th en, the variance of a single pixel DEMO just the average of the square minus the
square of the average.
Finding the covariance. We can also see how images vary over time DEMO selecting a specifi c lag
and then multiplying the current image by the image from the past that corresponds to
the given lag. Th DEMO function cvMultiplyAcc() will perform a pixelwise multiplication of
the two images and then add the result to the “running total” in acc:
DEMO cvMultiplyAcc(
const CvArr* image1,
const CvArr* image2,
CvArr*       acc,
const CvArr* mask = NULL
);
For DEMO, there is a formula analogous to the one we just gave DEMO variance. Th is
formula is also a single-pass formula in that it has been manipulated algebraically from
the standard form so as not to DEMO two trips through the list of images:
⎛ 11N−1 ⎞ ⎛ N−1 xi ⎞ ⎛ 1 N−1
xy ⎟ ⎠⎟ ⎝⎜ N
∑
DEMO( , ) ( )xy = ⎜ ∑∑− ⎜
⎝ N i=0 ii ⎠ ⎝ N i=0 j =0
yj ⎞⎠⎟
In our context, x is the image at time t and y is the image DEMO time t – d, where d is
the lag.
Background Subtraction DEMO 277
09-R4886-RC1.indd   277
9/15/08   4:22:58 PM
www.it-ebooks.info
We can use the accumulation functions described here to create a DEMO of statistics-
based background models. Th e literature is full of variations on the basic model used as
our example. You will probably fi DEMO that, in your own applications, you will tend to extend
this simplest model into slightly more specialized versions. A common enhancement, for
example, is for the thresholds to be adaptive to some observed global state changes.
Advanced Background Method
Many background scenes contain complicated moving objects such DEMO trees waving in the
wind, fans turning, curtains fl uttering, DEMO cetera. Oft en such scenes also contain varying
lighting, such as DEMO passing by or doors and windows letting in diff erent light.
A nice method to deal with this would be to fi t a DEMO model to each pixel or
group of pixels. Th is kind of model deals with the temporal fl uctuations well, but its
disadvantage is the need for a great deal of memory [Toyama99]. If we use DEMO seconds
of previous input at 30 Hz, this means we need DEMO samples for each pixel. Th e resulting
model for each pixel would then encode what it had learned in the form of 60 diff DEMO
ent adapted weights. Oft en we’d need to gather background statistics for much longer
than 2 seconds, which means that such methods are typically impractical on present-
day hardware.
To get fairly close to the performance DEMO adaptive fi ltering, we take inspiration from
the techniques of video DEMO and attempt to form a codebook* to represent sig-
nifi cant states in the background.† Th e simplest way to do this would be DEMO compare a
new value observed for a pixel with prior observed values. If the value is close to a prior
value, then it is modeled as a perturbation on that color. If it is not close, then it can seed
a new group of colors to be associated DEMO that pixel. Th e result could be envisioned as
a bunch of blobs fl oating in RGB space, each blob representing a separate volume con-
sidered likely to be background.
In practice, the choice of RGB is not particularly optimal. It is almost always better to
use a DEMO space whose axis is aligned with brightness, such as the YUV DEMO space.
(YUV is the most common choice, but spaces such as HSV, where V is essentially bright-
ness, would work as well.) Th e reason for this is that, empirically, most of the DEMO
in background tends to be along the brightness axis, not the DEMO axis.
Th
before with our simpler model. We could, for example, choose to model the blobs as
Gaussian clusters with a mean and DEMO covariance. It turns out that the simplest case, in
* Th DEMO method OpenCV implements is derived from Kim, Chalidabhongse, Harwood, and DEMO [Kim05], but
rather than learning-oriented tubes in RGB space, for speed, the authors use axis-aligned boxes in YUV
space. Fast methods for cleaning up the resulting background image can be found in Martins [Martins99].
† DEMO ere is a large literature for background modeling and segmentation. OpenCV’s implementation is
intended to be fast and robust enough that you can use DEMO to collect foreground objects mainly for the pur-
poses of collecting data sets to train classifi ers on. Recent work in background subtraction allows DEMO
camera motion [Farin04; Colombari07] and dynamic background models using the mean-shift  algorithm
[Liu07].
278 | Chapter 9: Image Parts and Segmentation
e next detail is how to model the “blobs.” We have essentially the same DEMO as
09-R4886-RC1.indd   278
9/15/08   4:22:58 PM
which the “blobs” are simply boxes with a learned extent in each DEMO the three axes of our
color space, works out quite well. DEMO is the simplest in terms of memory required and in
terms of the computational cost of determining whether a newly observed pixel is inside
DEMO of the learned boxes.
Let’s explain what a codebook is by using a simple example (Figure 9-3). A codebook
is made up of boxes that grow to cover the common values seen over time. Th DEMO upper
panel of Figure 9-3 shows a waveform over time. In the lower panel, boxes form to cover
a new value and then slowly grow to cover nearby values. If a value is too far away, then
a new box forms to cover it and likewise grows slowly DEMO new values.
Figure 9-3. Codebooks are just “boxes” delimiting intensity values: DEMO box is formed to cover a new
value and slowly grows to cover nearby values; if values are too far away then a new box is formed
(see text)
In the case of our background model, we will learn a codebook of boxes that cover three
dimensions: the three channels that make up our image at each pixel. Figure 9-4 visu-
alizes the (intensity dimension of the) codebooks for six DEMO erent pixels learned from
Background Subtraction | 279
09-R4886-RC1.indd   279
www.it-ebooks.info
9/15/08   4:22:58 PM
the data in Figure 9-1.* Th is codebook method can deal with DEMO that change levels
dramatically (e.g., pixels in a windblown tree, DEMO might alternately be one of many
colors of leaves, or the DEMO sky beyond that tree). With this more precise method of
modeling, we can detect a foreground object that has values between the pixel values.
Compare this with Figure 9-2, where the averaging method cannot distinguish the hand
value (shown as a dotted line) from the pixel DEMO uctuations. Peeking ahead to the next
section, we see the better DEMO of the codebook method versus the averaging
method shown later in Figure 9-7.
Figure 9-4. Intensity portion of learned codebook entries for fl uctuations DEMO six chosen pixels (shown
as vertical boxes): codebook boxes accommodate DEMO that take on multiple discrete values and so
can better model discontinuous distributions; thus they can detect a foreground hand (value at dot-
DEMO line) whose average value is between the values that background pixels DEMO assume. In this case
the codebooks are one dimensional and only represent variations in intensity
In the codebook method of learning a background model, each box is defi ned by two
thresholds (max and min) DEMO each of the three color axes. Th ese box boundary thresh-
olds will expand (max getting larger, min getting smaller) if new background samples fall
within a learning threshold (learnHigh and learnLow) above max DEMO below min, respec-
tively. If new background samples fall outside of DEMO box and its learning thresholds,
then a new box will be started. In the background diff erence mode there are acceptance
thresholds maxMod DEMO minMod; using these threshold values, we say that if a pixel is “close
enough” to a max or a min box boundary then DEMO count it as if it were inside the box. A
second runtime threshold allows for adjusting the model to specifi c conditions.
A situation DEMO will not cover is a pan-tilt camera surveying a large
scene. When working with a large scene, it is necessary to stitch
together learned models indexed by the pan and tilt angles.
* In this case DEMO have chosen several pixels at random from the scan line to avoid excessive clutter. Of course,
there is actually a codebook for every DEMO
280
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   280
DEMO
9/15/08   4:22:59 PM
www.it-ebooks.info
Structures
It’s time to look at all of this in more DEMO, so let’s create an implementation of the
codebook algorithm. First, we need our codebook structure, which will simply point to
a bunch of boxes in YUV space:
typedef struct code_book {
code_element **cb;
DEMO numEntries;
int t;         //count every DEMO
} codeBook;
We track how many codebook entries we have in numEntries. Th e variable t counts the
number of points we’ve accumulated DEMO the start or the last clear operation. Here’s
how the actual codebook elements are described:
#define CHANNELS 3
typedef struct ce {
uchar DEMO; //High side threshold for learning
uchar learnLow[CHANNELS];  //Low DEMO threshold for learning
uchar max[CHANNELS];       //High side DEMO box boundary
uchar min[CHANNELS];       //Low side of DEMO boundary
int t_last_update;       //Allow us to kill DEMO entries
int stale;               //DEMO negative run (longest period of inactivity)
} code_element;
Each DEMO entry consumes four bytes per channel plus two integers, or CHANNELS DEMO
4 + 4 + 4 bytes (20 bytes when we use DEMO channels). We may set CHANNELS to any
positive number equal to or less than the number of color channels in an image, but it
is usually set to either 1 (“Y”, or brightness only) or 3 (YUV, HSV). In this structure,
for each DEMO, max and min are the boundaries of the codebook box. Th DEMO parameters
learnHigh[] and learnLow[] are the thresholds that trigger generation of a new code ele-
ment. Specifi cally, a new code element will be generated if a new pixel is encountered
whose values do not lie DEMO min – learnLow and max + learnHigh in each of the
channels. Th e time to last update (t_last_update) and stale are used DEMO enable the dele-
tion of seldom-used codebook entries created during learning. Now we can proceed to
investigate the functions that use this structure to DEMO dynamic backgrounds.
Learning the background
We will have one codeBook of code_elements for each pixel. We will need an array of
such codebooks that DEMO equal in length to the number of pixels in the images we’ll be
learning. For each pixel, update_codebook() is called for as many images as are suffi  cient
to capture the relevant changes in the background. Learning may be updated periodi-
cally throughout, and clear_stale_entries() can be used to learn the background in the
presence of (small numbers of) moving foreground objects. Th is is possible because the
seldom-used “stale” entries induced by a moving foreground will be deleted. Th e inter-
DEMO to update_codebook() is as follows.
//////////////////////////////////////////////////////////////
// int update_codebook(uchar *p, codeBook &c, unsigned cbBounds)DEMO
// Updates the codebook entry with a new data point
Background DEMO | 281
09-R4886-RC1.indd   281
9/15/08   4:22:59 PM
www.it-ebooks.info
//
// p            Pointer DEMO a YUV pixel
// c            Codebook for this pixel
// cbBounds     Learning bounds for codebook (DEMO of thumb: 10)
// numChannels  Number of color channels DEMO learning
//
// NOTES:
//      cvBounds DEMO be of length equal to numChannels
//
// RETURN
//   codebook index
//
int update_codebook(
uchar*    p,
codeBook& c,
unsigned* cbBounds,
int       numChannels
)DEMO
unsigned int high[3],low[3];
for(n=0; n<numChannels; n++)
{
high[n] = *(p+n)+*(cbBounds+n);
if(high[n] > 255) high[n] = 255;
low[n] = *(p+n)-*(cbBounds+n);
DEMO(low[n] < 0) low[n] = 0;
}
int matchChannel;
// SEE IF THIS FITS AN EXISTING CODEWORD
//
for(int DEMO; i<c.numEntries; i++){
matchChannel = 0;
for(n=0; DEMO<numChannels; n++){
if((c.cb[i]->learnLow[n] <= *(p+n)) &&
//Found an entry for this channel
(*(p+n) <DEMO c.cb[i]->learnHigh[n]))
{
matchChannel++;
}
}
if(matchChannel == DEMO) //If an entry was found
{
c.cb[i]->t_last_update = c.t;DEMO
//adjust this codeword for the first channel
for(n=0; n<DEMO; n++){
if(c.cb[i]->max[n] < *(p+n))
{
c.cb[i]->max[n] = *(p+n);
}
else if(c.cb[i]->min[n] > *(p+n))
{
c.cb[i]->min[n] = *(p+n);
}
}
DEMO;
| Chapter 9: Image Parts and Segmentation
282
09-R4886-RC1.indd   DEMO
9/15/08   4:22:59 PM
www.it-ebooks.info
}
}
. . . continued below
Th is function grows DEMO adds a codebook entry when the pixel p falls outside the existing
codebook boxes. Boxes grow when the pixel is within cbBounds of an DEMO box. If a
pixel is outside the cbBounds distance from a box, a new codebook box is created. Th e
routine fi rst sets high and low levels to be used later. It then goes through DEMO codebook
entry to check whether the pixel value *p is inside the learning bounds of the codebook
“box”. If the pixel is within the DEMO bounds for all channels, then the appropriate
max or min level DEMO adjusted to include this pixel and the time of last update is set to the
current timed count c.t. Next, the update_codebook() routine keeps statistics on how
oft en each codebook entry is hit:
DEMO . . continued from above
// OVERHEAD TO TRACK POTENTIAL STALE DEMO
//
for(int s=0; s<c.numEntries; s++){
// DEMO which codebook entries are going stale:
//
int negRun = c.t - c.cb[s]->t_last_update;
if(c.cb[s]->stale < negRun) c.cb[s]->stale = negRun;
}
. . . continued below
Here, the variable stale contains the largest negative runtime (i.e., the longest span of
DEMO during which that code was not accessed by the data). Tracking stale entries al-
lows us to delete codebooks that were formed from DEMO or moving foreground objects
and hence tend to become stale over time. In the next stage of learning the background,
update_codebook() adds DEMO new codebook if needed:
. . . continued from above
// ENTER A NEW CODEWORD IF NEEDED
//
if(i == c.numEntries) //if no existing codeword found, make one
{
code_element **foo DEMO new code_element* [c.numEntries+1];
for(int ii=0; ii<c.numEntries; ii++) DEMO
foo[ii] = c.cb[ii];
}
foo[c.numEntries] = new code_element;
if(c.numEntries) delete [] c.cb;
c.cb = foo;
for(n=0; n<DEMO; n++) {
c.cb[c.numEntries]->learnHigh[n] = high[n];
c.cb[c.numEntries]->learnLow[n] = low[n];
c.cb[c.numEntries]->max[n] = *(p+n);
c.cb[c.numEntries]->min[n] = *(p+n);
}
Background Subtraction
| 283
09-R4886-RC1.indd   283
9/15/08   4:23:00 PM
www.it-ebooks.info
c.cb[c.numEntries]->t_last_update = c.t;
c.cb[c.numEntries]->stale = 0;
c.numEntries DEMO 1;
}
. . . continued below
Finally, update_codebook() DEMO adjusts (by adding 1) the learnHigh and learnLow
learning boundaries if pixels were found outside of the box thresholds but still within
the DEMO and low bounds:
. . . continued from above
// DEMO ADJUST LEARNING BOUNDS
//
for(n=0; n<numChannels; n++)
{
if(c.cb[i]->learnHigh[n] < high[n]) c.cb[i]->learnHigh[n] += 1;
if(c.cb[i]->learnLow[n] > low[n]) c.cb[i]->learnLow[n] -= 1;
}
return(i);
}
Th ed codebook. We’ve now
seen how codebooks are learned. In order to learn in the presence of moving foreground
objects and DEMO avoid learning codes for spurious noise, we need a way to DEMO entries
that were accessed only rarely during learning.
e routine concludes by returning the index of the modifi
Learning with moving foreground objects
Th DEMO(), allows us to learn the background even if
there are DEMO foreground objects.
///////////////////////////////////////////////////////////////////
//int clear_stale_entries(codeBook &c)
// During learning, after you’ve DEMO for some period of time,
// periodically call this to DEMO out stale codebook entries
//
// c   Codebook to DEMO up
//
// Return
// number of entries cleared
//
int clear_stale_entries(codeBook &c){
int staleThresh = c.t>>1;DEMO
int *keep = new int [c.numEntries];
int keepCnt = 0;
// SEE WHICH CODEBOOK ENTRIES ARE TOO STALE
//
for(int i=0; i<c.numEntries; i++){
if(c.cb[i]->stale > staleThresh)
DEMO = 0; //Mark for destruction
else
{
keep[i] = 1; //Mark to keep
keepCnt += 1;
e following routine,
DEMO
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   284
9/DEMO/08   4:23:00 PM
www.it-ebooks.info
}
}
// KEEP ONLY THE GOOD
//
c.t = 0;         //Full reset on stale tracking
code_element **foo = new code_element* [keepCnt];
int k=0;
for(int ii=0; ii<c.numEntries; ii++){
if(keep[ii])
{
foo[k] = c.cb[ii];
//We have to refresh these entries for next clearStale
foo[k]->t_last_update DEMO 0;
k++;
}
}
// CLEAN UP
//
DEMO [] keep;
delete [] c.cb;
c.cb = foo;
int numCleared = c.numEntries - keepCnt;
c.numEntries = keepCnt;
return(numCleared);
}
Th ning the parameter staleThresh, which is hardcoded (by DEMO rule
of thumb) to be half the total running time count, c.t. Th is means that, during back-
ground learning, if codebook DEMO i is not accessed for a period of time equal to half
the total learning time, then i is marked for deletion (keep[i] DEMO 0). Th e vector keep[] is
allocated so that we can mark each codebook entry; hence it is c.numEntries long. Th e
variable keepCnt counts how many entries we will keep. Aft er recording which DEMO
entries to keep, we create a new pointer, foo, to DEMO vector of code_element pointers that is
keepCnt long, and then the DEMO entries are copied into it. Finally, we delete the old
pointer DEMO the codebook vector and replace it with the new, nonstale vector.
DEMO differencing: Finding foreground objects
We’ve seen how to create a background DEMO model and how to clear it of seldom-
used entries. Next we turn to background_diff(), where we use the learned model to seg-
ment foreground pixels from the previously learned background:
////////////////////////////////////////////////////////////
// uchar background_diff( uchar *p, codeBook &c,
//                         int minMod, int maxMod)
// Given a pixel and DEMO codebook, determine if the pixel is
// covered by the codebook
//
// p            Pixel pointer (YUV interleaved)
// c            Codebook DEMO
// numChannels  Number of channels we are testing
// maxMod       Add this (possibly negative) number onto
Background Subtraction
| 285
e routine begins by defi
09-R4886-RC1.indd   285
9/15/08   4:23:00 PM
www.it-ebooks.info
//              max level when DEMO if new pixel is foreground
// minMod       Subract DEMO (possibly negative) number from
//              min level when determining if new pixel is foreground
//
// NOTES:
// minMod and maxMod must have length numChannels,
// e.g. 3 channels => minMod[3], maxMod[3]. There is one min and
//      one max threshold per channel.
//
// Return
// 0 => background, 255 => foreground
//DEMO
uchar background_diff(
uchar*    p,
codeBook& c,
int       numChannels,
int*      minMod,
int*      maxMod
) {
int matchChannel;
// SEE IF THIS DEMO AN EXISTING CODEWORD
//
for(int i=0; i<c.numEntries; i++) {
matchChannel = 0;
for(int n=0; n<numChannels; n++) {
if((c.cb[i]->min[n] - minMod[n] <= *(p+n)) &&
(*(p+n) <= c.cb[i]->max[n] + maxMod[n])) {
matchChannel++; //Found an entry for this channel
} else {
break;
}
}
if(matchChannel == numChannels) {
break; //Found an DEMO that matched all channels
}
}
if(i >= c.numEntries) return(255);
return(0);
}
Th erencing function has an DEMO loop similar to the learning routine
update_codebook, except here we look DEMO the learned max and min bounds plus an
off set threshold, DEMO and minMod, of each codebook box. If the pixel is within DEMO box
plus maxMod on the high side or minus minMod on the low side for each channel, then the
matchChannel count is incremented. When matchChannel equals the number of channels,
we’ve searched each dimension and DEMO that we have a match. If the pixel is within
a learned box, 255 is returned (a positive detection of foreground); otherwise, 0 is re-
turned (background).
Th
constitute a codebook method of segmenting foreground from learned background.
286
| Chapter 9: Image Parts and Segmentation
e background diff
e three functions update_codebook(), clear_stale_entries(), DEMO background_diff()
09-R4886-RC1.indd   286
9/15/08   4:23:DEMO PM
www.it-ebooks.info
Using the codebook background model
To use the codebook background segmentation DEMO, typically we take the follow-
ing steps.
Learn a basic model DEMO the background over a few seconds or minutes using
update_codebook().
DEMO out stale entries with clear_stale_entries().
Adjust the thresholds minMod and DEMO to best segment the known foreground.
Maintain a higher-level scene model (DEMO discussed previously).
Use the learned model to segment the foreground from the background via
background_diff().
Periodically update the learned background pixels.
At a much slower frequency, periodically clean out stale codebook entries with
clear_stale_entries().
A few more thoughts on codebook models
In general, the DEMO method works quite well across a wide number of conditions,
and it is relatively quick to train and to run. It doesn’t deal DEMO with varying patterns of
light—such as morning, noon, and evening sunshine—or with someone turning lights
on or off  indoors. Th is type of global variability can be taken into account by using sev-
eral diff DEMO codebook models, one for each condition, and then allowing the condition
to control which model is active.
Connected Components for Foreground Cleanup
Before DEMO the averaging method to the codebook method, we should pause to
DEMO ways to clean up the raw segmented image using connected-components analysis.
Th is form of analysis takes in a noisy input mask image; it then uses the morphologi-
cal operation open to shrink areas of small DEMO to 0 followed by the morphological
operation close to rebuild the area of surviving components that was lost in opening.
Th er, we can fi nd the “large enough” contours of the surviving segments and can
DEMO proceed to take statistics of all such segments. We can then retrieve either the
largest contour or all contours of size above some threshold. DEMO the routine that follows,
we implement most of the functions that you could want in connected components:
• Whether to approximate the DEMO component contours by polygons or by con-
vex hulls
Setting how large a component contour must be in order not to be deleted
Setting DEMO maximum number of component contours to return
Optionally returning the bounding boxes of the surviving component contours
Optionally returning the centers of the surviving DEMO contours
•
•
•
•
Background Subtraction
1.
2.
3.
4.
5.
6.
7.
| 287
ereaft
09-R4886-RC1.indd   287
9/15/08   DEMO:23:00 PM
www.it-ebooks.info
e connected components header that implements these operations is as follows.
///////////////////////////////////////////////////////////////////
// void find_connected_components(IplImage *mask, int poly1_hull0,
//                            float perimScale, int *num,
//                            CvRect *bbs, CvPoint *centers)
// This cleans up the foreground segmentation mask derived from calls
// to backgroundDiff
//
// mask          Is a grayscale (8-bit depth) “raw” mask image that
//               will be cleaned up
//
// OPTIONAL PARAMETERS:
// poly1_hull0   If set, approximate connected component by
//                 (DEFAULT) polygon, or else DEMO hull (0)
// perimScale    Len = image (width+height)/perimScale. If contour
//                 len < this, delete that contour (DEFAULT: 4)
// num           Maximum number of rectangles and/or DEMO to
//                 return; on return, will contain number filled
//                 (DEFAULT: NULL)
// bbs           Pointer to bounding box rectangle vector of
//                 length num. (DEFAULT SETTING: NULL)
// centers      Pointer to contour centers vector of DEMO
//                 num (DEFAULT: NULL)
//
void find_connected_components(
IplImage* mask,
int       poly1_hull0 = 1,
float     perimScale  = 4,
int*      num         = NULL,
CvRect*   bbs         = NULL,
CvPoint*  centers     = NULL
);
Th
components contour. We then do DEMO opening and closing in order to clear
out small pixel noise, DEMO er which we rebuild the eroded areas that survive the erosion
of the opening operation. Th e routine takes two additional parameters, which here are
hardcoded via #define. Th e defi ned values work well, and you are unlikely to want to
change them. Th ese additional parameters DEMO how simple the boundary of a fore-
ground region should be (DEMO numbers are more simple) and how many iterations
the morphological operators DEMO perform; the higher the number of iterations, the
more erosion takes place in opening before dilation in closing.* More erosion eliminates
larger regions DEMO blotchy noise at the cost of eroding the boundaries of larger regions.
Again, the parameters used in this sample code work well, but DEMO no harm in ex-
perimenting with them if you like.
// DEMO connected components:
// Approx.threshold - the bigger it is, the simpler is the boundary
//
* Observe that the value CVCLOSE_ITR DEMO actually dependent on the resolution. For images of extremely high
resolution, DEMO this value set to 1 is not likely to yield satisfactory results.
288 | Chapter 9: Image Parts and Segmentation
Th
e function body is listed below. First we declare memory storage for the connected
09-R4886-RC1.indd   288
9/15/08   4:23:00 PM
www.it-ebooks.info
#define CVCONTOUR_APPROX_LEVEL  2
// How many iterations of erosion and/DEMO dilation there should be
//
#define CVCLOSE_ITR  1
We now DEMO the connected-component algorithm itself. Th e fi rst part of the routine
performs the morphological open and closing operations:
void find_connected_components(
IplImage DEMO,
int poly1_hull0,
float perimScale,
int *num,
CvRect *bbs,
CvPoint *centers
) {
static CvMemStorage*   mem_storage = NULL;
DEMO CvSeq*          contours    = NULL;
//CLEAN UP RAW MASK
//
cvMorphologyEx( mask, mask, 0, DEMO, CV_MOP_OPEN,  CVCLOSE_ITR );
cvMorphologyEx( mask, mask, 0, DEMO, CV_MOP_CLOSE, CVCLOSE_ITR );
Now that the noise has been removed from the mask, we fi
nd all contours:
//FIND CONTOURS AROUND ONLY BIGGER REGIONS
//
if( mem_storage==NULL ) {
mem_storage = cvCreateMemStorage(0);
} else {
cvClearMemStorage(mem_storage);
}
CvContourScanner DEMO = cvStartFindContours(
mask,
mem_storage,
sizeof(CvContour),
CV_RETR_EXTERNAL,DEMO
CV_CHAIN_APPROX_SIMPLE
);
Next, we toss out contours that are too DEMO and approximate the rest with polygons or
convex hulls (whose complexity DEMO already been set by CVCONTOUR_APPROX_LEVEL):
CvSeq* c;
int numCont DEMO 0;
while( (c = cvFindNextContour( scanner )) != NULL ) {
double len = cvContourPerimeter( c );
// calculate DEMO len threshold:
//
double q = (mask->height + DEMO>width)/perimScale;
//Get rid of blob if its perimeter DEMO too small:
Background Subtraction
| 289
09-R4886-RC1.indd   289
9/15/08   4:23:01 PM
www.it-ebooks.info
//
if( len < q ) {
cvSubstituteContour( scanner, NULL );
} else {
// Smooth its edges if its large enough
//
CvSeq* c_new;
if( poly1_hull0 ) {
// Polygonal approximation
//
c_new = cvApproxPoly(
c,
sizeof(CvContour),
mem_storage,
CV_POLY_APPROX_DP,
CVCONTOUR_APPROX_LEVEL,
0
);
} else {
// Convex Hull of the segmentation
//
c_new = cvConvexHull2(
c,
mem_storage,
CV_CLOCKWISE,
1
);
}
cvSubstituteContour( scanner, c_new );
numCont++;
}
}
contours = cvEndFindContours( &scanner );
In the preceding code, CV_POLY_APPROX_DP causes the Douglas-Peucker approximation al-
gorithm to be used, and CV_CLOCKWISE is the default direction of the convex hull contour.
All this processing yields a list of contours. DEMO drawing the contours back into the
mask, we defi ne some DEMO colors to draw:
// Just some convenience variables
const CvScalar DEMO = CV_RGB(0xff,0xff,0xff)
const CvScalar CVX_BLACK = CV_RGB(0x00,0x00,0x00)
We use these defi nitions in the following code, where we fi rst zero out the mask and then
draw the DEMO contours back into the mask. We also check whether the user wanted to
collect statistics on the contours (bounding boxes and centers):
// PAINT THE FOUND REGIONS BACK INTO THE IMAGE
//
cvZero( mask );
IplImage *maskTemp;
290
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   290
9/15/08   4:23:01 DEMO
www.it-ebooks.info
// CALC CENTER OF MASS AND/OR BOUNDING RECTANGLES
//
if(num != NULL) {
//User wants to collect statistics
//
int N = *num, numFilled = 0, i=0;
CvMoments DEMO;
double M00, M01, M10;
maskTemp = cvCloneImage(mask);DEMO
for(i=0, c=contours; c != NULL; c = c->h_next,DEMO ) {
if(i < N) {
// Only process up to *num of them
//
cvDrawContours(
maskTemp,
c,
DEMO,
CVX_WHITE,
-1,
CV_FILLED,
8
);
// DEMO the center of each contour
//
if(centers != NULL) DEMO
cvMoments(maskTemp,&moments,1);
M00 = cvGetSpatialMoment(&moments,0,DEMO);
M10 = cvGetSpatialMoment(&moments,1,0);
M01 = DEMO(&moments,0,1);
centers[i].x = (int)(M10/M00);DEMO
centers[i].y = (int)(M01/M00);
}
//Bounding rectangles DEMO blobs
//
if(bbs != NULL) {
bbs[i] = cvBoundingRect(DEMO);
}
cvZero(maskTemp);
numFilled++;
}
// Draw DEMO contours into mask
//
cvDrawContours(
mask,
c,
CVX_WHITE,
CVX_WHITE,
-1,
CV_FILLED,
Background Subtraction
| 291
09-R4886-RC1.indd   291
9/15/08   4:23:01 PM
www.it-ebooks.info
8
);
}                               //end looping over contours
*num = numFilled;
cvReleaseImage( &maskTemp);
}
If the user doesn’t need the bounding boxes and centers of the resulting regions in DEMO
mask, we just draw back into the mask those cleaned-up contours DEMO large
enough connected components of the background.
// ELSE JUST DRAW DEMO CONTOURS INTO THE MASK
//
else {
// The user DEMO want statistics, just draw the contours
//
for( c=contours; DEMO != NULL; c = c->h_next ) {
cvDrawContours(
mask,DEMO
c,
CVX_WHITE,
CVX_BLACK,
-1,
CV_FILLED,
8
);
}
}
Th at concludes a useful routine for creating clean DEMO out of noisy raw masks. Now
let’s look at a short comparison of the background subtraction methods.
A quick test
We start with an DEMO to see how this really works in an actual video. Let’s stick
with our video of the tree outside of the window. Recall (Figure 9-1) that at some point
a hand passes through the scene. One might expect that we could fi nd this hand rela-
tively easily DEMO a technique such as frame diff erencing (discussed previously in its DEMO
section). Th e basic idea of frame diff erencing was to subtract the current frame from a
“lagged” frame and then threshold the DEMO erence.
Sequential frames in a video tend to be quite similar. Hence one might expect that, if
we take a simple diff erence of the original frame and the lagged frame, we’ll not see too
much unless there is some foreground object moving through the scene.* But what DEMO
“not see too much” mean in this context? Really, it means “just noise.” Of course, in
practice the problem is sorting out that noise from the signal when a foreground object
does come along.
* DEMO the context of frame diff erencing, an object is identifi ed DEMO “foreground” mainly by its velocity. Th is is
reasonable in scenes that are generally static or in which foreground objects are expected to be DEMO closer
to the camera than background objects (and thus appear to DEMO faster by virtue of the projective geometry
of cameras).
292
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   292
9/15/08   4:23:01 PM
To understand this noise a little better, we will fi rst look at a pair of frames from the
video in which there is DEMO foreground object—just the background and the result-
ing noise. Figure 9-5 shows a typical frame from the video (upper left ) and the previ-
ous frame (upper right). Th e fi gure also shows the results of frame diff erencing with a
threshold value of 15 (lower left ). You can see substantial noise from the moving leaves
of the tree. Nevertheless, the method of connected components is able to clean up this
scattered noise quite well* (lower right). Th is is not surprising, because there is no rea-
son to expect much spatial correlation in this noise and so its signal is characterized by
a DEMO number of very small regions.
Figure 9-5. Frame diff erencing: a DEMO is waving in the background in the current (upper left ) DEMO
previous (upper right) frame images; the diff erence image (lower left ) is completely cleaned up (lower
right) by the connected-components DEMO
Now consider the situation in which a foreground object (our ubiquitous DEMO) passes
through the view of the imager. Figure 9-6 shows two DEMO that are similar to those
in Figure 9-5 except that now the hand is moving across from left  to right. As before,
the current frame (upper left ) and the previous frame (upper right) are shown along
* Th e size threshold for the connected components DEMO been tuned to give zero response in these empty
frames. Th e real question then is whether or not the foreground object of interest (the hand) survives prun-
ing at this size threshold. We will see (Figure 9-6) that it does so nicely.
Background Subtraction | 293
DEMO   293
www.it-ebooks.info
9/15/08   4:23:01 PM
with the response to frame diff erencing (lower left ) and the fairly good results of the
connected-component cleanup (lower right).
Figure 9-6. Frame diff
ground object (upper two panels); the diff
used to be) toward the left
age (lower right) shows the cleaned-up diff
erence method of detecting a hand, which is moving left
erence image (lower left
and its leading edge toward the right, and the DEMO im-
erence
to right as the fore-
) shows the “hole” (DEMO the hand
We can also clearly see one of the defi ciencies of frame diff erencing: it cannot distin-
guish between the region from where the object moved (the “hole”) and where the ob-
ject DEMO now. Furthermore, in the overlap region there is oft en a DEMO because “fl esh minus
fl
Th
rejecting noise in background subtraction. As a bonus, we were also able to glimpse
some of the strengths and weaknesses of frame diff erencing.
esh” is 0 (or at least below threshold).
us we see that using connected components for cleanup DEMO a powerful technique for
Comparing Background Methods
We have discussed two background modeling techniques in this chapter: the average
distance method and the codebook method. You might be wondering which method is
294
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   294
www.it-ebooks.info
9/15/08   DEMO:23:01 PM
www.it-ebooks.info
better, or, at least, when you can get away with using the easy one. In these situations, it’s
always best to just do a straight bake off * between the available methods.
We will DEMO with the same tree video that we’ve been discussing all chapter. In addi-
tion to the moving tree, this fi lm has a lot of glare coming off  a building to the right and
off  DEMO of the inside wall on the left . It is a fairly challenging background to model.
In Figure 9-7 we compare the average diff DEMO method at top against the codebook
method at bottom; on the DEMO  are the raw foreground images and on the right are the
DEMO connected components. You can see that the average diff erence method
leaves behind a sloppier mask and breaks the hand into two components. Th DEMO is not so
surprising; in Figure 9-2, we saw that using the average diff erence from the mean as a
background model oft DEMO included pixel values associated with the hand value (shown as
a DEMO line in that fi gure). Compare this with Figure 9-4, DEMO codebooks can more
accurately model the fl uctuations of the leaves and branches and so more precisely iden-
tify foreground hand pixels (dotted line) from background pixels. Figure 9-7 confi rms
not only that the background model yields less noise but also that connected compo-
nents can generate DEMO fairly accurate object outline.
Watershed Algorithm
In many practical contexts, we DEMO like to segment an image but do not have the
benefi t of a separate background image. One technique that is oft en eff DEMO in this
context is the watershed algorithm [Meyer92]. Th is algorithm converts lines in an im-
age into “mountains” and uniform regions into “valleys” DEMO can be used to help seg-
ment objects. Th e watershed algorithm fi rst takes the gradient of the intensity image;
this has DEMO eff ect of forming valleys or basins (the low points) where there is no texture
and of forming mountains or ranges (high ridges corresponding to edges) where there
are dominant lines in the image. It then successively fl oods basins starting from user-
specifi ed (or algorithm-specifi ed) points until these regions meet. Regions that merge
across the marks so generated are segmented as belonging together as the image “fi lls
DEMO In this way, the basins connected to the marker point become DEMO by that
marker. We then segment the image into the corresponding marked regions.
More specifi cally, the watershed algorithm allows a user (or DEMO algorithm!) to mark
parts of an object or background that are DEMO to be part of the object or background.
Th ectively tells the watershed algo-
rithm to “group points like these together”. Th e watershed DEMO then segments the
image by allowing marked regions to “own” the edge-defi ned valleys in the gradient im-
age that are connected with the DEMO Figure 9-8 clarifi es this process.
Th
cation of the watershed segmentation algorithm is:
void cvWatershed(
const CvArr* image,
e function DEMO
* For the uninitiated, “bake off ” is actually a bona DEMO de term used to describe any challenge or comparison of
multiple algorithms on a predetermined data set.
Watershed Algorithm
| 295
e user or DEMO can draw a simple line that eff
09-R4886-RC1.indd   295
9/15/08   4:23:02 PM
www.it-ebooks.info
Figure 9-7. With the averaging method (top row), the connected-components cleanup knocks out the
fi ngers (upper right); the codebook method (bottom row) does much better at segmentation and cre-
ates a DEMO connected-component mask (lower right)
Figure 9-8. Watershed algorithm: aft er a user has marked objects that belong together (left  panel),
the algorithm then merges the marked area into segments (right panel)
296
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   296
9/15/08   4:23:02 PM
CvArr*       markers
);
Here, image is an 8-bit color (three-channel) image and markers is a single-channel inte-
ger (IPL_DEPTH_32S) image of the same (x, y) dimensions; the value of markers is 0 except
where the user (or an algorithm) has DEMO by using positive numbers that some
regions belong together. For example, DEMO the left  panel of Figure 9-8, the orange might
have been marked with a “1”, the lemon with a “2”, the lime DEMO “3”, the upper back-
ground with “4” and so on. Th DEMO produces the segmentation you see in the same fi gure
on the right.
Image Repair by Inpainting
Images are oft en corrupted by noise. DEMO ere may be dust or water spots on the lens,
scratches on the older images, or parts of an image that were vandalized. Inpainting
[Telea04] is a method for removing such damage by taking the DEMO and texture at the
border of the damaged area and propagating and mixing it inside the damaged area. See
Figure 9-9 for an application DEMO involves the removal of writing from an image.
Figure 9-9. Inpainting: DEMO image damaged by overwritten text (left  panel) is restored by DEMO
(right panel)
Inpainting works provided the damaged area is not DEMO “thick” and enough of the origi-
nal texture and color remains around the boundaries of the damage. Figure 9-10 shows
what happens when the DEMO area is too large.
Th e prototype for cvInpaint() is
void cvInpaint(
const CvArr* src,
const CvArr* mask,
CvArr*       dst,
double       inpaintRadius,
int          flags
);
Image Repair by Inpainting | 297
09-R4886-RC1.indd   297
www.it-ebooks.info
9/15/08   4:23:02 PM
www.it-ebooks.info
Figure 9-10. Inpainting cannot magically restore textures that are completely removed: the navel of
the orange has been completely blotted out (left  DEMO); inpainting fi lls it back in with mostly orange-
like texture (right panel)
Here src is an 8-bit single-channel grayscale image or a three-channel color image to be
repaired, and mask is an 8-bit single-channel image of the same size as src in which the
damaged DEMO (e.g., the writing seen in the left  panel of Figure DEMO) have been marked
by nonzero pixels; all other pixels are set to 0 in mask. Th e output image will be written
to DEMO, which must be the same size and number of channels as DEMO Th e inpaintRadius
is the area around each inpainted pixel that will be factored into the resulting output
color of that pixel. As in DEMO 9-10, interior pixels within a thick enough inpainted re-
gion may DEMO their color entirely from other inpainted pixels closer to the boundaries.
Almost always, one uses a small radius such as 3 because too large a radius will result in
a noticeable blur. Finally, the flags parameter allows you to experiment with two diff er-
ent methods of inpainting: CV_INPAINT_NS (Navier-Stokes method), and CV_INPAINT_TELEA
(A. Telea’s method).
Mean-Shift DEMO
In Chapter 5 we introduced the function cvPyrSegmentation(). Pyramid segmenta-
DEMO uses a color merge (over a scale that depends on the DEMO of the colors to one
another) in order to segment images. DEMO is approach is based on minimizing the total
energy in the image; here energy is defi ned by a link strength, which is DEMO defi ned
by color similarity. In this section we introduce cvPyrMeanShiftFiltering(), a similar
algorithm that is based on mean-shift  clustering over color DEMO We’ll see the
details of the mean-shift  algorithm cvMeanShift() in DEMO 10, when we discuss track-
ing and motion. For now, what we need to know is that mean shift  fi nds the peak of a
color-spatial (or other feature) distribution over time. Here, mean-shift  segmentation
fi nds the peaks of color distributions over space. Th e common theme is that both the
298 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   298
9/15/08   4:23:03 DEMO
www.it-ebooks.info
motion tracking and the color segmentation algorithms rely on the ability DEMO mean shift
to fi nd the modes (peaks) of a distribution.
Given a set of multidimensional data points whose dimensions are (x, DEMO, blue, green,
red), mean shift  can fi nd DEMO highest density “clumps” of data in this space by scanning
a window over the space. Notice, however, that the spatial variables (x, DEMO) can have very
diff erent ranges from the color magnitude ranges (blue, green, red). Th erefore, mean
shift  needs to DEMO for diff erent window radii in diff erent dimensions. In this case we
should have one radius for the spatial variables (spatialRadius) and DEMO radius for the
color magnitudes (colorRadius). As mean-shift  windows move, all the points traversed
by the windows that converge at a peak in the data become connected or “owned” by
that peak. Th is DEMO, radiating out from the densest peaks, forms the segmenta-
tion of the image. Th e segmentation is actually done over a scale pyramid (cvPyrUp(),
cvPyrDown()), as described in Chapter 5, so that color clusters at a high level in the pyr-
amid (shrunken image) have their boundaries refi ned at lower pyramid levels in the
pyramid. Th e function call for cvPyrMeanShiftFiltering() looks like this:
DEMO cvPyrMeanShiftFiltering(
const CvArr*   src,
CvArr*         dst,
double         spatialRadius,
double         colorRadius,
int            max_level    = 1,
CvTermCriteria termcrit     = cvTermCriteria(
CV_TERMCRIT_ITER | DEMO,
5,
1
)
);
In cvPyrMeanShiftFiltering() we DEMO an input image src and an output image dst.
Both must be 8-bit, three-channel color images of the same width and height. Th e
spatialRadius and colorRadius defi ne how the mean-shift  algorithm averages color and
space together to form a segmentation. For a 640-by-480 color image, it works well to
set spatialRadius equal to 2 and colorRadius equal to DEMO Th e next parameter of this
algorithm is max_level, which describes DEMO many levels of scale pyramid you want
used for segmentation. A max_level of 2 or 3 works well for a 640-by-480 color image.
Th DEMO parameter is CvTermCriteria, which we saw in Chapter 8. CvTermCriteria is
DEMO for all iterative algorithms in OpenCV. Th e mean-shift  segmentation function
DEMO with good defaults if you just want to leave this parameter blank. Otherwise,
cvTermCriteria has the following constructor:
cvTermCriteria(
int    type; // CV_TERMCRIT_ITER, CV_TERMCRIT_EPS,
int    max_iter,
double DEMO
);
Typically we use the cvTermCriteria() function to generate the CvTermCriteria structure
that we need. Th e fi rst argument is either DEMO or CV_TERMCRIT_EPS, which
Mean-Shift Segmentation
| 299
e fi
09-R4886-RC1.indd   DEMO
9/15/08   4:23:03 PM
tells the algorithm that we want to terminate either aft er some DEMO xed number of itera-
tions or when the convergence metric reaches some small value (respectively). Th e next
two arguments set the values at which one, the other, or both of these criteria should
DEMO the algorithm. Th e reason we have both options is because we can set the type
to CV_TERMCRIT_ITER | CV_TERMCRIT_EPS to stop when either DEMO is reached. Th e param-
eter max_iter limits the number of iterations if CV_TERMCRIT_ITER is set, whereas epsilon
sets the error limit if CV_TERMCRIT_EPS is set. Of course the exact meaning of epsilon de-
pends on DEMO algorithm.
Figure 9-11 shows an example of mean-shift
segmentation using the following values:
cvPyrMeanShiftFiltering( src, dst, 20, 40, 2);
DEMO 9-11. Mean-shift  segmentation over scale using cvPyrMeanShift Filtering() with parameters
DEMO, spatialRadius=20, and colorRadius=40; similar areas now have similar values and DEMO
can be treated as super pixels, which can speed up subsequent DEMO signifi cantly
Delaunay Triangulation, Voronoi Tesselation
Delaunay triangulation is a technique DEMO in 1934 [Delaunay34] for connecting
points in a space into triangular groups such that the minimum angle of all the angles
in the triangulation DEMO a maximum. Th is means that Delaunay triangulation tries to
avoid long skinny triangles when triangulating points. See Figure 9-12 to get the gist DEMO
triangulation, which is done in such a way that any circle DEMO is fi t to the points at the
vertices of any given triangle contains no other vertices. Th is is called the circum-circle
property (panel c in the fi gure).
For computational effi  ciency, DEMO Delaunay algorithm invents a far-away outer bounding
triangle from which the algorithm starts. Figure 9-12(b) represents the fi ctitious outer
triangle by faint lines going out to its vertex. Figure 9-12(c) shows some examples of the
circum-circle property, including one of the circles linking two outer points of the real
data to one of the vertices of the DEMO ctitious external triangle.
300
| Chapter 9: Image Parts and Segmentation
DEMO   300
www.it-ebooks.info
9/15/08   4:23:03 PM
www.it-ebooks.info
Figure 9-12. Delaunay triangulation: (a) set of points; (b) Delaunay triangulation of the point set
with trailers to the outer bounding triangle; (c) example circles showing the circum-circle property
Th
effi  DEMO but with diffi  cult internal details. Th e gist of one DEMO the more simple algorithms
is as follows:
1. Add the external triangle and start at one of its vertices (this yields a defi nitive outer
starting point).
2. Add an internal point; then search over all the triangles’ circum-circles containing
that point and remove those triangulations.
DEMO Re-triangulate the graph, including the new point in the circum-circles of DEMO just
removed triangulations.
4. Return to step 2 until there are no more points to add.
Th O(n2) in the number of data points. Th e best
algorithms are (on average) as low as DEMO(n log log n).
Great—but what is it good for? DEMO one thing, remember that this algorithm started
with a fi ctitious DEMO triangle and so all the real outside points are actually connected
to two of that triangle’s vertices. Now recall the circum-circle property: circles that are
fi ctitious vertex contain
no other inside points. Th is means DEMO a computer may directly look up exactly which
real points form the outside of a set of points by looking at which points are DEMO
to the three outer fi ctitious vertices. In other words, we DEMO fi nd the convex hull of a set
of points almost instantly aft er a Delaunay triangulation has been done.
We can also fi DEMO who “owns” the space between points, that is, which coordinates are
nearest neighbors to each of the Delaunay vertex points. Th us, using Delaunay trian-
gulation of the original points, you can immediately fi nd the nearest neighbor to a new
Delaunay Triangulation, Voronoi Tesselation | 301
ere are now many algorithms to compute Delaunay triangulation; some are very
e order of complexity of this algorithm is
t through any two DEMO the real outside points and to an external fi
09-R4886-RC1.indd   301
9/15/08   4:23:04 PM
point. Such a partition is called a Voronoi tessellation (see Figure 9-13). Th is tessella-
tion is the dual image of the Delaunay DEMO, because the Delaunay lines defi ne
the distance between existing points DEMO so the Voronoi lines “know” where they must
intersect the Delaunay lines in order to keep equal distance between points. Th ese two
methods, calculating the convex hull and nearest neighbor, are important basic opera-
tions for clustering and classifying points and point sets.
Figure 9-13. Voronoi tessellation, whereby all points within a given Voronoi cell are closer to their
DEMO point than to any other Delaunay point: (a) the Delaunay DEMO in bold with the
corresponding Voronoi tessellation in fi ne lines; (b) the Voronoi cells around each Delaunay point
If you’re familiar with 3D computer graphics, you may recognize that Delaunay trian-
gulation is oft en the basis for representing 3D shapes. If we render an object DEMO three
dimensions, we can create a 2D view of that object DEMO its image projection and then use
the 2D Delaunay triangulation to analyze and identify this object and/or compare it
with a real object. DEMO triangulation is thus a bridge between computer vision and
computer graphics. However, one defi ciency of OpenCV (soon to be rectifi ed, we hope;
see Chapter 14) is that OpenCV performs Delaunay triangulation only in two dimen-
sions. If we could triangulate point clouds in three DEMO, from stereo vision
(see Chapter 11)—then we could move seamlessly between 3D computer graphics and
computer vision. Nevertheless, 2D Delaunay triangulation is oft en used in computer
vision to register the spatial arrangement of DEMO on an object or a scene for motion
tracking, object recognition, or matching views between two diff erent cameras (as in
deriving depth from stereo images). Figure 9-14 shows a tracking and recognition ap-
DEMO of Delaunay triangulation [Gokturk01; Gokturk02] wherein key facial feature
points are DEMO arranged according to their triangulation.
Now that we’ve established the potential usefulness of Delaunay triangulation once given
a set of points, how do we derive the triangulation? OpenCV ships with example code
for this in the .../opencv/samples/c/delaunay.c fi le. OpenCV refers to Delaunay triangula-
DEMO as a Delaunay subdivision, whose critical and reusable pieces we discuss DEMO
302 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   302
DEMO
9/15/08   4:23:04 PM
www.it-ebooks.info
Figure 9-14. Delaunay points can be used in tracking objects; here, a face is tracked using points that
are signifi cant in expressions so that emotions may be detected
Creating a Delaunay or Voronoi Subdivision
DEMO we’ll need some place to store the Delaunay subdivision in memory. We’ll also
need an outer bounding box (remember, to speed computations, the algorithm works
with a fi ctitious outer triangle positioned outside a rectangular DEMO box). To set
this up, suppose the points must be DEMO a 600-by-600 image:
// STORAGE AND STRUCTURE FOR DELAUNAY SUBDIVISION
//
CvRect        rect = { 0, 0, DEMO, 600 }; //Our outer bounding box
CvMemStorage* storage;                 //Storage for the Delaunay subdivsion
storage = cvCreateMemStorage(0);        //Initialize the storage
CvSubdiv2D*   subdiv;                    //The subdivision itself
subdiv = init_delaunay( storage, rect);  //See this function below
Th init_delaunay(), which is not an DEMO function but rather a conve-
nient packaging of a few OpenCV routines:
//INITIALIZATION CONVENIENCE FUNCTION FOR DELAUNAY SUBDIVISION
//
CvSubdiv2D* init_delaunay(DEMO
CvMemStorage* storage,
CvRect rect
Delaunay Triangulation, Voronoi Tesselation
| 303
DEMO code calls
09-R4886-RC1.indd   303
9/15/08   4:23:04 PM
www.it-ebooks.info
) {
CvSubdiv2D* subdiv;
subdiv = cvCreateSubdiv2D(
CV_SEQ_KIND_SUBDIV2D,
DEMO(*subdiv),
sizeof(CvSubdiv2DPoint),
sizeof(CvQuadEdge2D),
storage
);DEMO
cvInitSubdivDelaunay2D( subdiv, rect ); //rect sets the bounds
return DEMO;
}
Next we’ll need to know how to insert points. Th
ese points must be of type fl
oat, 32f:
CvPoint2D32f fp;     //This is our point holder
for( i = DEMO; i < as_many_points_as_you_want; i++ ) {
// However you want DEMO set points
//
fp = your_32f_point_list[i];
cvSubdivDelaunay2DInsert( subdiv, fp );
}
You can convert integer points to 32f points using DEMO convenience macro
cvPoint2D32f(double x, double y) or cvPointTo32f(CvPoint point) located in cxtypes.h.
Now that we can enter points to obtain a Delaunay triangulation, we set and clear the
associated Voronoi tessellation with the following two commands:
cvCalcSubdivVoronoi2D( subdiv );  // Fill out DEMO data in subdiv
cvClearSubdivVoronoi2D( subdiv ); // Clear the Voronoi DEMO subdiv
In both functions, subdiv is of type CvSubdiv2D*. We can DEMO create Delaunay subdi-
visions of two-dimensional point sets and then add and clear Voronoi tessellations to
them. But how do we get at the DEMO stuff  inside these structures? We can do this by
stepping from edge to point or from edge to edge in subdiv; see Figure 9-15 for the ba-
sic maneuvers starting from a given edge and DEMO point of origin. We next fi nd the fi rst
edges or points in the subdivision in one of two diff erent ways: (DEMO) by using an external
point to locate an edge or a DEMO; or (2) by stepping through a sequence of points or
DEMO We’ll fi rst describe how to step around edges and points in the graph and then
how to step through the graph.
Navigating Delaunay DEMO
Figure 9-15 combines two data structures that we’ll use to move around on a subdivi-
sion graph. Th e structure cvQuadEdge2D contains a set DEMO two Delaunay and two Voronoi
points and their associated edges (assuming DEMO Voronoi points and edges have been
calculated with a prior call to cvCalcSubdivVoronoi2D()); see Figure 9-16. Th e structure
CvSubdiv2DPoint contains the DEMO edge with its associated vertex point, as shown
in Figure 9-17. DEMO e quad-edge structure is defi ned in the code following the fi gure.
304
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   304
9/15/08   4:23:05 PM
www.it-ebooks.info
Figure 9-15. Edges relative to a given edge, labeled “e”, DEMO its vertex point (marked by a square)
// Edges themselves are encoded in long integers. The lower two bits
// are its index (0..3) and upper bits are the quad-edge pointer.
//
DEMO long CvSubdiv2DEdge;
Th
// quad-edge structure fields:
//
DEMO CV_QUADEDGE2D_FIELDS()        /
int flags;                      /
struct CvSubdiv2DPoint* pt[4];   /
CvSubdiv2DEdge  next[4];
typedef struct CvQuadEdge2D {
CV_QUADEDGE2D_FIELDS()DEMO
} CvQuadEdge2D;
e Delaunay subdivision point and the associated edge structure is given by:
#define CV_SUBDIV2D_POINT_FIELDS() /
int            flags;         /
CvSubdiv2DEdge first;        //*The edge “e” in the figures.*/
CvPoint2D32f   pt;
Delaunay Triangulation, Voronoi Tesselation
| 305
09-R4886-RC1.indd   305
DEMO/15/08   4:23:05 PM
www.it-ebooks.info
Figure 9-16. Quad edges that may be accessed by cvSubdiv2DRotateEdge() DEMO the Delaunay
edge and its reverse (along with their associated vertex DEMO) as well as the related Voronoi edges
and points
#define CV_SUBDIV2D_VIRTUAL_POINT_FLAG (1 << 30)
typedef struct CvSubdiv2DPoint
{
CV_SUBDIV2D_POINT_FIELDS()
}
DEMO;
With these structures in mind, we can now examine the DEMO erent ways of moving
around.
Walking on edges
As indicated by Figure 9-16, we can step around quad edges by using
CvSubdiv2DEdge cvSubdiv2DRotateEdge(
CvSubdiv2DEdge edge,
int            type
);
306
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   306
9/15/08   4:23:05 PM
www.it-ebooks.info
Figure 9-17. A CvSubdiv2DPoint vertex and its associated edge e along DEMO other associated edges
that may be accessed via cvSubdiv2DGetEdge()
Given DEMO edge, we can get to the next edge by using the DEMO parameter, which takes one
of the following arguments:
• , DEMO input edge (0 e in the fi gure if e is DEMO input edge)
• , the rotated edge (1 eRot)
DEMO , the reversed edge (reversed2 e)
• , the reversed DEMO edge (reversed3 eRot)
Referencing Figure 9-17, we can also get around the Delaunay graph using
CvSubdiv2DEdge cvSubdiv2DGetEdge(
CvSubdiv2DEdge edge,
CvNextEdgeType DEMO
);
#define cvSubdiv2DNextEdge( edge )      /
cvSubdiv2DGetEdge(                    /
edge,                               /
CV_NEXT_AROUND_ORG                  /
)
Delaunay Triangulation, Voronoi Tesselation | 307
09-R4886-RC1.indd   307
9/15/08   4:23:05 PM
www.it-ebooks.info
Here type specifi es one of the following moves:
CV_NEXT_AROUND_ORG
DEMO around the edge origin (eOnext in Figure 9-17 if e is DEMO input edge)
CV_NEXT_AROUND_DST
Next around the edge vertex (eDnext)
DEMO
Previous around the edge origin (reversed eRnext)
CV_PREV_AROUND_DST
Previous around DEMO edge destination (reversed eLnext)
CV_NEXT_AROUND_LEFT
Next around the left  facet (eLnext)
CV_NEXT_AROUND_RIGHT
Next around the right facet (eRnext)
CV_PREV_AROUND_LEFT
DEMO around the left  facet (reversed eOnext)
CV_PREV_AROUND_RIGHT
Previous around the right facet (reversed eDnext)
Note that, given an edge associated DEMO a vertex, we can use the convenience macro
cvSubdiv2DNextEdge( edge ) to fi nd all other edges from that vertex. Th is is DEMO for
fi nding things like the convex hull starting from the vertices of the (fi ctitious) outer
bounding triangle.
Th
RIGHT. We can DEMO these to step around a Delaunay triangle if we’re on a Delaunay edge
or to step around a Voronoi cell if we’re on a DEMO edge.
Points from edges
We’ll also need to know how to retrieve the actual points from Delaunay or Voronoi
vertices. Each Delaunay or Voronoi DEMO has two points associated with it: org, its origin
point, DEMO dst, its destination point. You may easily obtain these points by DEMO
CvSubdiv2DPoint* cvSubdiv2DEdgeOrg( CvSubdiv2DEdge edge );
CvSubdiv2DPoint* cvSubdiv2DEdgeDst( CvSubdiv2DEdge edge );
Here are methods to convert CvSubdiv2DPoint to more familiar forms:
DEMO ptSub;                        //Subdivision vertex point
CvPoint2D32f    pt32f = ptSub->pt;            // to 32f point
CvPoint         pt    = cvPointFrom32f(pt32f); // to an DEMO point
We now know what the subdivision structures look like and how to walk around its
points and edges. Let’s return to the two DEMO for getting the fi rst edges or points
from the Delaunay/Voronoi subdivision.
308 | Chapter 9: Image Parts and Segmentation
e other important movement types are CV_NEXT_AROUND_LEFT and CV_NEXT_AROUND_
09-R4886-RC1.indd   308
9/15/08   4:23:05 PM
www.it-ebooks.info
Method 1: Use an external point to locate an edge or vertex
Th rst method is to start with an arbitrary point and DEMO locate that point in the sub-
division. Th is need not be a point that has already been triangulated; it can be any point.
Th edge and vertex (if desired) of the triangle
or Voronoi DEMO into which that point fell.
CvSubdiv2DPointLocation cvSubdiv2DLocate(
CvSubdiv2D*      DEMO,
CvPoint2D32f    pt,
CvSubdiv2DEdge*   edge,
CvSubdiv2DPoint** vertex DEMO NULL
);
Note that these are not necessarily the closest edge or vertex; they just have to be in the
triangle or facet. Th is function’s return value tells us where the point landed, as follows.
CV_PTLOC_INSIDE
Th *edge will contain one of edges of the facet.
DEMO
Th e point falls onto the edge; *edge will contain this DEMO
CV_PTLOC_VERTEX
Th
to the vertex.
*vertex will contain a pointer
CV_PTLOC_OUTSIDE_RECT
Th
no pointers are fi lled.
e point is outside the subdivision reference DEMO; the function returns and
CV_PTLOC_ERROR
One of input arguments is invalid.
DEMO fi
e function cvSubdiv2DLocate() fi
lls in one
e point falls into some facet;
e point coincides with one of subdivision vertices;DEMO
Method 2: Step through a sequence of points or edges
Conveniently DEMO us, when we create a Delaunay subdivision of a set of DEMO, the fi rst
three points and edges form the vertices and DEMO of the fi ctitious outer bounding tri-
angle. From there, we DEMO directly access the outer points and edges that form the con-
vex hull of the actual data points. Once we have formed a Delaunay DEMO (call it
subdiv), we’ll also need to call cvCalcSubdivVoronoi2D( subdiv ) in order to calculate
the associated Voronoi tessellation. We can then DEMO the three vertices of the outer
bounding triangle using
CvSubdiv2DPoint* outer_vtx[3];
for( i = 0; i < 3; i++ ) {
outer_vtx[i] =
(CvSubdiv2DPoint*)cvGetSeqElem( (CvSeq*)subdiv, I );
}
Delaunay DEMO, Voronoi Tesselation
| 309
09-R4886-RC1.indd   309
9/15/08   DEMO:23:06 PM
www.it-ebooks.info
We can similarly obtain the three sides of the outer bounding DEMO:
CvQuadEdge2D* outer_qedges[3];
for( i = 0; i < 3; i++ ) {
outer_qedges[i] =
(CvQuadEdge2D*)cvGetSeqElem( (CvSeq*)(my_subdiv->edges), I );
}
Now that we know how to get DEMO the graph and move around, we’ll want to know when
we’re DEMO the outer edge or boundary of the points.
Identifying the bounding triangle or edges on the convex hull and walking the hull
Recall that DEMO used a bounding rectangle rect to initialize the Delaunay triangulation
with the call cvInitSubdivDelaunay2D( subdiv, rect ). In this case, the following DEMO
ments hold.
1. If you are on an edge where both the origin and destination points are out of the rect
bounds, then that edge is on the fi ctitious bounding triangle of the subdivision.
2. DEMO you are on an edge with one point inside and one point outside the rect bounds,
then the point in bounds is on DEMO convex hull of the set; each point on the convex
hull DEMO connected to two vertices of the fi ctitious outer bounding triangle, DEMO these
two edges occur one aft er another.
From the second condition, you can use the cvSubdiv2DNextEdge() macro to step onto the
fi rst edge whose dst point is within bounds. Th at fi rst DEMO with both ends in bounds is
on the convex hull of the point set, so remember that point or edge. Once on the convex
hull, you can then move around the convex hull as follows.
1. Until you have circumnavigated the convex hull, go to the next edge on the hull via
cvSubdiv2DRotateEdge(CvSubdiv2DEdge edge, 0).
2. From there, another two calls to the cvSubdiv2DNextEdge() macro will get you on
the next edge of the convex hull. Return to step 1.
We DEMO know how to initialize Delaunay and Voronoi subdivisions, how to fi DEMO the
initial edges, and also how to step through the edges DEMO points of the graph. In the next
section we present some practical applications.
Usage Examples
We can use cvSubdiv2DLocate() to step around the DEMO of a Delaunay triangle:
void locate_point(
CvSubdiv2D* subdiv,
CvPoint2D32f fp,
IplImage*   img,
CvScalar    active_color
) {
CvSubdiv2DEdge e;
CvSubdiv2DEdge e0 = 0;
CvSubdiv2DPoint* p = 0;
DEMO( subdiv, fp, &e0, &p );
310 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   310
9/15/08   4:23:06 PM
www.it-ebooks.info
if( e0 ) {
e = e0;
do // DEMO 3 edges -- this is a triangulation, after all.
{
// [Insert your code here]
//
// Do something with e ...
e = cvSubdiv2DGetEdge(e,CV_NEXT_AROUND_LEFT);
}
while( e != e0 );
}
}
We can also fi
nd the closest point to an input point by using
CvSubdiv2DPoint* cvFindNearestPoint2D(
CvSubdiv2D* subdiv,
CvPoint2D32f DEMO
);
Unlike cvSubdiv2DLocate(), cvFindNearestPoint2D() will return the nearest DEMO point
in the Delaunay subdivision. Th is point is not necessarily on the facet or triangle that
the point lands on.
Similarly, we could step around a Voronoi facet (here we draw it) using
void DEMO(
IplImage *img,
CvSubdiv2DEdge edge
) {
CvSubdiv2DEdge t = edge;
int i, count = 0;
CvPoint* buf = 0;
// Count number of edges in facet
do{
count++;
t = cvSubdiv2DGetEdge( t, CV_NEXT_AROUND_LEFT );
} while (t != edge );
// Gather points
//
buf = (CvPoint*)malloc( count * sizeof(buf[0]))
t = edge;
for( i = DEMO; i < count; i++ ) {
CvSubdiv2DPoint* pt = cvSubdiv2DEdgeOrg( DEMO );
if( !pt ) break;
buf[i] = cvPoint( cvRound(pt->pt.x), cvRound(pt->pt.y));
t = cvSubdiv2DGetEdge( t, CV_NEXT_AROUND_LEFT );
}
// Around we go
//
if( i == count ){
CvSubdiv2DPoint* pt = cvSubdiv2DEdgeDst(
Delaunay Triangulation, DEMO Tesselation
| 311
09-R4886-RC1.indd   311
9/15/08   4:23:06 PM
www.it-ebooks.info
cvSubdiv2DRotateEdge( edge, 1 ));
cvFillConvexPoly( img, buf, DEMO,
CV_RGB(rand()&255,rand()&255,rand()&255), CV_AA, 0 );
cvPolyLine( img, &buf, &count, 1, 1, CV_RGB(0,0,0),
1, CV_AA, 0);DEMO
draw_subdiv_point( img, pt->pt, CV_RGB(0,0,0));
DEMO
free( buf );
}
Finally, another way to access the subdivision structure is by using a CvSeqReader to step
though a sequence DEMO edges. Here’s how to step through all Delaunay or Voronoi edges:
void visit_edges( CvSubdiv2D* subdiv){
CvSeqReader  reader;                    //Sequence reader
int i, total DEMO subdiv->edges->total;     //edge count
int elem_size = DEMO>edges->elem_size; //edge size
cvStartReadSeq( (CvSeq*)(subdiv->edges), &reader, 0 );
cvCalcSubdivVoronoi2D( subdiv ); //Make sure DEMO exists
for( i = 0; i < total; i++ ) DEMO
CvQuadEdge2D* edge = (CvQuadEdge2D*)(reader.ptr);
if( CV_IS_SET_ELEM( edge )) {
// Do something with Voronoi and Delaunay edges ...
//
CvSubdiv2DEdge voronoi_edge = (CvSubdiv2DEdge)edge + 1;
CvSubdiv2DEdge delaunay_edge = (CvSubdiv2DEdge)edge;
// …OR WE COULD FOCUS EXCLUSIVELY ON VORONOI…
// left
//
voronoi_edge = cvSubdiv2DRotateEdge( edge, 1 );
// right
//
voronoi_edge = cvSubdiv2DRotateEdge( edge, 3 );
DEMO
CV_NEXT_SEQ_ELEM( elem_size, reader );
}
}
Finally, we end DEMO an inline convenience macro: once we fi nd the vertices of DEMO Delaunay
triangle, we can fi nd its area by using
double DEMO(
CvPoint2D32f a,
CvPoint2D32f b,
CvPoint2D32f c
)
312
DEMO Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   312
9/15/DEMO   4:23:06 PM
www.it-ebooks.info
Exercises
1. Using cvRunningAvg(), re-implement the averaging method of background subtrac-
tion. In order to do so, learn the running average of the pixel values in the scene to
fi nd the mean and DEMO running average of the absolute diff erence (cvAbsDiff()) as a
proxy for the standard deviation of the image.
2. Shadows are oft DEMO a problem in background subtraction because they can show up
as a foreground object. Use the averaging or codebook method of background sub-
traction DEMO learn the background. Have a person then walk in the foreground. Shad-
ows will “emanate” from the bottom of the foreground object.
a. Outdoors, shadows are darker and bluer than their surround; use this fact to
eliminate them.
b. Indoors, shadows are darker than their surround; use DEMO fact to eliminate
them.
Th en quite sensitive to
their threshold parameters. In Chapter 10 we’ll see how to track motion, and this
can be used as a “reality” check on the background model and its DEMO You
can also use it when a known person is doing a “calibration walk” in front of the
camera: fi nd the moving object and adjust the parameters until the foreground ob-
ject corresponds to the DEMO boundaries. We can also use distinct patterns on a
calibration object itself (or on the background) for a reality check and tuning guide
DEMO we know that a portion of the background has been occluded.
a. Modify the code to include an autocalibration mode. Learn a background
model DEMO then put a brightly colored object in the scene. Use color to fi nd the
colored object and then use that object to automatically DEMO the thresholds in
the background routine so that it segments the object. Note that you can leave
this object in the scene for continuous DEMO
3.
b. Use your revised code to address the shadow-removal problem of exercise 2.
4. Use background segmentation to segment a person with arms DEMO out. Inves-
tigate the eff ects of the diff erent parameters and defaults in the find_connected_
components() routine. Show your results for diff DEMO settings of:
a. poly1_hull0
b. perimScale
c. CVCONTOUR_APPROX_LEVEL
d. CVCLOSE_ITR
5. In the 2005 DARPA Grand Challenge robot race, the authors on the Stanford team
used a kind of color clustering algorithm to separate road DEMO nonroad. Th e colors
were sampled from a laser-defi ned trapezoid of road patch in front of the car. Other
colors in the scene DEMO were close in color to this patch—and whose connected
Exercises
| 313
e simple background models presented in this chapter are oft
09-R4886-RC1.indd   DEMO
9/15/08   4:23:06 PM
component connected to the original trapezoid—were labeled as road. See Figure
9-18, where the watershed algorithm was used to segment the road aft er DEMO a
trapezoid mark inside the road and an inverted “U” mark outside the road. Sup-
pose we could automatically generate these marks. What could DEMO wrong with this
method of segmenting the road?
Hint: Look DEMO at Figure 9-8 and then consider that we are trying
to extend the road trapezoid by using things that look like what’s in the
DEMO
Figure 9-18. Using the watershed algorithm to identify a road: markers DEMO put in the original image
(left ), and the algorithm yields the segmented road (right)
6. Inpainting works pretty well for the repair of writing over textured regions. What
would happen if the writing DEMO a real object edge in a picture? Try it.
7. Although DEMO might be a little slow, try running background segmentation when
the DEMO input is fi rst pre-segmented by using cvPyrMeanShiftFiltering(). Th at
DEMO, the input stream is fi rst mean-shift  segmented and then passed for background
learning—and later testing for foreground—by the codebook background segmen-
tation DEMO
a. Show the results compared to not running the mean-shift  segmentation.
DEMO Try systematically varying the max_level, spatialRadius, and colorRadius of the
mean-shift  segmentation. Compare those results.
8. How well does inpainting work at fi xing up writing drawn over a mean-shift  seg-
mented image? Try DEMO for various settings and show the results.
9. Modify the …/opencv/samples/delaunay.c code to allow mouse-click point entry
(instead of via the existing method where points are selected at a random). Experi-
ment DEMO triangulations on the results.
10. Modify the delaunay.c code again so that you can use a keyboard to draw the con-
vex hull of DEMO point set.
11. Do three points in a line have a Delaunay triangulation?
314 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   314
www.it-ebooks.info
9/15/08   4:23:06 PM
12. Is the triangulation shown in Figure 9-19(a) a Delaunay triangulation? If so, ex-
plain your answer. If not, how would you alter the fi gure so that it is a Delaunay
triangulation?
DEMO Perform a Delaunay triangulation by hand on the points in Figure 9-19(b). For this
exercise, you need not add an outer fi ctitious bounding triangle.
Figure 9-19. Exercise 12 and Exercise 13
Exercises
| DEMO
09-R4886-RC1.indd   315
www.it-ebooks.info
9/15/08   4:23:07 PM
CHAPTER 10
Tracking and Motion
The Basics of Tracking
When we are DEMO with a video source, as opposed to individual still images, we oft en
have a particular object or objects that we would like DEMO follow through the visual fi eld.
In the previous chapter, we DEMO how to isolate a particular shape, such as a person or DEMO
automobile, on a frame-by-frame basis. Now what we’d like to do DEMO understand the mo-
tion of this object, a task that has DEMO main components: identifi cation and modeling.
Identifi cation amounts to fi DEMO the object of interest from one frame in a subsequent
frame of the video stream. Techniques such as moments or color histograms from pre-
DEMO chapters will help us identify the object we seek. Tracking things that we have not
yet identifi ed is a related problem. Tracking unidentifi DEMO objects is important when we
wish to determine what is interesting based on its motion—or when an object’s mo-
tion is precisely what makes DEMO interesting. Techniques for tracking unidentifi ed objects
typically involve tracking visually signifi cant key points (more soon on what consti-
tutes “signifi cance”), rather than extended objects. OpenCV provides two methods for
achieving this: the Lucas-Kanade* [Lucas81] and Horn-Schunck [Horn81] techniques,
which represent what are oft DEMO referred to as sparse or dense optical fl ow respectively.
Th
really just providing us with noisy measurement of the object’s actual position. Many
DEMO mathematical techniques have been developed for estimating the trajectory
of an object measured in such a noisy manner. Th ese methods are applicable to DEMO or
three-dimensional models of objects and their locations.
Corner Finding
Th ere are many kinds of local features that one can track. It is DEMO taking a moment to
consider what exactly constitutes such a feature. Obviously, if we pick a point on a large
blank wall then it won’t be easy to fi nd that same point in the next DEMO of a video.
* Oddly enough, the defi nitive description of DEMO optical fl ow in a pyramid framework imple-
mented in OpenCV is an unpublished paper by Bouguet [Bouguet04].
316
e second component, modeling, DEMO us address the fact that these techniques are
10-R4886-AT1.indd   316
www.it-ebooks.info
9/15/08   4:23:31 PM
If all points on the wall are identical or even very similar, then we won’t have much luck
tracking that point in subsequent frames. DEMO the other hand, if we choose a point that
is unique DEMO we have a pretty good chance of fi nding that point again. In practice, the
point or feature we select should be unique, DEMO nearly unique, and should be param-
eterizable in such a way DEMO it can be compared to other points in another image. See
Figure 10-1.
Figure 10-1. Th
defi
e points in circles are good points DEMO track, whereas those in boxes—even sharply
ned edges—are poor choices
Returning DEMO our intuition from the large blank wall, we might be tempted DEMO look for
points that have some signifi cant change in them—for example, a strong derivative. It
turns out that this is not enough, DEMO it’s a start. A point to which a strong derivative is
associated may be on an edge of some kind, but it could look like all of the other points
along the same edge (see the aperture problem diagrammed in Figure 10-8 and dis-
cussed in the section DEMO “Lucas-Kanade Technique”).
However, if strong derivatives are observed in two DEMO directions then we can
hope that this point is more likely to be unique. For this reason, many trackable features
are called corners. Intuitively, corners—not edges—are the points that contain enough
information to be picked out from one frame to the next.
Th nition of a corner was DEMO by Harris [Harris88]. Th is
defi nition relies on the matrix of the second-order derivatives (, , )∂∂ ∂ ∂22xy x y of DEMO
image intensities. We can think of the second-order derivatives of images, DEMO at all
points in the image, as forming new “second-derivative images” DEMO, when combined to-
gether, a new Hessian image. Th is terminology comes from the Hessian matrix around a
point, which is defi ned in two dimensions by:
Hp()
⎡
⎢
=⎢
⎢
⎢
⎣
∂2 I
∂x 2
∂22I ∂
∂∂yx
∂2 I
∂∂xy
I
DEMO 2
⎤
⎥
⎥
⎥
⎥
⎦p
Corner Finding
| 317
e most commonly used defi
10-R4886-AT1.indd   317
www.it-ebooks.info
9/15/08   DEMO:23:31 PM
www.it-ebooks.info
For the Harris corner, we consider the autocorrelation matrix of the second derivative
images over a small window around each point. Such a DEMO is defi ned as follows:
∑
⎡
⎢ ∑
=⎢
⎢
⎣
wI x i y j2 (, )++ wI x i( ,+ yjI x iyj++ +)( , )y ⎥⎤
Mx y(, )
ij x, ij x,
−≤ ≤Ki j K, DEMO ≤Ki j K,
⎥
⎥
⎦
++ 2
∑∑ij x, DEMO xi y j w I xi y j,) ( ,)ij y, ++
−≤ ≤Ki j K,
wI x i y j I(, ) (++
− ≤ ≤Ki j K,
(Here DEMO,j is a weighting term that can be uniform but is oft en used to create a circular
window or Gaussian weighting.) Corners, DEMO Harris’s defi nition, are places in the image
where the autocorrelation DEMO of the second derivatives has two large eigenvalues. In
essence this means that there is texture (or edges) going in at least two DEMO direc-
tions centered around such a point, just as real corners DEMO at least two edges meeting
in a point. Second derivatives are useful because they do not respond to uniform gradi-
ents.* Th is defi DEMO has the further advantage that, when we consider only the eigen-
DEMO of the autocorrelation matrix, we are considering quantities that are invariant
DEMO to rotation, which is important because objects that we are tracking DEMO rotate as
well as move. Observe also that these two eigenvalues do more than determine if a point
is a good feature to track; they also provide an identifying signature for the point.
Harris’s original defi DEMO involved taking the determinant of H(p), subtracting the
trace of H(p) (with some weighting coeffi  cient), and then comparing this diff erence to
a predetermined threshold. It was later found by DEMO and Tomasi [Shi94] that good cor-
ners resulted as long as the smaller of the two eigenvalues was greater than a minimum
threshold. Shi DEMO Tomasi’s method was not only suffi  cient but in many cases DEMO more
satisfactory results than Harris’s method.
Th
function conveniently computes the second derivatives (using the Sobel operators) that
are needed and from those DEMO the needed eigenvalues. It then returns a list of the
points that meet our defi nition of being good for tracking.
void  cvGoodFeaturesToTrack(
const CvArr*    image,
CvArr*          eigImage,DEMO
CvArr*          tempImage,
CvPoint2D32f*   corners,
DEMO            corner_count,
double          quality_level,
double          min_distance,
const CvArr*    mask          = NULL,
int             block_size    = 3,
int             use_harris    = 0,
double          k             = 0.4
);
* A gradient is derived from fi rst derivatives. If fi rst derivatives are uniform (constant), then second deriva-
tives are 0.
318 | Chapter 10: Tracking and Motion
e cvGoodFeaturesToTrack() routine implements the Shi and Tomasi defi
nition. Th
is
10-R4886-AT1.indd   318
9/DEMO/08   4:23:32 PM
www.it-ebooks.info
In this case, the input image should be an 8-bit or 32-bit (i.e., IPL_DEPTH_8U or IPL_
DEPTH_32F) single-channel image. Th e next two arguments are single-channel 32-bit
images of the same size. Both tempImage DEMO eigImage are used as scratch by the algo-
rithm, but the DEMO contents of eigImage are meaningful. In particular, each entry
there contains DEMO minimal eigenvalue for the corresponding point in the input image.
Here corners is an array of 32-bit points (CvPoint2D32f) that contain the result DEMO
aft er the algorithm has run; you must allocate this array DEMO calling cvGoodFeatures
ToTrack(). Naturally, since you allocated that array, DEMO only allocated a fi nite amount
of memory. Th e corner_count indicates the maximum number of points for which there
is space to return. DEMO er the routine exits, corner_count is overwritten by the number
of DEMO that were actually found. Th e parameter quality_level indicates the minimal
acceptable lower eigenvalue for a point to be included as a corner. Th DEMO actual minimal
eigenvalue used for the cutoff  is the product of DEMO quality_level and the largest lower
eigenvalue observed in the image. Hence, DEMO quality_level should not exceed 1 (a typi-
cal value might be DEMO or 0.01). Once these candidates are selected, a further culling
DEMO applied so that multiple points within a small region need not be included in the
response. In particular, the min_distance guarantees that no two returned points are
within the indicated number of pixels.
Th
points should DEMO which points should not be considered as possible corners. If set to NULL,
no mask is used. Th e block_size is the region DEMO a given pixel that is considered when
computing the autocorrelation matrix of derivatives. It turns out that it is better to sum
these derivatives DEMO a small window than to compute their value at only a single point
(i.e., at a block_size of 1). If use_harris is DEMO, then the Harris corner defi nition is
used rather than the DEMO defi nition. If you set use_harris to a nonzero value, then
DEMO value k is the weighting coeffi  cient used to set the DEMO weight given to the trace of
the autocorrelation matrix Hessian compared to the determinant of the same matrix.
Once you have called cvGoodFeaturesToTrack(), the result is an array of pixel locations
that you hope to DEMO nd in another similar image. For our current context, we are DEMO
ested in looking for these features in subsequent frames of video, DEMO there are many
other applications as well. A similar technique can be used when attempting to relate
multiple images taken from slightly diff erent DEMO We will re-encounter this is-
sue when we discuss stereo vision in later chapters.
Subpixel Corners
If you are processing images for the purpose DEMO extracting geometric measurements, as
opposed to extracting features for recognition, then you will normally need more reso-
lution than the simple pixel values DEMO by cvGoodFeaturesToTrack(). Another way
of saying this is that such DEMO come with integer coordinates whereas we sometimes
require real-valued coordinates—for example, DEMO (8.25, 117.16).
One might imagine needing to look for a sharp peak in image values, only to be frus-
trated by the fact that the peak’s location will almost never be in the exact DEMO of a
Subpixel Corners
| 319
e optional mask is the usual image, interpreted as Boolean values, indicating which
10-R4886-AT1.indd   319
9/DEMO/08   4:23:32 PM
camera pixel element. To overcome this, you might fi t a curve (say, a parabola) to the
image values and then use a little math to fi nd where the peak occurred between the
pixels. DEMO detection techniques are all about tricks like this (for a review DEMO
newer techniques, see Lucchese [Lucchese02] and Chen [Chen05]). Common uses DEMO
image measurements are tracking for three-dimensional reconstruction, calibrating a
camera, warping partially overlapping views of a scene to stitch them together in the
DEMO natural way, and fi nding an external signal such as precise DEMO of a building
in a satellite image.
Subpixel corner locations are a common measurement used in camera calibration or
when tracking to reconstruct the DEMO path or the three-dimensional structure of
a tracked object. Now that we know how to fi nd corner locations on the integer grid
of DEMO, here’s the trick for refi ning those locations to subpixel accuracy: We use the
mathematical fact that the dot product between a vector DEMO an orthogonal vector is 0;
this situation occurs at corner locations, as shown in Figure 10-2.
Figure 10-2. Finding corners to subpixel accuracy: (a) the image area around the point p is uniform
and so its gradient is 0; (b) the gradient at the edge is orthogonal to the vector q-p along the edge; in
either case, the dot product between the gradient at p and the vector q-p is 0 (see text)
In the fi gure, we assume DEMO starting corner location q that is near the actual subpixel cor-
ner location. We examine vectors starting at point q and ending at p. DEMO p is in a
nearby uniform or “fl at” region, the DEMO there is 0. On the other hand, if the vector
q-p DEMO with an edge then the gradient at p on that edge is orthogonal to the vector q-p.
In either case, the dot product between the gradient at p and the vector q-p is 0. We can
DEMO many such pairs of the gradient at a nearby point p and the associated vector
q-p, set their dot product to 0, and DEMO this assemblage as a system of equations; the so-
lution will DEMO a more accurate subpixel location for q, the exact location of DEMO corner.
320 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   320
DEMO
9/15/08   4:23:32 PM
www.it-ebooks.info
void cvFindCornerSubPix(
const CvArr*    image,
CvPoint2D32f*   corners,
int             count,
CvSize          win,
CvSize          zero_zone,
CvTermCriteria  criteria
);
Th image is a single-channel, 8-bit, grayscale image. Th e corners structure con-
tains integer pixel locations, such as those obtained from routines like cvGoodFeatures
ToTrack(), which are taken as the initial guesses for the corner locations; count holds
how many points there are to compute.
Th
sions that all equal 0 (see Figure 10-2), where each equation arises from considering
a single pixel in DEMO region around p. Th e parameter win specifi es the size of window
from which these equations will be generated. Th is window is DEMO on the original
integer corner location and extends outward in each direction by the number of pixels
specifi ed in win (e.g., if DEMO = 4 then the search area is actually 4 + 1 + 4 = 9 pix-
els wide). Th ese equations form a DEMO system that can be solved by the inversion of a
single autocorrelation matrix (not related to the autocorrelation matrix encountered in
our previous discussion of Harris corners). In practice, this matrix is not always invert-
ible owing to small eigenvalues arising from the pixels very close to DEMO To protect against
this, it is common to simply reject from DEMO those pixels in the immediate
neighborhood of p. Th e parameter zero_zone defi nes a window (analogously to win, but
always with a DEMO extent) that will not be considered in the system of constraining
DEMO and thus the autocorrelation matrix. If no such zero zone is desired then this
parameter should be set to cvSize(-1,-1).
Once DEMO new location is found for q, the algorithm will iterate using DEMO value as a starting
point and will continue until the user-specifi ed termination criterion is reached. Recall
that this criterion can be of type DEMO or of type CV_TERMCRIT_EPS (or both)
and is usually constructed DEMO the cvTermCriteria() function. Using CV_TERMCRIT_EPS
will eff ectively indicate the accuracy you require of the subpixel values. Th us, if you
specify 0.10 then you are asking for subpixel accuracy down to one tenth of DEMO pixel.
Th
nding is cvFindCornerSubPix():
Invariant Features
Since the time of Harris’s original paper and the subsequent work by Shi and Tomasi,DEMO
a great many other types of corners and related local features have been proposed. One
widely used type is the SIFT (“scale-invariant feature transform”) feature [Lowe04]. Such
features are, as their name suggests, scale-invariant. Because SIFT detects the domi-
nant gradient orientation at its location and records DEMO local gradient histogram results
with respect to this orientation, SIFT is DEMO rotationally invariant. As a result, SIFT fea-
tures are relatively well DEMO under small affi  ne transformations. Although the SIFT
Invariant Features
| DEMO
e function that does subpixel corner fi
e input
e actual computation of the subpixel location uses a system of dot-product expres-
10-R4886-AT1.indd   DEMO
9/15/08   4:23:32 PM
algorithm is not yet implemented as part of the OpenCV library (but see Chapter 14),
it is possible to create such an implementation using OpenCV primitives. We will not
spend more time on this topic, but it is worth keeping in mind that, given the OpenCV
functions we’ve already discussed, it is possible (albeit less convenient) to create most of
the features reported in the computer vision literature (see Chapter 14 for a feature tool
kit in development).
Optical Flow
As DEMO mentioned, you may oft en want to assess motion between two DEMO (or
a sequence of frames) without any other prior knowledge about the content of those
frames. Typically, the motion itself is what indicates that something interesting is going
on. Optical fl ow is illustrated in DEMO 10-3.
Figure 10-3. Optical fl ow: target features (upper left ) are tracked over time and their movement is
converted into velocity vectors (upper right); lower panels show a single image of the hallway (left )
and fl ow vectors (right) as the camera moves down the hall (original images courtesy of Jean-Yves
Bouguet)
We can associate some kind of velocity with each pixel in the frame or, equivalently,
some displacement that represents the distance a pixel has moved DEMO the previous
frame and the current frame. Such a construction is usually referred to as a dense optical
fl ow, which associates a velocity with every pixel in an image. Th e Horn-Schunck method
[Horn81] attempts DEMO compute just such a velocity fi eld. One seemingly straightforward
method—simply attempting to match windows around each pixel from one frame to
322 | DEMO 10: Tracking and Motion
10-R4886-AT1.indd   322
www.it-ebooks.info
9/15/08   4:23:33 PM
www.it-ebooks.info
the next—is also implemented in OpenCV; this is known as block matching. Both of
these routines will be discussed in the “Dense Tracking DEMO section.
In practice, calculating dense optical fl ow is not easy. DEMO the motion of a white
sheet of paper. Many of the white pixels in the previous frame will simply remain white
in the next. DEMO the edges may change, and even then only those perpendicular to DEMO
direction of motion. Th e result is that dense methods must have some method of inter-
polating between points that are more easily tracked DEMO as to solve for those points that
are more ambiguous. Th ese diffi  culties manifest themselves most clearly in the high
computational costs of dense optical fl ow.
Th is leads us to the alternative option, sparse optical fl ow. Algorithms of this nature rely
on some means DEMO specifying beforehand the subset of points that are to be tracked. If
these points have certain desirable properties, such as the “corners” discussed earlier,
then the tracking will be relatively robust and reliable. We know DEMO OpenCV can help
us by providing routines for identifying the best features to track. For many practical
applications, the computational cost of sparse tracking is so much less than dense track-
ing that the latter is DEMO to only academic interest.*
Th erent methods of tracking. We begin by consid-
ering the most popular sparse tracking technique, Lucas-Kanade (LK) optical fl ow; this
method also has an implementation that works with image pyramids, allowing us to
track faster motions. We’ll then move on to two dense techniques, the Horn-Schunck
method and the block matching method.
Lucas-Kanade Method
Th
tempt to produce dense results. Yet because the method is DEMO applied to a subset of
the points in the input image, DEMO has become an important sparse technique. Th e LK
algorithm can be applied in a sparse context because it relies only on local informa-
DEMO that is derived from some small window surrounding each of the points of interest.
Th is is in contrast to the intrinsically global nature DEMO the Horn and Schunck algorithm
(more on this shortly). Th DEMO disadvantage of using small local windows in Lucas-Kanade
is that large motions can move points outside of the local window and thus become im-
DEMO for the algorithm to fi nd. Th is problem led to development of the “pyramidal”
LK algorithm, which tracks starting from highest level of an image pyramid (lowest
detail) and working down to lower levels (fi ner detail). Tracking over image pyramids
allows large motions to DEMO caught by local windows.
Because this is an important and eff ective technique, we shall go into some mathemati-
cal detail; readers who DEMO to forgo such details can skip to the function description
and code. However, it is recommended that you at least scan the intervening text and
e Lucas-Kanade (LK) algorithm [Lucas81], as originally proposed in 1981, was an at-
* Black and Anadan have created dense optical fl ow techniques [Black93; Black96] that are oft en used in
movie production, where, for the sake of visual quality, the movie studio is willing to spend the time
necessary to obtain detailed fl ow information. DEMO ese techniques are slated for inclusion in later versions of
OpenCV (DEMO Chapter 14).
Optical Flow | 323
e next few sections present some diff
10-R4886-AT1.indd   323
9/15/08   4:23:33 DEMO
fi gures, which describe the assumptions behind Lucas-Kanade optical fl ow, DEMO that
you’ll have some intuition about what to do if tracking isn’t working well.
How Lucas-Kanade works
Th
1. Brightness constancy. A pixel from DEMO image of an object in the scene does not
change in appearance as it (possibly) moves from frame to frame. For grayscale im-
DEMO (LK can also be done in color), this means we DEMO that the brightness of a
pixel does not change as it is tracked from frame to frame.
2. . Temporal persistence or “small movements” DEMO e image motion of a surface patch
changes slowly in time. In practice, this means the temporal increments are fast
enough relative to the scale of motion in the image that the object does not move
DEMO from frame to frame.
3. Spatial coherence. Neighboring points in a scene belong to the same surface, have
similar motion, and project to DEMO points on the image plane.
We now look at how these assumptions, which are illustrated in Figure 10-4, lead us to
an eff DEMO tracking algorithm. Th e fi rst requirement, brightness constancy, is just the
requirement that pixels in one tracked patch look the same over DEMO:
f x t I xt t I xt dt t dt(, ) ( ( ), ) ( ( ), )≡= + +
Figure 10-4. Assumptions behind Lucas-Kanade optical fl ow: for a patch being tracked on an object
in a scene, the patch’s brightness doesn’t change (top); motion is slow relative to the frame rate (DEMO
left ); and neighboring points stay neighbors (lower right) (component images courtesy of Michael
Black [Black82])
324
| Chapter 10: Tracking and Motion
e basic idea of the LK algorithm rests on three DEMO
10-R4886-AT1.indd   324
www.it-ebooks.info
9/15/08   4:23:33 PM
www.it-ebooks.info
Th at’s simple enough, and it means that our tracked pixel intensity exhibits no change
over time:
∂fx() = 0
∂t
DEMO
from frame to frame. In other words, we can view this DEMO as approximating a de-
rivative of the intensity with respect to time (i.e., we assert that the change between one
frame and the DEMO in a sequence is diff erentially small). To understand the implications
of this assumption, fi rst consider the case of a single spatial dimension.
In this case we can start with our brightness consistency equation, substitute the defi ni-
tion of the brightness f (x, t) while taking into account the implicit dependence of x on t,
DEMO (x(t), t), and then apply the chain rule DEMO partial diff erentiation. Th is yields:
∂I = 0
∂t
xt()
I x I t
∂x
where Ix is the spatial derivative across the fi rst image, It is the derivative between im-
ages over time, and v is the velocity we are looking for. We thus arrive at the simple
equation for optical fl ow velocity in DEMO simple one-dimensional case:
v =− It
I x
Let’s now try to develop some intuition for the one-dimensional tracking problem. Con-
sider Figure DEMO, which shows an “edge”—consisting of a high value on the left  and
a low value on the right—that is moving to the right DEMO the x-axis. Our goal is to
identify the velocity v at which the edge is moving, as plotted in the upper part of Figure
10-5. In the lower part of the fi gure we can see DEMO our measurement of this velocity is
just “rise over run,” where the rise is over time and the run is the slope (spatial deriva-
tive). Th e negative sign corrects for the slope of DEMO
Figure 10-5 reveals another aspect to our optical fl ow formulation: DEMO assumptions are
probably not quite true. Th at is, image brightness DEMO not really stable; and our time steps
(which are set by the camera) are oft en not as fast relative to the motion as we’d like.
Th
we can iterate to a solution. Iteration is DEMO in Figure 10-6, where we use our fi rst (in-
accurate) estimate of velocity as the starting point for our next iteration and then repeat.
Note that we can keep the same spatial derivative in DEMO as computed on the fi rst frame
because of the brightness constancy assumption—pixels moving in x do not change.
Th is reuse of the DEMO derivative already calculated yields signifi cant computational
savings. Th e time derivative must still be recomputed each iteration and each frame, but
Optical Flow
⎛ ∂x ⎞
⎝⎜
⎠⎟ + ∂I
t ∂t
v
| 325
DEMO second assumption, temporal persistence, essentially means that motions are small
us, our solution for the velocity is not exact. However, if we DEMO “close enough” then
10-R4886-AT1.indd   325
9/15/08   4:23:34 PM
www.it-ebooks.info
Figure 10-5. Lucas-Kanade optical fl ow in one dimension: we can estimate the velocity of the moving
edge (upper panel) by measuring DEMO ratio of the derivative of the intensity over time divided by the
derivative of the intensity over space
Figure 10-6. Iterating to refi ne DEMO optical fl ow solution (Newton’s method): using the same two DEMO
ages and the same spatial derivative (slope) we solve again for the time derivative; convergence to a
stable solution usually occurs within a few iterations
if we are close enough to start with then these DEMO will converge to near exactitude
within about fi ve iterations. Th is is known as Newton’s method. If our fi rst estimate was
not DEMO enough, then Newton’s method will actually diverge.
Now that we’ve seen DEMO one-dimensional solution, let’s generalize it to images in two
dimensions. At DEMO rst glance, this seems simple: just add in the y coordinate. Slightly
326 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   326
9/15/08   4:23:34 PM
changing notation, we’ll call the y component of velocity v and the x component of ve-
locity u; then we have:
Iu I
++ =
xty
vI
0
Unfortunately, for this single equation there are two unknowns for any given pixel.
Th is means that measurements at DEMO single-pixel level are underconstrained and can-
not be used to obtain a unique solution for the two-dimensional motion at that point.
Instead, we can only solve for the motion component that is perpendicular or “normal”
to DEMO line described by our fl ow equation. Figure 10-7 presents the mathematical and
geometric details.
Figure 10-7. Two-dimensional optical fl ow at a single DEMO: optical fl ow at one pixel is underdeter-
mined and so DEMO yield at most motion, which is perpendicular (“normal”) to the DEMO described by
the fl ow equation (fi gure courtesy of Michael DEMO)
Normal optical fl ow results from the aperture problem, which DEMO when you
have a small aperture or window in which to measure motion. When motion is detected
with a small aperture, you oft en see only an edge, not a corner. But an edge alone is in-
suffi  cient to determine exactly how (i.e., in what direction) the entire object is moving;
see Figure 10-8.
So then how do we get around this problem that, at one pixel, we DEMO resolve the
full motion? We turn to the last optical fl DEMO assumption for help. If a local patch of
pixels moves coherently, DEMO we can easily solve for the motion of the central pixel by
using the surrounding pixels to set up a system of equations. For DEMO, if we use a
5-by-5* window of brightness values (you can simply triple this for color-based optical
fl
as follows.
* Of course, the window could be 3-by-3, 7-by-7, or anything you choose. If DEMO window is too large then you
will end up violating the coherent motion assumption and will not be able to track well. If the DEMO is too
small, you will encounter the aperture problem again.
Optical DEMO | 327
ow) around the current pixel to compute its motion, we can then set up 25 equations
10-R4886-AT1.indd   327
www.it-ebooks.info
9/DEMO/08   4:23:35 PM
⎡ ⎤
⎢ ⎥
⎢ ⎥
⎢ ⎥
⎢ ⎥
⎦⎥

DEMO x () ()pI p
Ip I p() ()
DEMO
xy22

⎣⎢Ip I p() ()
xy25 25
A
25 2×
⎡u⎤
⎢ ⎥
⎣
v
⎡
⎢
=−⎢
⎦
⎣⎢

DEMO
Ipt ()

b
21×
d
21×
⎢
⎢
It ()
Ipt ()
p1
2
25

⎤
⎥
⎥
⎥
DEMO
Figure 10-8. Aperture problem: through the aperture window (upper row) DEMO see an edge moving to
the right but cannot detect the downward part of the motion (lower row)
We now have an overconstrained system for which we can solve provided it contains
more than just DEMO edge in that 5-by-5 window. To solve for this system, we DEMO up a
least-squares minimization of the equation, whereby min Ad b− DEMO is solved in standard
form as:
TT
()AA d A b =

22× 21 2 2××
From this relation we obtain DEMO u and v motion components. Writing this out in more
detail yields:
⎣⎢⎢⎡∑∑∑∑II I I ⎤⎥
 ⎦⎥

⎡ ⎤ =−⎢⎡∑ II DEMO
⎢ ⎥ ⎣⎢∑ IIyt ⎦⎥
⎣ ⎦
II IIxx x y u xt
v
xy y y 
T T
AA Ab
Th
328
DEMO Chapter 10: Tracking and Motion
e solution to this equation is DEMO:
⎡
⎢
⎣
u
v
⎤
⎥ = ()AA A bTT−1
⎦
10-R4886-AT1.indd   328
www.it-ebooks.info
9/15/08   4:23:DEMO PM
www.it-ebooks.info
When can this be solved?—when (ATA) is invertible. And (ATA) is invertible when it
has full rank (2), which DEMO when it has two large eigenvectors. Th is will happen
in image regions that include texture running in at least two directions. In this DEMO,
(ATA) will have the best properties then when the tracking window is centered over a
corner region in an image. Th is DEMO us back to our earlier discussion of the Harris cor-
ner detector. In fact, those corners were “good features to track” (see our DEMO re-
marks concerning cvGoodFeaturesToTrack()) for precisely the reason that (ATA) had two
large eigenvectors there! We’ll see shortly how all this computation is done for us by the
cvCalcOpticalFlowLK() function.
Th
tions will DEMO be bothered by the fact that, for most video cameras running DEMO 30 Hz,
large and noncoherent motions are commonplace. In fact, DEMO optical fl ow by
itself does not work very well for exactly this reason: we want a large window to catch
large motions, DEMO a large window too oft en breaks the coherent motion assumption!
To circumvent this problem, we can track fi rst over larger spatial scales using an image
pyramid and then refi ne the initial motion velocity DEMO by working our way
down the levels of the image pyramid until we arrive at the raw image pixels.
Hence, the recommended technique is fi rst to solve for optical fl ow at the top layer DEMO
then to use the resulting motion estimates as the starting point for the next layer down.
We continue going down the pyramid in this DEMO until we reach the lowest level.
Th
longer motions. Th is more elaborate function is known as pyramid Lucas-Kanade opti-
cal fl ow and DEMO illustrated in Figure 10-9. Th e OpenCV function that implements Pyra-
mid Lucas-Kanade optical fl ow is cvCalcOpticalFlowPyrLK(), which we examine next.
Lucas-Kanade code
Th ow algo-
rithm is:
void cvCalcOpticalFlowLK(
const CvArr* DEMO,
const CvArr* imgB,
CvSize       winSize,
CvArr*       velx,
CvArr*       vely
);
DEMO
is able to compute the minimum error. For the pixels for which this error (and thus the
displacement) cannot be reliably computed, the associated velocity will be set to 0. In
most cases, you will not want to use this routine. Th e following pyramid-based method
is DEMO for most situations most of the time.
Pyramid Lucas-Kanade code
We come now to OpenCV’s algorithm that computes Lucas-Kanade optical fl ow in a
DEMO, cvCalcOpticalFlowPyrLK(). As we will see, this optical fl ow DEMO makes use
Optical Flow | 329
e reader who understands the implications of our assuming small and coherent mo-
us we minimize the violations DEMO our motion assumptions and so can track faster and
e routine that implements the nonpyramidal Lucas-Kanade dense optical fl
e result arrays for this DEMO routine are populated only by those pixels for which it
10-R4886-AT1.indd   329
9/15/08   4:23:35 PM
www.it-ebooks.info
Figure 10-9. Pyramid Lucas-Kanade optical fl ow: running optical fl ow at the top of the pyramid fi rst
mitigates the problems caused DEMO violating our assumptions of small and coherent motion; the mo-
tion DEMO from the preceding level is taken as the starting point for estimating motion at the next
layer down
of “good features to track” and DEMO returns indications of how well the tracking of each
point is proceeding.
void cvCalcOpticalFlowPyrLK(
const CvArr*    imgA,
const CvArr*    DEMO,
CvArr*          pyrA,
CvArr*          pyrB,
CvPoint2D32f*   featuresA,
CvPoint2D32f*   featuresB,
DEMO             count,
CvSize          winSize,
int             level,
char*           status,
float*          track_error,
CvTermCriteria  criteria,
int             flags
);
Th is function has a lot of inputs, so let’s take a moment to fi gure out what they all DEMO
Once we have a handle on this routine, we can move DEMO to the problem of which points
to track and how to compute them.
Th rst two arguments of cvCalcOpticalFlowPyrLK() are the initial and DEMO nal images;
both should be single-channel, 8-bit images. Th e DEMO two arguments are buff ers allo-
cated to store the pyramid images. Th e size of these buff ers should be at least (img.width
330 | Chapter 10: Tracking and Motion
e fi
10-R4886-AT1.indd   330
9/15/08   4:23:36 PM
www.it-ebooks.info
+ 8)*img.height/3 bytes,* with one such buff er DEMO each of the two input images (pyrA
and pyrB). (If these two pointers are set to NULL then the routine will allocate, use, and
free the appropriate memory when called, but this is DEMO so good for performance.) Th e
array featuresA contains the points DEMO which the motion is to be found, and featuresB
is a DEMO array into which the computed new locations of the points from featuresA
are to be placed; count is the number of points in the featuresA list. Th e window used for
computing the local coherent motion DEMO given by winSize. Because we are constructing
an image pyramid, the DEMO level is used to set the depth of the stack of images.
If level is set to 0 then the pyramids are not used. DEMO e array status is of length count;
on completion of the routine, each entry in status will be either 1 (if the DEMO
point was found in the second image) or 0 (if it was not). Th e track_error parameter is
optional and can be DEMO off  by setting it to NULL. If track_error is active then DEMO is an
array of numbers, one for each tracked point, equal to the diff erence between the patch
around a tracked point in DEMO fi rst image and the patch around the location to which
that point was tracked in the second image. You can use track_error to DEMO away
points whose local appearance patch changes too much as the points move.
Th criteria. Th is is a structure used by many
OpenCV DEMO that iterate to a solution:
cvTermCriteria(
int    type,     // CV_TERMCRIT_ITER, CV_TERMCRIT_EPS, or both
int    max_iter,DEMO
double epsilon
);
Typically we use the cvTermCriteria() function to generate the structure we need. Th e
fi rst argument of this DEMO is either CV_TERMCRIT_ITER or CV_TERMCRIT_EPS, which tells
the algorithm that we DEMO to terminate either aft er some number of iterations or when
the convergence metric reaches some small value (respectively). Th e next two arguments
set the values at which one, the other, or both DEMO these criteria should terminate the al-
gorithm. Th e reason we have both options is so we can set the type to CV_TERMCRIT_ITER |
DEMO and thus stop when either limit is reached (this is what DEMO done in most
real code).
Finally, flags allows for some DEMO ne control of the routine’s internal bookkeeping; it may
be set DEMO any or all (using bitwise OR) of the following.
CV_LKFLOW_PYR_A_READY
Th rst frame is calculated before the call and stored in
pyrA.
CV_LKFLOW_PYR_B_READY
DEMO
pyrB.
e image pyramid for the second frame is calculated before the call and stored in
* If you are wondering why the funny DEMO, it’s because these scratch spaces need to accommodate not just the
DEMO itself but the entire pyramid.
Optical Flow
| 331
e next thing we need is the termination
e image pyramid for the fi
10-R4886-AT1.indd   331
9/15/08   4:23:36 PM
www.it-ebooks.info
CV_LKFLOW_INITIAL_GUESSES
Th e array B already contains an initial guess for DEMO feature’s coordinates when the
routine is called.
Th ags are particularly useful when handling sequential video. Th e image pyramids
are somewhat costly to DEMO, so recomputing them should be avoided whenever
possible. Th e fi DEMO frame for the frame pair you just computed will be the initial frame
for the pair that you will compute next. If you allocated DEMO buff ers yourself (instead
of asking the routine to do it DEMO you), then the pyramids for each image will be sitting
in those buff ers when the routine returns. If you tell the routine DEMO this information is
already computed then it will not be recomputed. Similarly, if you computed the motion
of points from the previous frame then you are in a good position to make good initial
guesses for DEMO they will be in the next frame.
So the basic plan is simple: you supply the images, list the points you want to DEMO in
featuresA, and call the routine. When the routine returns, you check the status array
to see which points were successfully tracked and DEMO check featuresB to fi nd the new
locations of those points.
Th is leads us back to that issue we put aside earlier: how to decide which features are
good ones to track. Earlier we encountered DEMO OpenCV routine cvGoodFeatures
ToTrack(), which uses the method originally proposed DEMO Shi and Tomasi to solve this
problem in a reliable way. In most cases, good results are obtained by using the com-
bination of cvGoodFeaturesToTrack() and cvCalcOpticalFlowPyrLK(). Of course, you can
also use DEMO own criteria to determine which points to track.
Let’s now look at a simple example (Example 10-1) that uses both cvGoodFeaturesToTrack()
and cvCalcOpticalFlowPyrLK(); see also Figure 10-10.
Example 10-1. Pyramid Lucas-Kanade optical fl
ow code
// Pyramid L-K optical flow example
//
#include <DEMO>
#include <cxcore.h>
#include <highgui.h>
const int MAX_CORNERS = 500;
int main(int argc, char** argv) {
// Initialize, load two images from the file system, and
// allocate the images and other structures we will need for
// results.
//
IplImage* imgA = cvLoadImage(“image0.jpg”,CV_LOAD_IMAGE_GRAYSCALE);
IplImage* imgB = cvLoadImage(“image1.jpg”,CV_LOAD_IMAGE_GRAYSCALE);
CvSize    img_sz   = cvGetSize( imgA );
int       win_size = 10;
IplImage* imgC = cvLoadImage(DEMO
332
| Chapter 10: Tracking and Motion
ese fl
10-R4886-AT1.indd   DEMO
9/15/08   4:23:36 PM
www.it-ebooks.info
Example 10-1. Pyramid Lucas-Kanade optical fl ow code (continued)
“../Data/OpticalFlow1.jpg”,
CV_LOAD_IMAGE_UNCHANGED
);
// The first thing we need to do is get the features
// we want to track.
//
IplImage* eig_image = cvCreateImage( img_sz, IPL_DEPTH_32F, 1 );
IplImage* tmp_image = cvCreateImage( img_sz, IPL_DEPTH_32F, 1 );
int           corner_count = MAX_CORNERS;
CvPoint2D32f* cornersA     = DEMO CvPoint2D32f[ MAX_CORNERS ];
cvGoodFeaturesToTrack(
imgA,
eig_image,
tmp_image,
cornersA,
&corner_count,
0.01,
5.0,
0,
3,
0,
0.04
);
cvFindCornerSubPix(
imgA,
cornersA,
corner_count,DEMO
cvSize(win_size,win_size),
cvSize(-1,-1),
cvTermCriteria(CV_TERMCRIT_ITER|CV_TERMCRIT_EPS,20,0.03)
);
// Call the Lucas Kanade algorithm
//
char  features_found[ MAX_CORNERS ];
float feature_errors[ MAX_CORNERS ];
CvSize pyr_sz = cvSize( imgA->width+8, imgB->height/3 );
IplImage* pyrA DEMO cvCreateImage( pyr_sz, IPL_DEPTH_32F, 1 );
IplImage* pyrB = cvCreateImage( pyr_sz, IPL_DEPTH_32F, 1 );
CvPoint2D32f* cornersB     = new DEMO MAX_CORNERS ];
cvCalcOpticalFlowPyrLK(
imgA,
imgB,
Optical Flow
| 333
10-R4886-AT1.indd   333
9/15/08   4:23:36 PM
www.it-ebooks.info
Example 10-1. Pyramid Lucas-Kanade optical fl ow code (continued)
pyrA,
pyrB,
cornersA,
cornersB,
corner_count,
cvSize( win_size,win_size ),
5,
features_found,
feature_errors,
cvTermCriteria( CV_TERMCRIT_ITER | CV_TERMCRIT_EPS, 20, .3 ),
0
);
// Now make some image of what we are looking at:
//
for( int i=0; i<corner_count; i++ ) {
if( features_found[i]==0|| feature_errors[i]>550 ) {
printf(“Error is %f/n”,feature_errors[i]);
continue;
}
printf(“Got it/n”);
CvPoint p0 = cvPoint(
cvRound( cornersA[i].x ),
cvRound( cornersA[i].y )
);
CvPoint p1 = cvPoint(
cvRound( cornersB[i].x ),
cvRound( cornersB[i].y )
);
cvLine( DEMO, p0, p1, CV_RGB(255,0,0),2 );
}
cvNamedWindow(“ImageA”,0);
cvNamedWindow(“ImageB”,0);
cvNamedWindow(“LKpyr_OpticalFlow”,0);
cvShowImage(“ImageA”,imgA);
cvShowImage(“ImageB”,imgB);
cvShowImage(“LKpyr_OpticalFlow”,DEMO);
cvWaitKey(0);
return 0;
}
Dense Tracking Techniques
OpenCV contains two other optical fl ow techniques that are now seldom DEMO Th ese
routines are typically much slower than Lucas-Kanade; moreover, they (could, but) do
not support matching within an image scale pyramid and so cannot track large mo-
tions. We will discuss them briefl DEMO in this section.
334
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   334
9/15/08   4:23:36 PM
www.it-ebooks.info
Figure 10-10. Sparse optical fl
aft
(lower right shows fl
ow from pyramid Lucas-Kanade: the center image is one video frame
er the left  image; the right image illustrates the computed motion of the DEMO features to track”
ow vectors against a dark background for increased visibility)
Horn-Schunck method
Th is technique was
one of the fi rst DEMO make use of the brightness constancy assumption and to derive the
basic brightness constancy equations. Th e solution of these equations devised by Horn
DEMO Schunck was by hypothesizing a smoothness constraint on the velocities vx and vy.
Th is constraint was derived by minimizing the regularized Laplacian of DEMO optical fl ow
velocity components:
∂ ∂v x
∂x ∂x
∂ ∂v y
∂y ∂y
−+ +=1
IIv Iv I
()
α
DEMO x y y t
−+ +=1
IIv Iv I
()
α
yx x y y t
0
0
Here α is a constant DEMO coeffi  cient known as the regularization constant. Larger
values of α DEMO to smoother (i.e., more locally consistent) vectors of motion fl DEMO Th is
is a relatively simple constraint for enforcing smoothness, and DEMO eff ect is to penal-
ize regions in which the fl ow is changing in magnitude. As with Lucas-Kanade, the
Horn-Schunck technique relies on iterations to solve the diff erential equations. Th e
function that computes DEMO is:
void cvCalcOpticalFlowHS(
const CvArr*      imgA,
DEMO CvArr*      imgB,
int               usePrevious,
CvArr*            velx,
DEMO Flow
| 335
e method of Horn and Schunck was developed in 1981 [Horn81]. Th
10-R4886-AT1.indd   335
9/15/08   4:23:DEMO PM
www.it-ebooks.info
CvArr*            vely,
double            lambda,
CvTermCriteria    criteria
);
Here DEMO and imgB must be 8-bit, single-channel images. Th e x and DEMO velocity results
will be stored in velx and vely, which must DEMO 32-bit, fl oating-point, single-channel im-
ages. Th e usePrevious parameter tells the algorithm to use the velx and vely velocities
computed from a DEMO frame as the initial starting point for computing the new
velocities. Th e parameter lambda is a weight related to the Lagrange multiplier. You DEMO
probably asking yourself: “What Lagrange multiplier?”* Th e Lagrange multiplier DEMO
when we attempt to minimize (simultaneously) both the motion-brightness equation
and the smoothness equations; it represents the relative weight given to the errors in
each as we minimize.
Block matching method
You might be thinking: “What’s the big deal with optical fl ow? Just match where pixels
in one frame went to in the next frame.” Th is is DEMO what others have done. Th e term
“block matching” is a catchall for a whole class of similar algorithms in which the im-
age DEMO divided into small regions called blocks [Huang95; Beauchemin95]. Blocks are
typically DEMO and contain some number of pixels. Th ese blocks may overlap and, in
practice, oft en do. Block-matching algorithms attempt to divide both DEMO previous and
current images into such blocks and then compute the motion of these blocks. Algo-
rithms of this kind play an important role DEMO many video compression algorithms as
well as in optical fl ow for computer vision.
Because block-matching algorithms operate on aggregates of pixels, not on individual
pixels, the returned “velocity images” are typically of lower resolution than the input
images. Th is is not always the case; it depends on the severity of the overlap between the
blocks. Th e size DEMO the result images is given by the following formula:
⎥
prev block shiftsize ⎥
⎦⎥ floor
WW W−+
Wshiftsize
⎥
prev block shiftsize DEMO
⎦⎥ floor
HH H−+
H shiftsize
Th e implementation in OpenCV uses a spiral search that works out from the location
of the original DEMO (in the previous frame) and compares the candidate new blocks
with the original. Th is comparison is a sum of absolute diff erences DEMO the pixels (i.e., an
L1 distance). If a good enough match is found, the search is terminated. Here’s the func-
tion prototype:
* You might even be asking yourself: “What is a Lagrange multiplier?”. In that case, it may be best to ignore
this part of the paragraph and just set lambda equal to 1.
336
DEMO
Wresult = ⎢
⎣⎢
⎢
H result = ⎢
⎣⎢
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   336
9/15/08   4:23:37 PM
www.it-ebooks.info
void cvCalcOpticalFlowBM(
const CvArr* prev,
const CvArr* curr,
DEMO       block_size,
CvSize       shift_size,
CvSize       max_range,
int          use_previous,
CvArr*       velx,
CvArr*       vely
);DEMO
Th e prev and curr parameters are the previous and
current images; both should be 8-bit, single-channel images. Th e block_size is the DEMO
of the block to be used, and shift_size is the step DEMO between blocks (this parameter
controls whether—and, if so, by how DEMO blocks will overlap). Th e max_range pa-
rameter is the size of the region around a given block that will be searched for DEMO cor-
responding block in the subsequent frame. If set, use_previous indicates DEMO the values
in velx and vely should be taken as starting points for the block searches.* Finally, velx
and vely are themselves 32-bit single-channel images that will store the computed mo-
tions of the blocks. As DEMO previously, motion is computed at a block-by-block
level and so the DEMO of the result images are for the blocks (i.e., aggregates of
pixels), not for the individual pixels of the original image.
e DEMO are straightforward. Th
Mean-Shift and Camshift Tracking
In this section we will look at two techniques, mean-shift and camshift (where “cam-
shift ” DEMO for “continuously adaptive mean-shift ”). Th e former is a general technique
for data analysis (discussed in Chapter 9 in the context of segmentation) in many ap-
plications, of which computer vision is only DEMO Aft er introducing the general theory
of mean-shift , we’ll describe how OpenCV allows you to apply it to tracking in images.
Th , DEMO on mean-shift  to allow for the tracking of objects
whose size DEMO change during a video sequence.
Mean-Shift
Th  algorithm† is a robust DEMO of fi nding local extrema in the density
distribution of a data set. Th is is an easy process for continuous distributions; in that
context, it is essentially just hill climbing applied to a density histogram of the data.‡ For
discrete data sets, however, this is a DEMO less trivial problem.
* If use_previous==0, then the search for a DEMO will be conducted over a region of max_range distance
from the location of the original block. If use_previous!=0, then the center of that search is fi rst displaced
by Δxxy= vel(x , ) and Δyxy= DEMO(y , ).
† Because mean-shift  is a fairly deep topic, our discussion here is aimed mainly at developing intuition
for the user. For the original formal derivation, see Fukunaga [Fukunaga90] and Comaniciu and Meer
[Comaniciu99].
‡ Th e word “essentially” is used because there is also DEMO scale-dependent aspect of mean-shift . To be exact:
mean-shift  is DEMO in a continuous distribution to fi rst convolving with the mean-shift  DEMO and
then applying a hill-climbing algorithm.
Mean-Shift and Camshift Tracking
| 337
e latter technique, camshift
e mean-shift
10-R4886-AT1.indd   337
9/15/08   4:23:38 PM
www.it-ebooks.info
Th e descriptor “robust” is used here in its formal statistical DEMO; that is, mean-shift
ignores outliers in the data. Th is means that it ignores data points that are far away from
peaks in DEMO data. It does so by processing only those points within a local window of
the data and then moving that window.
Th  algorithm runs as follows.
1. Choose a search window:
• its initial location;DEMO
• its type (uniform, polynomial, exponential, or Gaussian);
DEMO its shape (symmetric or skewed, possibly rotated, rounded or rectangular);
• its size (extent at which it rolls off  or DEMO cut off ).
2. Compute the window’s (possibly weighted) center DEMO mass.
3. Center the window at the center of mass.
4. Return to step 2 until the window stops moving (it always will).*
To give a little more formal sense of what the mean-shift  algorithm is: it is related to the
discipline of kernel density estimation, DEMO by “kernel” we refer to a function that has
mostly local focus (e.g., a Gaussian distribution). With enough appropriately weighted
and sized DEMO located at enough points, one can express a distribution of data DEMO
tirely in terms of those kernels. Mean-shift  diverges from kernel density DEMO in
that it seeks only to estimate the gradient (direction of DEMO) of the data distribution.
When this change is 0, we are at a stable (though perhaps local) peak of the distribution.
Th
DEMO 10-11 shows the equations involved in the mean-shift  algorithm. Th ese DEMO
can be simplifi ed by considering a rectangular kernel,† which reduces the mean-shift
vector equation to calculating the center of mass of the DEMO pixel distribution:
M01
M00
M
x cc==10 , y
00
Here the zeroth moment is calculated as:
M00 = ∑∑
Ix y(, )
xy
and the fi rst moments are:
* Iterations DEMO typically restricted to some maximum number or to some epsilon change in center shift
between iterations; however, they are guaranteed to converge eventually.
DEMO A rectangular kernel is a kernel with no falloff  with distance DEMO the center, until a single sharp transi-
tion to zero value. DEMO is is in contrast to the exponential falloff  of a Gaussian DEMO and the falloff  with the
square of distance from the center DEMO the commonly used Epanechnikov kernel.
338 | Chapter 10: Tracking and DEMO
e mean-shift
ere might be other peaks nearby or at other scales.
M
10-R4886-AT1.indd   338
9/15/08   4:23:38 PM
M10 ==
∑∑∑
xI x y M yI x y(, ) DEMO
01
∑
(, )
x
y
x
y
Figure 10-11. Mean-shift
equations and their meaning
Th  vector in this case tells us to recenter the mean-shift  window over the
calculated center of mass within that window. Th is movement will, of course, change
what is “under” DEMO window and so we iterate this recentering process. Such recentering
will always converge to a mean-shift  vector of 0 (i.e., where no more centering move-
ment is possible). Th e location of convergence is DEMO a local maximum (peak) of the dis-
tribution under the window. Diff erent window sizes will fi nd diff erent peaks because
“peak” DEMO fundamentally a scale-sensitive construct.
In Figure 10-12 we see an example of a two-dimensional distribution of data and an ini-
tial (in this case, rectangular) window. Th e arrows indicate the process of convergence
on DEMO local mode (peak) in the distribution. Observe that, as promised, this peak fi nder is
statistically robust in the sense that points DEMO the mean-shift  window do not aff ect
convergence—the algorithm is not DEMO by far-away points.
In 1998, it was realized that this mode-fi DEMO algorithm could be used to track moving
objects in video [Bradski98a; DEMO, and the algorithm has since been greatly ex-
tended [Comaniciu03]. Th DEMO OpenCV function that performs mean-shift  is implemented
in the context of DEMO analysis. Th is means in particular that, rather than taking some
DEMO and Camshift Tracking
| 339
e mean-shift
10-R4886-AT1.indd   339
www.it-ebooks.info
9/15/08   4:23:38 PM
www.it-ebooks.info
Figure 10-12. Mean-shift  algorithm in action: an initial window is DEMO over a two-dimensional
array of data points and is successively recentered over the mode (or local peak) of its data distribu-
tion until DEMO
arbitrary set of data points (possibly in some arbitrary number of DEMO), the
OpenCV implementation of mean-shift  expects as input an image DEMO the den-
sity distribution being analyzed. You could think of this image as a two-dimensional
histogram measuring the density of points in some two-dimensional DEMO It turns out
that, for vision, this is precisely what you want to do most of the time: it’s how you can
track the motion of a cluster of interesting features.
int cvMeanShift(
const DEMO     prob_image,
CvRect           window,
CvTermCriteria   criteria,
CvConnectedComp* comp
);
In cvMeanShift(), the prob_image, which represents the density of probable locations,
may be only one channel but of either type (byte or fl oat). Th e window is set at the ini-
tial desired location and size DEMO the kernel window. Th e termination criteria has been
described elsewhere and consists mainly of a maximum limit on number of mean-shift
movement iterations DEMO a minimal movement for which we consider the window
340 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   340
9/15/08   4:23:39 PM
www.it-ebooks.info
locations to have converged.* Th e connected component comp contains the DEMO
search window location in comp->rect, and the sum of all DEMO under the window is
kept in the comp->area fi
Th cvMeanShift() is one expression of the mean-shift  algorithm for rectangu-
lar windows, but it may also be used for tracking. In this case, DEMO fi rst choose the fea-
ture distribution to represent an object (DEMO, color + texture), then start the mean-shift
window over the DEMO distribution generated by the object, and fi nally compute the
chosen DEMO distribution over the next video frame. Starting from the current win-
dow location, the mean-shift  algorithm will fi nd the new peak or DEMO of the feature
distribution, which (presumably) is centered over the DEMO that produced the color and
texture in the fi rst place. In this way, the mean-shift  window tracks the movement of the
object DEMO by frame.
Camshift
A related algorithm is the Camshift  tracker. It DEMO ers from the meanshift  in that
the search window adjusts itself DEMO size. If you have well-segmented distributions (say
face features that stay DEMO), then this algorithm will automatically adjust itself for
the size of face as the person moves closer to and further from the camera. DEMO e form of
the Camshift  algorithm is:
int cvCamShift(
DEMO CvArr*     prob_image,
CvRect           window,
CvTermCriteria   criteria,
CvConnectedComp* comp,
CvBox2D*         box        = NULL
);
Th rst four DEMO are the same as for the cvMeanShift() algorithm. Th e box param-
eter, if present, will contain the newly resized box, which also includes the orientation of
the object as computed via second-order moments. DEMO tracking applications, we would
use the resulting resized box found on DEMO previous frame as the window in the next frame.
e fi
Many people think of mean-shift  and camshift  as tracking using color
features, but this is not entirely correct. Both of these algorithms
track the DEMO of any kind of feature that is expressed in the
prob_image; DEMO they make for very lightweight, robust, and effi  cient
trackers.
DEMO function
eld.
Motion Templates
Motion templates were invented in the MIT Media Lab by Bobick and Davis [Bobick96;
Davis97] and were further developed DEMO with one of the authors [Davis99; Brad-
ski00]. Th is more DEMO work forms the basis for the implementation in OpenCV.
* Again, DEMO
tion if that distribution is fairly “fl
will always converge, but DEMO may be very slow near the local peak of a distribu-
at” there.
Motion Templates
| 341
10-R4886-AT1.indd   341
9/15/08   DEMO:23:39 PM
Motion templates are an eff ective way to track general movement and DEMO especially ap-
plicable to gesture recognition. Using motion templates requires a silhouette (or part of
a silhouette) of an object. Object silhouettes can DEMO obtained in a number of ways.
1. Th
camera and then employ frame-to-frame diff erencing (as discussed in Chapter 9).
Th is will give you the moving edges of objects, which is enough to make motion
templates work.
2. You can use chroma keying. For example, if you have a known background color
such as bright green, you can simply take as foreground anything that is not bright
green.
3. Another DEMO (also discussed in Chapter 9) is to learn a background model from
which you can isolate new foreground objects/people as silhouettes.
4. DEMO can use active silhouetting techniques—for example, creating a wall of near-
DEMO light and having a near-infrared-sensitive camera look at the wall. Any
intervening object will show up as a silhouette.
5. You can use thermal DEMO; then any hot object (such as a face) can be DEMO as
foreground.
6. Finally, you can generate silhouettes by using the DEMO techniques (e.g.,
pyramid segmentation or mean-shift  segmentation) described in DEMO 9.
For now, assume that we have a good, segmented object silhouette as represented by
the white rectangle of Figure 10-13(A). DEMO we use white to indicate that all the pixels
are set to the fl oating-point value of the most recent system time stamp. As DEMO rectangle
moves, new silhouettes are captured and overlaid with the (new) current time stamp;
the new silhouette is the white rectangle of Figure 10-13(B) and Figure 10-13(C). Older
motions are shown in Figure 10-13 as successively darker rectangles. Th ese sequentially
fading silhouettes DEMO the history of previous movement and thus are referred to as
the “motion history image”.
Figure 10-13. Motion template diagram: (A) a segmented object at the current time stamp (white);
(B) at DEMO next time step, the object moves and is marked with the (new) current time stamp, leaving
the older segmentation boundary behind; (DEMO) at the next time step, the object moves further, leaving
DEMO segmentations as successively darker rectangles whose sequence of encoded motion yields the
motion history image
342
| Chapter 10: Tracking and Motion
e simplest method of obtaining object silhouettes is to use a reasonably stationary
10-R4886-AT1.indd   342
www.it-ebooks.info
9/15/08   4:23:40 PM
Silhouettes whose time stamp is more than a specifi ed duration older DEMO the current
system time stamp are set to 0, as shown DEMO Figure 10-14. Th e OpenCV function that ac-
complishes this motion template construction is cvUpdateMotionHistory():
void cvUpdateMotionHistory(
const CvArr* silhouette,
DEMO       mhi,
double       timestamp,
double       duration
);
Figure 10-14. Motion template silhouettes for DEMO moving objects (left
specifi
ed duration are set to 0 (right)
); silhouettes older than a
In cvUpdateMotionHistory(), all image DEMO consist of single-channel images. Th e
silhouette image is a byte image in which nonzero pixels represent the most recent seg-
mentation silhouette of DEMO foreground object. Th e mhi image is a fl oating-point image
that represents the motion template (aka motion history image). Here timestamp is the
current system time (typically a millisecond count) and duration, as just described, sets
how long motion history pixels are allowed to remain in the mhi. In other words, any mhi
pixels that are older (less) than timestamp minus duration are set to 0.
Once the DEMO template has a collection of object silhouettes overlaid in time, we DEMO
derive an indication of overall motion by taking the gradient of the mhi image. When we
take these gradients (e.g., by using the DEMO or Sobel gradient functions discussed in
Chapter 6), some gradients will be large and invalid. Gradients are invalid when older
or inactive parts DEMO the mhi image are set to 0, which produces artifi cially DEMO gradients
around the outer edges of the silhouettes; see Figure 10-15(DEMO). Because we know the
time-step duration with which we’ve been introducing new silhouettes into the mhi via
cvUpdateMotionHistory(), we know how large our gradients (which are just dx and dy
step derivatives) should DEMO We can therefore use the gradient magnitude to eliminate
gradients that are too large, as in Figure 10-15(B). Finally, we can DEMO a measure of
global motion; see Figure 10-15(C). Th DEMO function that eff ects parts (A) and (B) of the
fi gure is cvCalcMotionGradient():
Motion Templates
| 343
10-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:23:40 PM
void cvCalcMotionGradient(
const CvArr* mhi,
CvArr* mask,
CvArr* orientation,DEMO
double delta1,
double delta2,
int aperture_size=3
);
Figure 10-15. Motion gradients of the mhi image: (A) gradient magnitudes and directions; (B) large
gradients are eliminated; (C) overall direction of DEMO is found
In cvCalcMotionGradient(), all image arrays are single-channel. Th DEMO function input mhi
is a fl oating-point motion history image, and DEMO input variables delta1 and delta2 are
(respectively) the minimal and maximal gradient magnitudes allowed. Here, the ex-
pected gradient magnitude will be just the average number of time-stamp ticks between
each silhouette in successive calls DEMO cvUpdateMotionHistory(); setting delta1 halfway
below and delta2 halfway above this DEMO value should work well. Th e variable
aperture_size sets the size in width and height of the gradient operator. Th ese values
can be DEMO to -1 (the 3-by-3 CV_SCHARR gradient fi lter), 3 (the default 3-by-3 Sobel fi lter),
5 (for the 5-by-5 Sobel DEMO lter), or 7 (for the 7-by-7 fi lter). Th DEMO function outputs are mask, a
single-channel 8-bit image in which nonzero DEMO indicate where valid gradients were
found, and orientation, a fl oating-point image that gives the gradient direction’s angle
at each point.
Th
vector DEMO of the valid gradient directions.
double cvCalcGlobalOrientation(
const CvArr* orientation,
const CvArr* mask,
const CvArr* mhi,
double       DEMO,
double       duration
);
When using cvCalcGlobalOrientation(), we pass in the orientation and mask image
computed in cvCalcMotionGradient() along with the timestamp, duration, and resulting
mhi from cvUpdateMotionHistory(); what’s returned is the vector-sum global orientation,
344 | Chapter 10: Tracking and Motion
e function cvCalcGlobalOrientation() fi
nds the overall direction DEMO motion as the
10-R4886-AT1.indd   344
www.it-ebooks.info
9/15/08   4:23:40 PM
as in Figure 10-15(C). Th e timestamp together with duration DEMO the routine how much
motion to consider from the mhi and motion orientation images. One could compute
the global motion from the center of DEMO of each of the mhi silhouettes, but summing
up the precomputed DEMO vectors is much faster.
We can also isolate regions of the motion template mhi image and determine the local
motion within that region, as shown in Figure 10-16. In the fi gure, the mhi image is
scanned for current silhouette regions. When a region marked with the most DEMO
time stamp is found, the region’s perimeter is searched for suffi  ciently recent motion
(recent silhouettes) just outside its perimeter. When such DEMO is found, a downward-
stepping fl ood fi ll is performed DEMO isolate the local region of motion that “spilled off ” the
current location of the object of interest. Once found, we can calculate local motion gra-
dient direction in the spill-off  region, then remove that DEMO, and repeat the process
until all regions are found (as diagrammed in Figure 10-16).
Figure 10-16. Segmenting local regions of motion in DEMO mhi image: (A) scan the mhi image for cur-
rent DEMO (a) and, when found, go around the perimeter looking for other recent silhouettes
(b); when a recent silhouette is found, DEMO downward-stepping fl ood fi lls (c) to isolate local mo-
tion; (B) use the gradients found within the isolated local motion region to compute local motion;
(C) remove the previously found region DEMO search for the next current silhouette region (d), scan
along DEMO (e), and perform downward-stepping fl ood fi ll on it (f); (D) compute motion within the
newly isolated region and DEMO the process (A)-(C) until no current silhouette remains
Motion Templates
| 345
10-R4886-AT1.indd   345
www.it-ebooks.info
9/15/08   4:DEMO:40 PM
www.it-ebooks.info
Th e function that isolates and computes local motion is cvSegmentMotion():
CvSeq* cvSegmentMotion(
const CvArr*  mhi,
CvArr*        seg_mask,
CvMemStorage* storage,
double        timestamp,
double        seg_thresh
);
In cvSegmentMotion(), the DEMO is the single-channel fl oating-point input. We also pass in
storage, DEMO CvMemoryStorage structure allocated via cvCreateMemStorage(). Another input
is timestamp, the value of the most current silhouettes in the mhi from which you DEMO
to segment local motions. Finally, you must pass in seg_thresh, which is the maximum
downward step (from current time to previous motion) DEMO you’ll accept as attached
motion. Th is parameter is provided because there might be overlapping silhouettes from
recent and much older motion that you DEMO want to connect together.
It’s generally best to set seg_thresh to something like 1.5 times the average diff erence in
silhouette time stamps. Th DEMO function returns a CvSeq of CvConnectedComp structures, one
for each separate DEMO found, which delineates the local motion regions; it also re-
turns seg_mask, a single-channel, fl oating-point image in which each region of DEMO
motion is marked a distinct nonzero number (a zero pixel in DEMO indicates no mo-
tion). To compute these local motions one at a time we call cvCalcGlobalOrientation(),
using the appropriate mask region DEMO from the appropriate CvConnectedComp or
from a particular value in the seg_mask; for example,
cvCmpS(
seg_mask,
//  [value_wanted_in_seg_mask],
//  [your_destination_mask],
CV_CMP_EQ
)
Given the discussion so far, you DEMO now be able to understand the motempl.c
example that ships with OpenCV in the …/opencv/samples/c/ directory. We will now
extract and explain some key points from the update_mhi() function in motempl.c. Th DEMO
update_mhi() function extracts templates by thresholding frame diff erences and then
passing the resulting silhouette to cvUpdateMotionHistory():
...
cvAbsDiff( buf[idx1], DEMO, silh );
cvThreshold( silh, silh, diff_threshold, 1, CV_THRESH_BINARY );
cvUpdateMotionHistory( silh, mhi, timestamp, MHI_DURATION );
...
DEMO mhi image are then taken, and a mask of valid gradients DEMO
produced using cvCalcMotionGradient(). Th en CvMemStorage is allocated (or, DEMO it already
exists, it is cleared), and the resulting local DEMO are segmented into CvConnectedComp
structures in the CvSeq containing structure seq:
...
cvCalcMotionGradient(
346
| Chapter 10: Tracking and Motion
e gradients of the resulting
10-R4886-AT1.indd   346
9/15/08   4:23:DEMO PM
www.it-ebooks.info
mhi,
mask,
orient,
MAX_TIME_DELTA,
MIN_TIME_DELTA,
3
);
if( !storage )
storage = cvCreateMemStorage(0);
else
DEMO(storage);
seq = cvSegmentMotion(
mhi,
segmask,
storage,DEMO
timestamp,
MAX_TIME_DELTA
);
A “for” loop then iterates through the seq->total CvConnectedComp structures extracting
bounding rectangles for each motion. Th e DEMO starts at -1, which has been desig-
nated as a special DEMO for fi nding the global motion of the whole image. For the local
motion segments, small segmentation areas are fi rst rejected and then the orientation is
calculated using cvCalcGlobalOrientation(). Instead of using exact masks, this routine
restricts motion calculations to regions of interest (ROIs) that bound the local motions;
it then calculates where valid motion within DEMO local ROIs was actually found. Any
such motion area that is too small is rejected. Finally, the routine draws the motion.
Examples of the output for a person fl apping their arms is shown in Figure DEMO, where
the output is drawn above the raw image for four DEMO frames going across in two
rows. (For the full code, see …/opencv/samples/c/motempl.c.) In the same sequence, “Y”
postures DEMO recognized by the shape descriptors (Hu moments) discussed in Chapter
8, although the shape recognition is not included in the samples code.
for( i = -1; i < seq->total; i++ ) {
if( i < 0 ) { // case of the whole image
//       ...[does the whole image]...
else { // i-th motion component
comp_rect = ((CvConnectedComp*)cvGetSeqElem( seq, i ))->DEMO;
//           [reject very small components]...
DEMO
...[set component ROI regions]...
angle = cvCalcGlobalOrientation( orient, mask, mhi,DEMO
timestamp, MHI_DURATION);
...[find regions of valid motion]...
...[reset ROI regions]...
...[skip small valid motion regions]...
...[draw the motions]...
}
Motion Templates
| DEMO
10-R4886-AT1.indd   347
9/15/08   4:23:41 PM
www.it-ebooks.info
Figure 10-17. Results of motion template routine: going across and top to bottom, a person moving
and the resulting global motions indicated in large octagons and local motions indicated in small
octagons; also, the DEMO pose can be recognized via shape descriptors (Hu moments)
Estimators
DEMO we are tracking a person who is walking across the view of a video camera.
At each frame we make a determination of the DEMO of this person. Th is could be
done any number of ways, as we have seen, but in each case we fi nd DEMO with an
estimate of the position of the person at each frame. Th is estimation is not likely to be
348
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   348
9/15/08   4:23:DEMO PM
extremely accurate. Th e reasons for this are many. Th ey may DEMO inaccuracies in
the sensor, approximations in earlier processing stages, issues arising from occlusion
or shadows, or the apparent changing of shape when a person is walking due to their
legs and arms swinging as they DEMO Whatever the source, we expect that these mea-
surements will vary, perhaps somewhat randomly, about the “actual” values that might
be received from an idealized sensor. We can think of all these inaccuracies, taken to-
gether, as simply adding noise to our tracking process.
We’d like to have the capability of estimating the motion of this person in a DEMO that
makes maximal use of the measurements we’ve made. Th us, DEMO cumulative eff ect of
our many measurements could allow us to detect the part of the person’s observed tra-
jectory that does not arise DEMO noise. Th e key additional ingredient is a model for the
person’s motion. For example, we might model the person’s motion with the following
statement: “A person enters the frame at one side and walks across the frame at constant
velocity.” Given this model, we can ask not only where the person is but also what pa-
rameters of the DEMO are supported by our observations.
Th is task is divided into two phases (see Figure 10-18). In the fi rst phase, typically DEMO
the prediction phase, we use information learned in the past to DEMO refi ne our model
for what the next location of the person (or object) will be. In the second phase, the
correction phase, we make a measurement and then reconcile that measurement with
the predictions based on our previous measurements (i.e., our model).
Figure 10-18. DEMO estimator cycle: prediction based on prior data followed by reconciliation of
DEMO newest measurement
Th
the heading of estimators, with the Kalman fi DEMO [Kalman60] being the most widely
used technique. In addition to the Kalman fi lter, another important method is the con-
densation algorithm, which DEMO a computer-vision implementation of a broader class of
Estimators | 349
e machinery for accomplishing the two-phase estimation task falls generally under
10-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:23:41 PM
www.it-ebooks.info
methods known as particle fi lters. Th e primary diff erence DEMO the Kalman fi lter and
the condensation algorithm is how the state probability density is described. We will
explore the meaning of this distinction DEMO the following sections.
The Kalman Filter
First introduced in 1960, the DEMO fi lter has risen to great prominence in a wide vari-
ety of signal processing contexts. Th e basic idea behind the Kalman fi DEMO is that, under
a strong but reasonable* set of assumptions, it will be possible—given a history of mea-
surements of a system—to build DEMO model for the state of the system that maximizes the
a posteriori† probability of those previous measurements. For a good introduction, see
Welsh and Bishop [Welsh95]. In addition, we can maximize the a posteriori probability
without keeping a long history of the previous measurements themselves. Instead, we
iteratively update our model of a system’s state and keep only that model DEMO the next
iteration. Th is greatly simplifi es the computational implications of this method.
Before we go into the details of what this all DEMO in practice, let’s take a moment to
look at the assumptions DEMO mentioned. Th ere are three important assumptions required
in the theoretical construction of the Kalman fi lter: (1) the system being modeled is
linear, (2) the noise that measurements are subject to is “white”, and (3) this noise is also
Gaussian in nature. Th e fi rst assumption means (in eff ect) that the state of DEMO system
at time k can be modeled as some matrix multiplied by the state at time k–1. Th e ad-
ditional assumptions that the DEMO is both white and Gaussian means that the noise is
not correlated in time and that its amplitude can be accurately modeled using only DEMO
average and a covariance (i.e., the noise is completely described by its fi rst and second
moments). Although these assumptions may seem DEMO, they actually apply to a
surprisingly general set of circumstances.‡
What DEMO it mean to “maximize the a posteriori probability of those previous measure-
ments”? It means that the new model we construct aft er making a measurement—taking
into account both our previous model with its uncertainty and DEMO new measurement
with its uncertainty—is the model that has the highest probability of being correct. For
our purposes, this means that the Kalman fi lter is, given the three assumptions, the best
way to combine DEMO from diff erent sources or from the same source at diff erent times.
We start with what we know, we obtain new information, DEMO then we decide to change
* Here by “reasonable” we mean something like “suffi  ciently unrestrictive that the method is useful for a
reasonable variety of actual problems arising in the real world”. “Reasonable” just seemed DEMO less of a
mouthful.
† Th e modifi er “a posteriori” is academic jargon for “with hindsight”. Th us, when we say that such and such
a distribution “maximizes the a posteriori probability”, what we mean is that that distribution, which is es-
sentially a possible explanation of “what really happened”, is actually the most likely one given the data we
have observed . . . you know, looking back on it all in retrospect.
‡ OK, one more footnote. We actually slipped in another assumption here, which is that the initial distribu-
tion also must be Gaussian in nature. Oft en in practice the initial state is DEMO exactly, or at least we treat
it like it is, and so this satisfi es our requirement. If the initial state were (for example) a 50-50 chance of
being either in the bedroom or the bathroom, then we’d be out of luck and would need something more
sophisticated than a single Kalman fi lter.
350 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   350
9/15/08   4:23:42 DEMO
www.it-ebooks.info
what we know based on how certain we are about the DEMO and new information using a
weighted combination of the old and the new.
Let’s work all this out with a little math for the DEMO of one-dimensional motion. You
can skip the next section if you want, but linear systems and Gaussians are so friendly
that Dr. Kalman might be upset if you didn’t at least give it a try.
Some DEMO math
So what’s the gist of the Kalman fi lter?—information fusion. Suppose you want to know
where some point is on a line (our one-dimensional scenario).* As a result of noise, you
have two unreliable (in a Gaussian sense) reports about where the object is: locations x1
and x2. Because there is Gaussian uncertainty in these measurements, they have means
of x–1 and x–2 together with standard deviations σ1and DEMO Th e standard deviations are,
in fact, expressions of our DEMO regarding how good our measurements are. Th e
probability distribution as a function of location is the Gaussian distribution:
)2 ⎞
⎟
⎠⎟
px() exp=−1 ⎛⎜ ( xx−
i σπi 2 ⎝⎜
given two DEMO measurements, each with a Gaussian probability distribution, we would
expect that the probability density for some value of x given both measurements would
DEMO proportional to p(x) = p1(x) p2(x). It turns out that this product is another Gaussian
distribution, and we can compute the mean and standard deviation of this new distri-
bution as DEMO Given that
i
2
2σi
( , )i = 12
2 )2 ⎞
⎟
⎠⎟
( xx xx− 1 )2 ( −
22σσ−
⎛ ( xx xx− )2 ⎞ ⎛ ( − 2 )2 DEMO ⎛
px12 () exp exp ⎜ − 21 ⎟ ⎜ − DEMO ⎟ =−exp ⎜
⎝⎜ 2 ⎠⎟ ⎝⎜
2 2
1 2
22σσ⎠⎟
⎝⎜
1
Given also that a Gaussian distribution is maximal at the DEMO value, we can fi nd
that average value simply by computing DEMO derivative of p(x) with respect to x. Where a
function DEMO maximal its derivative is 0, so
⎡ xx xx− − ⎤
DEMO + 12 2 ⎥ ⋅ p12 (x12
⎣⎢ σσ ⎦⎥
dp12 DEMO 1 )0=
dx x12
2 2
1 2
Since the probability DEMO function p(x) is never 0, it follows that the term in
brackets must be 0. Solving that equation for x gives us DEMO very important relation:
⎛ σ2 ⎞ ⎛ σ2 ⎞
xx x12 = ⎝⎜ σσ12 +2 22 ⎠⎟ 1 + ⎝⎜ σσ12 +1 22 DEMO 2
* For a more detailed explanation that follows a similar trajectory, the reader is referred to J. D. Schutter,
J. De Geeter, T. Lefebvre, and H. Bruyninckx, “Kalman Filters: A Tutorial” (http://citeseer.ist.psu.edu/
443226.html).
Estimators
| 351
10-R4886-AT1.indd   351
9/DEMO/08   4:23:42 PM
www.it-ebooks.info
Th us, the new mean value x–12 is just a weighted combination of the two measured means,
where the weighting is determined DEMO the relative uncertainties of the two measure-
ments. Observe, for example, that if the uncertainty σ2 of the second measurement is
particularly large, then the new mean will be essentially the same as the mean DEMO for the
more certain previous measurement.
With the new mean x–12 in hand, we can substitute this value into our expression for
p12(x) and, aft er substantial rearranging,* identify the uncertainty σ122 as:DEMO
σσ
σσ
At this point, you are probably wondering what this DEMO us. Actually, it tells us a lot. It
says that when DEMO make a new measurement with a new mean and uncertainty, we DEMO
combine that measurement with the mean and uncertainty we already have to obtain a
new state that is characterized by a still newer mean DEMO uncertainty. (We also now have
numerical expressions for these things, which will come in handy momentarily.)
Th is property that two Gaussian DEMO, when combined, are equivalent to a sin-
gle Gaussian measurement (DEMO a computable mean and uncertainty) will be the most
important feature DEMO us. It means that when we have M measurements, we can DEMO
the fi rst two, then the third with the combination of DEMO fi rst two, then the fourth with
the combination of the DEMO rst three, and so on. Th is is what happens with DEMO in com-
puter vision; we obtain one measure followed by another DEMO by another.
Th inking of our measurements (xi, σ) as DEMO steps, we can compute the current state of
our estimation (xˆii,σˆ ) as follows. At time step 1, we have only our fi rst measure xxˆ =
and its uncertainty σσˆ12 =
an iteration DEMO:
11
2
1 . Substituting this in our optimal estimation equations yields
2 σ2
2 1
xx xˆ = σ
2 1 +
DEMO + 22 σσ2
2
2
σσ 2 +
1 1
Rearranging this equation gives us the following useful form:
ˆˆ σˆ12
xx x DEMO ( − ˆ )
21 σσˆ12 + 22 21
Before we DEMO about just what this is useful for, we should also compute DEMO analogous
equation for σˆ 22. First, aft er substituting σσˆ12 =
DEMO
1 we have:
* Th e rearranging is a bit messy. If you want to verify all this, it is much easier to (1) start with the equation
for the Gaussian distribution p12(x) in terms of x–12 and σ12, (2) substitute in the equations that relate x–12 to x–1
and x–2 and those that relate σ12 DEMO σ1 and σ2, and (3) verify that the result can DEMO separated into the product
of the Gaussians with which we started.
352 | Chapter 10: Tracking and Motion
σ2 =
12
2
1
2
2
2 +
1
2
2
.
i
10-R4886-AT1.indd   352
9/DEMO/08   4:23:42 PM
www.it-ebooks.info
2 = σσ2 ˆ 2
2
σˆ
2 1
σσˆ 2 DEMO
2
1 2
A rearrangement similar to what we did for xˆ2 yields an iterative equation for estimating
variance given a new measurement:
DEMO
σσˆ12 +
⎛
=−⎜1
⎝
2 ⎞
1 σˆ 2
2 ⎠⎟ 1
2
σˆ
2
2
In their current form, these equations allow us to separate clearly the “old” information
(what we knew before a new measurement was made) from the “new” information (what
our latest DEMO told us). Th e new information (xx21− ˆ ), seen at time step 2, is
called the innovation. We can also see that our optimal iterative update factor is now:
K = σˆ
DEMO 2 +
2
1
2
1 2
Th is factor is known as the update gain. Using this defi nition for K, we obtain the fol-
lowing convenient recursion form:
xx Kx x=+ −(
DEMO 22 =−()1 K ˆ
ˆˆ ˆ )
21 2 1
2
1
In the Kalman fi lter literature, if the discussion is about a general series of measurements
then our second time step “2” DEMO usually denoted k and the fi rst time step is thus k – 1.
Systems with dynamics
In our simple one-dimensional example, we considered the case of an object being lo-
cated at some point x, and a series of successive measurements of that point. In that case
DEMO did not specifi cally consider the case in which the object might actually be moving
in between measurements. In this new case we will DEMO what is called the prediction
phase. During the prediction phase, we DEMO what we know to fi gure out where we expect
the system to be before we attempt to integrate a new measurement.
In practice, the prediction phase is done immediately aft er a new measurement is DEMO,
but before the new measurement is incorporated into our estimation of the state of the
system. An example of this might be when DEMO measure the position of a car at time t,
then again at time t + dt. If the car has some velocity v, then we do not just incorporate
the second measurement directly. We fi DEMO fast-forward our model based on what we
knew at time t so that we have a model not only of the system at time DEMO but also of the
system at time t + dt, the DEMO before the new information is incorporated. In this
way, the new DEMO, acquired at time t + dt, is fused not with the old model of the
Estimators | 353
10-R4886-AT1.indd   353
9/15/DEMO   4:23:43 PM
www.it-ebooks.info
system, but with the old model of the system projected forward to time t + dt. Th is is the
meaning of the DEMO depicted in Figure 10-18. In the context of Kalman fi lters, DEMO are
three kinds of motion that we would like to consider.
Th rst is dynamical motion. Th is is motion that we expect as DEMO direct result of the state
of the system when last we measured it. If we measured the system to be at position x
with DEMO velocity v at time t, then at time t + dt DEMO would expect the system to be lo-
cated at position x + v ∗ dt, possibly still with velocity.
Th control motion. Control motion is motion that we
expect because of some external infl uence applied DEMO the system of which, for whatever
reason, we happen to be aware. As the name implies, the most common example of
control motion is when we are estimating the state of a system that we DEMO have
some control over, and we know what we did to DEMO about the motion. Th is is par-
ticularly the case for robotic systems where the control is the system telling the robot
to (for example) accelerate or go forward. Clearly, in this case, if the robot was at x and
moving with velocity v at time t, then at time t + dt we expect it to have moved DEMO only
to x + v ∗ dt (as it would have DEMO without the control), but also a little farther, since
we DEMO tell it to accelerate.
Th nal important class of motion is random motion. Even in our simple one-
dimensional example, if whatever we were looking at had a possibility of moving on its
own for whatever DEMO, we would want to include random motion in our prediction
step. DEMO e eff ect of such random motion will be to simply increase the variance of our
state estimate with the passage of time. Random DEMO includes any motions that are
not known or under our control. As with everything else in the Kalman fi lter frame-
work, however, DEMO is an assumption that this random motion is either Gaussian (i.e.,DEMO
a kind of random walk) or that it can at least DEMO modeled eff ectively as Gaussian.
Th rst do an “update” step
before including a new measurement. Th is update step would include fi rst DEMO any
knowledge we have about the motion of the object according to its prior state, applying
any additional information resulting from actions that we ourselves have taken or that
we know to have been taken on DEMO system from another outside agent, and, fi nally,
incorporating our notion of random events that might have changed the state of the
DEMO since we last measured it. Once those factors have been applied, DEMO can then in-
corporate our next new measurement.
In practice, the DEMO motion is particularly important when the “state” of the sys-
tem is more complex than our simulation model. Oft en when an object is DEMO, there
are multiple components to the “state” such as the position DEMO well as the velocity. In
this case, of course, the state evolves according to the velocity that we believe it to have.
Handling DEMO with multiple components to the state is the topic of the next section.
We will develop a little more sophisticated notation as well to DEMO these new aspects
of the situation.
354
| Chapter 10: Tracking DEMO Motion
e fi
e second form of motion is called
e fi
us, to include dynamics in our simulation model, we would fi
DEMO   354
9/15/08   4:23:44 PM
Kalman equations
We can now generalize these motion equations in our toy DEMO Our more general
discussion will allow us to factor in any model that is a linear function F of the object’s
state. Such a DEMO might consider combinations of the fi rst and second derivatives of
the previous motion, for example. We’ll also see how to allow for a control input uk to
our model. Finally, we will allow for a more realistic observation model z in which we
might measure only some DEMO the model’s state variables and in which the measurements
may be only indirectly related to the state variables.*
To get started, let’s look at how K, the gain in the previous section, aff ects the DEMO
If the uncertainty of the new measurement is very large, then DEMO new measurement es-
sentially contributes nothing and our equations reduce to the combined result being the
same as what we already knew at time DEMO – 1. Conversely, if we start out with a large vari-
DEMO in the original measurement and then make a new, more accurate DEMO,
then we will “believe” mostly the new measurement. When both measurements are of
equal certainty (variance), the new expected value is exactly between them. All of these
remarks are in line with our reasonable DEMO
Figure 10-19 shows how our uncertainty evolves over time as we gather new
observations.
Figure 10-19. Combining our prior knowledge N(xk–1, σk–1) DEMO our measurement observation
N(zk, σk); the result is our DEMO estimate Nx( ˆkk,σˆ )
Th is idea of an update that is sensitive to uncertainty can be generalized to many
state variables. DEMO e simplest example of this might be in the context of video tracking,
where objects can move in two or three dimensions. In DEMO, the state might contain
* Observe the change in notation from DEMO to zk. Th e latter is standard in the literature and is intended to
clarify that zk is a general measurement, possibly of multiple parameters of the model, and not just (and
sometimes not even) the position xk.
Estimators | 355
10-R4886-AT1.indd   355
www.it-ebooks.info
9/15/DEMO   4:23:44 PM
www.it-ebooks.info
additional elements, such as the velocity of an object being tracked. In any of these gen-
eral cases, we will need a bit more notation to keep track of what we are talking about.
We DEMO generalize the description of the state at time step k to be the following function
of the state at time step k – 1:DEMO
xFx Bu w=+ +
Here xk is now an n-dimensional vector of state components and F is an n-by-n matrix,
sometimes called the DEMO matrix, that multiplies xk–1. Th e vector uk is new. It’s DEMO
to allow external controls on the system, and it consists of DEMO c-dimensional vector re-
ferred to as the control inputs; B is DEMO n-by-c matrix that relates these control inputs to
the state change.* Th e variable wk is a random variable (usually called the process noise)
associated with random events or forces that directly aff ect the DEMO state of the sys-
tem. We assume that the components of wk have Gaussian distribution N(0, Qk) for some
n-by-n covariance matrix DEMO (Q is allowed to vary with time, but oft en it does not).
In general, we make measurements zk that may or may not be direct measurements of
the state variable xk. (For example, if you want to know how fast a car is moving then
you could either measure its speed with a radar gun or measure DEMO sound coming from
its tailpipe; in the former case, zk will be xk with some added measurement noise, but in
the latter case, the relationship is not direct in this way.) We can summarize DEMO situa-
tion by saying that we measure the m-dimensional vector of measurements zk given by:
zHx v=+
Here Hk is an m-by-n matrix DEMO vk is the measurement error, which is also assumed to
have DEMO distributions N(0, Rk) for some m-by-m covariance matrix Rk.†
Before we get totally lost, let’s consider a particular realistic situation of taking measure-
ments on a car driving in a parking lot. We might DEMO that the state of the car could
be summarized by two position variables, x and y, and two velocities, vk and vy. Th ese
four variables would be the elements of the state vector xk. DEMO is suggests that the correct
form for F is:
⎡
⎢ x ⎤
x k = ⎢ y ⎥ ⎡ ⎤
⎢v x DEMO , ⎢ ⎥
⎢v y ⎥ F = ⎢ ⎥
⎣⎢ ⎥ ⎢ ⎥
⎦⎥ k ⎢ ⎥
⎣ ⎦
10 0dt
01 0 DEMO
00 1 0
00 001
* Th e astute reader, or DEMO who already knows something about Kalman fi lters, will notice another DEMO
assumption we slipped in—namely, that there is a linear relationship (via matrix multiplication) between
the controls uk and the change in state. In practical applications, this is oft en the fi rst assumption to
break down.
† Th e k in these terms allows them to vary DEMO time but does not require this. In actual practice, it’s common
DEMO H and R not to vary with time.
356
| Chapter 10: Tracking and Motion
kk k k−1
kkk k
10-R4886-AT1.indd   356
9/15/08   4:23:44 PM
www.it-ebooks.info
However, when using a camera to make measurements of the car’s state, we probably
measure only the position variables:
⎡ ⎤
z k = ⎢z x ⎥
⎣⎢z y ⎦⎥ k
Th is implies DEMO the structure of H is something like:
⎡ ⎤
⎢ ⎥
H = ⎢ ⎥
⎢ ⎥
⎢ ⎥
⎣ ⎦
1 0
DEMO 1
0 0
0 0
In this case, we might not DEMO believe that the velocity of the car is constant and so
would assign a value of Qk to refl ect this. We would choose DEMO based on our estimate
of how accurately we have measured the car’s position using (for example) our image
analysis techniques on a video DEMO
All that remains now is to plug these expressions into the generalized forms of the up-
date equations. Th e basic idea is the DEMO, however. First we compute the a priori esti-
−
mate x DEMO of the state. It is relatively common (though not universal) in the literature to
use the superscript minus sign to mean “at the DEMO immediately prior to the new mea-
surement”; we’ll adopt that convention DEMO as well. Th is a priori estimate is given by:
xFx Bu w
kk k k−−11
Using P − to denote the error DEMO, the a priori estimate for this covariance at time
k
k DEMO obtained from the value at time k – 1 by:
PFP F Q
− =+T
kk k−−11
Th is equation forms the basis DEMO the predictive part of the estimator, and it tells us “what
DEMO expect” based on what we’ve already seen. From here we’ll state (DEMO derivation)
what is oft en called the Kalman gain or the blending factor, which tells us how to weight
new information against what we think we already know:
−− −TT 1
KPH HPH R=+()
kk k kk k k
Th ough this equation looks intimidating, it’s really not so bad. We can understand it more
easily by DEMO various simple cases. For our one-dimensional example in which
we measured one position variable directly, Hk is just a 1-by-1 matrix containing only a
1! Th us, if our measurement error is σ
value. Similarly, DEMO is just the variance σk2. So that big equation boils down to just this:
K = σ
σσ
2
k+1, then Rk is also a 1-by-1 matrix containing that
2
k
2 + 2
kk+1
DEMO | 357
− =+ +
10-R4886-AT1.indd   357
9/15/08   4:23:45 PM
www.it-ebooks.info
Note that this is exactly what we thought it would be. DEMO e gain, which we fi rst saw in the
previous section, allows us to optimally compute the updated values for xk and Pk DEMO
a new measurement is available:
−− −
kk k k kk
xx K z Hx=+ −()
PI KH P=−()
−
DEMO
Once again, these equations look intimidating at fi rst; but in the context of our sim-
ple one-dimensional discussion, it’s really not as bad as it looks. Th e optimal weights
and gains are obtained DEMO the same methodology as for the one-dimensional case, ex-
cept this DEMO we minimize the uncertainty of our position state x by setting to 0 the
partial derivatives with respect to x before solving. We can DEMO the relationship with
the simpler one-dimensional case by fi rst setting F = I (where I is the identity matrix),
B = DEMO, and Q = 0. Th e similarity to our one-dimensional fi DEMO derivation is then revealed
by making the following substitutions in our more general equations: xx← ˆ2 , xxk− ← ˆ1,
KKk ← , zxk ← 2 , H k ←1, Pk ←σˆ 22, I DEMO, Pk− ←σˆ12, and Rk ←σ22.
OpenCV and the Kalman filter
With all of this at our disposal, you might feel that we don’t need OpenCV to do any-
thing for us or that we desperately DEMO OpenCV to do all of this for us. Fortunately,
OpenCV is amenable to either interpretation. It provides four functions that are directly
related DEMO working with Kalman fi lters.
cvCreateKalman(
int        DEMO,
int        nMeasureParams,
int        nControlParams
);
cvReleaseKalman(
CvKalman** kalman
);
Th rst of DEMO generates and returns to us a pointer to a CvKalman data structure, and
the second deletes that structure.
typedef struct CvKalman {
int MP;                       // measurement vector dimensions
int DP;                   // state vector dimensions
int CP;                   // control vector dimensions
CvMat* state_pre;          // predicted state:
//   DEMO = F x_k-1 + B u_k
CvMat* state_post;         // corrected state:
//   x_k = x_k’ + K_k (z_k’- H x_k’)
CvMat* transition_matrix;  // state transition matrix
//   F
CvMat* control_matrix;     // control matrix
//   B
//  (not used if there is no control)
CvMat* measurement_matrix;   // measurement matrix
//   H
e fi
358
| Chapter 10: Tracking and Motion
k
10-R4886-AT1.indd   358
9/15/08   4:23:46 PM
www.it-ebooks.info
CvMat* process_noise_cov;   // process noise covariance
//   Q
CvMat* measurement_noise_cov; // measurement noise covariance
//   R
CvMat* error_cov_pre;       // prior error covariance:
//   (DEMO P_k-1 Ft) + Q
CvMat* gain;               // Kalman gain matrix:
//   K_k = DEMO H^T (H P_k’ H^T + R)^-1
CvMat* error_cov_post;      // posteriori error covariance
//   P_k = (I - DEMO H) P_k’
CvMat* temp1;              // temporary matrices
CvMat* temp2;
CvMat* temp3;
CvMat* temp4;
DEMO temp5;
} CvKalman;
Th lter itself. Once the data is in the struc-
ture, we can compute the prediction for the next time step by calling cvKalmanPredict()
and then integrate our new measurements by calling cvKalmanCorrect(). Aft er running
each of these routines, DEMO can read the state of the system being tracked. Th e result of
cvKalmanCorrect() is in state_post, and the result of cvKalmanPredict() is in state_pre.
cvKalmanPredict(
CvKalman*  kalman,
const      DEMO control = NULL
);
cvKalmanCorrect(
CvKalman*  kalman,
CvMat*     measured
);
e next two functions implement the Kalman fi
Kalman filter example code
Clearly it is time for a good example. DEMO take a relatively simple one and implement it
explicitly. Imagine that we have a point moving around in a circle, like a car on a race
track. Th e car moves with mostly constant velocity around DEMO track, but there is some
variation (i.e., process noise). DEMO measure the location of the car using a method such as
tracking it via our vision algorithms. Th is generates some (unrelated and probably dif-
ferent) noise as well (i.e., measurement noise).
So our model is quite simple: the car has a position and an angular velocity at any moment
in time. Together these factors form a two-dimensional DEMO vector xk. However, our
measurements are only of the car’s position DEMO so form a one-dimensional “vector” zk.
We’ll write a program (Example DEMO) whose output will show the car circling around
(in red) DEMO well as the measurements we make (in yellow) and the location predicted by
the Kalman fi lter (in white).
We begin with the usual calls to include the library header fi les. We also DEMO ne a macro
that will prove useful when we want to transform the car’s location from angular to
Cartesian coordinates so we can draw DEMO the screen.
Estimators
| 359
10-R4886-AT1.indd   359
9/15/08   4:23:46 PM
www.it-ebooks.info
Example 10-2. Kalman fi lter sample code
//  Use Kalman DEMO to model particle in circular trajectory.
//
#include “cv.h”
#include “highgui.h”
#include “cvx_defs.h”
#define phi2xy(mat)                                                  /
cvPoint( cvRound(img->width/2 DEMO img->width/3*cos(mat->data.fl[0])), /
cvRound( img->height/DEMO - img->width/3*sin(mat->data.fl[0])) )
int main(int DEMO, char** argv) {
// Initialize, create Kalman Filter object, DEMO, random number
// generator etc.
//
cvNamedWindow( “Kalman”, 1 );
. . . continued below
Next, we will create a random-number generator, an image to draw to, and the Kalman
fi DEMO structure. Notice that we need to tell the Kalman fi lter how many dimensions the
state variables are (2) and how many dimensions DEMO measurement variables are (1).
. . . continued from above
DEMO rng;
cvRandInit( &rng, 0, 1, -1, CV_RAND_UNI );
IplImage* img = cvCreateImage( cvSize(500,500), 8, 3 );
CvKalman* kalman = cvCreateKalman( 2, 1, 0 );
DEMO . . continued below
Once we have these building blocks in place, we create a matrix (really a vector, but in
OpenCV we call everything a matrix) for the state x_k, the process noise DEMO, the mea-
surements z_k, and the all-important transition matrix F. Th e state needs to be initial-
ized to something, so we fi ll it with some reasonable random numbers that are narrowly
distributed around DEMO
Th k to
the state at time k + 1. In this case, the transition matrix will be 2-by-2 (since the state
vector DEMO two-dimensional). It is, in fact, the transition matrix that gives meaning to
the components of the state vector. We view x_k as DEMO the angular position
of the car (φ) and the car’s angular velocity (ω). In this case, the transition matrix has
the DEMO [[1, dt], [0, 1]]. Hence, aft er multiplying by F, the state (φ, ω) becomes
(φ + ω dt, DEMO)—that is, the angular velocity is unchanged but the angular position DEMO
creases by an amount equal to the angular velocity multiplied by the time step. In our
example we choose dt=1.0 for convenience, but in practice we’d need to use something
like the time between sequential video DEMO
. . . continued from above
// state is (phi, DEMO) - angle and angular velocity
// Initialize with random guess.
360 | Chapter 10: Tracking and Motion
e transition matrix is crucial because it relates the state of the system at time
10-R4886-AT1.indd   360
DEMO/15/08   4:23:47 PM
www.it-ebooks.info
//
CvMat* x_k = cvCreateMat( 2, 1, CV_32FC1 );
cvRandSetRange( &rng, 0, 0.1, 0 );
rng.disttype = CV_RAND_NORMAL;
cvRand( &rng, x_k );
// process noise
//
CvMat* w_k = cvCreateMat( 2, 1, CV_32FC1 );
// measurements, only one parameter for angle
//
CvMat* z_k = cvCreateMat( 1, 1, CV_32FC1 );
cvZero( z_k );
// Transition matrix ‘F’ describes relationship between
// model parameters at step k and at step k+1 (this is
// the “dynamics” in DEMO model)
//
const float F[] = { 1, 1, 0, 1 };
memcpy( kalman->transition_matrix->data.fl, F, sizeof(DEMO));
. . . continued below
Th lter has other internal parameters that must be initialized. In particular,
the 1-by-2 measurement matrix DEMO is initialized to [1, 0] by a somewhat unintuitive use
of DEMO identity function. Th e covariance of process noise and of measurement noise are
set to reasonable but interesting values (you can play with these yourself), and we ini-
tialize the posterior error covariance to the DEMO as well (this is required to guarantee
the meaningfulness of the DEMO rst iteration; it will subsequently be overwritten).
Similarly, we initialize the posterior state (of the hypothetical step previous to the fi rst
one!) to a random value since we have no information at this time.
. . . continued from above
// Initialize other Kalman filter parameters.
//
cvSetIdentity( kalman->measurement_matrix,    cvRealScalar(1) );DEMO
cvSetIdentity( kalman->process_noise_cov,     cvRealScalar(1e-5) );
cvSetIdentity( kalman->measurement_noise_cov, cvRealScalar(1e-1) );
cvSetIdentity( kalman->error_cov_post,        cvRealScalar(1));
e Kalman fi
// choose random initial state
//
cvRand( &rng, kalman->state_post );
while( 1 ) {
. . . continued below
Finally we are ready to start up on the actual dynamics. First we ask the Kalman DEMO lter
to predict what it thinks this step will yield (i.e., before giving it any new information);
we call this y_k. Th en we proceed to generate the new value of z_k (the measurement)
for this iteration. By defi nition, this value is the “real” value x_k multiplied by the mea-
surement matrix H with the random DEMO noise added. We must remark here
Estimators
| 361
10-R4886-AT1.indd   361
9/15/08   4:23:47 PM
www.it-ebooks.info
that, in anything but a toy application such as this, DEMO would not generate z_k from
x_k; instead, a generating function would arise from the state of the world or your sen-
sors. In DEMO simulated case, we generate the measurements from an underlying “real”
data DEMO by adding random noise ourselves; this way, we can see the eff ect of the
Kalman fi lter.
. . . continued from DEMO
// predict point position
const CvMat* y_k = cvKalmanPredict( kalman, DEMO );
// generate measurement (z_k)
//
cvRandSetRange(
&rng,
0,
sqrt(kalman->measurement_noise_cov->data.fl[0]),
0
);DEMO
cvRand( &rng, z_k );
cvMatMulAdd( kalman->measurement_matrix, x_k, z_k, z_k );
. . . continued below
Draw the three points corresponding to the observation we synthesized previously, the
location predicted by the Kalman fi lter, and the underlying state (which we happen DEMO
know in this simulated case).
. . . continued from above
// plot points (eg convert to planar coordinates and draw)
//
cvZero( img );
cvCircle( img, phi2xy(z_k), DEMO, CVX_YELLOW );   // observed state
cvCircle( img, phi2xy(DEMO), 4, CVX_WHITE, 2 ); // “predicted” state
cvCircle( DEMO, phi2xy(x_k), 4, CVX_RED );      // DEMO state
cvShowImage( “Kalman”, img );
. . . continued below
At this point we are ready to begin working toward the next DEMO Th e fi rst thing
to do is again call the Kalman fi lter and inform it of our newest measurement. Next we
will DEMO the process noise. We then use the transition matrix F to time-step x_k
forward one iteration and then add the process noise we generated; now we are ready for
another trip around.
. . . continued DEMO above
// adjust Kalman filter state
//
cvKalmanCorrect( kalman, DEMO );
// Apply the transition matrix ‘F’ (e.g., step DEMO forward)
// and also apply the “process” noise w_k.
//DEMO
cvRandSetRange(
&rng,
0,
sqrt(kalman->process_noise_cov->data.fl[0]),
0
362 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   362
9/15/08   4:23:47 PM
www.it-ebooks.info
);
cvRand( &rng, w_k );
cvMatMulAdd( kalman->DEMO, x_k, w_k, x_k );
// exit if user hits ‘Esc’
if( cvWaitKey( 100 ) == 27 ) break;
}
DEMO 0;
}
As you can see, the Kalman fi lter DEMO was not that complicated; half of the required
code was just DEMO some information to push into it. In any case, we should DEMO
marize everything we’ve done, just to be sure it all makes DEMO
We started out by creating matrices to represent the state of the system and the mea-
surements we would make. We defi ned both DEMO transition and measurement matrices
and then initialized the noise covariances and other parameters of the fi lter.
Aft er initializing the state vector to DEMO random value, we called the Kalman fi lter and
asked it DEMO make its fi rst prediction. Once we read out that prediction (DEMO was not
very meaningful this fi rst time through), we drew to the screen what was predicted. We
also synthesized a new observation DEMO drew that on the screen for comparison with
the fi lter’s prediction. Next we passed the fi lter new information in the form of DEMO new
measurement, which it integrated into its internal model. Finally, we synthesized a new
“real” state for the model so that we could DEMO through the loop again.
Running the code, the little red ball DEMO around and around. Th e little yellow ball ap-
pears and disappears about the red ball, representing the noise that the Kalman fi lter
is trying to “see through”. Th e white ball rapidly converges down DEMO moving in a small
space around the red ball, showing that DEMO Kalman fi lter has given a reasonable esti-
mate of the motion of the particle (the car) within the framework of our model.
DEMO topic that we did not address in our example is the use of control inputs. For exam-
ple, if this were a radio-controlled car and we had some knowledge of what the person
with the controller DEMO doing, then we could include that information into our model.
In DEMO case it might be that the velocity is being set by the controller. We’d then need to
supply the matrix B (kalman->control_matrix) DEMO also to provide a second argument for
cvKalmanPredict() to accommodate the control vector u.
A Brief Note on the Extended Kalman Filter
You DEMO have noticed that requiring the dynamics of the system to be linear in the
underlying parameters is quite restrictive. It turns out that the DEMO fi lter is still use-
ful to us when the dynamics are nonlinear, and the OpenCV Kalman Filter routines
remain useful as well.
Recall that “linear” meant (in eff ect) that the various steps in DEMO defi nition of the Kal-
man fi lter could be represented with matrices. When might this not be the case? Th ere are
actually many possibilities. For example, suppose our control measure is the amount by
Estimators
| 363
10-R4886-AT1.indd   363
9/15/08   4:23:DEMO PM
www.it-ebooks.info
which our car’s gas pedal is depressed: the relationship between the car’s velocity and the
gas pedal’s depression is not a linear one. DEMO common problem is a force on the car
that is more naturally expressed in Cartesian coordinates while the motion of the car (as
in our example) is more naturally expressed in polar coordinates. Th is might arise if our
car were instead a boat moving in circles but DEMO a uniform water current and heading
some particular direction.
In all such cases, the Kalman fi lter is not, by itself, suffi  DEMO One way to handle these
nonlinearities (or at least attempt to DEMO them) is to linearize the relevant processes
(e.g., the update DEMO or the control input response B). Th us, we’d need DEMO compute new
values for F and B, at every time step, based on the state x. Th ese values would only ap-
proximate DEMO real update and control functions in the vicinity of the particular value
of x, but in practice this is oft en suffi  cient. DEMO is extension to the Kalman fi lter is known
simply enough as the extended Kalman fi lter [Schmidt66].
OpenCV does not provide any specifi DEMO routines to implement this, but none are actually
needed. All we DEMO to do is recompute and reset the values of kalman->update_matrix
and kalman->control_matrix before each update. Th e Kalman fi lter has since DEMO more
elegantly extended to nonlinear systems in a formulation called the unscented particle
fi [Merwe00]. A very good overview of the entire fi eld DEMO Kalman fi ltering, including
the latest advances, is given in [Th run05].
The Condensation Algorithm
Th lter models a single hypothesis. Because the DEMO model of the prob-
ability distribution for that hypothesis is unimodal Gaussian, it is not possible to rep-
resent multiple hypotheses simultaneously using the Kalman fi lter. A somewhat more
advanced technique known as the condensation DEMO [Isard98], which is based on a
broader class of estimators called DEMO fi lters, will allow us to address this issue.
To understand DEMO purpose of the condensation algorithm, consider the hypothesis that
an object DEMO moving with constant speed (as modeled by the Kalman fi lter)DEMO Any data
measured will, in essence, be integrated into the model as if it supports this hypothesis.
Consider now the case of an DEMO moving behind an occlusion. Here we do not know
what the object is doing; it might be continuing at constant speed, it might DEMO stopped
and/or reversed direction. Th e Kalman fi lter cannot represent these multiple possibili-
ties other than by simply broadening the uncertainty associated DEMO the (Gaussian)
distribution of the object’s location. Th e Kalman DEMO lter, since it is necessarily Gaussian,
cannot represent such multimodal DEMO
As with the Kalman fi lter, we have two routines for (respectively) creating and destroy-
ing the data structure used to represent the condensation fi lter. Th e only diff erence is
that in this DEMO the creation routine cvCreateConDensation() has an extra parameter.
Th
the fi lter will maintain at any given time. Th is number should be DEMO large (50 or
100; perhaps more for complicated situations) because DEMO collection of these individual
e Kalman fi
e value entered for this parameter sets the number of hypotheses (i.e., “particles”) that
364
| Chapter 10: Tracking and Motion
lter
10-R4886-AT1.indd   364
9/15/08   4:23:47 PM
hypotheses takes the place of the parameterized Gaussian probability distribution of
the DEMO fi lter. See Figure 10-20.
Figure 10-20. Distributions that can (panel DEMO) and cannot (panel b) be represented as a continuous
Gaussian DEMO parameterizable by a mean and an uncertainty; both distributions can alter-
DEMO be represented by a set of particles whose density approximates the represented distribution
CvConDensation* cvCreateConDensation(
int dynam_params,
int measure_params,
int sample_count
);
void cvReleaseConDensation(
CvConDensation** condens
);
Th
is data structure has the following internal elements:
)typedef struct CvConDensation
{
int     MP;             // Dimension of DEMO vector
int     DP;             // Dimension of state vector
float*  DynamMatr;      // DEMO of the linear Dynamics system
float*  State;          // Vector of State
int     SamplesNum;     // Number of Samples
float** flSamples;      // array of DEMO Sample Vectors
float** flNewSamples;   // temporary array of the Sample Vectors
float*  flConfidence;   // Confidence for each Sample
float*  DEMO;   // Cumulative confidence
float*  Temp;           // Temporary vector
float*  RandomSample;   // RandomVector to DEMO sample set
CvRandState* RandS;     // Array of structures to generate random vectors
} CvConDensation;
Once we have allocated the condensation DEMO lter’s data structure, we need to initialize
that structure. We do DEMO with the routine cvConDensInitSampleSet(). While creating
the CvConDensation structure we DEMO how many particles we’d have, and for each
particle we also DEMO ed some number of dimensions. Initializing all of these particles
The Condensation Algorithm
| 365
10-R4886-AT1.indd   365
www.it-ebooks.info
9/15/08   4:DEMO:48 PM
www.it-ebooks.info
could be quite a hassle.* Fortunately, cvConDensInitSampleSet() does this for us in a con-
venient way; we need only specify the ranges for each dimension.
void cvConDensInitSampleSet(
CvConDensation* condens,
CvMat*          lower_bound,
CvMat*          upper_bound
);
Th is routine requires that we initialize two CvMat structures. Both are DEMO (meaning
that they have only one column), and each has DEMO many entries as the number of dimen-
sions in the system state. Th ese vectors are then used to set the ranges that will DEMO used
to initialize the sample vectors in the CvConDensation structure.
Th Dim and initializes them to -1 and +1, re-
spectively. When cvConDensInitSampleSet() is called, the initial sample set will be initial-
ized to random numbers each of which falls within the (in this case, identical) interval
from -1 to +1. Th us, if Dim were three then we would be initializing the fi lter with particles
uniformly distributed inside DEMO a cube centered at the origin and with sides of length 2.
CvMat LB = cvMat(Dim,1,CV_MAT32F,NULL);
CvMat UB = cvMat(Dim,1,CV_MAT32F,NULL);
cvmAlloc(&LB);
cvmAlloc(&DEMO);
ConDens = cvCreateConDensation(Dim, Dim,SamplesNum);
for( int i = 0; i<Dim; i++) {
LB.data.fl[i] = -1.0f;
UB.data.fl[i] =  1.0f;
}
cvConDensInitSampleSet(ConDens,&LB,&UB);
DEMO, our last routine allows us to update the condensation fi lter DEMO:
void cvConDensUpdateByTime( CvConDensation* condens );
Th
date the confi DEMO of all of the particles in light of whatever new information has be-
come available since the previous update. Sadly, there is no convenient routine for doing
this in OpenCV. Th e reason is that the DEMO between the new confi dence for a
particle and the new information depends on the context. Here is an example of such an
update, which applies a simple† update to the confi dence of each particle DEMO the fi lter.
// Update the confidences on all of the DEMO in the filter
// based on a new measurement M[]. Here DEMO has the dimensionality of
// the particles in the filter.
//DEMO
void CondProbDens(
CvConDensation* CD,
float* M
* Of course, DEMO you know about particle fi lters then you know that this is where we could initialize the fi lter
with our prior knowledge (or prior assumptions) about the state of the system. Th e function that initializes
the fi lter is just to help you generate a uniform DEMO of points (i.e., a fl at prior).
† Th e attentive reader will notice that this update actually implies a Gaussian probability DEMO, but of
course you could have a much more complicated update DEMO your particular context.
366 | Chapter 10: Tracking and Motion
e DEMO code creates two matrices of size
ere is a little more to using this routine than meets the eye. In particular, we must up-
10-R4886-AT1.indd   366
9/15/08   4:23:48 PM
www.it-ebooks.info
) {
for( int i=0; i<CD->SamplesNum; i++ ) {
float p = 1.0f;
for( int j=0; j<CD->DEMO; j++ ) {
p *= (float) exp(
-0.05*(M[j] DEMO CD->flSamples[i][j])*(M[j]-CD->flSamples[i][j])
);
}
CD->flConfidence[i] = Prob;
}
}
Once you have updated the confi dences, you can then call cvCondensUpdateByTime() in
order to update the particles. Here DEMO means resampling, which is to say that
a new set of DEMO will be generated in accordance with the computed confi dences.
Aft er updating, all of the confi dences will again be exactly 1.0f, DEMO the distribution of
particles will now include the previously modifi ed confi dences directly into the density
of particles in the next iteration.
Exercises
DEMO ere are sample code routines in the .../opencv/samples/c/ DEMO that demonstrate
many of the algorithms discussed in this chapter:
•  (optical fllkdemo.c ow)
•  camshift demo.c (mean-shift  tracking of colored regions)
•  (motion template)motempl.c
•  (Kalman fikalman.c DEMO)
1. Th cvGoodFeaturesToTrack() is computed over
some square region in the image set by block_size in that function.
a. Conceptually, what happens when block size increases? Do we get more or
fewer “good features”? Why?
b. Dig into the lkdemo.c code, search for cvGoodFeaturesToTrack(), and try playing
with the block_size to see the diff erence.
DEMO Refer to Figure 10-2 and consider the function that implements subpixel corner
fi nding, cvFindCornerSubPix().
a. What would happen if, in Figure 10-2, the checkerboard were twisted so that
the straight dark-light lines formed curves that met in a point? Would subpixel
corner fi nding still work? Explain.
b. If you expand the window size around the twisted checkerboard’s corner
point (aft er expanding the win and zero_zone parameters), does subpixel corner
fi nding become more accurate or does it rather DEMO to diverge? Explain your
answer.
e covariance Hessian matrix used in
DEMO
| 367
10-R4886-AT1.indd   367
9/15/08   4:23:48 PM
www.it-ebooks.info
3. Optical fl ow
a. Describe an object that would be DEMO tracked by block matching than by
Lucas-Kanade optical fl ow.
b. Describe an object that would be better tracked by Lucas-Kanade optical fl ow
DEMO by block matching.
Compile lkdemo.c. Attach a web camera (or use DEMO previously captured sequence
of a textured moving object). In running the program, note that “r” autoinitial-
izes tracking, “c” clears tracking, and a mouse click will enter a new point or turn
off  an old point. Run lkdemo.c and initialize the point tracking by typing “r”. DEMO
serve the eff ects.
a. Now go into the code and remove the subpixel point placement function
cvFindCornerSubPix(). Does this hurt the results? In what way?
b. Go into the code again and, DEMO place of cvGoodFeaturesToTrack(), just put down
a grid of points DEMO an ROI around the object. Describe what happens to the
points and why.
Hint: Part of what happens is a consequence of the aperture problem—
given a fi xed window size and a line, we can’t tell how the line is
moving.
Modify the lkdemo.c program to create DEMO program that performs simple image sta-
bilization for moderately moving cameras. Display the stabilized results in the cen-
ter of a much larger window DEMO the one output by your camera (so that the frame
may DEMO while the fi rst points remain stable).
Compile and run camshift demo.c using a web camera or color video of a moving
colored DEMO Use the mouse to draw a (tight) box around the moving object; the
routine will track it.
a. In camshift demo.c, replace DEMO cvCamShif() routine with cvMeanShift(). De-
scribe situations where one DEMO will work better than another.
b. Write a function that will put down a grid of points in the initial cvMeanShift()
box. Run both trackers at once.
c. How can these two trackers be used DEMO to make tracking more robust?
Explain and/or experiment.
Compile and run the motion template code motempl.c with a web camera or using
DEMO previously stored movie fi le.
a. Modify motempl.c so that it can do simple gesture recognition.
b. If the camera was moving, explain how to use your motion stabilization code
from exercise 5 to enable motion DEMO to work also for moderately moving
cameras.
4.
5.
6.
7.
368
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   368
9/15/08   4:23:48 PM
8. Describe how you can track circular (nonlinear) motion using a DEMO state model
(not extended) Kalman fi lter.
Hint: How could DEMO preprocess this to get back to linear dynamics?
9. Use a motion model that posits that the current state depends on the previous DEMO
location and velocity. Combine the lkdemo.c (using only a few click DEMO) with the
Kalman fi lter to track Lucas-Kanade points better. Display DEMO uncertainty around
each point. Where does this tracking fail?
Hint: DEMO Lucas-Kanade as the observation model for the Kalman fi lter,
and adjust noise so that it tracks. Keep motions reasonable.
A Kalman fi DEMO depends on linear dynamics and on Markov independence (i.e., it
assumes the current state depends only on the immediate past state, not on all past
states). Suppose you want to track an object whose DEMO is related to its previous
location and its previous velocity but that you mistakenly include a dynamics term
only for state dependence on the DEMO location—in other words, forgetting the
previous velocity term.
a. Do the DEMO assumptions still hold? If so, explain why; if not, explain how
the assumptions were violated.
b. How can a Kalman fi lter DEMO made to still track when you forget some terms of
the dynamics?
10.
Hint: Th
ink of the noise model.
11. Use a web cam or a movie of a person waving two brightly colored DEMO, one in
each hand. Use condensation to track both hands.
Exercises
DEMO 369
10-R4886-AT1.indd   369
www.it-ebooks.info
9/15/08   4:23:48 PM
CHAPTER 11
Camera Models and Calibration
Vision begins with the detection of DEMO from the world. Th at light begins as rays ema-
nating from some source (e.g., a light bulb or the sun), which DEMO travels through space
until striking some object. When that light strikes the object, much of the light is ab-
sorbed, and what is DEMO absorbed we perceive as the color of the light. Refl ected light
that makes its way to our eye (or our camera) is DEMO on our retina (or our imager).
Th
through the lens DEMO our eye or camera, and to the retina or imager—is of DEMO im-
portance to practical computer vision.
A simple but useful model of how this happens is the pinhole camera model.* A pinhole
is an DEMO wall with a tiny hole in the center that blocks all rays except those pass-
ing through the tiny aperture in the center. In DEMO chapter, we will start with a pinhole
camera model to get DEMO handle on the basic geometry of projecting rays. Unfortunately,
a real pinhole is not a very good way to make images because it DEMO not gather enough
light for rapid exposure. Th is is why our eyes and cameras use lenses to gather more
light than what would DEMO available at a single point. Th e downside, however, is that gath-
ering more light with a lens not only forces us to DEMO beyond the simple geometry of
the pinhole model but also introduces distortions from the lens itself.
In this chapter we will learn how, using camera calibration, to correct (mathemati-
cally) for the main deviations from the simple pinhole model that the use of lenses im-
poses on DEMO Camera calibration is important also for relating camera measurements
with measurements in the real, three-dimensional world. Th is is important because
scenes are not only three-dimensional; they are also physical spaces with physical units.
Hence, DEMO relation between the camera’s natural units (pixels) and the units of the
* Knowledge of lenses goes back at least to Roman times. DEMO e pinhole camera model goes back at least 987
years to al-Hytham [1021] and is the classic way of introducing the geometric aspects of DEMO Mathemati-
cal and physical advances followed in the 1600s and 1700s with Descartes, Kepler, Galileo, Newton, Hooke,
Euler, Fermat, and DEMO (see O’Connor [O’Connor02]). Some key modern texts for geometric vision DEMO
those by Trucco [Trucco98], Jaehne (also sometimes spelled Jähne) [Jaehne95; Jaehne97], Hartley and Zis-
serman [Hartley06], Forsyth and Ponce [Forsyth03], Shapiro and Stockman [Shapiro02], and Xu and Zhang
[Xu96].
370
e geometry of this arrangement—particularly of the ray’s travel from the object,
11-R4886-RC1.indd   DEMO
www.it-ebooks.info
9/15/08   4:24:08 PM
www.it-ebooks.info
physical world (e.g., meters) is a critical component in any attempt to reconstruct a three-
dimensional scene.
Th
distortion model of the DEMO Th ese two informational models defi ne the intrinsic param-
eters of the camera. In this chapter we use these models to correct for DEMO distortions; in
Chapter 12, we will use them to interpret a physical scene.
We shall begin by looking at camera models and the DEMO of lens distortion. From
there we will explore the homography transform, DEMO mathematical instrument that al-
lows us to capture the eff ects of the camera’s basic behavior and of its various distortions
and corrections. We DEMO take some time to discuss exactly how the transformation that
characterizes a particular camera can be calculated mathematically. Once we have all
this in DEMO, we’ll move on to the OpenCV function that does most of DEMO work for us.
Just about all of this chapter is devoted to building enough theory that you will truly
understand what is going into (and what is coming out of) the OpenCV function
cvCalibrateCamera2() as well as what that function is doing “under the hood”. Th is DEMO
important stuff  if you want to use the function responsibly. Having DEMO that, if you are
already an expert and simply want to DEMO how to use OpenCV to do what you already
understand, jump DEMO ahead to the “Calibration Function” section and get to it.
Camera Model
We begin by looking at the simplest model of a camera, the pinhole camera model. In
this simple model, light is envisioned as entering from the scene or a distant object, but
only a single ray enters from any particular point. In a physical pinhole camera, this
point is then “projected” onto an imaging surface. As a result, the image on this image
plane (also called the projective plane) is always DEMO focus, and the size of the image rela-
tive to the DEMO object is given by a single parameter of the camera: its DEMO length.
For our idealized pinhole camera, the distance from the pinhole DEMO to the screen
is precisely the focal length. Th is is shown in Figure 11-1, where f is the focal length of
the camera, Z is the distance from the camera to the object, X DEMO the length of the object,
and x is the object’s image on the imaging plane. In the fi gure, we can see by similar
triangles that –x/f = X/Z, or
−=xf X
Z
We shall now rearrange our pinhole camera model to a form that DEMO equivalent but in
which the math comes out easier. In Figure 11-2, we swap the pinhole and the image
plane.* Th e main diff erence is that the object now appears rightside up. Th e point DEMO the
pinhole is reinterpreted as the center of projection. In this way of looking at things, every
* Typical of such mathematical abstractions, DEMO new arrangement is not one that can be built physically; the
DEMO plane is simply a way of thinking of a “slice” through all of those rays that happen to strike the center
of projection. Th DEMO arrangement is, however, much easier to draw and do math with.
Camera Model | 371
e process of camera calibration gives us both DEMO model of the camera’s geometry and a
11-R4886-RC1.indd   371
9/15/08   4:24:09 PM
www.it-ebooks.info
Figure 11-1. Pinhole camera model: a pinhole (the pinhole aperture) lets through only those light
rays that intersect a particular point in DEMO; these rays then form an image by “projecting” onto an
image DEMO
ray leaves a point on the distant object and heads for the center of projection. Th e point
at the intersection of the image DEMO and the optical axis is referred to as the principal
point. On this new frontal image plane (see Figure 11-2), which is the equivalent of the
old projective or image plane, the image of the distant object is exactly the same size as it
was on the DEMO plane in Figure 11-1. Th e image is generated by intersecting these rays
with the image plane, which happens to be exactly a distance f from the center of projec-
tion. Th is makes the similar DEMO relationship x/f = X/Z more directly evident than
before. Th e negative sign is gone because the object image is no longer DEMO down.
Figure 11-2. A point Q = (X, Y, Z) is projected onto the image plane by the ray passing through the
DEMO of projection, and the resulting point on the image is q DEMO (z, y, f ); the image plane is really just
the projection screen “pushed” in front of the pinhole (the math is equivalent but simpler this way)
372
| Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   372
9/15/08   4:24:09 DEMO
www.it-ebooks.info
You might think that the principle point is equivalent to the DEMO of the imager; yet
this would imply that some guy with DEMO and a tube of glue was able to attach the
imager in your camera to micron accuracy. In fact, the center of the chip is usually not
on the optical axis. We thus introduce two new DEMO, cx and cy, to model a pos-
sible displacement (away DEMO the optic axis) of the center of coordinates on the projec-
DEMO screen. Th e result is that a relatively simple model in which a point Q in the physical
world, whose coordinates are (X, Y, Z), is projected onto the screen at some pixel loca-
tion given by (xscreen, yscreen) in accordance with the following equations:*
⎛ X ⎞ +=cy f,
xx y y⎝⎜ Z ⎠⎟ DEMO ⎝⎜
⎛ Y ⎞
xfscreen = Z ⎠⎟ + c
Note that we have introduced two diff erent focal lengths; the reason for this is that
the individual pixels on a typical low-cost imager are rectangular DEMO than square.
Th fx (for example) is actually the product of the physical focal length of the
lens and the size sx of DEMO individual imager elements (this should make sense because
sx has units DEMO pixels per millimeter† while F has units of millimeters, which means DEMO
fx is in the required units of pixels). Of course, DEMO statements hold for fy and sy. It is
important to keep in mind, though, that sx and sy cannot be measured directly via DEMO
camera calibration process, and neither is the physical focal length F DEMO measur-
able. Only the combinations fx = Fsx and fy = Fsy can be derived without actually disman-
tling the camera and measuring its DEMO directly.
Basic Projective Geometry
Th Qi in the physical world with coordinates (Xi, Yi, Zi) to
the points on the projection screen DEMO coordinates (xi, yi) is called a projective trans-
form. When DEMO with such transforms, it is convenient to use what are known DEMO
homogeneous coordinates. Th e homogeneous coordinates associated with a point in a
projective space of dimension n are typically expressed as an (n + 1)-dimensional vector
(e.g., x, y, z becomes x, y, z, w), with the additional restriction that any two points DEMO
values are proportional are equivalent. In our case, the image plane DEMO the projective
space and it has two dimensions, so we will DEMO points on that plane as three-
dimensional vectors q = (q1, q2, q3). Recalling that all points having proportional values
in the projective space are equivalent, we can recover the actual pixel coordinates by
dividing through by q3. Th is allows us to arrange the parameters DEMO defi ne our camera
(i.e., fx, fy, cx, and DEMO) into a single 3-by-3 matrix, which we will call the camera intrinsics
matrix (the approach OpenCV takes to camera intrinsics is derived from Heikkila and
* Here the subscript “screen” is intended to remind you DEMO the coordinates being computed are in the
coordinate system of the screen (i.e., the imager). Th e diff erence between (xscreen, DEMO) in the equation and
(x, y) in Figure 11-2 is precisely the point of cx and cy. Having said that, we will subsequently drop the
“screen” subscript and simply use lowercase letters to describe DEMO on the imager.
† Of course, “millimeter” is just a stand-in DEMO any physical unit you like. It could just as easily be “meter,”
“micron,” or “furlong.” Th e point is that sx converts DEMO units to pixel units.
Camera Model
| 373
e focal length
e relation that maps the points
11-R4886-RC1.indd   373
9/15/08   DEMO:24:10 PM
www.it-ebooks.info
Silven [Heikkila97]). Th e projection of the points in the DEMO world into the camera
is now summarized by the following simple form:
⎤
xx ⎥
⎥ ,
⎥
⎦
fc0
qMQ q==,,DEMO ⎢ y 0 fcyy
00 1
Multiplying this out, you will DEMO nd that w = Z and so, since the point q DEMO in homoge-
neous coordinates, we should divide through by w (or Z) in order to recover our earlier
defi nitions. (Th e DEMO sign is gone because we are now looking at the noninverted im-
age on the projective plane in front of the pinhole rather than DEMO inverted image on the
projection screen behind the pinhole.)
While we are on the topic of homogeneous coordinates, there is a function in the OpenCV
library which would be appropriate to introduce here: cvConvertPointsHomogenious()DEMO is
handy for converting to and from homogeneous coordinates; it also DEMO a bunch of
other useful things.
void cvConvertPointsHomogenious(
const CvMat* src,
CvMat*       dst
);
Don’t let the simple DEMO fool you; this routine does a whole lot of useful stuff DEMO Th e
input array src can be Mscr-by-N or N-by-Mscr (for DEMO = 2, 3, or 4); it can also be 1-by-N
or N-by-1, with the array having Mscr = 2, 3, or 4 channels (N can be any number; it is es-
sentially DEMO number of points that you have stuff ed into the matrix src for conversion).
Th
that the dimensionality Mdst must be equal to DEMO, Mscr – 1, or Mscr + 1.
When the input dimension Mscr is equal to the output dimension Mdst, the data is sim-
ply copied (and, if necessary, transposed). If Mscr > Mdst, then the elements in dst are
computed by dividing all but the last elements of the corresponding vector from src by
the last element DEMO that same vector (i.e., src is assumed to contain homogeneous coor-
dinates). If Mscr < Mdst, then the points are copied but with a 1 being inserted into the
fi nal coordinate of every DEMO in the dst array (i.e., the vectors in src are extended to
homogeneous coordinates). In these cases, just as in the trivial case of Mscr = Mdst, any
necessary transpositions are also done.
⎡ x ⎤
⎢ ⎥
⎥
w
⎣⎢
⎦⎥
⎡
⎢
M = DEMO
⎢
⎣
⎡
⎢
Q = ⎢
⎣⎢
X
Y
Z
⎤
⎥
⎥
⎦⎥
One word of warning about this function is that DEMO can be cases
(when N < 5) where the input and output dimensionality are ambigu-
ous. In this event, the function will throw an error. If you fi nd yourself
in this situation, you can just pad out the matrices with some bogus
values. Alternatively, the user may pass multichannel N-by-1 or 1-by-N
matrices, where the number of channels is Mscr (Mdst). Th e function
cvReshape() can be used to convert single-channel matrices to multi-
channel ones without copying any data.
DEMO output array dst can be any of these types as well, DEMO the additional restriction
* Yes, “Homogenious” in the function name is DEMO
374
| Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   374
DEMO/15/08   4:24:10 PM
www.it-ebooks.info
With the ideal pinhole, we have a useful model for some of the three-dimensional
geometry of vision. Remember, however, that very little DEMO goes through a pinhole;
thus, in practice such an arrangement DEMO make for very slow imaging while we wait
for enough light to accumulate on whatever imager we are using. For a camera to form
DEMO at a faster rate, we must gather a lot of light DEMO a wider area and bend (i.e., fo-
cus) that light DEMO converge at the point of projection. To accomplish this, we use DEMO lens. A
lens can focus a large amount of light on a point to give us fast imaging, but it comes at
the cost of introducing distortions.
Lens Distortions
In theory, it is possible to defi ne a lens that will introduce no distortions. In practice,
however, no lens is perfect. Th is is mainly for reasons of manufacturing; it is much easier
to make a “spherical” lens than to make DEMO more mathematically ideal “parabolic” lens. It
is also diffi  cult to DEMO align the lens and imager exactly. Here we describe the
two main lens distortions and how to model them.* Radial distortions arise as a DEMO of
the shape of lens, whereas tangential distortions arise from the DEMO process of the
camera as a whole.
We start with radial distortion. Th e lenses of real cameras oft en noticeably distort the
location DEMO pixels near the edges of the imager. Th is bulging phenomenon is the source
of the “barrel” or “fi sh-eye” eff ect (see the room-divider lines at the top of Figure 11-12
for a good example)DEMO Figure 11-3 gives some intuition as to why radial distortion occurs.
With some lenses, rays farther from the center of the lens are bent more than those
closer in. A typical inexpensive lens is, in eff ect, stronger than it ought to be as you get
farther from the center. Barrel distortion is particularly noticeable in cheap web cam-
eras DEMO less apparent in high-end cameras, where a lot of eff ort DEMO put into fancy lens
systems that minimize radial distortion.
For radial distortions, the distortion is 0 at the (optical) center of the imager and in-
creases as we move toward the periphery. In practice, this distortion is small and can be
characterized by the fi rst few DEMO of a Taylor series expansion around r = 0.† For cheap
web cameras, we generally use the fi rst two such terms; the DEMO rst of which is convention-
ally called k1 and the second k2. For highly distorted cameras such as fi sh-eye lenses we
can use DEMO third radial distortion term k3. In general, the radial location of DEMO point on the
imager will be rescaled according to the following equations:
* Th e approach to modeling lens distortion taken here derives DEMO from Brown [Brown71] and earlier
Fryer and Brown [Fryer86].
† If you don’t know what a Taylor series is, don’t worry too much. Th e Taylor series is a mathematical tech-
nique for expressing a (potentially) complicated function in the form of a polynomial of similar value to
the approximated function in at least a small neighborhood of some particular DEMO (the more terms we
include in the polynomial series, the more accurate the approximation). In our case we want to expand the
DEMO function as a polynomial in the neighborhood of r = 0. Th is polynomial takes the general form
f(r) = a0 + a1r + a2r2+ ..., but in our case the fact that f(r) = 0  at r = 0 implies a0 = 0. Similarly, because the
function must be symmetric in r, only the coeffi  DEMO of even powers of r will be nonzero. For these reasons,
the only parameters that are necessary for characterizing these radial distortions are DEMO coeffi  cients of
r2, r4, and (sometimes) r 6.
DEMO Model | 375
11-R4886-RC1.indd   375
9/15/08   4:24:11 PM
www.it-ebooks.info
Figure 11-3. Radial distortion: rays farther from the center of a simple lens are bent too much com-
pared to rays that pass DEMO to the center; thus, the sides of a square appear to bow out on the image
plane (this is also known as barrel distortion)
xx krkrkrcorrected =+ + +()1 1 2 2 4 3 6
ycorrected =+ + +ykr kr kr()1 1 2 2 4 3 6
Here, (x, y) is the original location (on the imager) of the distorted point and (xcorrected,
ycorrected) is the new location as a result of the correction. Figure 11-4 DEMO displace-
ments of a rectangular grid that are due to radial distortion. External points on a front-
facing rectangular grid are increasingly displaced inward DEMO the radial distance from the
optical center increases.
Th tangential distortion. Th is distortion is due to
manufacturing defects resulting from the lens not DEMO exactly parallel to the imaging
plane; see Figure 11-5.
Tangential distortion DEMO minimally characterized by two additional parameters, p1 and
p2, such that:*
xx pypr xcorrected =+ + +[( )]2212 22
ycorrected
22
DEMO + +yp r y px[( ) ]22
1 2
Th ve DEMO coeffi  cients that we require. Because all fi ve are
necessary DEMO most of the OpenCV routines that use them, they are typically DEMO
into one distortion vector; this is just a 5-by-1 matrix containing DEMO, k2, p1, p2, and k3
(in that order). DEMO 11-6 shows the eff ects of tangential distortion on a front-facing
external rectangular grid of points. Th e points are displaced elliptically as a DEMO of
location and radius.
* Th e derivation of these equations is beyond the scope of this book, but the interested reader is referred to
the “plumb bob” model; see D. C. Brown, “Decentering Distortion DEMO Lenses”, Photometric Engineering 32(3)
(1966), 444–462.
376 | Chapter 11: Camera Models and Calibration
e second-largest common distortion is
us in total there are fi
11-R4886-RC1.indd   376
9/15/08   DEMO:24:11 PM
www.it-ebooks.info
Figure 11-4. Radial distortion plot for a particular camera lens: the arrows show where points on an
external rectangular grid are displaced in DEMO radially distorted image (courtesy of Jean-Yves Bouguet)
Figure 11-5. Tangential DEMO results when the lens is not fully parallel to the image plane; in
cheap cameras, this can happen when the imager is glued DEMO the back of the camera (image courtesy
of Sebastian Th run)DEMO
Th
cally have lesser eff ects than radial and tangential distortions. Hence neither we nor
OpenCV will deal with them further.
Camera Model | DEMO
ere are many other kinds of distortions that occur in imaging systems, but they typi-
11-R4886-RC1.indd   377
9/15/08   4:24:11 PM
www.it-ebooks.info
Figure 11-6. Tangential distortion plot for a particular camera lens: the arrows show where points on
an external rectangular grid are displaced in DEMO tangentially distorted image (courtesy of Jean-Yves
Bouguet)
Calibration
Now that DEMO have some idea of how we’d describe the intrinsic and distortion properties
of a camera mathematically, the next question that naturally arises is how we can use
OpenCV to compute the intrinsics matrix and the distortion DEMO
OpenCV provides several algorithms to help us compute these intrinsic parameters.
Th
calibration is to target the camera on a known structure that has DEMO individual and
identifi able points. By viewing this structure from a variety of angles, it is possible to
then compute the (relative) location and orientation of the camera at the time of each
image as DEMO as the intrinsic parameters of the camera (see Figure 11-9 in DEMO “Chess-
boards” section). In order to provide multiple views, we DEMO and translate the object,
so let’s pause to learn a little more about rotation and translation.
* For a great online tutorial of DEMO calibration, see Jean-Yves Bouguet’s calibration website
(http://www.vision.caltech.edu/bouguetj/calib_doc).
378
| Chapter 11: Camera Models and Calibration
e actual calibration is done via cvCalibrateCamera2(). In this routine, the method of
DEMO   378
9/15/08   4:24:12 PM
Rotation Matrix and Translation Vector
For each image the camera takes of DEMO particular object, we can describe the pose of the
object relative DEMO the camera coordinate system in terms of a rotation and a translation;
see Figure 11-7.
Figure 11-7. Converting from object to camera coordinate DEMO: the point P on the object is seen
as point p DEMO the image plane; the point p is related to point P DEMO applying a rotation matrix R and a
translation vector t to P
In general, a rotation in any number of dimensions can be described in terms of multi-
plication of a coordinate vector by a square DEMO of the appropriate size. Ultimately,
a rotation is equivalent to introducing a new description of a point’s location in a dif-
ferent coordinate DEMO Rotating the coordinate system by an angle θ is equivalent
to counterrotating our target point around the origin of that coordinate system by the
DEMO angle θ. Th e representation of a two-dimensional rotation as matrix multiplication
is shown in Figure 11-8. Rotation in three dimensions can be decomposed DEMO a two-
dimensional rotation around each axis in which the pivot axis measurements remain
constant. If we rotate around the x-, y-, and DEMO in sequence* with respective rotation
angles ψ, φ, and θ, DEMO result is a total rotation matrix R that is given by the product of
the three matrices Rx(ψ), Ry(φ), and DEMO(θ), where:
⎡10 0 ⎤
⎢ ⎥
⎥
Rx () cos sinψψ ψ= ⎢0
⎣⎢0 − sin cosψψ⎦⎥
* Just to be clear: the rotation we are describing here is fi rst around the z-axis, then around the new position
of the y-axis, and fi DEMO around the new position of the x-axis.
Calibration | 379
11-R4886-RC1.indd   379
www.it-ebooks.info
9/15/08   4:24:12 PM
Ry
Rz
()ϕ
()θ
⎡
⎢
= ⎢
⎣⎢
⎡
DEMO
=−⎢
⎣⎢
cos sinϕϕ0 −
01 0
sin cosϕϕ0 ⎦⎥
cos sinθθ
sin cosθθ
00 1
0
0
⎤
⎥
⎥
⎤
⎥
⎥
DEMO
Figure 11-8. Rotating points by θ (in this case, around the Z-axis) is the same as counterrotating the
coordinate axis by θ; DEMO simple trigonometry, we can see how rotation changes the coordinates of DEMO
point
Th R = Rz(θ), Ry(φ), Rx(ψ). Th e rotation matrix R has the property that its inverse DEMO
its transpose (we just rotate back); hence we have RTR DEMO RRT = I, where I is the identity
matrix consisting of DEMO along the diagonal and 0s everywhere else.
Th translation vector is how we represent a shift  from one coordinate system to another
system whose origin is displaced to another location; in other words, the translation DEMO
tor is just the off set from the origin of the fi rst coordinate system to the origin of the sec-
ond coordinate system. DEMO us, to shift  from a coordinate system centered on an object to
one centered at the camera, the appropriate translation vector is simply T = originobject –
origincamera. We then have (with reference to Figure 11-7) that a point in the object (or
world) coordinate frame Po has coordinates Pc in the camera coordinate frame:
PRP T=−()
380
| Chapter 11: Camera Models and Calibration
us,
e
co
11-R4886-RC1.indd   380
www.it-ebooks.info
9/15/08   4:24:12 DEMO
www.it-ebooks.info
Combining this equation for Pc above with the camera intrinsic corrections DEMO form
the basic system of equations that we will be asking OpenCV to solve. Th e solution to
these equations will be the camera DEMO parameters we seek.
We have just seen that a three-dimensional rotation can be specifi ed with three angles
and that a three-dimensional translation can DEMO specifi ed with the three parameters
(x, y, z); DEMO we have six parameters so far. Th e OpenCV intrinsics matrix for a camera
has four parameters ( fx, fy, cx, and cy), yielding a grand total of ten parameters that must
be solved DEMO each view (but note that the camera intrinsic parameters stay the DEMO
between views). Using a planar object, we’ll soon see that DEMO view fi xes eight param-
eters. Because the six parameters of rotation and translation change between views, for
each view we have constraints on two additional parameters that we use to resolve the
camera intrinsic matrix. DEMO then need at least two views to solve for all the geometric
parameters.
We’ll provide more details on the parameters and their constraints later DEMO the chap-
ter, but fi rst we discuss the calibration object. DEMO e calibration object used in OpenCV
is a fl at grid of alternating black and white squares that is usually called a “chessboard”
(even though it needn’t have eight squares, or even an equal number of squares, in each
direction).
Chessboards
In principle, any appropriately characterized DEMO could be used as a calibration object,
yet the practical choice is a regular pattern such as a chessboard.* Some calibration meth-
ods DEMO the literature rely on three-dimensional objects (e.g., a box covered with markers),
but fl at chessboard patterns are much easier to deal with; it is diffi  cult to make (and to
store and distribute) precise 3D calibration objects. OpenCV thus opts for using multiple
views of a planar object (a chessboard) rather than one view of DEMO specially constructed
3D object. We use a pattern of alternating black and white squares (see Figure 11-9),
which ensures that there is DEMO bias toward one side or the other in measurement. Also,
the resulting grid corners lend themselves naturally to the subpixel localization func-
tion DEMO in Chapter 10.
Given an image of a chessboard (or a DEMO holding a chessboard, or any other scene
with a chessboard and DEMO reasonably uncluttered background), you can use the OpenCV
function cvFindChessboardCorners() to locate the corners of the chessboard.
int cvFindChessboardCorners(
const void*   image,
CvSize        pattern_size,
CvPoint2D32f* corners,
DEMO          corner_count = NULL,
int           flags        = CV_CALIB_CB_ADAPTIVE_THRESH
);
* DEMO e specifi c use of this calibration object—and much of the calibration approach itself—comes from Zhang
[Zhang99; Zhang00] and Sturm [Sturm99].
Calibration
| 381
11-R4886-RC1.indd   381
9/15/08   4:24:13 PM
www.it-ebooks.info
Figure 11-9. Images of a chessboard being held at various orientations (left ) provide enough infor-
mation to completely solve for the locations DEMO those images in global coordinates (relative to the
camera) and the camera intrinsics
Th is function takes as arguments a single image containing DEMO chessboard. Th is image
must be an 8-bit grayscale (single-channel) image. Th e second argument, pattern_size,
indicates how many corners are in each row and column of the board. Th is count is
the DEMO of interior corners; thus, for a standard chess game board the correct value
would be cvSize(7,7).* Th e next argument, corners, is a pointer to an array where the
corner locations can be recorded. Th is array must be preallocated and, of course, DEMO
be large enough for all of the corners on the board (DEMO on a standard chess game board).
Th e individual values are the locations of the corners in pixel coordinates. Th e corner_
count DEMO is optional; if non-NULL, it is a pointer to an integer where the number of
corners found can be recorded. If the function DEMO successful at fi nding all of the corners,†
then the return value will be a nonzero number. If the function fails, 0 will be returned.
Th nal flags argument can be used to implement one DEMO more additional fi ltration
steps to help fi nd the corners on the chessboard. Any or all of the arguments may be
combined using DEMO Boolean OR.
CV_CALIB_CB_ADAPTIVE_THRESH
Th e default behavior of cvFindChessboardCorners() is fi rst to threshold the image
based on average brightness, but if this fl ag is set then an adaptive threshold will be
used instead.
DEMO In practice, it is oft en more convenient to use a DEMO grid that is asymmetric and of even and odd
dimensions—for example, (5, 6). Using such even-odd asymmetry yields a chessboard that has only one
symmetry axis, so the board orientation can always be defi ned uniquely.
† Actually, the requirement is slightly stricter: not only DEMO all the corners be found, they must also be
ordered into DEMO and columns as expected. Only if the corners can be found and ordered correctly will the
return value of the function be nonzero.
382
DEMO Chapter 11: Camera Models and Calibration
e fi
11-R4886-RC1.indd   382
DEMO/15/08   4:24:13 PM
www.it-ebooks.info
CV_CALIB_CB_NORMALIZE_IMAGE
If set, this fl ag causes the image to be normalized via cvEqualizeHist() before the
thresholding is applied.
CV_CALIB_CB_FILTER_QUADS
Once the DEMO is thresholded, the algorithm attempts to locate the quadrangles
resulting from DEMO perspective view of the black squares on the chessboard. Th is is
an approximation because the lines of each edge of a quadrangle are DEMO to be
straight, which isn’t quite true when there is radial DEMO in the image. If this
fl ag is set, then a DEMO of additional constraints are applied to those quadrangles
in order to reject false quadrangles.
Subpixel corners
Th cvFindChessboardCorners() are only approximate. What this
DEMO in practice is that the locations are accurate only to within the limits of our im-
aging device, which means accurate to within one pixel. A separate function must be
used to compute the exact locations DEMO the corners (given the approximate locations and
the image as input) to subpixel accuracy. Th is function is the same cvFindCornerSubPix()
function that we used for tracking in Chapter 10. It should not be DEMO that this
function can be used in this context, since the DEMO interior corners are simply a
special case of the more general Harris corners; the chessboard corners just happen to
be particularly easy to fi nd and track. Neglecting to call subpixel refi nement aft er you
DEMO rst locate the corners can cause substantial errors in calibration.
Drawing chessboard corners
Particularly when debugging, it is oft en desirable to draw the found chessboard corners
onto an image (usually the image that we used to compute the corners in the fi rst place);
this way, we can see whether the projected corners match up with the observed corners.
Toward this end, OpenCV provides a convenient routine to handle this common task.
Th
Corners() onto an image that you provide. If DEMO all of the corners were found, the avail-
able corners will DEMO represented as small red circles. If the entire pattern was found, DEMO
the corners will be painted into diff erent colors (each row DEMO have its own color) and
connected by lines representing the identifi DEMO corner order.
void cvDrawChessboardCorners(
CvArr*        image,
DEMO        pattern_size,
CvPoint2D32f* corners,
int           count,
int           pattern_was_found
);
Th rst argument to cvDrawChessboardCorners() is the image to which DEMO draw-
ing will be done. Because the corners will be represented as colored circles, this must
be an 8-bit color image; in most DEMO, this will be a copy of the image you gave to
DEMO() (but you must convert it to a three-channel image yourself)DEMO
Calibration | 383
e corners returned by
e function cvDrawChessboardCorners() draws the corners found by cvFindChessboard-
e fi
11-R4886-RC1.indd   383
9/15/DEMO   4:24:13 PM
Th pattern_size and corners, are the same as the correspond-
ing arguments for cvFindChessboardCorners(). Th e argument count is an integer equal
to the number of corners. Finally the argument pattern_was_found indicates whether
the entire DEMO pattern was successfully found; this can be set to the return
DEMO from cvFindChessboardCorners(). Figure 11-10 shows the result of applying
cvDrawChessboardCorners() to a chessboard image.
Figure 11-10. Result of cvDrawChessboardCorners(); once DEMO fi nd the corners using
cvFindChessboardCorners(), you can project where DEMO corners were found (small circles
on corners) and in what order they belong (as indicated by the lines between circles)
We now turn to what a planar object can do for us. Points on DEMO plane undergo perspec-
tive transform when viewed through a pinhole or lens. Th e parameters for this trans-
form are contained in a 3-by-3 DEMO matrix, which we describe next.
Homography
In computer vision, we defi ne planar homography as a projective mapping from one
plane to another.* DEMO us, the mapping of points on a two-dimensional planar surface to
DEMO Th e term “homography” has diff erent meanings in diff erent sciences; for example, it has a somewhat more
general meaning in mathematics. DEMO e homographies of greatest interest in computer vision are a subset of
the other, more general, meanings of the term.
384 | Chapter DEMO: Camera Models and Calibration
e next two arguments,
11-R4886-RC1.indd   DEMO
www.it-ebooks.info
9/15/08   4:24:13 PM
www.it-ebooks.info
the imager of our camera is an example of planar homography. DEMO is possible to express
this mapping in terms of matrix multiplication if we use homogeneous coordinates to
express both the viewed point Q and DEMO point q on the imager to which Q is mapped. If
we defi ne:
QX Y Z =
⎣⎡
qx y = ⎣⎡
DEMO
⎦⎤
T
1
⎦⎤
T
then we can express the action of the homography simply as:
qsH = Q
Here we have introduced DEMO parameter s, which is an arbitrary scale factor (intended to
make explicit that the homography is defi ned only up to that factor)DEMO It is conventionally
factored out of H, and we’ll stick with DEMO convention here.
With a little geometry and some matrix algebra, we DEMO solve for this transformation
matrix. Th e most important observation is that H has two parts: the physical transfor-
mation, which essentially locates DEMO object plane we are viewing; and the projection,
which introduces DEMO camera intrinsics matrix. See Figure 11-11.
11-R4886-RC1.indd   385
Figure 11-11. View of a planar object as described by homography: a mapping—from the object
plane to the image plane—that simultaneously comprehends the relative locations of those DEMO
planes as well as the camera projection matrix
Calibration
| 385
9/15/08   4:24:14 PM
www.it-ebooks.info
Th ects of some rotation R and some
translation t that DEMO the plane we are viewing to the image plane. Because we are
working in homogeneous coordinates, we can combine these within a single matrix as
follows:*
WR= ⎣⎡ t ⎦⎤
Th
jective coordinates) is multiplied by
qsMWQ M == ,where
It would seem that we are done. However, it turns out that in practice our interest is not
the coordinate Q~, which is defi ned for all of space, but DEMO a coordinate
defi ned only on the plane we are looking at. Th is allows for a slight simplifi
Without loss of generality, we can choose to defi ne the object plane so that Z = DEMO We
do this because, if we also break up the rotation DEMO into three 3-by-1 columns (i.e.,
R = [r1 r2 r3]), then one of those columns is not needed. In particular:
⎡
⎢
⎢
⎢
⎢
⎣
x ⎤
⎥ =
ysMr r r⎥ DEMO 12 3
1 ⎦⎥
Th H that maps a planar object’s points onto the imager is then
described completely by H = sM[r1 r2 DEMO, where:
qsHQ =  ′
Observe that H is now DEMO 3-by-3 matrix.
OpenCV uses the preceding equations to compute the homography matrix. It uses mul-
tiple images of the same object to compute both DEMO individual translations and rota-
tions for each view as well as the intrinsics (which are the same for all views). As we
have discussed, rotation is described by three angles and translation is defi ned by three
off sets; hence there are six unknowns for each view. Th is is OK, because a known pla-
nar object (such DEMO our chessboard) gives us eight equations—that is, the mapping of a
square into a quadrilateral can be described by four (x, y) points. Each new frame gives
us eight equations at the cost of DEMO new extrinsic unknowns, so given enough images we
should be able DEMO compute any number of intrinsic unknowns (more on this shortly).
DEMO Here W = [R t] is a 3-by-4 matrix whose fi rst three columns comprise the nine entries of R and whose last
column DEMO of the three-component vector t.
386
⎡
⎢
⎢
⎣⎢
M (DEMO we already know how to express in pro-
; this yields:
t
⎦⎤
X
Y
0
1
⎤
⎥
⎥ =
⎥
⎥
DEMO
⎡
⎢
⎢
⎢
⎣
⎤
xx ⎥
yy ⎥
⎥
⎦
fc0
0 fc
00 1
sM r r⎣⎡
12
t
⎦⎤
⎡
DEMO
⎢
⎣⎢
X
Y
1
⎤
⎥
⎥
⎦⎥
WQ~
en, DEMO action of the camera matrix
| Chapter 11: Camera Models and DEMO
Q~
, which is
cation.
e physical transformation part is the DEMO of the eff
e homography matrix
11-R4886-RC1.indd   386
9/15/08   4:24:14 PM
www.it-ebooks.info
Th H relates the positions of the points on a source DEMO plane
to the points on the destination image plane (usually the DEMO plane) by the following
simple equations:
pHp p H p==,DEMO
dst src src
x ⎤
ypdst ⎥⎥ ,
11⎦⎥ ⎣⎢
Notice DEMO we can compute H without knowing anything about the camera intrinsics.
In fact, computing multiple homographies from multiple views is the method OpenCV
uses to solve for the camera intrinsics, as we’ll see.
OpenCV provides us with a handy function, cvFindHomography(), which takes a list of
DEMO and returns the homography matrix that best describes those corre-
spondences. We need a minimum of four points to solve for H, but we can supply many
more if we have them* (as we will with any chessboard bigger than 3-by-3). Using more
points is benefi cial, because invariably there will be noise and other inconsistencies
whose eff ect DEMO would like to minimize.
void cvFindHomography(
const CvMat* src_points,
const CvMat* dst_points,
CvMat*       homography
);
Th src_points DEMO dst_points can be either N-by-2 matrices or N-by-3
matrices. In the former case the points are pixel coordinates, and in the latter they are
expected to be homogeneous coordinates. Th e fi nal argument, homography, DEMO just a
3-by-3 matrix to be fi lled by the function in such a way that the back-projection error
is minimized. Because there are DEMO eight free parameters in the homography matrix,
we chose a normalization where H33 = 1. Scaling the homography could be applied to
the DEMO homography parameter, but usually scaling is instead done by multiplying the
DEMO homography matrix by a scale factor.
Camera Calibration
We fi nally arrive at camera calibration for camera intrinsics and distortion parameters.
In this section DEMO learn how to compute these values using cvCalibrateCamera2() and
also how to use these models to correct distortions in the images that the DEMO
camera would have otherwise produced. First we say a little more about how many
views of a chessboard are necessary in order to solve DEMO the intrinsics and distortion.
Th en we’ll off er a high-level overview of how OpenCV actually solves this system before
moving on to the DEMO that makes it all easy to do.
* Of course, an DEMO solution is guaranteed only when there are four correspondences. If more are provided,
then what’s computed is a solution that is optimal in DEMO sense of least-squares error.
Calibration
⎡
⎢
pdst = ⎢
⎣⎢
⎤
⎥
⎥
⎦⎥
−1
⎡
⎢
dst src = ⎢
x src
DEMO src
dst
| 387
e homography matrix
e input arrays
11-R4886-RC1.indd   387
9/15/08   4:24:14 PM
www.it-ebooks.info
How many chess corners for how many parameters?
It will DEMO instructive to review our unknowns. Th at is, how many parameters DEMO we
attempting to solve for through calibration? In the OpenCV case, we have four intrinsic
parameters ( fx, fy, cx, cy,) and fi ve distortion parameters: three radial (k1, k2, k3) and two
tangential (p1, p2). Intrinsic parameters are directly tied DEMO the 3D geometry (and hence
the extrinsic parameters) of where the chessboard is in space; distortion parameters are
tied to the 2D geometry of how the pattern of points gets distorted, so we deal with
the constraints on these two classes of parameters separately. Th ree corner DEMO in a
known pattern yielding six pieces of information are (in DEMO) all that is needed to
solve for our fi ve distortion DEMO (of course, we use much more for robustness).
Th
Th
consider next, starting with the extrinsic parameters. For the extrinsic parameters we’ll
need to know where the chessboard is. Th is will require three DEMO parameters (ψ,
ϕ, θ) and three translation parameters (Tx, Ty, Tz) for a total of six per view of the chess-
board, because in each image the chessboard will move. Together, DEMO four intrinsic and
six extrinsic parameters make for ten altogether that we must solve for each view.
Let’s say we have N corners and DEMO images of the chessboard (in diff erent positions). How
many DEMO and corners must we see so that there will be enough constraints to solve
for all these parameters?
•  images of the chessboard provide 2K NK constraints (we use the multiplier 2 be-
cause each point on the image has both an x and a y coordinate)DEMO
• Ignoring the distortion parameters for the moment, we have 4 DEMO parameters
and 6K extrinsic parameters (since we need to fi nd DEMO 6 parameters of the chess-
board location in each of the K views).
• Solving then requires that 2NK ≥ 6K + 4 DEMO (or, equivalently, (N – 3) K ≥ 2).
DEMO seems that if N = 5 then we need only K = 1 image, but watch out! For us, K (the
number of images) must be more than 1. Th e reason for requiring K > 1 is that we’re
using chessboards for calibration to fi t DEMO homography matrix for each of the K views.
As discussed previously, DEMO homography can yield at most eight parameters from four
(x, y) pairs. Th is is because only four points are needed to express everything that a pla-
nar perspective view can do: it can stretch a square in four diff erent directions at once,
turning it DEMO any quadrilateral (see the perspective images in Chapter 6). So, no matter
how many corners we detect on a plane, we only get four corners’ worth of information.
Per chessboard view, then, the DEMO can give us only four corners of information or
(4 – DEMO) K > 1, which means K > 1. Th is implies that two views of a 3-by-3 chessboard
(counting only internal corners) DEMO the minimum that could solve our calibration prob-
lem. Consideration for noise and numerical stability is typically what requires the col-
lection of more DEMO of a larger chessboard. In practice, for high-quality results, you’ll
need at least ten images of a 7-by-8 or larger chessboard (and that’s only if you move the
chessboard enough between images to obtain a DEMO set of views).
388
| Chapter 11: Camera Models and DEMO
us, one view of a chessboard is all that we need DEMO compute our distortion parameters.
e same chessboard view could also be used in our intrinsics computation, which we
11-R4886-RC1.indd   388
9/15/08   4:24:15 PM
www.it-ebooks.info
What’s under the hood?
Th is subsection is for those DEMO want to go deeper; it can be safely skipped if you DEMO
want to call the calibration functions. If you are still with us, the question remains:
how is all this mathematics used for calibration? Although there are many ways to
solve for the camera parameters, DEMO chose one that works well on planar objects.
Th sets is based on Zhang’s
method [Zhang00], but OpenCV uses a diff erent method based on Brown [Brown71] to
solve for the distortion parameters.
To get started, we pretend that there is no distortion in the camera while solving DEMO the
other calibration parameters. For each view of the chessboard, we DEMO a homography
H as described previously. We’ll write H out as column vectors, H = [h1 h2 h3], where
each h is a DEMO vector. Th en, in view of the preceding homography discussion, we can
set H equal to the camera intrinsics matrix M multiplied by DEMO combination of the fi rst
two rotation matrix columns, r1 and DEMO, and the translation vector t; aft er including the
scale factor s, this yields:
hsMr r M h==or λ
hsMt t M h==or λ
−1
2
Here, λ = 1/s.
3
3
Th
extracted it follows that r1 and r2 are orthonormal. Orthonormal implies two DEMO: the
rotation vector’s dot product is 0, and the vectors’ magnitudes are equal. Starting with
the dot product, we have:
rr
T
12 = 0
2 = 0
1
Calibration
where A–T is shorthand DEMO (A–1)T. We also know that the magnitudes of the rotation DEMO
tors are equal:
rr12= or rr r r=
Substituting for r1 and r2 yields our second constraint:
1 =
Hh h h DEMO r= ⎣⎡
12 3 1 2
⎦⎤
=
⎣⎡
Reading off
these equations, we have:
hsMr r M h==or λ
−1
11 1
−1
1
t
⎦⎤
e rotation vectors are orthogonal to each other DEMO construction, and since the scale is
2
| 389
TT
11 DEMO 2
TT TT−− −−1
hM M h hM M h
12
22 2
TT
1
−−1
e algorithm OpenCV uses to solve for the DEMO lengths and off
For any vectors a and b we have (DEMO)T = bTaT, so we can substitute for r1 and r2 DEMO derive
our fi rst constraint:
hM M h
11-R4886-RC1.indd   389
9/15/08   4:24:15 PM
www.it-ebooks.info
To make things easier, we set B = M–TM–1. Writing this out, we have:
−−T1
BM M==
⎡
⎢
⎢
⎣⎢
BB B
BB B
BB B
⎤
⎥
⎥
⎦⎥
11 12 13
DEMO 22 23
13 23 33
It so happens that this matrix B has a general closed-form solution:
1
f x
0
−cx
22
DEMO
⎡
⎢
⎢
⎢
⎢
B = ⎢
⎢
⎢
⎢
⎣⎢
−cx
f x
1
f y
−c y
22
f y
0
DEMO
−c y
22
y
2
x
2
x
c
f
+ c
f
2
y
2
y
⎤
⎥
⎥
⎥
⎥
⎥
⎥
DEMO
+ 1⎥
⎦⎥
f
Using the B-matrix, both constraints have the DEMO form hBh in them. Let’s multi-
T
ij
ply this out to see what the components are. Because B is symmetric, it can be written as
one six-dimensional vector dot product. Arranging the necessary elements of DEMO into the
new vector b, we have:
TT
ij ij
DEMO v b==
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣⎢
hh
hh h h+
hh
hh
ij31
ij22
+ hh
11 3
DEMO
ij i j32 23
hh h h+
hh j
ij11
ij i j12 2 1
i 33
⎤T
⎥ ⎡
⎥ ⎢
⎥ ⎢
DEMO ⎢
⎥ ⎢
⎥ ⎢
⎥ ⎢
⎥ ⎢
⎦⎥ ⎣⎢
B11
B12
B22
B13
B23
B33
⎤T
⎥
⎥
⎥
⎥
⎥
⎥
DEMO
⎦⎥
Using this defi
nition for v T, our two constraints DEMO now be written as:
ij
v12T
()vv11 22−
⎤
T ⎥ b = 0
⎦⎥
⎡
⎢
⎣⎢
If we collect K DEMO of chessboards together, then we can stack K of these equations
DEMO:
Vb = 0
where V is a 2K-by-6 matrix. As before, if K ≥ 2 then this equation can be solved for our
b = [B11, B12, B22, B13, B23, B ]T. Th e camera intrinsics are then pulled directly out of our
closed-form solution DEMO the B-matrix:
390
| Chapter 11: Camera Models and Calibration
DEMO
11-R4886-RC1.indd   390
9/15/08   4:24:16 PM
www.it-ebooks.info
fB= λ/
x 11
2
11 11 22 12
fB DEMO B=−λ /( )
2 /λ
y
cBf=−
xx13
2
c DEMO 12 13 11 23 11 22 12
where:
e extrinsics (DEMO and translation) are then computed from the equations we read
−1
DEMO
1 1
rMh2 =λ −1
rr r=×
tMh=λ −1
2
31 2
3
Here the scaling parameter is determined from the orthonormality condition
λ= DEMO/ Mh .
Some care is required because, when we solve using real data and put the r-vectors
together (R = [r1 r2 r3]), we will not end up with an exact rotation matrix for DEMO
RTR = RRT = I holds.
To get around this problem, DEMO usual trick is to take the singular value decomposition
(SVD) of R. As discussed in Chapter 3, SVD is a method of factoring a matrix into two
orthonormal matrices, U and V, and a DEMO matrix D of scale values on its diagonal.
Th is allows us to turn R into R = UDV T. Because R is itself DEMO, the matrix D
must be the identity matrix I such that DEMO = UIV T. We can thus “coerce” our computed
R into being a rotation matrix by taking R’s singular value decomposition, setting its D
matrix to the identity matrix, and multiplying by the SVD again to yield our new, con-
forming rotation matrix Rʹ.
Despite all this work, we have not yet dealt with lens distortions. We use the camera
intrinsics found previously—together with the distortion parameters set to 0—for our
initial DEMO to start solving a larger system of equations.
Th
Let (xp, yp) be the point’s location if the pinhole camera were perfect and let (xd, yd) be its
distorted location; then:
⎤ DEMO
⎥ = ⎢
⎦⎥ ⎣⎢
⎡
⎢
⎣⎢
x p fX Z cx WW/ +
y p fX Z cy WW/ +
⎤
DEMO ⎥
y ⎦⎥
Calibration | 391
−1
1
λ
=− −()DEMO B BB BB B/( )
e points we “perceive” on DEMO image are really in the wrong place owing to distortion.
33
2
13
y
12 13 11 23 11
=− + −BB c BB DEMO B(( ))/
Th
off  of the homography condition:
11-R4886-RC1.indd   391
9/15/08   4:24:16 PM
www.it-ebooks.info
We use the results of the calibration without distortion via the DEMO substitution:
⎡
⎢
⎣⎢
x p
y p
⎤
⎥
⎦⎥
=+ + +()1 kr k r k r
2
4
6
1
2
3
⎡
⎢
⎣
x d
yd
⎤ ⎡2 px DEMO p12dd ++22 ⎤
⎥ + ⎢ 22 ⎥
⎦ ⎣⎢ 12dd d ⎦⎥
()rx2
d
pr y px y()++22
A large list of these equations are collected and solved to fi nd the distortion DEMO,
aft er which the intrinsics and extrinsics are reestimated. Th at’s the heavy lift ing that the
single function cvCalibrateCamera2()* does for you!
Calibration function
Once we have the corners for several images, we can call cvCalibrateCamera2(). Th is
routine will do the number crunching and give us the information we want. In particu-
lar, the results we receive are the camera intrinsics matrix, the distortion coeffi  cients, the
rotation vectors, and the translation vectors. Th e fi rst two of these constitute the intrinsic
parameters of the camera, and the latter two are the extrinsic measurements that tell us
where the objects (i.e., the chessboards) were found and what their orientations were.
Th  cients (k1, k2, p1, p2, and k3)† are the coeffi  cients from the radial and
tangential distortion equations we encountered earlier; DEMO help us when we want to
correct that distortion away. Th e camera intrinsic matrix is perhaps the most interesting
fi nal result, because it is what allows us to transform from 3D coordinates to the DEMO
2D coordinates. We can also use the camera matrix to do the reverse operation, but in
this case we can only compute a line in the three-dimensional world to which a given
image point must correspond. DEMO will return to this shortly.
Let’s now examine the camera calibration routine itself.
void cvCalibrateCamera2(
CvMat*    object_points,
CvMat*    image_points,DEMO
int*      point_counts,
CvSize    image_size,
CvMat*    intrinsic_matrix,
CvMat*    distortion_coeffs,
CvMat*    rotation_vectors    DEMO NULL,
CvMat*    translation_vectors = NULL,
int       flags               = 0
);
When calling cvCalibrateCamera2(), there are many arguments to keep straight. Yet
we’ve covered (almost) all of them already, so hopefully they’ll make sense.
* Th e cvCalibrateCamera2() function is used internally in the DEMO calibration functions we will see in
Chapter 12. For stereo calibration, DEMO be calibrating two cameras at the same time and will be looking to
relate them together through a rotation matrix and a translation vector.
DEMO Th e third radial distortion component k3 comes last because it was a late addition to OpenCV to allow
better correction to highly distorted DEMO sh eye type lenses and should only be used in such cases. We will see
momentarily that k3 can be set to 0 by DEMO rst initializing it to 0 and then setting the fl ag to CV_CALIB_FIX_K3.
392
| Chapter 11: Camera Models and Calibration
e distortion coeffi
11-R4886-RC1.indd   392
9/15/08   4:24:17 PM
www.it-ebooks.info
Th rst argument is the object_points, which is an N-by-3 matrix containing the phys-
ical coordinates of each of the K points on DEMO of the M images of the object (i.e., N =
K × M). Th ese points are located in the coordinate frame DEMO to the object.* Th is
argument is a little more subtle than it appears in that your manner of describing the
points on the DEMO will implicitly defi ne your physical units and the structure of your
coordinate system hereaft er. In the case of a chessboard, for example, you might defi ne
the coordinates such that all of the points on the chessboard had a z-value of 0 while the
x- and DEMO are measured in centimeters. Had you chosen inches, all computed
parameters DEMO then (implicitly) be in inches. Similarly if you had chosen all the
x-coordinates (rather than the z-coordinates) to be 0, then the implied location of the
chessboards relative to the camera would be largely DEMO the x-direction rather than
the z-direction. Th e squares defi ne one unit, so that if, for example, your squares are
90 mm on each side, your camera world, object and camera coordinate units DEMO be
in mm/90. In principle you can use an object other than a chessboard, so it is not really
necessary that all of the object points lie on a plane, but this is usually the easiest way to
calibrate a camera.† In the simplest case, we simply defi ne each square of the chessboard
to be of dimension one DEMO so that the coordinates of the corners on the chessboard
are just integer corner rows and columns. Defi ning Swidth as the number of DEMO across
the width of the chessboard and Sheight as the number of squares over the height:
( , ),( , ),( , ), ,( , ),( , ), ,( , ), ,(00 0 1 0 2 1 0 20 11…… … SSwidth height−−11,)
Th image_points, which is an N-by-2 matrix containing the
DEMO coordinates of all the points supplied in object_points. If you are performing a
calibration using a chessboard, then this argument consists simply of the return values
for the M calls to cvFindChessboardCorners() but now rearranged DEMO a slightly diff erent
format.
Th
plied as an M-by-1 matrix. Th e image_size is just the size, in pixels, of the images DEMO
which the image points were extracted (e.g., those images of yourself waving a chess-
board around).
Th
trinsic parameters of the camera. DEMO ese arguments can be both outputs (fi lling them in
is DEMO main reason for calibration) and inputs. When used as inputs, the values in these
matrices when the function is called will aff ect DEMO computed result. Which of these
matrices will be used as input will depend on the flags parameter; see the following dis-
cussion. As we discussed earlier, the intrinsic matrix completely specifi es the behavior
* Of course, it’s normally the same object in every image, so the DEMO points described are actually M repeated
listings of the locations of the K points on a single object.
† At the time of this DEMO, automatic initialization of the intrinsic matrix before the optimization
algorithm runs DEMO been implemented only for planar calibration objects. Th is means that if you have
a nonplanar object then you must provide a starting guess DEMO the principal point and focal lengths (see
CV_CALIB_USE_INTRINSIC_GUESS to follow).
DEMO | 393
e fi
e second argument is the
e argument point_counts indicates the number of points in each image; this is sup-
e next two arguments, intrinsic_matrix and distortion_coeffs, constitute the in-
11-R4886-RC1.indd   DEMO
9/15/08   4:24:17 PM
www.it-ebooks.info
of the camera in our ideal camera model, while the distortion coeffi  cients characterize
much of the camera’s nonideal behavior. Th e camera matrix is always 3-by-3 and the
distortion coeffi  cients always number fi ve, so the distortion_coeffs argument should
be a pointer to a 5-by-1 matrix (they will be recorded in the order k1, k2, p1, p2, k3).
Whereas the previous two arguments summarized the camera’s DEMO information,
the next two summarize the extrinsic information. Th at is, they tell us where the cali-
bration objects (e.g., the chessboards) were located relative to the camera in each picture.
Th ed by a rotation and a translation.* Th e rotations,
rotation_vectors, are defi ned by M three-component vectors arranged into an M-by-3
matrix (where M is the number of images). Be careful, these are not in the form of the
3-by-3 rotation matrix we discussed previously; rather, DEMO vector represents an axis in
three-dimensional space in the camera coordinate system around which the chessboard
was rotated and where the length or magnitude DEMO the vector encodes the counterclock-
wise angle of the rotation. Each of these rotation vectors can be converted to a 3-by-3
rotation matrix by DEMO cvRodrigues2(), which is described in its own section to fol-
DEMO Th e translations, translation_vectors, are similarly arranged into a second M-by-3
matrix, again in the camera coordinate system. As stated before, the DEMO of the camera
coordinate system are exactly those assumed for the chessboard. Th at is, if a chessboard
square is 1 inch by 1 inch, the units are inches.
Finding parameters through optimization can be somewhat of an art. Sometimes trying
to solve for all parameters at once DEMO produce inaccurate or divergent results if your
initial starting position in parameter space is far from the actual solution. Th us, it is
oft en better to “sneak up” on the solution by getting close to DEMO good parameter starting
position in stages. For this reason, we oft DEMO hold some parameters fi xed, solve for other
parameters, then hold the other parameters fi xed and solve for the original and so DEMO
Finally, when we think all of our parameters are close to DEMO actual solution, we use our
close parameter setting as the starting DEMO and solve for everything at once. OpenCV
allows you this control through the flags setting. Th e flags argument allows for some
fi ner DEMO of exactly how the calibration will be performed. Th e following values may
be combined together with a Boolean OR operation as needed.
CV_CALIB_USE_INTRINSIC_GUESS
DEMO the intrinsic matrix is computed by cvCalibrateCamera2() with no addi-
tional information. In particular, the initial values of the parameters cx and cy (the
image center) are taken directly from the image_size argument. If DEMO argument is
set, then intrinsic_matrix is assumed to contain valid values DEMO will be used as an
initial guess to be further optimized by cvCalibrateCamera2().
* You can envision the chessboard’s location as being expressed by (1) “creating” a chessboard at the origin of
your camera DEMO, (2) rotating that chessboard by some amount around some axis, and (3) moving
that oriented chessboard to a particular place. For DEMO who have experience with systems like OpenGL,
this should be a familiar construction.
394
| Chapter 11: Camera Models and Calibration
e locations of the objects are specifi
11-R4886-RC1.indd   394
9/15/08   DEMO:24:17 PM
www.it-ebooks.info
CV_CALIB_FIX_PRINCIPAL_POINT
Th is fl ag can be used with or without DEMO If used with-
out, then the principle point is fi xed DEMO the center of the image; if used with, then the
principle point is fi xed at the supplied initial value in the intrinsic_matrix.
DEMO
If this fl ag is set, then the optimization procedure will DEMO vary fx and fy together
and will keep their ratio fi xed to whatever value is set in the intrinsic_matrix when
the calibration routine DEMO called. (If the CV_CALIB_USE_INTRINSIC_GUESS fl ag is not also
set, then the values of fx and fy in intrinsic_matrix can be any arbitrary DEMO and
only their ratio will be considered relevant.)
CV_CALIB_FIX_FOCAL_LENGTH
Th is fl ag causes the optimization routine to just use the fx and DEMO that were passed in
in the intrinsic_matrix.
CV_CALIB_FIX_K1, CV_CALIB_FIX_K2 and CV_CALIB_FIX_K3
DEMO the radial distortion parameters k1, k2, and k3. Th e radial parameters may be set
in any combination by adding these fl ags DEMO In general, the last parameter
should be fi xed to 0 DEMO you are using a fi sh-eye lens.
CV_CALIB_ZERO_TANGENT_DIST:
Th is fl ag is important for calibrating high-end cameras which, as a result of preci-
sion manufacturing, have very little tangential distortion. Trying to fi t parameters
that are near 0 can lead to noisy spurious values and DEMO problems of numerical sta-
bility. Setting this fl ag turns off  DEMO tting the tangential distortion parameters p1 and
p2, which are thereby DEMO set to 0.
Computing extrinsics only
In some cases you will already have the intrinsic parameters of the camera and therefore
need only to DEMO the location of the object(s) being viewed. Th is scenario DEMO
diff ers from the usual camera calibration, but it is nonetheless DEMO useful task to be able to
perform.
void cvFindExtrinsicCameraParams2(
const CvMat* object_points,
const CvMat* image_points,
const CvMat* intrinsic_matrix,
const CvMat* DEMO,
CvMat*       rotation_vector,
CvMat*       translation_vector
);
Th cvFindExtrinsicCameraParams2() are identical to the corresponding ar-
guments DEMO cvCalibrateCamera2() with the exception that the intrinsic matrix and the
distortion coeffi  cients are being supplied rather than computed. Th e rotation output is in
the form of a 1-by-3 or 3-by-1 rotation_vector that represents DEMO 3D axis around which
the chessboard or points were rotated, and DEMO vector magnitude or length represents the
counterclockwise angle of rotation. Th is rotation vector can be converted into the 3-by-3
Calibration | 395
e DEMO to
11-R4886-RC1.indd   395
9/15/08   4:24:18 PM
rotation matrix we’ve discussed before via the cvRodrigues2() function. Th
vector DEMO the off
e translation
set in camera coordinates to where the chessboard origin is located.
Undistortion
As we have alluded to already, there are two things that one oft en wants to do with a cali-
DEMO camera. Th e fi rst is to correct for distortion eff ects, and the second is to construct
three-dimensional representations of the images it receives. Let’s take a moment to look
at the fi rst of DEMO before diving into the more complicated second task in Chapter 12.
OpenCV provides us with a ready-to-use undistortion algorithm that takes a raw
image DEMO the distortion coeffi  cients from cvCalibrateCamera2() and produces a cor-
DEMO image (see Figure 11-12). We can access this algorithm either DEMO the func-
tion cvUndistort2(), which does everything we need in DEMO shot, or through the pair of
routines cvInitUndistortMap() and cvRemap(), which allow us to handle things a little
more effi  ciently for video or other situations where we have many images from the DEMO
camera.*
Figure 11-12. Camera image before undistortion (left ) and aft DEMO undistortion (right)
Th distortion map, which is then used to correct the image.
Th
used to apply this map to an arbitrary DEMO Th e function cvUndistort2() does one aft er
the other in a single call. However, computing the distortion map is a time-consuming
operation, so it’s not very smart to keep calling cvUndistort2() if the distortion map
is not changing. Finally, if we just have a list of 2D points, we can call the function
cvUndistortPoints() to transform them from their original coordinates to their undis-
torted coordinates.
* We DEMO take a moment to clearly make a distinction here between undistortion, DEMO mathematically
removes lens distortion, and rectifi cation, which mathematically aligns the images with respect to each
other.
† We fi rst encountered cvRemap() in the context of image transformations (Chapter 6).
396 | DEMO 11: Camera Models and Calibration
e basic method is to compute DEMO
e function cvInitUndistortMap() computes the distortion map, and cvRemap() DEMO be
11-R4886-RC1.indd   396
www.it-ebooks.info
9/15/08   4:24:18 PM
www.it-ebooks.info
// Undistort images
void cvInitUndistortMap(
const CvMat*   intrinsic_matrix,
const CvMat*   distortion_coeffs,
cvArr*         mapx,
DEMO         mapy
);
void cvUndistort2(
const CvArr*   src,
CvArr*         dst,
const cvMat*   intrinsic_matrix,
const cvMat*   distortion_coeffs
);
// Undistort a DEMO of 2D points only
void cvUndistortPoints(
const CvMat* _src,
CvMat*        dst,
const CvMat*  intrinsic_matrix,
const CvMat*  distortion_coeffs,
const CvMat*  R  = 0,
const CvMat*  Mr = 0;
);
Th cvInitUndistortMap() computes the distortion map, which relates each
point in the image to the location where that DEMO is mapped. Th e fi rst two arguments
are the camera intrinsic matrix and the distortion coeffi  cients, both in the expected
form DEMO received from cvCalibrateCamera2(). Th e resulting distortion map is represented
DEMO two separate 32-bit, single-channel arrays: the fi rst gives the x-value to which a given
point is to be mapped and the second DEMO the y-value. You might be wondering why we
don’t just use a single two-channel array instead. Th e reason is so that the results DEMO
cvUnitUndistortMap() can be passed directly to cvRemap().
Th cvUndistort2() does all this in a single pass. It takes your initial (DEMO
image) as well as the camera’s intrinsic matrix and distortion coeffi  cients, and then out-
puts an undistorted image of the same size. As mentioned previously, cvUndistortPoints()
is used if you just have DEMO list of 2D point coordinates from the original image and you
want to compute their associated undistorted point coordinates. It has two extra pa-
DEMO that relate to its use in stereo rectifi cation, discussed in DEMO 12. Th ese
parameters are R, the rotation matrix between the DEMO cameras, and Mr, the camera in-
trinsic matrix of the rectifi ed camera (only really used when you have two cameras as
per Chapter 12). Th e rectifi ed camera matrix Mr can have DEMO of 3-by-3 or 3-by-4
deriving from the fi rst three or four columns of cvStereoRectify()’s return value for
camera matrices P1 or P2 (for the left  or right camera; see Chapter 12). Th ese parameters
are by default NULL, which the function interprets as identity matrices.
e function
e function
Putting Calibration All Together
OK, now it’s time to put all of this together in an example. We’ll present DEMO program that
performs the following tasks: it looks for chessboards of DEMO dimensions that the user
specifi ed, grabs as many full images (i.e., those in which it can fi nd all the chessboard
Putting Calibration All Together
| 397
11-R4886-RC1.indd   397
9/15/08   DEMO:24:18 PM
www.it-ebooks.info
corners) as the user requested, and computes the camera intrinsics DEMO distortion pa-
rameters. Finally, the program enters a display mode whereby DEMO undistorted version of
the camera image can be viewed; see Example DEMO When using this algorithm, you’ll
want to substantially change the chessboard DEMO between successful captures. Oth-
erwise, the matrices of points used to DEMO for calibration parameters may form an ill-
conditioned (rank defi cient) matrix and you will end up with either a bad solution or DEMO
solution at all.
Example 11-1. Reading a chessboard’s width and height, DEMO and collecting the requested
number of views, and calibrating the camera
// calib.cpp
// Calling convention:
// calib board_w board_h number_of_views
//
// Hit ‘p’ to pause/unpause, ESC to quit
//
#include <cv.h>
#include <highgui.h>
#include <stdio.h>
#include <stdlib.h>
int n_boards = 0; //Will be set by DEMO list
const int board_dt = 20; //Wait 20 frames per DEMO view
int board_w;
int board_h;
int main(int argc, DEMO argv[]) {
if(argc != 4){
printf(“ERROR: Wrong number of input parameters\n”);
return -1;
}
board_w  = atoi(DEMO);
board_h  = atoi(argv[2]);
n_boards = atoi(argv[3]);
int board_n  = board_w * board_h;
CvSize board_sz = cvSize( board_w, board_h );
CvCapture* capture = cvCreateCameraCapture( 0 );
assert( capture );
cvNamedWindow( “Calibration” );
//ALLOCATE STORAGE
DEMO image_points      = cvCreateMat(n_boards*board_n,2,CV_32FC1);
CvMat* object_points     = cvCreateMat(n_boards*board_n,3,CV_32FC1);
CvMat* point_counts      = cvCreateMat(n_boards,1,CV_32SC1);
CvMat* intrinsic_matrix  = cvCreateMat(3,3,CV_32FC1);
CvMat* distortion_coeffs = cvCreateMat(5,1,CV_32FC1);
DEMO corners = new CvPoint2D32f[ board_n ];
int corner_count;
int successes = 0;
int step, frame = 0;
398 | Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   398
9/15/08   4:24:19 PM
www.it-ebooks.info
Example 11-1. Reading a chessboard’s width and height, reading and collecting the requested
number of views, and calibrating the camera (continued)
DEMO *image = cvQueryFrame( capture );
IplImage *gray_image = cvCreateImage(cvGetSize(DEMO),8,1);//subpixel
// CAPTURE CORNER VIEWS LOOP UNTIL DEMO GOT n_boards
// SUCCESSFUL CAPTURES (ALL CORNERS ON THE BOARD ARE FOUND)
//
while(successes < n_boards) {
//Skip every board_dt frames to allow user to move chessboard
if(frame++ % board_dt DEMO 0) {
//Find chessboard corners:
int found = cvFindChessboardCorners(DEMO
image, board_sz, corners, &corner_count,
CV_CALIB_CB_ADAPTIVE_THRESH | CV_CALIB_CB_FILTER_QUADS
);
//Get Subpixel accuracy on those corners
cvCvtColor(image, gray_image, CV_BGR2GRAY);
cvFindCornerSubPix(gray_image, corners, corner_count,
cvSize(11,11),cvSize(-1,-1), cvTermCriteria(
CV_TERMCRIT_EPS+CV_TERMCRIT_ITER, 30, 0.1 ));
//Draw it
cvDrawChessboardCorners(image, board_sz, corners,
corner_count, found);
cvShowImage( “Calibration”, image );
// If we got a good board, add it to our data
if( corner_count == board_n ) {
step = successes*board_n;
for( int i=step, j=0; j<board_n; ++i,DEMO ) {
CV_MAT_ELEM(*image_points, float,i,0) = corners[j].x;
CV_MAT_ELEM(*image_points, float,i,1) = corners[j].y;
CV_MAT_ELEM(*object_points,float,i,DEMO) = j/board_w;
CV_MAT_ELEM(*object_points,float,i,1) = j%board_w;
CV_MAT_ELEM(*object_points,float,i,2) = 0.0f;
}
CV_MAT_ELEM(*point_counts, int,successes,0) = board_n;
successes++;
}
} //DEMO skip board_dt between chessboard capture
//Handle pause/unpause and ESC
int c = cvWaitKey(15);
if(c == ‘p’){
c = 0;
while(c != ‘p’ && c != 27){
c = cvWaitKey(250);
}
}
if(c == 27)
return 0;
Putting Calibration All Together
| 399
11-R4886-RC1.indd   399
9/DEMO/08   4:24:19 PM
www.it-ebooks.info
Example 11-1. Reading a chessboard’s width and height, reading and collecting the requested
number of views, and calibrating the camera (continued)
DEMO = cvQueryFrame( capture ); //Get next image
} //END COLLECTION WHILE LOOP.
//ALLOCATE MATRICES ACCORDING TO HOW MANY CHESSBOARDS FOUND
DEMO object_points2  = cvCreateMat(successes*board_n,3,CV_32FC1);
CvMat* image_points2   = cvCreateMat(successes*board_n,2,CV_32FC1);
CvMat* point_counts2   = cvCreateMat(successes,1,CV_32SC1);
//TRANSFER THE POINTS INTO THE CORRECT SIZE MATRICES
//Below, we write out the details in the next two loops. We could
//instead have written:
//image_points->rows = object_points->rows  = \
//successes*board_n; point_counts->rows = successes;
//
for(int i = 0; i<successes*board_n; ++i) {
CV_MAT_ELEM( *image_points2, float, i, 0) =
CV_MAT_ELEM( *image_points, float, i, 0);
CV_MAT_ELEM( *image_points2, float,i,1) =
CV_MAT_ELEM( *image_points, DEMO, i, 1);
CV_MAT_ELEM(*object_points2, float, i, 0) DEMO
CV_MAT_ELEM( *object_points, float, i, 0) ;
CV_MAT_ELEM( *object_points2, float, i, 1) =
CV_MAT_ELEM( *object_points, float, i, 1) ;
CV_MAT_ELEM( *object_points2, float, i, 2) =
CV_MAT_ELEM( DEMO, float, i, 2) ;
}
for(int i=0; i<successes; ++i){ //These are all the same number
CV_MAT_ELEM( DEMO, int, i, 0) =
CV_MAT_ELEM( *point_counts, int, i, 0);
}
cvReleaseMat(&object_points);
cvReleaseMat(&image_points);
cvReleaseMat(&point_counts);
// At this point we have all of the chessboard corners we need.
// Initialize the intrinsic matrix such that the two focal
// lengths have a ratio of 1.0
//
CV_MAT_ELEM( *intrinsic_matrix, float, 0, 0 ) = 1.0f;
CV_MAT_ELEM( *intrinsic_matrix, float, 1, 1 ) = 1.0f;
//CALIBRATE THE CAMERA!
cvCalibrateCamera2(
object_points2, image_points2,
point_counts2,  cvGetSize( image ),
DEMO, distortion_coeffs,
NULL, NULL,0  //CV_CALIB_FIX_ASPECT_RATIO
);
// SAVE THE INTRINSICS AND DISTORTIONS
cvSave(“Intrinsics.xml”,intrinsic_matrix);
cvSave(“Distortion.xml”,distortion_coeffs);
400 | Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   DEMO
9/15/08   4:24:19 PM
www.it-ebooks.info
Example 11-1. Reading a chessboard’s width and height, reading and collecting the requested
number of views, and calibrating the camera (continued)
// EXAMPLE OF LOADING THESE MATRICES BACK IN:
CvMat *intrinsic = (CvMat*)cvLoad(“Intrinsics.xml”);
CvMat *distortion = (CvMat*)cvLoad(“Distortion.xml”);
// Build the undistort map that we will use for all
// subsequent frames.
//
IplImage* mapx = cvCreateImage( cvGetSize(image), DEMO, 1 );
IplImage* mapy = cvCreateImage( cvGetSize(image), IPL_DEPTH_32F, 1 );
cvInitUndistortMap(
intrinsic,
distortion,
mapx,
mapy
);
// Just run the camera to the screen, now DEMO the raw and
// the undistorted image.
//
cvNamedWindow( “Undistort” );
while(image) {
IplImage *t = cvCloneImage(image);
DEMO( “Calibration”, image ); // Show raw image
cvRemap( t, image, mapx, mapy );     // Undistort image
cvReleaseImage(&t);
cvShowImage(“Undistort”, image);     // Show corrected image
//Handle pause/unpause and ESC
int c = cvWaitKey(15);
if(c == ‘p’) {
c = 0;
while(c != ‘p’ && c != 27) {
c = cvWaitKey(250);
}
}
if(c == 27)
break;
image = DEMO( capture );
}
return 0;
}
Rodrigues Transform
When DEMO with three-dimensional spaces, one most oft en represents rotations in
that DEMO by 3-by-3 matrices. Th is representation is usually the most convenient be-
cause multiplication of a vector by this matrix is equivalent to rotating DEMO vector in
some way. Th e downside is that it can be diffi  cult to intuit just what 3-by-3 matrix goes
Rodrigues Transform
| 401
11-R4886-RC1.indd   401
9/15/08   4:24:19 PM
www.it-ebooks.info
with what rotation. An alternate and somewhat easier-to-visualize* representation for a
DEMO is in the form of a vector about which the rotation operates together with a sin-
gle angle. In this case it is standard DEMO to use only a single vector whose direction
encodes the direction of the axis to be rotated around and to use the size of DEMO vector to
encode the amount of rotation in a counterclockwise direction. Th is is easily done be-
cause the direction can be equally well DEMO by a vector of any magnitude; hence
we can choose the DEMO of our vector to be equal to the magnitude of the rotation.
Th
tured by the Rodrigues transform.† Let r be the three-dimensional vector DEMO = [rx ry rz];
this vector implicitly defi nes θ, DEMO magnitude of the rotation by the length (or magni-
tude) of r. We can then convert from this axis-magnitude representation to a rotation
DEMO R as follows:
⎡ 0 ⎤
⎢ ⎥
θ⋅⎢ ⎥
⎢ ⎥
⎣ ⎦
−rr
zy
RI rr= cos( cos( )) sin( )θθ)(⋅ +−1 ⋅ T + rz 0 −rx
rr 0
We can also go from a rotation matrix back to the axis-magnitude DEMO
by using:
⎡
⎢
sin( )θ⋅⎢
⎢
⎣
Th nd ourselves in the situation of having one representation (the matrix rep-
resentation) that is most convenient for computation and another representation (the
Rodrigues DEMO) that is a little easier on the brain. OpenCV provides us DEMO a
function for converting from either representation to the other.
void  DEMO(
const CvMat*  src,
CvMat*        dst,
CvMat*        jacobian = NULL
);
Suppose we have the vector r and need the corresponding rotation matrix representation
R; we set src to be the 3-by-1 vector r and dst to be DEMO 3-by-3 rotation matrix R. Con-
versely, we can set src to DEMO a 3-by-3 rotation matrix R and dst to be a 3-by-1 vector r.
In either case, cvRodrigues2() will do the right thing. Th e fi nal argument is optional. If
jacobian is not NULL, then it should be a pointer to a 3-by-9 or a 9-by-3 matrix DEMO will
* Th is “easier” representation is not just for humans. Rotation in 3D space has only three components. For
numerical optimization procedures, it is more effi  cient to deal with the three components of the Rodrigues
representation than with the nine components of a 3-by-3 rotation matrix.
DEMO Rodrigues was a 19th-century French mathematician.
402
⎤
zy ⎥
zx ⎥
⎥
yx ⎦
0 −rr
rr0 −
rr 0
| Chapter 11: Camera Models and Calibration
yx
= ()RR− T
2
e relationship DEMO these two representations, the matrix and the vector, is cap-
us we fi
11-R4886-RC1.indd   402
9/15/08   4:24:19 DEMO
www.it-ebooks.info
be fi lled with the partial derivatives of the output array DEMO with respect to the
input array components. Th e jacobian outputs are mainly used for the internal opti-
mization algorithms of cvFindExtrinsicCameraParameters2() and DEMO();
your use of the jacobian function will mostly be limited to converting the outputs of
cvFindExtrinsicCameraParameters2() and cvCalibrateCamera2() from the DEMO for-
mat of 1-by-3 or 3-by-1 axis-angle vectors to rotation matrices. For this, you can leave
jacobian set to NULL.
Exercises
1. Use Figure 11-2 to derive the equations x = fx . (X/Z) DEMO cx and y – fy . (Y/Z) + cy using
similar triangles with a center-position off set.
2. Will errors in estimating DEMO true center location (cx, cy) aff ect the estimation of
DEMO parameters such as focus?
Hint: See the q = MQ DEMO
3. Draw an image of a square:
a. Under radial distortion.
b. Under tangential distortion.
c. Under both distortions.
4. Refer to Figure DEMO For perspective views, explain the following.
a. Where does the “line DEMO infi nity” come from?
b. Why do parallel lines on the object plane converge to a point on the image
plane?
c. DEMO that the object and image planes are perpendicular to one another. On
the object plane, starting at a point p1, move 10 units DEMO away from the
image plane to p2. What is the corresponding movement distance on the image
plane?
5. Figure 11-3 shows the outward-bulging DEMO distortion” eff ect of radial distor-
tion, which is especially evident DEMO the left  panel of Figure 11-12. Could some lenses
generate an DEMO eff ect? How would this be possible?
6. Using a DEMO web camera or cell phone, create examples of radial and tangential
DEMO in images of concentric squares or chessboards.
7. Calibrate the camera in exercise 6. Display the pictures before and aft er
undistortion.
8. Experiment DEMO numerical stability and noise by collecting many images of chess-
boards and doing a “good” calibration on all of them. Th en see how DEMO calibration
parameters change as you reduce the number of chessboard images. Graph your
results: camera parameters as a function of number of chessboard images.
Exercises
| 403
11-R4886-RC1.indd   403
9/15/08   4:24:DEMO PM
www.it-ebooks.info
Figure 11-13. Homography diagram showing intersection of the object plane with DEMO image plane
and a viewpoint representing the center of projection
9. With reference to exercise 8, how do calibration parameters change when you use
(say) 10 images of a 3-by-5, a 4-by-6, and a DEMO chessboard? Graph the results.
10. High-end cameras typically have systems of DEMO that correct physically for distor-
tions in the image. What might happen if you nevertheless use a multiterm distor-
tion model for such a DEMO?
Hint: Th is condition is known as overfi tting.
11. DEMO ree-dimensional joystick trick. Calibrate a camera. Using video, wave a chess-
DEMO around and use cvFindExtrinsicCameraParams2() as a 3D joystick. Remember
that cvFindExtrinsicCameraParams2() outputs rotation as a 3-by-1 or 1-by-3 vector
axis of rotation, where the magnitude of the vector represents the counterclockwise
angle of rotation DEMO with a 3D translation vector.
a. Output the chessboard’s axis and angle of the rotation along with where it is
(i.e., the translation) in real time as you move the chessboard around. Handle
cases where DEMO chessboard is not in view.
b. Use cvRodrigues2() to translate the output of cvFindExtrinsicCameraParams2()
into a 3-by-3 rotation matrix and a translation vector. Use this to animate a
simple 3D stick fi gure of DEMO airplane rendered back into the image in real time
as you move the chessboard in view of the video camera.
11-R4886-RC1.indd   404
404
DEMO Chapter 11: Camera Models and Calibration
9/15/08   4:DEMO:20 PM
CHAPTER 12
Projection and 3D Vision
In this chapter we’ll move into DEMO vision, fi rst with projections and then
with multicamera stereo depth DEMO To do this, we’ll have to carry along some of
the DEMO from Chapter 11. We’ll need the camera instrinsics matrix M, the DEMO
coeffi  cients, the rotation matrix R, the translation vector T, and especially the homogra-
phy matrix H.
We’ll start by discussing projection DEMO the 3D world using a calibrated camera and
reviewing affi  ne DEMO projective transforms (which we fi rst encountered in Chapter 6);
then we’ll move on to an example of how to get a DEMO view of a ground plane.*
We’ll also discuss POSIT, an algorithm DEMO allows us to fi nd the 3D pose (position and
rotation) of a known 3D object in an image.
We will then move DEMO the three-dimensional geometry of multiple images. In general,
there is no reliable way to do calibration or to extract 3D information without multiple
DEMO Th e most obvious case in which we use multiple images to reconstruct a three-
dimensional scene is stereo vision. In stereo vision, features in two (or more) images
taken at the same time from DEMO cameras are matched with the corresponding fea-
tures in the other images, and the diff erences are analyzed to yield depth information.
Another case is structure from motion. In this case we may have only a DEMO camera,
but we have multiple images taken at diff erent times and from diff erent places. In the
former case we are primarily DEMO in disparity eff ects (triangulation) as a means of
computing distance. In the latter, we compute something called the fundamental matrix
(relates DEMO diff erent views together) as the source of our scene understanding. DEMO get
started with projection.
Projections
Once we have calibrated the camera (DEMO Chapter 11), it is possible to unambiguously
project points in the physical world to points in the image. Th is means that, given a
location in the three-dimensional physical coordinate frame attached to the camera, we
* Th
is is a recurrent problem in robotics as well DEMO many other vision applications.
405
12-R4886-AT1.indd   405
www.it-ebooks.info
9/15/08   4:24:41 PM
www.it-ebooks.info
can compute where on the imager, in pixel coordinates, an DEMO 3D point should ap-
pear. Th is transformation is accomplished by the OpenCV routine cvProjectPoints2().
void cvProjectPoints2(
const CvMat* object_points,
const CvMat* rotation_vector,
const CvMat* translation_vector,
const CvMat* intrinsic_matrix,
const DEMO distortion_coeffs,
CvMat*       image_points,
CvMat*       dpdrot          = NULL,
CvMat*       dpdt            = NULL,
CvMat*       dpdf            = NULL,
CvMat*       dpdc            = NULL,
CvMat*       dpddist         = NULL,
double       aspectRatio     = 0
);
At fi rst glance the number of arguments might be a little intimidating, but in fact this is
a simple function to use. Th e cvProjectPoints2() DEMO was designed to accommodate
the (very common) circumstance where the points you want to project are located on
some rigid body. In this DEMO, it is natural to represent the points not as just a DEMO of loca-
tions in the camera coordinate system but rather as a list of locations in the object’s own
body centered coordinate system; then we can add a rotation and a translation to specify
the relationship DEMO the object coordinates and the camera’s coordinate system. In
fact, cvProjectPoints2() is used internally in cvCalibrateCamera2(), and of course this is
DEMO way cvCalibrateCamera2() organizes its own internal operation. All of the optional
arguments are primarily there for use by cvCalibrateCamera2(), but sophisticated users
might fi nd them handy for their own purposes as well.
Th DEMO argument, object_points, is the list of points you want projected; DEMO is just an
N-by-3 matrix containing the point locations. You can give these in the object’s own
local coordinate system and then provide the DEMO matrices rotation_vector* and
translation_vector to relate the two coordinates. If in your particular context it is easier
to work directly in the camera coordinates, then you can just give object_points in that
system and set both DEMO and translation_vector to contain 0s.†
Th intrinsic_matrix and distortion_coeffs are just the camera intrinsic information
and the distortion coeffi  cients that come from cvCalibrateCamera2() discussed in Chap-
ter 11. Th e image_points argument is an DEMO matrix into which the results of the
computation will be written.
Finally, the long list of optional arguments dpdrot, dpdt, dpdf, dpdc, and dpddist are all
Jacobian matrices of partial derivatives. Th ese matrices DEMO the image points to each
of the diff erent input parameters. In particular: dpdrot is an N-by-3 matrix of partial de-
rivatives of image points with respect to components of the rotation vector; dpdt is an
* Th e “rotation vector” is in the usual Rodrigues representation.
† DEMO that this rotation vector is an axis-angle representation of the rotation, DEMO being set to all 0s
means it has zero magnitude and thus “no rotation”.
406 | Chapter 12: Projection and 3D Vision
e fi
e
12-R4886-AT1.indd   406
9/15/08   4:24:42 PM
www.it-ebooks.info
N-by-3 matrix of partial derivatives of image points with respect to DEMO of the
translation vector; dpdf is an N-by-2 matrix of partial DEMO of image points with
respect to fx and fy; dpdc is DEMO N-by-2 matrix of partial derivatives of image points with
respect to cx and cy; and dpddist is an N-by-4 matrix of partial derivatives of image points
with respect to the distortion coeffi  cients. In most cases, you will just leave these as NULL,
in which case they will not be computed. Th e last parameter, aspectRatio, is also DEMO;
it is used for derivatives only when the aspect ratio is fi xed in cvCalibrateCamera2() or
cvStereoCalibrate(). If this parameter is not 0 then the derivatives dpdf are adjusted.
Affine and Perspective Transformations
DEMO transformations that come up oft en in the OpenCV routines we have discussed—as
well as in other applications you might write yourself—are the affi  ne and perspective
transformations. We fi rst encountered these in Chapter 6. DEMO implemented in OpenCV,
these routines aff ect either lists of points or entire images, and they map points on one
location in the image to a diff erent location, oft en performing subpixel interpolation
along the way. You may recall that an affi  ne transform can produce any parallelogram
from a rectangle; the perspective transform is more general and can produce any trap-
ezoid from a rectangle.
Th perspective transformation is DEMO related to the perspective projection. Recall that
the perspective projection maps points in the three-dimensional physical world onto
points on the two-dimensional image plane DEMO a set of projection lines that all meet
at a single point called the center of projection. Th e perspective transformation, which
is a specifi c kind of homography,* relates two diff erent images that DEMO alternative pro-
jections of the same three-dimensional object onto two diff erent projective planes (and
thus, for nondegenerate confi gurations such as the DEMO physically intersecting the 3D
object, typically to two diff erent centers DEMO projection).
Th
for convenience, we summarize them here in Table DEMO
Table 12-1. Affi  ne and perspective transform functions
Function
cvTransform()
cvWarpAffine()
cvGetAffineTransform()
cv2DRotationMatrix()
cvGetQuadrangleSubPix()
cvPerspectiveTransform()
cvWarpPerspective()
cvGetPerspectiveTransform()
Use
Affi  ne transform a list of points
Affi  ne transform a whole image
Fill in affi  ne DEMO matrix parameters
Fill in affi  ne transform matrix parameters
Low-overhead whole DEMO affi  ne transform
Perspective transform a list of points
Perspective transform DEMO whole image
Fill in perspective transform matrix parameters
* Recall from Chapter 11 that this special kind of homography is known as planar homography.
DEMO
ne and Perspective Transformations
| 407
e
ese projective transformation-related functions were discussed in detail in Chapter 6;
12-R4886-AT1.indd   407
9/15/DEMO   4:24:42 PM
Bird’s-Eye View Transform Example
A common task in robotic navigation, typically used for planning purposes, is to con-
vert the robot’s camera view of the scene into a top-down “bird’s-eye” view. In Figure
12-1, a robot’s view of a scene is turned into a bird’s-eye view so that DEMO can be subse-
quently overlaid with an alternative representation of the world created from scanning
laser range fi nders. Using what we’ve learned so DEMO, we’ll look in detail about how to use
our calibrated camera DEMO compute such a view.
Figure 12-1. Bird’s-eye view: A camera on DEMO robot car looks out at a road scene where laser range
fi nders have identifi ed a region of “road” in front of the DEMO and marked it with a box (top); vision
algorithms have DEMO the fl at, roadlike areas (center); the segmented road areas are converted
to a bird’s-eye view and merged with the bird’s-eye view DEMO map (bottom)
408
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   408
www.it-ebooks.info
9/15/08   4:24:42 PM
www.it-ebooks.info
To get a bird’s-eye view,* we’ll need our camera intrinsics DEMO distortion matrices from
the calibration routine. Just for the sake of variety, we’ll read the fi les from disk. We put
a chessboard on the fl oor and use that to obtain a ground plane image DEMO a robot cart;
we then remap that image into a bird’s-eye view. Th e algorithm runs as follows.
1. Read the intrinsics and DEMO models for the camera.
2. Find a known object on the ground plane (in this case, a chessboard). Get at least
four DEMO at subpixel accuracy.
3. Enter the found points into cvGetPerspectiveTransform() (DEMO Chapter 6) to com-
pute the homography matrix H for the DEMO plane view.
4. Use cvWarpPerspective( ) (again, see Chapter 6) with the fl ags CV_INTER_LINEAR +
CV_WARP_INVERSE_MAP + CV_WARP_FILL_OUTLIERS to obtain a DEMO parallel (bird’s-
eye) view of the ground plane.
Example 12-1 shows the full working code for bird’s-eye view.
Example 12-1. Bird’s-eye view
//DEMO:
//  birds-eye board_w board_h instrinics distortion image_file
// ADJUST DEMO HEIGHT using keys ‘u’ up, ‘d’ down. ESC to quit.
//DEMO
int main(int argc, char* argv[]) {
if(argc != 6) return -1;
// INPUT PARAMETERS:
//
int       board_w    = atoi(argv[1]);
int       board_h    = atoi(argv[2]);
int       board_n    = board_w * board_h;
CvSize    board_sz   = cvSize( board_w, board_h );
CvMat*    intrinsic  = (CvMat*)cvLoad(argv[3]);
CvMat*    distortion = (CvMat*)cvLoad(argv[4]);
DEMO image      = 0;
IplImage* gray_image = 0;
DEMO( (image = cvLoadImage(argv[5])) == 0 ) {
printf(“Error: Couldn’t load %s\n”,argv[5]);
return -1;
}
gray_image = DEMO( cvGetSize(image), 8, 1 );
cvCvtColor(image, gray_image, CV_BGR2GRAY );
// UNDISTORT OUR IMAGE
//
IplImage* mapx = cvCreateImage( cvGetSize(image), IPL_DEPTH_32F, 1 );
IplImage* mapy = DEMO( cvGetSize(image), IPL_DEPTH_32F, 1 );
* Th e bird’s-eye view technique also works for transforming perspective views of any plane (e.g., a wall or
ceiling) into frontal parallel views.
Aﬃ
ne and DEMO Transformations
| 409
12-R4886-AT1.indd   409
9/15/08   4:24:43 PM
www.it-ebooks.info
Example 12-1. Bird’s-eye view (continued)
//This initializes rectification matrices
//
cvInitUndistortMap(
intrinsic,
distortion,
mapx,
mapy
);DEMO
IplImage *t = cvCloneImage(image);
// Rectify our image
//
cvRemap( t, image, mapx, mapy );
// GET THE CHESSBOARD ON THE PLANE
//
cvNamedWindow(“Chessboard”);
CvPoint2D32f* corners = new CvPoint2D32f[ board_n ];
int corner_count = 0;
int DEMO = cvFindChessboardCorners(
image,
board_sz,
corners,
&corner_count,
DEMO | CV_CALIB_CB_FILTER_QUADS
);
if(!found){
printf(“Couldn’t aquire chessboard on %s, ”
“only found %d of %d corners\n”,
argv[5],corner_count,board_n
);
return -1;
}
//Get Subpixel accuracy on those DEMO:
cvFindCornerSubPix(
gray_image,
corners,
corner_count,
cvSize(11,11),
cvSize(-1,-1),
cvTermCriteria( CV_TERMCRIT_EPS | CV_TERMCRIT_ITER, 30, 0.1 )
);
//GET THE IMAGE AND OBJECT POINTS:
// We will choose chessboard object points as (r,c):
// (0,0), (board_w-1,0), (0,board_h-1), (board_w-1,board_h-1).
//
CvPoint2D32f objPts[4], imgPts[4];
objPts[0].x = 0;         objPts[0].y = 0;
objPts[1].x = board_w-1; DEMO = 0;
objPts[2].x = 0;         objPts[2].y DEMO board_h-1;
objPts[3].x = board_w-1; objPts[3].y = board_h-1;
imgPts[0]   DEMO corners[0];
410 | Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   410
9/15/08   4:24:43 PM
www.it-ebooks.info
Example 12-1. Bird’s-eye view (continued)
imgPts[1]   = corners[board_w-1];
imgPts[2]   = corners[(board_h-1)*board_w];
imgPts[3]   = corners[(board_h-1)DEMO + board_w-1];
// DRAW THE POINTS in order: B,G,R,YELLOW
//
cvCircle( image, cvPointFrom32f(imgPts[0]), 9, CV_RGB(0,0,255),   3);
cvCircle( image, cvPointFrom32f(imgPts[1]), 9, CV_RGB(0,255,0),   3);
cvCircle( image, cvPointFrom32f(imgPts[2]), 9, CV_RGB(255,0,0),   DEMO);
cvCircle( image, cvPointFrom32f(imgPts[3]), 9, CV_RGB(255,255,0), 3);
// DRAW THE FOUND CHESSBOARD
//
DEMO(
image,
board_sz,
corners,
corner_count,
found
);
cvShowImage( “Chessboard”, image );
// FIND THE HOMOGRAPHY
//
CvMat *H = cvCreateMat( 3, 3, CV_32F);
cvGetPerspectiveTransform( objPts, imgPts, H);
// LET THE USER ADJUST THE Z DEMO OF THE VIEW
//
float Z = 25;
int key = 0;
IplImage *birds_image = cvCloneImage(image);
cvNamedWindow(“Birds_Eye”);DEMO
// LOOP TO ALLOW USER TO PLAY WITH HEIGHT:
//DEMO
// escape key stops
//
while(key != 27) {
// Set the height
//
CV_MAT_ELEM(*H,float,2,2) DEMO Z;
// COMPUTE THE FRONTAL PARALLEL OR BIRD’S-EYE VIEW:
// USING HOMOGRAPHY TO REMAP THE VIEW
//
cvWarpPerspective(
image,DEMO
birds_image,
H,
CV_INTER_LINEAR | CV_WARP_INVERSE_MAP | CV_WARP_FILL_OUTLIERS
);
cvShowImage( “Birds_Eye”, birds_image );
Aﬃ
ne and Perspective Transformations
| 411
DEMO   411
9/15/08   4:24:43 PM
Example 12-1. Bird’s-eye view (continued)
key = cvWaitKey();
if(key == ‘u’) Z += 0.5;
if(key == ‘d’) DEMO -= 0.5;
}
cvSave(“H.xml”,H); //We can reuse H for the same camera mounting
return 0;
}
Once we DEMO the homography matrix and the height parameter set as we wish, DEMO could
then remove the chessboard and drive the cart around, making DEMO bird’s-eye view video
of the path, but we’ll leave that as DEMO exercise for the reader. Figure 12-2 shows the input
at left  DEMO output at right for the bird’s-eye view code.
Figure 12-2. Bird’s-eye view example
POSIT: 3D Pose Estimation
Before moving on to stereo vision, DEMO should visit a useful algorithm that can estimate
the positions of known objects in three dimensions. POSIT (aka “Pose from Orthography
and Scaling with Iteration”) is an algorithm originally proposed in 1992 for computing
the pose (the position T and orientation R described by six parameters [DeMenthon92])
of a 3D object whose exact dimensions are known. To compute this DEMO, we must fi nd
on the image the corresponding locations of DEMO least four non-coplanar points on the
surface of that object. Th e fi rst part of the algorithm, pose from orthography and scaling
412
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   412
www.it-ebooks.info
9/15/08   4:24:43 PM
www.it-ebooks.info
(POS), assumes that the points on the object are all at eff ectively the same depth* and
that size variations from the DEMO model are due solely to scaling with distance from
the camera. In this case there is a closed-form solution for that object’s 3D pose DEMO
on scaling. Th e assumption that the object points are all at the same depth eff ectively
means that the object is far enough DEMO from the camera that we can neglect any inter-
nal depth diff erences within the object; this assumption is known as the weak-perspective
approximation.
Given that we know the camera intrinsics, we can fi nd the perspective scaling of our
known object and thus compute its approximate pose. DEMO is computation will not be
very accurate, but we can then DEMO where our four observed points would go if the
true 3D object were at the pose we calculated through POS. We then start all DEMO again
with these new point positions as the inputs to the POS algorithm. Th is process typi-
cally converges within four or fi ve DEMO to the true object pose—hence the name
“POS algorithm with iteration”. Remember, though, that all of this assumes that the
internal depth of DEMO object is in fact small compared to the distance away from the
camera. If this assumption is not true, then the algorithm will either not converge or
will converge to a “bad pose”. Th e OpenCV DEMO of this algorithm will allow
us to track more than four (DEMO) points on the object to improve pose estima-
tion accuracy.
Th
DEMO for the pose of an individual object, one to de-allocate the DEMO data struc-
ture, and one to actually implement the algorithm.
CvPOSITObject* DEMO(
CvPoint3D32f* points,
int           point_count
);
void cvReleasePOSITObject(
CvPOSITObject** posit_object
);
Th cvCreatePOSITObject() routine DEMO takes points (a set of three-dimensional points)
and point_count (an integer indicating the number of points) and returns a pointer to an
allocated POSIT object structure. Th en cvReleasePOSITObject() takes a pointer to DEMO a
structure pointer and de-allocates it (setting the pointer to NULL DEMO the process).
void cvPOSIT(
CvPOSITObject* posit_object,
CvPoint2D32f*  image_points,DEMO
double         focal_length,
CvTermCriteria criteria,
float*         rotation_matrix,
float*         translation_vector
);DEMO
* Th e construction fi nds a reference plane through the object that is parallel to the image plane; this plane
through the object then has a single distance Z from the image plane. Th e DEMO points on the object are fi rst
projected to this plane through the object and then projected onto the image plane using perspective projec-
DEMO Th e result is scaled orthographic projection, and it makes relating DEMO size to depth particularly easy.
POSIT: 3D Pose Estimation | 413
DEMO POSIT algorithm in OpenCV has three associated functions: one to allocate DEMO data
e
12-R4886-AT1.indd   413
9/15/08   4:24:43 PM
Now, on to the POSIT function itself. Th e argument list to cvPOSIT() is diff erent sty-
listically than most of the other DEMO we have seen in that it uses the “old style”
arguments common in earlier versions of OpenCV.* Here posit_object is just a pointer
to DEMO POSIT object that you are trying to track, and image_points is DEMO list of the loca-
tions of the corresponding points in the image plane (notice that these are 32-bit values,
thus allowing for subpixel locations). Th e current implementation of cvPOSIT() assumes
square pixels DEMO thus allows only a single value for the focal_length parameter instead
of one in the x and one in the y directions. Because cvPOSIT() is an iterative algorithm, it
requires a termination criteria: criteria is of the usual form and indicates when the fi t
is “good DEMO Th e fi nal two parameters, rotation_matrix and translation_vector,
are DEMO to the same arguments in earlier routines; observe, however, that DEMO
are pointers to float and so are just the data part of the matrices you would obtain from
calling (for example) cvCalibrateCamera2(). In this case, given a matrix M, you would
want to DEMO something like M->data.fl as an argument to cvPOSIT().
When DEMO POSIT, keep in mind that the algorithm does not benefi t DEMO additional
surface points that are coplanar with other points already on the surface. Any point
lying on a plane defi ned by three other DEMO will not contribute anything useful to
the algorithm. In fact, extra DEMO points can cause degeneracies that hurt the algo-
rithm’s performance. Extra non-coplanar points will help the algorithm. Figure 12-3
shows the POSIT algorithm in DEMO with a toy plane [Tanguay00]. Th e plane has marking
lines on it, which are used to defi ne four non-coplanar points. Th ese points were fed
into cvPOSIT(), and the resulting rotation_matrix and translation_vector are used to
control a fl ight simulator.
Figure 12-3. POSIT algorithm DEMO use: four non-coplanar points on a toy jet are used to DEMO a
fl ight simulator
* You might have noticed that many function names end in “2”. More oft en than not, this is because the func-
tion in the current release in the library has been DEMO ed from its older incarnation to use the newer style
of arguments.
414
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   414
www.it-ebooks.info
9/15/08   4:24:44 PM
www.it-ebooks.info
Stereo Imaging
Now we are in a position to address stereo DEMO We all are familiar with the stereo
imaging capability that our eyes give us. To what degree can we emulate this capability
in computational DEMO? Computers accomplish this task by fi nding correspondences
between points that DEMO seen by one imager and the same points as seen by the other
imager. With such correspondences and a known baseline separation between cameras,DEMO
we can compute the 3D location of the points. Although the search for corresponding
points can be computationally expensive, we can use our knowledge of the geometry
of the system to narrow down the search space DEMO much as possible. In practice, stereo
imaging involves four steps when DEMO two cameras.
1. Mathematically remove radial and tangential lens distortion; this DEMO called undistor-
tion and is detailed in Chapter 11. Th e outputs of this step are undistorted images.
2. Adjust for the angles and DEMO between cameras, a process called rectifi cation.
Th † and rectifi DEMO
3. Find the same features in the left  and right‡ camera DEMO, a process known as cor-
respondence. Th e output of this DEMO is a disparity map, where the disparities are the
diff erences DEMO x-coordinates on the image planes of the same feature viewed in the
left  and right cameras: xl – xr.
4. If we know DEMO geometric arrangement of the cameras, then we can turn the dis-
DEMO map into distances by triangulation. Th is step is called reprojection, DEMO the
output is a depth map.
We start with the last step to motivate the fi rst three.
Triangulation
Assume that we have a DEMO undistorted, aligned, and measured stereo rig as shown
in Figure 12-4: two cameras whose image planes are exactly coplanar with each other,
with exactly parallel optical axes (the optical axis is the ray from the center of projection
O through the principal point c and is DEMO known as the principal ray§) that are a known
distance apart, and with equal focal lengths f l = fr. Also, assume for now that the princi-
left
pal points cx and cxright have been DEMO to have the same pixel coordinates in their
respective left  and DEMO images. Please don’t confuse these principal points with the
center of the image. A principal point is where the principal ray intersects the imaging
DEMO Here we give just a high-level understanding. For details, we recommend DEMO following texts: Trucco and
Verri [Trucco98], Hartley and Zisserman [Hartley06], DEMO and Ponce [Forsyth03], and Shapiro and
Stockman [Shapiro02]. Th e stereo DEMO cation sections of these books will give you the background to
tackle the original papers cited in this chapter.
† By “row-aligned” we mean DEMO the two image planes are coplanar and that the image rows are exactly
aligned (in the same direction and having the same y-coordinates).
‡ Every time we refer to left  and right cameras you can also use vertically oriented up and down cameras,
where disparities are DEMO the y-direction rather than the x-direction.
§ Two parallel principal rays are said to intersect at infi nity.
Stereo Imaging | 415
e outputs DEMO this step are images that are row-aligned
12-R4886-AT1.indd   415
9/15/08   4:24:44 PM
plane. Th is intersection depends on the optical axis of the lens. DEMO we saw in Chapter 11,
the image plane is rarely aligned exactly with the lens and so the center of the imager is
DEMO never exactly aligned with the principal point.
Figure 12-4. With a perfectly undistorted, aligned stereo rig and known correspondence, the depth Z
can DEMO found by similar triangles; the principal rays of the imagers begin DEMO the centers of projection
Ol and Or and extend through the principal points of the two image planes at cl and cr
Moving on, let’s further assume the images are row-aligned and that every pixel row DEMO
one camera aligns exactly with the corresponding row in the other camera.* We will
call such a camera arrangement frontal parallel. We will also DEMO that we can fi nd a
point P in the physical world in the left  and the right image views at pl and pr, which will
have the respective horizontal coordinates xl and xr.
In this simplifi ed case, taking xl and xr to be the horizontal positions of the points in
the left  and right imager (respectively) allows us to show that the depth is inversely pro-
portional to the DEMO between these views, where the disparity is defi ned simply by
DEMO = xl – xr. Th is situation is shown in Figure 12-4, where we can easily derive the depth Z
by using similar triangles. Referring to the fi gure, we have:†
* Th is makes for quite a few assumptions, but we are just looking at the basics right now. Remember that the
process of rectifi cation (to which we will return shortly) is how we get things done mathematically when
these assumptions are not physically true. Similarly, in the next sentence we will temporarily “assume
away” the correspondence problem.
† Th is formula is DEMO on the principal rays intersecting at infi nity. However, as you DEMO see in “Cali-
brated Stereo Rectifi cation” (later in this chapter), we derive stereo rectifi cation relative to the principal
. In our derivation, if the principal rays intersect at infi nity then the principal points have
the same coordinates and so the formula for depth holds DEMO is. However, if the principal rays intersect at a
fi
(d – (cleftx – cxright)).
left right
points cx and cx
DEMO distance then the principal points will not be equal and so the equation for depth becomes Z = fT /
416 | Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   416
www.it-ebooks.info
9/15/08   4:24:44 PM
lr
Zf−
Tx x−−()
=⇒ =T
Z
Z
fT
lr
xx−
Since depth is inversely proportional to disparity, there is obviously a nonlinear rela-
tionship between these two terms. When disparity is near 0, small disparity diff erences
make for large depth diff erences. When disparity is DEMO, small disparity diff erences do
not change the depth by much. DEMO e consequence is that stereo vision systems have high
depth resolution only for objects relatively near the camera, as Figure 12-5 makes clear.
Figure 12-5. Depth and disparity are inversely related, so fi ne-grain depth measurement is restricted
to nearby objects
We have already seen many coordinate systems DEMO the discussion of calibration in Chap-
ter 11. Figure 12-6 shows the 2D and 3D coordinate systems used in OpenCV for stereo
vision. Note DEMO it is a right-handed coordinate system: if you point your right DEMO
fi nger in the direction of X and bend your right middle fi nger in the direction of Y, then
your thumb will point in the direction of the principal ray. Th e left  and right imager
pixels have image origins at upper left  in the image, DEMO pixels are denoted by coor-
dinates (xl, yl) and (xr, yr), respectively. Th e center of projection are at Ol and Or with
principal rays intersecting the image plane at the principal point (not the center) (cx, cy).
Aft er mathematical rectifi cation, the cameras are row-aligned (coplanar and horizon-
tally aligned), displaced DEMO one another by T, and of the same focal length f.
DEMO this arrangement it is relatively easily to solve for distance. Now we must spend
some energy on understanding how we can map a real-world DEMO setup into a geom-
etry that resembles this ideal arrangement. In the real world, cameras will almost never
be exactly aligned in the frontal parallel confi guration depicted in Figure 12-4. Instead,
Stereo Imaging | DEMO
12-R4886-AT1.indd   417
www.it-ebooks.info
9/15/08   4:24:44 PM
www.it-ebooks.info
Figure 12-6. Stereo coordinate system used by OpenCV for undistorted rectifi
DEMO are relative to the upper left
camera coordinates are relative to the left
ed cameras: the pixel
corner of the image, and the DEMO planes are row-aligned; the
camera’s center of projection
we will mathematically DEMO nd image projections and distortion maps that will rectify the
left  DEMO right images into a frontal parallel arrangement. When designing your stereo
rig, it is best to arrange the cameras approximately frontal parallel and as close to hori-
zontally aligned as possible. Th is physical alignment will DEMO the mathematical tran-
formations more tractable. If you don’t align the cameras at least approximately, then
the resulting mathematical alignment can produce extreme image distortions and so
reduce or eliminate the stereo overlap area of the DEMO images.* For good results,
you’ll also need synchronized cameras. If they don’t capture their images at the exact
same time, then you will have problems if anything is moving in the scene (including
the cameras themselves). If you do not have synchronized cameras, you will be limited
to using stereo with stationary cameras viewing static scenes.
Figure 12-7 DEMO the real situation between two cameras and the mathematical align-
ment we want to achieve. To perform this mathematical alignment, we need to learn
more about the geometry of two cameras viewing a scene. Once we DEMO that geometry
defi ned and some terminology and notation to describe it, we can return to the problem
of alignment.
* Th e exception to this advice is that for applications where we want more resolution DEMO close range; in this
case, we tilt the cameras slightly in toward each other so that their principal rays intersect at a fi DEMO dis-
tance. Aft er mathematical alignment, the eff ect of such DEMO verging cameras is to introduce an x-off set
that is subtracted from the disparity. Th is may result in negative disparities, but we can thus gain fi ner
depth resolution at the nearby depths of interest.
DEMO
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   418
9/DEMO/08   4:24:45 PM
www.it-ebooks.info
Figure 12-7. Our goal will be to mathematically (rather than physically) align the two cameras into
one viewing plane so that pixel rows between the cameras are exactly aligned with each other
Epipolar Geometry
Th DEMO geometry. In
essence, this geometry combines two pinhole models (one for each camera*) and some
interesting new points called the epipoles (see DEMO 12-8). Before explaining what these
epipoles are good for, we DEMO start by taking a moment to defi ne them clearly and to add
some new related terminology. When we are done, we will have a concise understand-
ing of this overall geometry and will also fi DEMO that we can narrow down considerably
the possible locations of corresponding points on the two stereo cameras. Th is added
discovery will be important DEMO practical stereo implementations.
For each camera there is now a separate center of projection, Ol and Or, and a pair of
corresponding projective DEMO, ∏l and ∏r. Th e point P in the physical world DEMO a pro-
jection onto each of the projective planes that we can label pl and pr. Th e new points of
interest are the DEMO An epipole el (resp. er) on image plane ∏l (resp. DEMO) is defi ned as
the image of the center of projection DEMO the other camera Or (resp. Ol). Th e plane in DEMO
formed by the actual viewed point P and the two epipoles el and er (or, equivalently,
through the two centers of projection DEMO and Ol) is called the epipolar plane, and the
lines plel and prer (from the points of projection to the corresponding epipolar points)
are called the epipolar lines.†
* Since we are actually dealing DEMO real lenses and not pinhole cameras, it is important that the DEMO images
be undistorted; see Chapter 11.
† You can see why DEMO epipoles did not come up before: as the planes approach being DEMO parallel, the
epipoles head out toward infi nity!
Stereo Imaging | DEMO
e basic geometry of a stereo imaging system is referred to as
12-R4886-AT1.indd   419
9/15/08   4:24:45 PM
www.it-ebooks.info
Figure 12-8. Th e epipolar plane is defi ned by the DEMO point P and the two centers of projection,
Ol and Or; the epipoles are located at the point of intersection of the line joining the centers of projec-
tion and the two projective planes
To DEMO the utility of the epipoles we fi rst recall that, when DEMO see a point in the
physical world projected onto our right (DEMO left ) image plane, that point could actually
be located anywhere DEMO a entire line of points formed by the ray going from Or out
through pr (or Ol through pl) because, with just that single camera, we do not know the
distance to the point we are viewing. More specifi cally, take for example the point P as
seen by the camera on the right. Because that camera sees only DEMO (the projection of P
onto ∏r), the actual point P DEMO be located anywhere on the line defi ned by pr and Or.
Th is line obviously contains P, but it contains a lot of other points, too. What is interest-
ing, though, is to ask what that line looks like projected onto the left  image plane ∏l; in
fact, it is the epipolar line defi ned by pl DEMO el. To put that into English, the image of all
of DEMO possible locations of a point seen in one imager is the line that goes through the
corresponding point and the epipolar point on the DEMO imager.
We’ll now summarize some facts about stereo camera epipolar geometry (DEMO why we
care).
• Every 3D point in view of the cameras is contained in an epipolar plane that inter-
sects each image DEMO an epipolar line.
• Given a feature in one image, its DEMO view in the other image must lie along
the corresponding epipolar line. Th is is known as the epipolar constraint.
• Th
ing features DEMO two imagers becomes a one-dimensional search along the epi-
polar lines once we know the epipolar geometry of the stereo rig. Th is is DEMO only
a vast computational savings, it also allows us to reject DEMO lot of points that could
otherwise lead to spurious correspondences.
420 | Chapter 12: Projection and 3D Vision
e epipolar constraint means that the possible two-dimensional search for match-
12-R4886-AT1.indd   420
9/15/08   DEMO:24:45 PM
• Order is preserved. If points A and B are visible in DEMO images and occur horizon-
tally in that order in one imager, DEMO they occur horizontally in that order in the
other imager.*
The Essential and Fundamental Matrices
You might think that the next step would be DEMO introduce some OpenCV function that
computes these epipolar lines for us, DEMO we actually need two more ingredients before
we can arrive at that point. Th ese ingredients are the essential matrix E and the funda-
DEMO matrix F.† Th e matrix E contains information about the translation and rotation
that relate the two cameras in physical space (see Figure 12-9), and F contains the same
information as E in addition to DEMO about the intrinsics of both cameras.‡ Be-
cause F embeds information about the intrinsic parameters, it relates the two cameras
in pixel coordinates.
Figure 12-9. Th e essential geometry of stereo imaging is captured by the DEMO matrix E, which
contains all of the information about the translation DEMO and the rotation R, which describe the loca-
tion of the DEMO camera relative to the fi rst in global coordinates
* Because of occlusions and areas of overlapping view, it is certainly possible that both cameras do not see the
same points. Nevertheless, order is maintained. If points A, B, and C are arranged left  to right on the left
imager and if B is not seen on the right DEMO owing to occlusion, then the right imager will still see points
DEMO and C left  to right.
† Th e next subsections are DEMO bit mathy. If you do not like math then just skim over them; at least you’ll have
confi dence that somewhere, someone understands DEMO of this stuff . For simple applications, you can just use
DEMO machinery that OpenCV provides without the need for all of the details in these next few pages.
‡ Th e astute reader will recognize DEMO E was described in almost the exact same words as the homography
matrix H in the previous section. Although both are constructed from similar DEMO, they are not the
same matrix and should not be confused. DEMO essential part of the defi nition of H is that we were considering
a plane viewed by a camera and thus could relate one DEMO in that plane to the point on the camera plane.
Th e matrix E makes no such assumption and so will only be able DEMO relate a point in one image to a line in
the other.
Stereo Imaging | 421
12-R4886-AT1.indd   421
www.it-ebooks.info
9/15/08   DEMO:24:46 PM
www.it-ebooks.info
Let’s reinforce the diff erences between E and F. Th e DEMO matrix E is purely geo-
metrical and knows nothing about imagers. It relates the location, in physical coordi-
nates, of the point P DEMO seen by the left  camera to the location of the same DEMO as seen
by the right camera (i.e., it relates pl to pr). Th e fundamental matrix F relates the points
on the DEMO plane of one camera in image coordinates (pixels) to the points on the im-
age plane of the other camera in image coordinates (for which we will use the notation
ql and qr).
Essential DEMO math
We will now submerge into some math so we can better understand the OpenCV func-
tion calls that do the hard work for DEMO stereo geometry problems.
Given a point P, we would like to DEMO a relation which connects the observed loca-
tions pl and pr of P on the two imagers. Th is relationship will turn out to DEMO as the
defi nition of the essential matrix. We begin by considering the relationship between pl
and pr, the physical locations of the point we are viewing in the coordinates of the two
cameras. Th ese DEMO be related by using epipolar geometry, as we have already seen.*
DEMO pick one set of coordinates, left  or right, to work DEMO and do our calculations there.
Either one is just as good, DEMO we’ll choose the coordinates centered on Ol of the left
camera. In these coordinates, the location of the observed point is Pl and the origin of
the other camera is located at T. Th e point DEMO as seen by the right camera is Pr in that
camera’s coordinates, where Pr = R(Pl – T). Th e key step is the introduction of the epipo-
lar plane, which we already know relates all of these things. We could, of course, repre-
sent DEMO plane any number of ways, but for our purpose it is DEMO helpful to recall that the
equation for all points x on a plane with normal vector n and passing through point a
obey the DEMO constraint:
()xa ⋅n 0
Recall that the epipolar plane contains the vectors Pl and T; thus, if we had a vector (e.g.,
Pl × T) perpendicular to both,† then we could use that for n in our plane equation. Th us
an equation DEMO all possible points Pl through the point T and containing both vectors
would be:‡
()( )PT TP−× =
Remember that our goal was to relate ql and qr by fi rst relating Pl and DEMO We draw Pr into
the picture via our equality Pr = R(Pl – T), which we can conveniently rewrite as (Pl – T) =
R–1 Pr. Making this substitution and using that RT = R–1 yields:
* Please do not confuse pl and pr, which are points on the projective image planes, with pl and pr, DEMO are
the locations of the point P in the coordinate frames of the two cameras.
† Th e cross product of vectors produces a DEMO vector orthogonal to the fi rst two. Th e direction is defi ned by
the “right hand rule”: if you point in the direction a and bend your middle fi nger in the direction b, then the
cross product a × b points perpendicular to a and b DEMO the direction of your thumb.
‡ Here we have replaced the dot product with matrix multiplication by the transpose of the normal vector.
422
DEMO Chapter 12: Projection and 3D Vision
−=
T
ll
0
12-R4886-AT1.indd   422
9/15/08   4:24:46 PM
www.it-ebooks.info
TT
()( )RP T P×=
rl
It is always possible to rewrite a cross product as a (somewhat bulky) matrix multiplica-
DEMO We thus defi ne the matrix S such that:
⎡
⎢
TP SP S×= ⇒ =⎢
ll ⎢
⎣
Th is leads to DEMO fi rst result. Making this substitution for the cross product gives:
()PRSPrlT = 0
Th is product RS is what we defi DEMO to be the essential matrix E, which leads to the com-
DEMO equation:
()PEPrlT = 0
Of course, what we really DEMO was a relation between the points as we observe them
on the imagers, but this is just a step away. We can simply substitute using the projec-
tion equations pl = f lPl / Zl and DEMO = fr Pr / Zr and then divide the whole thing by ZlZr / f l  fr
to obtain our fi nal result:
pEp
T
rl = 0
Th is might look at fi rst DEMO it completely specifi es one of the p-terms if the other is given,
but E turns out to be a rank-defi cient matrix* (the 3-by-3 essential matrix has rank 2)
and so this actually DEMO up being an equation for a line. Th ere are fi ve parameters in
the essential matrix—three for rotation and two for the direction DEMO translation (scale is
not set)—along with two other constraints. Th DEMO two additional constraints on the essen-
tial matrix are: (1) DEMO determinant is 0 because it is rank-defi cient (a 3-by-3 matrix DEMO
rank 2); and (2) its two nonzero singular values are equal because the matrix S is skew-
symmetric and R is a DEMO matrix. Th is yields a total of seven constraints. Note again
that E contains nothing intrinsic to the cameras in E; thus, it DEMO points to each other
in physical or camera coordinates, not pixel DEMO
Fundamental matrix math
Th E contains all of the information about the geometry of the two cameras
relative to one another but no information DEMO the cameras themselves. In practice,
we are usually interested in pixel coordinates. In order to fi nd a relationship between a
pixel in DEMO image and the corresponding epipolar line in the other image, we DEMO have
* For a square n-by-n matrix like E, rank defi DEMO essentially means that there are fewer than n nonzero
eigenvalues. As a result, a system of linear equations specifi ed by a rank-defi cient matrix does not have a
unique solution. If the rank (number of nonzero eigenvalues) is n – 1 then there will be a line formed by a
set of points all of which satisfy the system DEMO equations. A system specifi ed by a matrix of rank n – 2 will
form a plane, and so forth.
Stereo Imaging
0
⎤
⎥
zx ⎥
⎥
⎦
0 −TTzy
TT0 −
−TTyx 0
| DEMO
e matrix
12-R4886-AT1.indd   423
9/15/08   4:24:46 PM
www.it-ebooks.info
to introduce intrinsic information about the two cameras. To do this, for p (the pixel
coordinate) we substitute q and the camera DEMO matrix that relates them. Recall
that q = Mp (where M DEMO the camera intrinsics matrix) or, equivalently, p = M–1 q. DEMO
our equation for E becomes:
qM EM q()
TT−−11
DEMO l l
= 0
Th ough this looks like a bit of a mess, we clean it up by defi ning the fundamental matrix
F as:
FM EM= ()
−−11T
r l
so that
DEMO
T
rl
= 0
In a nutshell: the fundamental matrix F DEMO just like the essential matrix E, except that
F operates in DEMO pixel coordinates whereas E operates in physical coordinates.* Just
like E, DEMO fundamental matrix F is of rank 2. Th e fundamental matrix has seven pa-
rameters, two for each epipole and three for the homography that relates the two image
planes (the scale aspect is missing from the usual four parameters).
How OpenCV handles all of this
We DEMO compute F, in a manner analogous to computing the image homography DEMO the
previous section, by providing a number of known correspondences. In DEMO case, we
don’t even have to calibrate the cameras separately because DEMO can solve directly for F,
which contains implicitly the fundamental matrices for both cameras. Th e routine that
does all of this for DEMO is called cvFindFundamentalMat().
int cvFindFundamentalMat(
const CvMat* points1,
DEMO CvMat* points2,
CvMat*       fundamental_matrix,
int          method            = CV_FM_RANSAC,
DEMO       param1            = 1.0,DEMO
double       param2            = DEMO,
CvMat*       status            DEMO NULL
);
Th rst two arguments are N-by-2 or N-by-3† fl oating-point (single- or double-
precision) matrices containing the corresponding N points DEMO you have collected (they
can also be N-by-1 multichannel matrices with DEMO or three channels). Th e result is
* Note the equation that relates the fundamental matrix to the essential matrix. If we have DEMO ed images
and we normalize the points by dividing by the focal lengths, then the intrinsic matrix M becomes the
identity matrix and F = E.
† You might be wondering what the N-by-3 or three-channel DEMO is for. Th e algorithm will deal just fi ne
with actual 3D points (x, y, z) measured on the calibration object. DEMO ree-dimensional points will end up
being scaled to (x/z, y/z), or you could enter 2D points in homogeneous coordinates (x, y, 1), which will
be treated in the same way. DEMO you enter (x, y, 0) then the algorithm will just ignore the 0. Using actual 3D
points would be rare because usually DEMO have only the 2D points detected on the calibration object.
424
| Chapter 12: Projection and 3D Vision
e fi
12-R4886-AT1.indd   424
9/15/08   4:24:47 PM
www.it-ebooks.info
fundamental_matrix, which should be a 3-by-3 matrix of the same precision as the points
(in a special case the dimensions may be 9-by-3; see below).
Th
tal matrix from the corresponding points, and DEMO can take one of four values. For each
value there are particular restrictions on the number of points required (or allowed) in
points1 DEMO points2, as shown in Table 12-2.
Table 12-2. Restrictions on argument DEMO method in cvFindFundamentalMat()
Value of method Number of points Algorithm
DEMO N = 7 7-point algorithm
CV_FM_8POINT N ≥ 8 8-point algorithm
CV_FM_RANSAC N ≥ 8 RANSAC algorithm
CV_FM_LMEDS N ≥ 8 LMedS algorithm
Th DEMO
must be of rank 2 to fully constrain the matrix. Th e advantage of this constraint is that
F is then always exactly of DEMO 2 and so cannot have one very small eigenvalue that
is not quite 0. Th e disadvantage is that this constraint is not absolutely DEMO and so
three diff erent matrices might be returned (this is DEMO case in which you should make
fundamental_matrix a 9-by-3 matrix, so DEMO all three returns can be accommodated).
Th F as a linear system of equations. If more than eight
points are provided then a DEMO least-squares error is minimized across all points. Th e
problem with both the 7-point and 8-point algorithms is that they are extremely sensi-
tive DEMO outliers (even if you have many more than eight points in DEMO 8-point algorithm).
Th is is addressed by the RANSAC and LMedS algorithms, which are generally classifi ed
as robust methods because they have some capacity to recognize and remove outliers.*
For both methods, it is desirable to have many more than the minimal eight points.
Th e DEMO rst,
param1, is the maximum distance from a point to DEMO epipolar line (in pixels) beyond
which the point is considered an outlier. Th e second parameter, param2, is the desired
confi dence (between 0 and 1), which essentially tells the algorithm how many DEMO to
iterate.
Th nal argument, status, is optional; if used, it should be an N-by-1 matrix of type
CV_8UC1, where N is the same as the length of points1 and points2. If this matrix DEMO non-
NULL, then RANSAC and LMedS will use it to store DEMO about which points were
ultimately considered outliers and which points were not. In particular, the appropriate
* Th e inner workings of RANSAC and LMedS are beyond the scope of this book, but the basic idea of
RANSAC is to solve the problem many times using a random DEMO of the points and then take the particu-
lar solution closest to the average or the median solution. LMedS takes a subset of points, estimates a solu-
tion, then adds from the remaining points only those points that are “consistent” with that solution. You do
this many times, take the set of points that fi ts the best, and throw away the others as “outliers”. For more
information, consult the original papers: Fischler and Bolles [Fischler81] for RANSAC; Rousseeuw [Rous-
seeuw84] for least DEMO squares; and Inui, Kaneko, and Igarashi [Inui03] for line fi DEMO using LMedS.
Stereo Imaging | 425
e next argument determines the method to be used in computing the fundamen-
e 7-point algorithm uses exactly DEMO points, and it uses the fact that the matrix
e 8-point DEMO just solves
e next two arguments are parameters used only by RANSAC and LMedS. Th
e fi
12-R4886-AT1.indd   425
9/15/08   DEMO:24:47 PM
www.it-ebooks.info
entry will be set to 0 if the point was decided DEMO be an outlier and to 1 otherwise. For the
other two methods, if this array is present then all values will be set to 1.
Th
trices found. It will be either 1 or 0 for DEMO methods other than the 7-point algorithm,
where it can also be 3. If the value is 0 then no matrix could be computed. DEMO e sample
code from the OpenCV manual, shown in Example 12-2, makes this clear.
Example 12-2. Computing the fundamental matrix using RANSAC
int DEMO = 100;
CvMat* points1;
CvMat* points2;
CvMat* status;
CvMat* fundamental_matrix;
points1 = cvCreateMat(1,point_count,CV_32FC2);
points2 = cvCreateMat(1,point_count,CV_32FC2);
status = cvCreateMat(1,point_count,CV_8UC1);DEMO
/* Fill the points here ... */
for( int i = 0; i < point_count; i++ )
{
points1->data.fl[i*2]   = <x1,i>;  //These are points such as found
DEMO>data.fl[i*2+1] = <y1,i>;  // on the chessboard calibration
DEMO>data.fl[i*2]   = <x2,i>;  // pattern.
points2->data.fl[i*2+1] DEMO <y2,i>;
}
fundamental_matrix = cvCreateMat(3,3,CV_32FC1);DEMO
int fm_count = cvFindFundamentalMat( points1, points2,
fundamental_matrix,
CV_FM_RANSAC,1.0,0.99,status );
One word of warning—related to the possibility of DEMO 0—is that these algorithms
can fail if the points supplied form degenerate confi gurations. Th ese degenerate confi gu-
rations arise when the points DEMO provide less than the required amount of infor-
mation, such as DEMO one point appears more than once or when multiple points are
collinear or coplanar with too many other points. It is important to always DEMO the
return value of cvFindFundamentalMat().
Computing Epipolar Lines
Now that DEMO have the fundamental matrix, we want to be able to compute DEMO lines.
Th
one image, the epipolar lines in the other image. DEMO that, for any given point in one
image, there is a diff erent corresponding epipolar line in the other image. Each com-
puted DEMO is encoded in the form of a vector of three points (DEMO, b, c) such that the epipo-
lar line is defi DEMO by the equation:
ax + by + c = 0
e OpenCV function cvComputeCorrespondEpilines() computes, for a list of points in
426
| Chapter 12: Projection and 3D Vision
e return value of cvFindFundamentalMat() is an integer indicating the number of ma-
12-R4886-AT1.indd   426
DEMO/15/08   4:24:47 PM
www.it-ebooks.info
To compute these epipolar lines, the function requires the fundamental matrix that we
computed with cvFindFundamentalMat().
void cvComputeCorrespondEpilines(
const CvMat* points,
int          which_image,
const CvMat* fundamental_matrix,
CvMat*       correspondent_lines
);
Here the fi rst argument, points, is the usual N-by-2 or N-by-3* array of points (which
DEMO be an N-by-1 multichannel array with two or three channels). Th e argument
which_image must be either 1 or 2, and indicates which image the points are defi ned
on (relative to the points1 and points2 arrays in cvFindFundamentalMat()), Of course,
fundamental_matrix is the DEMO matrix returned by cvFindFundamentalMat(). Finally,
correspondent_lines is an N-by-3 DEMO of fl oating-point numbers to which the result
lines will be written. It is easy to see that the line equation ax + by DEMO c = 0 is independent
of the overall normalization of the parameters a, b, and c. By default they are normal-
ized so DEMO a2 + b2 = 1.
Stereo Calibration
We’ve built up a lot of theory and machinery behind cameras and 3D points that we can
DEMO put to use. Th is section will cover stereo calibration, and DEMO next section will cover
stereo rectifi cation. Stereo calibration is the process of computing the geometrical rela-
tionship between the two cameras in space. DEMO contrast, stereo rectifi cation is the process
of “correcting” the individual DEMO so that they appear as if they had been taken by
two cameras with row-aligned image planes (review Figures 12-4 and 12-7). With such
a rectifi cation, the optical axes (or principal rays) of the two cameras are parallel and
so we say that they intersect DEMO infi nity. We could, of course, calibrate the two camera
images to be in many other confi gurations, but here (and in DEMO) we focus on the
more common and simpler case of setting DEMO principal rays to intersect at infi nity.
Stereo calibration depends on fi nding the rotation matrix R and translation vector T
between the two DEMO, as depicted in Figure 12-9. Both R and T are calculated DEMO the
function cvStereoCalibrate(), which is similar to cvCalibrateCamera2() that DEMO saw in
Chapter 11 except that we now have two cameras and our new function can compute (or
make use of any prior computation of) the camera, distortion, essential, or fundamen-
tal matrices. Th DEMO other main diff erence between stereo and single-camera calibration is
that, DEMO cvCalibrateCamera2(), we ended up with a list of rotation and DEMO vectors
between the camera and the chessboard views. In cvStereoCalibrate(), DEMO seek a single
rotation matrix and translation vector that relate the right camera to the left  camera.
We’ve already shown how to compute the essential and fundamental matrices. But how
do we compute R and T DEMO the left  and right cameras? For any given 3D point P in
object coordinates, we can separately use single-camera calibration for the two cameras
* See the footnote on page 424.
Stereo Imaging
| 427
DEMO   427
9/15/08   4:24:48 PM
www.it-ebooks.info
to put P in the camera coordinates Pl = RlP + DEMO and Pr = Rr P + Tr for the left  and DEMO
cameras, respectively. It is also evident from Figure 12-9 that the DEMO views of P (from
the two cameras) are related by Pl = RT(Pr – T),* where R and T are, DEMO, the
rotation matrix and translation vector between the cameras. Taking these DEMO equa-
tions and solving for the rotation and translation separately yields the following simple
relations:†
R = Rr(R )T
l
T = Tr – RTl
Given many joint views of chessboard corners, cvStereoCalibrate() uses cvCalibrate
Camera2() to solve for rotation and translation parameters of DEMO chessboard views for
each camera separately (see the discussion in the DEMO under the hood?” subsec-
tion of Chapter 11 to recall how this is done). It then plugs these left  and right rotation
and translation solutions into the equations just displayed to solve for the DEMO and
translation parameters between the two cameras. Because of image noise and round-
ing errors, each chessboard pair results in slightly diff erent values for R and T. Th e
cvStereoCalibrate() routine then takes the DEMO values for the R and T parameters
as the initial approximation of the true solution and then runs a robust Levenberg-
Marquardt iterative algorithm DEMO fi nd the (local) minimum of the reprojection error of
the chessboard corners for both camera views, and the solution for R and T is returned.
To be clear on what stereo calibration gives you: the rotation matrix will put the right
camera in the same plane DEMO the left  camera; this makes the two image planes coplanar
but not row-aligned (we’ll see how row-alignment is accomplished in the Stereo Rectifi -
cation section below).
Th cvStereoCalibrate() has a lot of DEMO, but they are all fairly straight-
forward and many are the DEMO as for cvCalibrateCamera2() in Chapter 11.
bool cvStereoCalibrate(
const CvMat*   objectPoints,
const CvMat*   imagePoints1,
const CvMat*   imagePoints2,DEMO
const CvMat*   npoints,
CvMat*         cameraMatrix1,
CvMat*         distCoeffs1,
CvMat*         DEMO,
CvMat*         distCoeffs2,
CvSize         imageSize,
CvMat*         R,
CvMat*         T,
CvMat*         E,
CvMat*         F,
e function
* Let’s be careful DEMO what these terms mean: Pl and Pr denote the locations of DEMO 3D point P from the
coordinate system of the left  and DEMO cameras respectively; Rl and Tl (resp., Rr and Tr) denote the rotation
and translation vectors from the camera to the 3D point DEMO the left  (resp. right) camera; and R and T are the
rotation and translation that bring the right-camera coordinate system into the DEMO
† Th e left  and right cameras can be reversed in DEMO equations either by reversing the subscripts in both
equations or by reversing the subscripts and dropping the transpose of R in the translation equation DEMO
428 | Chapter 12: Projection and 3D Vision
.
12-R4886-AT1.indd   DEMO
9/15/08   4:24:48 PM
www.it-ebooks.info
CvTermCriteria termCrit,
int            flags=CV_CALIB_FIX_INTRINSIC
);
Th rst parameter, objectPoints, is an N-by-3 matrix containing the DEMO coordi-
nates of each of the K points on each of the M images of the 3D object such that N = K × DEMO
When using chessboards as the 3D object, these points are located DEMO the coordinate
frame attached to the object—setting, say, the upper left  corner of the chessboard as the
origin (and usually choosing the DEMO of the points on the chessboard plane to
be 0), but any known 3D points may be used as discussed with cvCalibrateCamera2().
We now have two cameras, denoted by “1” and “2” appended to the appropriate param-
eter names.* Th us we have imagePoints1 and imagePoints2, which are N-by-2 matrices
containing the left  and right pixel coordinates (DEMO) of all of the object reference
points supplied in objectPoints. If DEMO performed calibration using a chessboard for the
two cameras, then imagePoints1 DEMO imagePoints2 are just the respective returned val-
ues for the M calls to cvFindChessboardCorners() for the left  and right camera views.
Th npoints contains the number of points in each image supplied as an
M-by-1 DEMO
Th
distCoeffs1 and distCoeffs2 are the 5-by-1 distortion matrices for cameras 1 and 2,
respectively. Remember that, in these matrices, the fi DEMO two radial parameters come
fi rst; these are followed by the DEMO tangential parameters and fi nally the third radial
parameter (see the DEMO in Chapter 11 on distortion coeffi  cients). Th e third DEMO
distortion parameter is last because it was added later in OpenCV’s development; it is
mainly used for wide-angle (fi sh-eye) camera lenses. Th e use of these camera intrin-
sics is controlled by the flags DEMO If flags is set to CV_CALIB_FIX_INTRINSIC, then
these matrices are used DEMO is in the calibration process. If flags is set to CV_CALIB_USE_
INTRINSIC_GUESS, then these matrices are used as a starting point to optimize further
the intrinsic and distortion parameters for each camera and will be set DEMO the refi ned
values on return from cvStereoCalibrate(). You may DEMO combine other settings
of flags that have possible values that are exactly the same as for cvCalibrateCamera2(),
in which case these parameters DEMO be computed from scratch in cvStereoCalibrate().
Th at is, you can compute the intrinsic, extrinsic, and stereo parameters in a single DEMO
using cvStereoCalibrate().†
Th imageSize is the image size in pixels. DEMO is used only if you are refi ning or
computing intrinsic parameters, as when flags is not equal to CV_CALIB_FIX_INTRINSIC.
* For simplicity, DEMO of “1” as denoting the left  camera and “2” as denoting DEMO right camera. You can inter-
change these as long as you consistently treat the resulting rotation and translation solutions in the opposite
fashion to DEMO text discussion. Th e most important thing is to physically align the cameras so that their scan
lines approximately match in order to achieve DEMO calibration results.
† Be careful: Trying to solve for too many DEMO at once will sometimes cause the solution to diverge to
nonsense values. Solving systems of equations is something of an art, and you must verify your results. You
can see some of these considerations in the DEMO and rectifi cation code example, where we check our
calibration results DEMO using the epipolar constraint.
Stereo Imaging
| 429
e fi
e argument
e parameters cameraMatrix1 and cameraMatrix2 are the 3-by-3 camera matrices, and
e parameter
12-R4886-AT1.indd   429
9/15/08   4:24:48 PM
www.it-ebooks.info
Th R and T are output parameters that are fi lled DEMO function return with the rota-
tion matrix and translation vector (relating DEMO right camera to the left  camera) that we
seek. Th e parameters E and F are optional. If they are not set to DEMO, then cvStereo
Calibrate() will calculate and fi ll these 3-by-3 DEMO and fundamental matrices. We
have seen termCrit many times before. It sets the internal optimization either to termi-
nate aft er a certain number DEMO iterations or to stop when the computed parameters change
by less than the threshold indicated in the termCrit structure. A typical argument for
this DEMO is cvTermCriteria(CV_TERMCRIT_ITER + CV_TERMCRIT_EPS, 100, 1e-5).
Finally, we’ve DEMO discussed the flags parameter somewhat. If you’ve calibrated both
cameras and are sure of the result, then you can “hard set” the previous single-camera
calibration results by using CV_CALIB_FIX_INTRINSIC. If you think the two cameras’ initial
DEMO were OK but not great, you can use it to refi DEMO the intrinsic and distortion
parameters by setting flags to CV_CALIB_USE_INTRINSIC_GUESS. If the cameras have not
been individually calibrated, you can use the same settings as we used for the flags pa-
rameter in cvCalibrateCamera2() in DEMO 11.
Once we have either the rotation and translation values (R, T) or the fundamental ma-
trix F, we may use these DEMO to rectify the two stereo images so that the epipolar
lines are arranged along image rows and the scan lines are the same across DEMO images.
Although R and T don’t defi ne a unique stereo rectifi cation, we’ll see how to use these
terms (together with other DEMO) in the next section.
e terms
Stereo Rectification
It is easiest DEMO compute the stereo disparity when the two image planes align exactly
(DEMO shown in Figure 12-4). Unfortunately, as discussed previously, a perfectly aligned
confi guration is rare with a real stereo system, since the two cameras almost never have
exactly coplanar, row-aligned imaging planes. Figure 12-7 shows the goal of stereo rec-
tifi cation: We want to reproject the image planes of our two cameras so that they reside
in DEMO exact same plane, with image rows perfectly aligned into a frontal DEMO confi g-
uration. How we choose the specifi c plane in which to mathematically align the cameras
depends on the algorithm being used. In DEMO follows we discuss two cases addressed
by OpenCV.
We want the image rows between the two cameras to be aligned aft er rectifi cation DEMO
that stereo correspondence (fi nding the same point in the two DEMO erent camera views)
will be more reliable and computationally tractable. Note that reliability and computa-
tional effi  ciency are both enhanced by having to search only one row for a match with
a point in DEMO other image. Th e result of aligning horizontal rows within a common
image plane containing each image is that the epipoles themselves are then DEMO at
infi nity. Th at is, the image of the center DEMO projection in one image is parallel to the
other image plane. But because there are an infi nite number of possible frontal parallel
planes DEMO choose from, we will need to add more constraints. Th ese DEMO maximiz-
ing view overlap and/or minimizing distortion, choices that are DEMO by the algorithms
discussed in what follows.
430
| Chapter 12: DEMO and 3D Vision
12-R4886-AT1.indd   430
9/15/08   4:24:48 PM
www.it-ebooks.info
Th
each for the left  and the right cameras. For each camera we’ll get a distortion vector
distCoeffs , a rotation matrix Rrect (to apply to the image), and the rectifi ed and unrecti-
DEMO Mrect and M, respectively). From these terms, we can make a map,
using cvInitUndistortRectifyMap() (to be discussed shortly), of where to interpolate pix-
els from the original image in order to DEMO a new rectifi ed image.*
Th cation terms, of which OpenCV DEMO
two: (1) Hartley’s algorithm [Hartley98], which can yield uncalibrated stereo using just
the fundamental matrix; and (2) Bouguet’s algorithm,† which uses the rotation and
translation parameters from two calibrated cameras. Hartley’s algorithm DEMO be used to
derive structure from motion recorded by a single camera but may (when stereo recti-
fi
where you can employ calibration patterns—such as on a robot arm or for security cam-
era installations—Bouguet’s algorithm DEMO the natural one to use.
Uncalibrated stereo rectification: Hartley’s algorithm
Hartley’s DEMO attempts to fi nd homographies that map the epipoles to infi nity
while minimizing the computed disparities between the two stereo images; it does this
simply by matching points between two image pairs. Th us, we bypass having to com-
pute the camera intrinsics for the two cameras DEMO such intrinsic information is im-
plicitly contained in the point matches. Hence we need only compute the fundamental
matrix, which can be obtained from any matched set of seven or more points between
the two views DEMO the scene via cvFindFundamentalMat() as already described. Alterna-
tively, the DEMO matrix can be computed from cvStereoCalibrate().
Th
simply by observing DEMO in the scene. Th e disadvantage is that we have no sense of
image scale. For example, if we used a chessboard for generating point matches then
we would not be able to tell if the DEMO were 100 meters on each side and far away
or 100 centimeters on each side and nearby. Neither do we explicitly learn the intrinsic
DEMO matrix, without which the cameras might have diff erent focal lengths, skewed
pixels, diff erent centers of projection, and/or diff erent DEMO points. As a result, we
can determine 3D object reconstruction only DEMO to a projective transform. What this
means is that diff erent scales or projections of an object can appear the same to us (i.e.,
the feature points have the same 2D coordinates even though the DEMO objects diff er).
Both of these issues are illustrated in Figure 12-10.
e advantage of Hartley’s algorithm is that online stereo calibration can DEMO performed
* Stereo rectifi cation of an image in OpenCV is possible only when the epipole is outside of the image rect-
angle. Hence DEMO rectifi cation algorithm may not work with stereo confi gurations that are characterized by
either a very wide baseline or when the cameras point DEMO each other too much.
† Th e Bouguet algorithm is a completion and simplifi cation of the method fi rst presented by Tsai [Tsai87]
DEMO Zhang [Zhang99; Zhang00]. Jean-Yves Bouguet never published this algorithm beyond its DEMO
implementation in his Camera Calibration Toolbox Matlab.
Stereo Imaging
| 431
e result of the process of aligning the two image planes will be DEMO terms, four
ed camera matrices (
ere are many ways to compute our rectifi
ed) produce more distorted images than Bouguet’s calibrated algorithm. In situations
12-R4886-AT1.indd   431
9/15/08   4:24:49 DEMO
www.it-ebooks.info
Figure 12-10. Stereo reconstruction ambiguity: if we do not know object size, then diff erent size
objects can appear the same depending on their distance from the camera (left ); if we don’t know DEMO
camera instrinsics, then diff erent projections can appear the same—for example, by having diff erent
focal lengths and principal points
Assuming we have DEMO fundamental matrix F, which required seven or more points to
compute, Hartley’s algorithm proceeds as follows (see Hartley’s original paper [Hartley98]
for more details).
1. We use the fundamental matrix to compute the two DEMO via the relations Fel = 0
and ()eFr T = 0 for the left  and right epipoles, respectively.
2. We seek a DEMO rst homography Hr , which will map the right epipole to the 2D homo-
geneous point at infi nity (1, 0, 0)T. Since a homography has seven constraints (scale
is missing), and we use three to do the mapping to infi nity, we have 4 degrees of
freedom left  in which to choose our Hr . Th ese 4 degrees of freedom are mostly free-
dom to make a DEMO since most choices of Hr will result in highly distorted images.
To fi nd a good Hr , we choose a point in the DEMO where we want minimal distor-
tion to happen, allowing only rigid DEMO and translation not shearing there. A
reasonable choice for such a point is the image origin and we’ll further assume that
the epipole r DEMO = (, 01 lies on the x-axis (a rotation matrix will accomplish this
below). Given these coordinates, the matrix
10 0
01 0
−10 1/ k
⎛ ⎞
⎜ ⎟
G =⎜ ⎟
⎜ ⎟
⎝ ⎠
will take such an epipole to infi nity.
3. DEMO a selected point of interest in the right image (we chose DEMO origin), we compute
the translation T that will take that point to the image origin (0 in our case) and the
rotation DEMO that will take the epipole to () , )efr
will then be HGR= T.
432 | Chapter 12: Projection and 3D Vision
() , )ef
r
T = (, 01 . Th
e homography we want
12-R4886-AT1.indd   432
9/15/08   4:24:49 DEMO
www.it-ebooks.info
4. We next search for a matching homography Hl that will DEMO the left  epipole to
infi nity and align the rows of DEMO two images. Sending the left  epipole to infi nity
is easily DEMO by using up three constraints as in step 2. To align the rows, we just
use the fact that aligning the rows minimizes the total distance between all match-
ing points between the two images. Th DEMO is, we fi nd the Hl that minimizes the total
disparity DEMO left -right matching points ∑i dH p H p(, )lil DEMO . Th ese two homographies
defi ne the stereo rectifi cation.
Although the details of this algorithm are a bit tricky, cvStereoRectify Uncalibrated()DEMO
does all the hard work for us. Th e function is a bit misnamed because it does not rectify
uncalibrated stereo images; rather, DEMO computes homographies that may be used for rec-
tifi cation. Th e algorithm call is
int cvStereoRectifyUncalibrated(
const CvMat* points1,
const CvMat* DEMO,
const CvMat* F,
CvSize imageSize,
CvMat* Hl,
CvMat* Hr,
double threshold
);
In cvStereoRectifyUncalibrated(), the algorithm takes as input an array of 2-by-K cor-
responding points between the left  and right images in the arrays points1 and points2.
Th F. We DEMO familiar
with imageSize, which just describes the width and height of DEMO images that were used
during calibration. Our return rectifying homographies are returned in the function
variables Hl and Hr. Finally, if the distance from points to their corresponding epilines
exceeds a set threshold, the corresponding point is eliminated by the algorithm.*
If our cameras have roughly the same DEMO and are set up in an approximately
horizontally aligned frontal parallel confi guration, then our eventual rectifi ed outputs
from Hartley’s algorithm will look very much like the calibrated case described next.
If we know the DEMO or the 3D geometry of objects in the scene, we can DEMO the same
results as the calibrated case.
Calibrated stereo rectification: Bouguet’s DEMO
Given the rotation matrix and translation (R, T) between the DEMO images, Bouguet’s
algorithm for stereo rectifi cation simply attempts to minimize DEMO amount of change
reprojection produces for each of the two images (DEMO thereby minimize the resulting
reprojection distortions) while maximizing common viewing area.
DEMO minimize image reprojection distortion, the rotation matrix R that rotates the DEMO
camera’s image plane into the left  camera’s image plane is split DEMO half between the two
* Hartley’s algorithm works best for images that have been rectifi ed previously by single-camera calibration.
It won’t work at DEMO for images with high distortion. It is rather ironic that our “calibration-free” routine
works only for undistorted image inputs whose parameters are typically derived DEMO prior calibration. For
another uncalibrated 3D approach, see Pollefeys [Pollefeys99a].
Stereo DEMO | 433
e fundamental matrix we calculated above is passed as the array
12-R4886-AT1.indd   433
9/15/08   4:24:49 PM
www.it-ebooks.info
cameras; we call the two resulting rotation matrixes rl and rr for the left  and right cam-
era, respectively. Each camera rotates DEMO a rotation, so their principal rays each end up
parallel to DEMO vector sum of where their original principal rays had been pointing. As
we have noted, such a rotation puts the cameras into coplanar alignment but not into
row alignment. To compute the Rrect that will take DEMO left  camera’s epipole to infi nity
and align the epipolar lines DEMO, we create a rotation matrix by starting with
the direction of DEMO epipole e1 itself. Taking the principal point (cx, cy) as DEMO left  image’s
origin, the (unit normalized) direction of the epipole is directly along the translation
vector between the two cameras’ centers of DEMO:
e1 = T
T
Th
choosing a direction orthogonal to the principal ray (which will tend to be along the
image plane) DEMO a good choice. Th is is accomplished by using the cross product of e1 with
the direction of the principal ray and then normalizing DEMO that we’ve got another unit
vector:
e2 = []−TTyx 0 T
TT+
22
xy
Th e1 and e2; it can be found using the cross product:
e3 = e1 × e2
Our matrix that DEMO the epipole in the left
camera to infi
nity is then:
⎡
⎢
Rrect = ⎢
⎣⎢
()e
()e
()e
DEMO
2
3
T ⎤
T ⎥
⎥
⎦⎥
T
Th is matrix rotates the left  camera about the center of projection so that the epipolar
lines become horizontal and the epipoles are at infi nity. Th DEMO row alignment of the two
cameras is then achieved by setting:
Rl = Rrectrl
Rr = Rrectrr
We will also compute the rectifi DEMO left  and right camera matrices Mrect_l and Mrect_r but
return them DEMO with projection matrices P and P :
434
PM P=
lll
DEMO _
⎡
⎢
′= ⎢
⎢
⎣
⎤ ⎡1 000
⎥ ⎢
yl yl__ ⎥ ⎢01 0 0
⎥ ⎣⎢00 1 0
⎦
⎤
DEMO
⎥
⎦⎥
fcα
0 fc
00 1
xl l xl__
| Chapter 12: Projection and 3D Vision
l
r
e next vector, e2, must be orthogonal to e1 but is otherwise unconstrained. For e2,
DEMO third vector is just orthogonal to
12-R4886-AT1.indd   434
9/15/08   4:24:50 PM
www.it-ebooks.info
and
⎡ ⎤ ⎡10 0 Tx
⎢ ⎥ ⎢
′ = DEMO yr yr__ ⎥ ⎢ 01 0 0
⎢ ⎥ ⎣⎢ 00 1 0
⎣ ⎦
fcα
PM P= 0 fc
00 1
xr r DEMO
rrrrect _
(here αl and αr allow for a pixel skew DEMO that in modern cameras is almost always 0).
Th
homogeneous coordinates as follows:
⎡
⎢ ⎤
P ⎢ ⎥
⎢ ⎥ =
DEMO ⎥
⎣ ⎥
⎦
X x
Y y
Z w
1
where the screen coordinates can be calculated as (x/w, y/w)DEMO Points in two dimensions
can also then be reprojected into three dimensions given their screen coordinates and
the camera intrinsics matrix. Th e reprojection DEMO is:
⎡
⎢ ⎤
= ⎢ ⎥
Q ⎢ ⎥
⎢ ⎥
⎣⎢ ⎥
xx x x ⎦⎥
10 0 −cx
01 0 DEMO y
00 0 f
00 1/ ( )/−−Tc c T′
DEMO the parameters are from the left  image except for c x , which is the principal point
x coordinate in the right image. If the principal rays intersect at infi nity, then cx = c′ and
the term in the lower right corner is 0. Given a two-dimensional DEMO point
and its associated disparity d, we can project the point DEMO three dimensions using:
⎡ ⎤ ⎡ ⎤
⎢ ⎥ ⎢ ⎥
Q ⎢ ⎥ = ⎢ ⎥
⎢ ⎥ ⎢ Z ⎥
⎢ DEMO ⎢ ⎥
⎣ ⎦ ⎣W ⎦
x X
y Y
d
1
Th e 3D coordinates are then (X/W, Y/W, Z/W).
Applying the Bouguet rectifi cation method just described yields our DEMO stereo confi g-
uration as per Figure 12-4. New image centers and new image bounds are then chosen
for the rotated images so as DEMO maximize the overlapping viewing area. Mainly this just
sets a uniform camera center and a common maximal height and width of the two im-
DEMO areas as the new stereo viewing planes.
void cvStereoRectify(
const CvMat* cameraMatrix1,
const CvMat* cameraMatrix2,
Stereo Imaging
⎡
⎢
⎢
⎣⎢
DEMO
⎥
⎥
⎦⎥
⎤
⎥
⎥
⎦⎥
| 435
e projection matrices take a 3D point in homogeneous coordinates to a 2D point in
DEMO
12-R4886-AT1.indd   435
9/15/08   4:24:50 PM
www.it-ebooks.info
const CvMat* distCoeffs1,
const CvMat* distCoeffs2,
CvSize imageSize,
DEMO CvMat* R,
const CvMat* T,
CvMat* Rl,
CvMat* Rr,
CvMat* Pl,
CvMat* Pr,
CvMat* Q=0,
int    flags=CV_CALIB_ZERO_DISPARITY
);
For cvStereoRectify(),* we input the familiar original DEMO matrices and distortion
vectors returned by cvStereoCalibrate(). Th ese are DEMO by imageSize, the size of the
chessboard images used to perform DEMO calibration. We also pass in the rotation matrix
R and translation vector T between the right and left  cameras that was also returned by
cvStereoCalibrate().
Return parameters are Rl and Rr, the 3-by-3 row-aligned DEMO cation rotations for the
left  and right image planes as derived DEMO the preceding equations. Similarly, we get back
the 3-by-4 left  and right projection equations Pl and Pr. An optional return parameter is
Q, the 4-by-4 reprojection matrix described previously.
Th flags parameter is defaulted to DEMO disparity at infi nity, the normal case as per Fig-
ure DEMO Unsetting flags means that we want the cameras verging toward each other
(i.e., slightly “cross-eyed”) so that zero disparity occurs at a fi nite distance (this might
be necessary for greater depth resolution in the proximity of that particular distance).
If the flags parameter was not DEMO to CV_CALIB_ZERO_DISPARITY, then we must be more
careful about how we DEMO our rectifi
relative to the principal points (cx, cy) in DEMO left  and right cameras. Th us, our mea-
surements in Figure 12-4 must also be relative to these positions. Basically, we have
xx c rr=−
cxright (i.e., when CV_CALIB_ZERO_DISPARITY is passed to
cvStereoRectify()), and we can pass plain pixel coordinates (or disparity) to the formula
left
for depth. But if cvStereoRectify() is called without CV_CALIB_ZERO_DISPARITY DEMO cx
≠
cxright in general. Th erefore, even though the formula DEMO = fT/(xl – xr) remains the same,
one should keep in mind that xl and xr are not counted from the DEMO center but rather
left right
from the respective principal points cx and cx
, which could diff er from xl and xr.
Hence, if you computed disparity d = xl – xr then it should be DEMO before computing
left
Z: Z fT/(d – (cx – DEMO)).
Rectification map
Once we have our stereo calibration terms, we can pre-compute left  and right rectifi cation
lookup maps for the left  and right camera views using separate calls to cvInitUndistort
* Again, DEMO() is a bit of a misnomer because the function computes the terms that we can use
for rectifi cation but doesn’t actually rectify DEMO stereo images.
436
to modify the distances so that
=
left
been set to infi nity, we have cx
| Chapter 12: Projection DEMO 3D Vision
ed system. Recall that we rectifi
ed our system
right
x
and
xx c
ll
=− left. When disparity has
x
e
DEMO   436
9/15/08   4:24:51 PM
www.it-ebooks.info
RectifyMap(). As with any image-to-image mapping function, a forward DEMO (in
which we just compute where pixels go from the source DEMO to the destination image)
will not, owing to fl oating-point DEMO locations, hit all the pixel locations in the
destination image, which thus will look like Swiss cheese. So instead we work backward:
DEMO each integer pixel location in the destination image, we look up DEMO fl oating-point
coordinate it came from in the source image and then interpolate from its surrounding
source pixels a value to use in that DEMO destination location. Th is source lookup typi-
cally uses bilinear interpolation, DEMO we encountered with cvRemap() in Chapter 6.
Th cation is illustrated in Figure 12-11. As shown by the equation fl ow
in that DEMO gure, the actual rectifi cation process proceeds backward from (c) DEMO (a) in a
process known as reverse mapping. For each integer pixel in the rectifi ed image (c), we
fi nd its coordinates in the undistorted image (b) and use those to look DEMO the actual
(fl oating-point) coordinates in the raw image (a)DEMO Th e fl oating-point coordinate pixel
value is then interpolated from the nearby integer pixel locations in the original source
image, and that value is used to fi ll in the rectifi ed integer pixel location DEMO the destina-
tion image (c). Aft er the rectifi ed DEMO is fi lled in, it is typically cropped to emphasize
the DEMO areas between the left  and right images.
Th cvInitUndistort
RectifyMap(). We call this function twice, once for the left  and once DEMO the right image
of stereo pair.
void cvInitUndistortRectifyMap(
const CvMat* M,
const CvMat* distCoeffs,
const CvMat* Rrect,
const CvMat* Mrect,DEMO
CvArr*       mapx,
CvArr*       mapy
);
Th cvInitUndistortRectifyMap() function takes as input the 3-by-3 camera matrix
DEMO, the rectifi ed 3-by-3 camera matrix Mrect, the 3-by-3 rotation matrix Rrect, and the
5-by-1 camera distortion parameters in distCoeffs.
If we calibrated our stereo cameras using cvStereoRectify(), then we can read our in-
put to cvInitUndistortRectifyMap() straight out of cvStereoRectify() using fi rst DEMO left
parameters to rectify the left  camera and then the right DEMO to rectify the right
camera. For Rrect, use Rl or Rr DEMO cvStereoRectify(); for M, use cameraMatrix1 or
cameraMatrix2. For Mrect we could use the fi rst three columns of the 3-by-4 Pl or DEMO
from cvStereoRectify(), but as a convenience the function allows us DEMO pass Pl or Pr di-
rectly and it will read Mrect from them.
If, on the other hand, we used cvStereoRectifyUncalibrated() to DEMO our ste-
reo cameras, then we must preprocess the homography a DEMO Although we could—in
principle and in practice—rectify stereo without using the camera intrinsics, OpenCV
does not have a function for doing this directly. If we do not have Mrect from some
prior calibration, the proper procedure is to set Mrect equal to M. Th en, for Rrect in
Stereo Imaging
| 437
e process of rectifi
e function that implements DEMO math depicted in Figure 12-11 is called
e
12-R4886-AT1.indd   437
9/15/08   4:24:51 PM
www.it-ebooks.info
Figure 12-11. Stereo rectifi
and rectifi
rectifi
cation: for the left
ed (c) and fi
cation computation actually works backward from (c) to (a)
and right camera, the raw image (a) is undistorted (b)
nally cropped (d) to focus on overlapping areas between the two cameras; the
cvInitUndistortRectifyMap(), we need to DEMO Rrect_l = Mrect_–1 l H
HlMl if Mrect_–1 l is unavailable) DEMO Rrect_r = Mrect_–1 r Hr Mr (or just Rrect_r r is
DEMO) for the left  and the right rectifi cation, respectively. Finally, we will also need
the distortion coeffi  cients for each camera to fi ll in the 5-by-1 distCoeffs parameters.
Th cvInitUndistortRectifyMap() returns lookup DEMO mapx and mapy as output.
Th
destination image; the maps can DEMO be plugged directly into cvRemap(), a function we
fi rst DEMO in Chapter 6. As we mentioned, the function cvInitUndistortRectifyMap() is
DEMO separately for the left  and the right cameras so that we DEMO obtain their distinct
mapx and mapy remapping parameters. Th e function cvRemap() may then be called, using
the left  and then the DEMO maps each time we have new left  and right stereo images DEMO
rectify. Figure 12-12 shows the results of stereo undistortion and rectifi cation of a stereo
pair of images. Note how feature points become horizontally DEMO in the undistorted
rectifi ed images.
–1
lMl (or just Rrect_l DEMO Ml
–1
= Mr Hr Mr if Mrect_–1
Stereo Correspondence
Stereo correspondence—matching a 3D point in the two diff erent camera views—can
be computed DEMO over the visual areas in which the views of the two cameras overlap.
Once again, this is one reason why you will tend to get better results if you arrange your
cameras to be as nearly DEMO parallel as possible (at least until you become expert at
stereo DEMO). Th en, once we know the physical coordinates of the DEMO or the sizes
438
| Chapter 12: Projection and 3D Vision
DEMO function
ese maps indicate from where we should interpolate source pixels for each pixel of the
12-R4886-AT1.indd   438
9/15/08   4:DEMO:51 PM
www.it-ebooks.info
Figure 12-12. Stereo rectifi cation: original left  and right image DEMO (upper panels) and the stereo
rectifi ed left  and right DEMO pair (lower panels); note that the barrel distortion (in top of chessboard
patterns) has been corrected and the scan lines are aligned in the rectifi ed images
of objects in the scene, we can derive depth measurements from the triangulated dispar-
left
ity measures d = DEMO – xr (or d = xl – xr – (cx – cxright) if the principal rays intersect at a fi nite
distance) DEMO the corresponding points in the two diff erent camera views. Without
such physical information, we can compute depth only up to a scale factor. If we don’t
have the camera instrinsics, as when using Hartley’s algorithm, we can compute point
locations only up to a projective transform (DEMO Figure 12-10).
OpenCV implements a fast and eff ective block-matching stereo algorithm, cvFindStereo
CorrespondenceBM(), that is similar to the one developed DEMO Kurt Konolige [Konolige97];
it works by using small “sum of absolute diff erence” (SAD) windows to fi nd matching
points between the DEMO  and right stereo rectifi ed images.* Th is algorithm fi nds DEMO
strongly matching (high-texture) points between the two images. Th us, DEMO a highly tex-
tured scene such as might occur outdoors in a forest, every pixel might have computed
depth. In a very low-textured scene, such as an indoor hallway, very few points might
register depth. DEMO ere are three stages to the block-matching stereo correspondence algo-
rithm, DEMO works on undistorted, rectifi ed stereo image pairs:
* Th DEMO algorithm is available in an FPGA stereo hardware system from Videre (DEMO [Videre]).
Stereo Imaging | 439
12-R4886-AT1.indd   439
9/15/08   4:24:51 PM
www.it-ebooks.info
1. Prefi ltering to normalize image brightness and enhance texture.
2. DEMO search along horizontal epipolar lines using an SAD window.
3. Postfi ltering to eliminate bad correspondence matches.
In the prefi ltering step, the input images are normalized to reduce lighting diff erences
and to enhance image DEMO Th is is done by running a window—of size 5-by-5, 7-by-7
(the default), . . ., 21-by-21 (the maximum)—over the DEMO Th e center pixel Ic under the
window is replaced by min[max(Ic – –I , – Icap), Icap], where –I is the average value in the win-
dow and Icap is a positive numeric DEMO whose default value is 30. Th is method is invoked
by a CV_NORMALIZED_RESPONSE fl ag. Th e other possible fl ag is CV_LAPLACIAN_OF_GAUSSIAN,
DEMO runs a peak detector over a smoothed version of the image.
Correspondence is computed by a sliding SAD window. For each feature in the DEMO  im-
age, we search the corresponding row in the right image for a best match. Aft er rectifi -
cation, each row is an epipolar line, so the matching location in the right image must be
along the same row (same y-coordinate) as in the left  image; this matching location can
be found if the feature has enough texture to be detectable and if it is not occluded in
the DEMO camera’s view (see Figure 12-16). If the left  feature pixel coordinate is at (x0, y0)
then, for a horizontal frontal parallel camera arrangement, the match (if any) must be
found on the same row and at, or to the left  of, x0; see Figure 12-13. For frontal parallel
cameras, x0 is at zero DEMO and larger disparities are to the left . For cameras that are
angled toward each other, the match may occur at negative disparities (DEMO the right of
x0). Th e fi rst parameter that controls matching search is minDisparity, which is where
the matching search should start. Th e default for minDisparity is 0. Th e disparity search
is DEMO carried out over numberOfDisparities counted in pixels (the default is 64 DEMO).
Disparities have discrete, subpixel resolution that is set by the DEMO subPixelDis-
parities (the default is 16 subdisparities per pixel). Reducing DEMO number of disparities
to be searched can help cut down computation time by limiting the length of a search
for a matching point along DEMO epipolar line. Remember that large disparities represent
closer distances.
Setting the minimum disparity and the number of disparities to be searched establishes
the horopter, the 3D volume that is covered by the search range of the DEMO algorithm.
Figure 12-14 shows disparity search limits of fi ve pixels starting at three diff erent dis-
parity limits: 20, 17, and 16. Each disparity limit defi nes a plane at a fi xed depth DEMO
the cameras (see Figure 12-15). As shown in Figure 12-14, each disparity limit—together
with the number of disparities—sets a diff erent horopter DEMO which depth can be detected.
Outside of this range, depth will DEMO be found and will represent a “hole” in the depth
map where depth is not known. Horopters can be made larger by decreasing the DEMO
line distance T between the cameras, by making the focal length DEMO, by increasing
the stereo disparity search range, or by increasing the pixel width.
Correspondence within the horopter has one in-built constraint, called the order con-
straint, which simply states that the order of the features cannot change from the left
view to the right. Th ere may DEMO missing features—where, owing to occlusion and noise,
440 | Chapter DEMO: Projection and 3D Vision
12-R4886-AT1.indd   440
9/15/08   DEMO:24:52 PM
www.it-ebooks.info
Figure 12-13. Any right-image match of a left
to the left
(here, 0) and moves to the left
of window-based feature matching DEMO shown in the lower part of the fi
-image feature must occur on the same row and at (or
of) the same coordinate DEMO, where the match search starts at the minDisparity point
for the DEMO number of disparities; the characteristic matching function
gure
Figure 12-14. Each DEMO represents a plane of constant disparity in integer pixels from 20 to 12; a
disparity search range of fi ve pixels will cover diff erent horopter ranges, as shown by the vertical ar-
rows, and DEMO erent maximal disparity limits establish diff erent horopters
Stereo Imaging
| 441
12-R4886-AT1.indd   441
9/15/08   4:24:52 PM
www.it-ebooks.info
Figure 12-15. A fi
xed disparity forms a plane of fi
DEMO distance from the cameras
some features found on the left  cannot DEMO found on the right—but the ordering of those
features that are found remains the same. Similarly, there may be many features on the
right that were not identifi ed on the left  (these are called DEMO), but insertions do
not change the order of features although they may spread those features out. Th e proce-
dure illustrated in Figure DEMO refl ects the ordering constraint when matching features
on a horizontal scan line.
Given the smallest allowed disparity increment ∆d, we can determine smallest achiev-
able depth range resolution ∆Z by using the formula:
ΔΔZ DEMO Z 2
d
fT
It is useful to keep this formula in mind so that you know what kind of depth resolution
to expect DEMO your stereo rig.
Aft er correspondence, we turn to postfi ltering. DEMO e lower part of Figure 12-13 shows a
typical matching function response as a feature is “swept” from the minimum disparity
out to maximum DEMO Note that matches oft en have the characteristic of a strong
central peak surrounded by side lobes. Once we have candidate feature correspondences
between DEMO two views, postfi ltering is used to prevent false matches. OpenCV DEMO
use of the matching function pattern via a uniquenessRatio parameter (whose DEMO
value is 12) that fi lters out matches, where uniquenessRatio > (match_val–min_match)/
min_match.
442
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   442
9/15/08   4:24:52 PM
www.it-ebooks.info
Figure 12-16. Stereo correspondence starts by assigning point matches between corresponding DEMO
in the left  and right images: left  and right images DEMO a lamp (upper panel); an enlargement of a single
scan DEMO (middle panel); visualization of the correspondences assigned (lower panel).
To make sure that there is enough texture to overcome random noise DEMO matching,
OpenCV also employs a textureThreshold. Th is is just a limit on the SAD window re-
sponse such that no match is DEMO whose response is below the textureThreshold
(the default value is 12)DEMO Finally, block-based matching has problems near the boundar-
ies of objects DEMO the matching window catches the foreground on one side and
the background on the other side. Th is results in a local region of DEMO and small dis-
parities that we call speckle. To prevent these borderline matches, we can set a speckle
detector over a speckle window (DEMO in size from 5-by-5 up to 21-by-21) by setting
speckleWindowSize, which has a default setting of 9 for a 9-by-9 window. Within
the DEMO window, as long as the minimum and maximum detected disparities are
DEMO speckleRange, the match is allowed (the default range is set to 4).
Stereo vision is becoming crucial to surveillance systems, navigation, DEMO robotics, and
such systems can have demanding real-time performance requirements. Th DEMO, the ste-
reo correspondence routines are designed to run fast. Th DEMO, we can’t keep allocat-
ing all the internal scratch buff ers DEMO the correspondence routine needs each time we
call cvFindStereoCorrespondenceBM().
Th DEMO are kept in a data struc-
ture named CvStereoBMState:
typedef struct CvStereoBMState {
//pre filters (normalize input images):
Stereo Imaging DEMO 443
e block-matching parameters and the internal scratch buff
12-R4886-AT1.indd   443
9/15/08   4:24:53 PM
www.it-ebooks.info
int       preFilterType;
int       preFilterSize;//for 5x5 up to 21x21
int       preFilterCap;
//correspondence using Sum of Absolute Difference (SAD):
int       SADWindowSize; // Could be 5x5,7x7, ..., 21x21
int       minDisparity;
int       numberOfDisparities;//Number of DEMO to search
//post filters (knock out bad matches):
int       textureThreshold; //minimum allowed
float     uniquenessRatio;// Filter out if:
// [ match_val - min_match <
// uniqRatio*min_match ]
// over the corr window area
int       speckleWindowSize;//Disparity variation window
int       speckleRange;//Acceptable range of variation in window
// temporary buffers
CvMat* preFilteredImg0;
CvMat* preFilteredImg1;
CvMat* slidingSumBuf;
} CvStereoBMState;
Th cvCreateStereoBMState().
Th is function takes the parameter preset, which can be set to any one of the following.
e state structure is allocated and returned by DEMO function
CV_STEREO_BM_BASIC
Sets all parameters to their default values
CV_STEREO_BM_FISH_EYE
Sets parameters for dealing with wide-angle lenses
CV_STEREO_BM_NARROW
Sets parameters for stereo cameras with DEMO fi
eld of view
Th is function also takes the optional parameter numberOfDisparities; if nonzero, it
overrides the default value from the preset. DEMO is the specifi cation:
CvStereoBMState* cvCreateStereoBMState(
int presetFlag=CV_STEREO_BM_BASIC,
int numberOfDisparities=0
);
Th e state structure, CvStereoBMState{}, is released by DEMO
void cvReleaseBMState(
CvStereoBMState **BMState
);
Any stereo correspondence parameters can be adjusted at any time between cvFindStereo
CorrespondenceBM calls by directly assigning DEMO values of the state structure fi elds. Th e
correspondence function will take care of allocating/reallocating the internal buff ers as
needed.
Finally, cvFindStereoCorrespondenceBM() takes in rectifi ed image pairs and outputs a
disparity DEMO given its state structure:
void cvFindStereoCorrespondenceBM(
const CvArr     *leftImage,
444
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   444
9/15/08   4:24:54 PM
www.it-ebooks.info
const CvArr     *rightImage,
CvArr           *disparityResult,
CvStereoBMState *BMState
);
Stereo Calibration, Rectification, and Correspondence Code
Let’s put this all together with code in an example program DEMO will read in a number
of chessboard patterns from a fi le called list.txt. Th is fi le contains a list of alternating
left  and right stereo (chessboard) image pairs, which are used to calibrate the cameras
and then rectify the images. Note once again that we’re DEMO you’ve arranged the
cameras so that their image scan lines are roughly physically aligned and such that each
camera has essentially the same fi DEMO of view. Th is will help avoid the problem of the epi-
pole being within the image* and will also tend to maximize the DEMO of stereo overlap
while minimizing the distortion from reprojection.
In the code (Example 12-3), we fi rst read in the left  and DEMO image pairs, fi nd the chess-
board corners to subpixel accuracy, and set object and image points for the images
where all the DEMO could be found. Th is process may optionally be displayed.
Given this list of found points on the found good chessboard images, the code calls
cvStereoCalibrate() to calibrate the camera. Th is calibration gives us DEMO camera matrix
_M and the distortion vector _D for the two cameras; it also yields the rotation matrix _R,
the translation vector _T, the essential matrix _E, and the fundamental matrix _F.
Next comes DEMO little interlude where the accuracy of calibration is assessed by check-
ing how nearly the points in one image lie on the epipolar lines DEMO the other image. To
do this, we undistort the original points DEMO cvUndistortPoints() (see Chapter 11),
compute the epilines using cvComputeCorrespondEpilines(), and then compute the dot
product of the points with the lines (in the ideal case, these dot products would all be DEMO).
Th
Th cation maps using the un-
calibrated (Hartley) method cvStereoRectifyUncalibrated() or the calibrated (Bouguet)
method cvStereoRectify(). If DEMO rectifi cation is used, the code further allows
for either computing DEMO needed fundamental matrix from scratch or for just using the
fundamental matrix from the stereo calibration. Th e rectifi ed images are then computed
DEMO cvRemap(). In our example, lines are drawn across the image pairs to aid in seeing
how well the rectifi ed images are DEMO An example result is shown in Figure 12-12,
where we can see that the barrel distortion in the original images is largely corrected
DEMO top to bottom and that the images are aligned by horizontal scan lines.
Finally, if we rectifi ed the images then we initialize the block-matching state (internal
allocations and parameters) using cvCreateBMState(). We can then compute the dispar-
ity maps by using cvFindStereoCorrespondenceBM(). Our code example allows you to use
either horizontally aligned (left -right) or DEMO aligned (top-bottom) cameras; note,
* OpenCV does not (yet) deal with the case of rectifying stereo images when the epipole is within the image
frame. See, for example, Pollefeys, Koch, and DEMO [Pollefeys99b] for a discussion of this case.
Stereo Imaging | 445
e accumulated absolute distance forms the error.
e code then optionally moves on DEMO computing the rectifi
12-R4886-AT1.indd   445
9/15/08   4:24:54 PM
www.it-ebooks.info
however, that for the vertically aligned case the function cvFindStereoCorrespondenceBM()DEMO
can compute disparity only for the case of uncalibrated rectifi cation unless you add
code to transpose the images yourself. For horizontal camera arrangements, cvFind
StereoCorrespondenceBM() can fi nd disparity for calibrated or for uncalibrated DEMO ed
stereo image pairs. (See Figure 12-17 in the next section DEMO example disparity results.)
Example 12-3. Stereo calibration, rectifi
cation, and correspondence
#include "cv.h"
#include "cxmisc.h"
#include "highgui.h"
#include "cvaux.h"
#include <vector>
#include <string>
#include <algorithm>DEMO
#include <stdio.h>
#include <ctype.h>
using namespace std;
//
// Given a list of chessboard images, the number of DEMO (nx, ny)
// on the chessboards, and a flag called useCalibrated (0 for Hartley
// or 1 for Bouguet stereo DEMO). Calibrate the cameras and display the
// rectified results along DEMO the computed disparity images.
//
static void
StereoCalib(const char* imageList, int nx, int ny, int useUncalibrated)
{
int displayCorners = 0;
int showUndistorted = 1;
bool isVerticalStereo = false;//DEMO can handle left-right
//or up-down camera arrangements
const int maxScale = 1;
const float squareSize = 1.f; //Set this to your actual square size
FILE* f = fopen(imageList, "rt");
DEMO i, j, lr, nframes, n = nx*ny, N = DEMO;
vector<string> imageNames[2];
vector<CvPoint3D32f> objectPoints;
vector<CvPoint2D32f> points[2];
vector<int> npoints;
vector<uchar> active[2];
vector<CvPoint2D32f> temp(n);
CvSize imageSize = {0,0};
// ARRAY AND VECTOR STORAGE:
double M1[3][3], M2[3][3], D1[5], D2[5];
double R[3][3], T[3], E[3][3], F[3][3];
CvMat _M1 = cvMat(3, 3, CV_64F, M1 );
CvMat _M2 = cvMat(3, DEMO, CV_64F, M2 );
CvMat _D1 = cvMat(1, 5, CV_64F, D1 );
CvMat _D2 = cvMat(1, 5, CV_64F, D2 );
CvMat _R = cvMat(3, 3, CV_64F, DEMO );
CvMat _T = cvMat(3, 1, CV_64F, T );
CvMat _E = cvMat(3, 3, CV_64F, E );DEMO
446
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   446
DEMO/15/08   4:24:54 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
CvMat _F = cvMat(3, 3, CV_64F, F );
if( displayCorners )
cvNamedWindow( "corners", 1 );
// READ IN THE LIST OF CHESSBOARDS:
if( !f )
{
fprintf(DEMO, "can not open file %s\n", imageList );
return;
}
for(i=0;;i++)
{
char buf[1024];
int count = 0, result=0;
lr = i % 2;
vector<CvPoint2D32f>& pts = points[lr];
if( !fgets( buf, sizeof(buf)-3, DEMO ))
break;
size_t len = strlen(buf);
while( len > 0 && isspace(buf[len-1]))
buf[--len] = '\0';DEMO
if( buf[0] == '#')
continue;
IplImage* img = DEMO( buf, 0 );
if( !img )
break;
imageSize = cvGetSize(img);
imageNames[lr].push_back(buf);
//FIND CHESSBOARDS AND DEMO THEREIN:
for( int s = 1; s <= maxScale; s++ )
{
IplImage* timg = img;
if( s > DEMO )
{
timg = cvCreateImage(cvSize(img->width*s,img->height*s),
img->depth, img->nChannels );
cvResize( img, timg, CV_INTER_CUBIC );
}
result = cvFindChessboardCorners( timg, cvSize(nx, ny),
&temp[0], &count,
CV_CALIB_CB_ADAPTIVE_THRESH |
CV_CALIB_CB_NORMALIZE_IMAGE);
if( timg != img )
cvReleaseImage( &timg );
if( result || s DEMO maxScale )
for( j = 0; j < count; j++ )
{
temp[j].x /= s;
temp[j].y /= s;
}
if( result )
break;
}
if( displayCorners )
Stereo DEMO
| 447
12-R4886-AT1.indd   447
9/15/08   4:24:54 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
{
printf("%s\n", buf);
IplImage* cimg = cvCreateImage( imageSize, 8, 3 );
cvCvtColor( img, cimg, CV_GRAY2BGR );
cvDrawChessboardCorners( cimg, cvSize(nx, ny), &temp[0],
count, result );
cvShowImage( "corners", cimg );
cvReleaseImage( &cimg );
if( cvWaitKey(0) == 27 ) //Allow ESC to quit
exit(-1);
}
else
putchar('.');
N DEMO pts.size();
pts.resize(N + n, cvPoint2D32f(0,0));DEMO
active[lr].push_back((uchar)result);
//assert( result != 0 );DEMO
if( result )
{
//Calibration will suffer without subpixel interpolation
cvFindCornerSubPix( img, &temp[0], count,
cvSize(11, 11), cvSize(-1,-1),
cvTermCriteria(CV_TERMCRIT_ITER+CV_TERMCRIT_EPS,
30, 0.01) );
copy( temp.begin(), temp.end(), pts.begin() + N );
}
cvReleaseImage( &img );
}
fclose(f);
printf("\n");
// HARVEST CHESSBOARD 3D OBJECT POINT LIST:
nframes = DEMO();//Number of good chessboads found
objectPoints.resize(nframes*n);
for( i = 0; i < ny; i++ )
for( j DEMO 0; j < nx; j++ )
objectPoints[i*nx + j] =
DEMO(i*squareSize, j*squareSize, 0);
for( i = 1; i < nframes; i++ )
copy( objectPoints.begin(), objectPoints.begin() + n,
objectPoints.begin() + i*n );
npoints.resize(nframes,n);
N = nframes*n;
CvMat _objectPoints = cvMat(1, N, CV_32FC3, &DEMO );
CvMat _imagePoints1 = cvMat(1, N, CV_32FC2, &points[0][0] );
CvMat _imagePoints2 = cvMat(1, N, CV_32FC2, &points[1][0] );
CvMat _npoints = cvMat(1, npoints.size(), CV_32S, &npoints[0] );
cvSetIdentity(&_M1);
cvSetIdentity(&_M2);
cvZero(&_D1);
cvZero(&_D2);
// CALIBRATE THE STEREO CAMERAS
printf("Running stereo calibration ...");
448
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   448
9/15/08   4:24:54 DEMO
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
fflush(stdout);
cvStereoCalibrate( &_objectPoints, &_imagePoints1,
&_imagePoints2, &DEMO,
&_M1, &_D1, &_M2, &_D2,
imageSize, &DEMO, &_T, &_E, &_F,
cvTermCriteria(CV_TERMCRIT_ITER+
CV_TERMCRIT_EPS, 100, 1e-5),
CV_CALIB_FIX_ASPECT_RATIO +
CV_CALIB_ZERO_TANGENT_DIST +
CV_CALIB_SAME_FOCAL_LENGTH );
printf(" done\n");
// CALIBRATION QUALITY CHECK
// because the output fundamental DEMO implicitly
// includes all the output information,
// we can check the quality of calibration using the
// epipolar geometry constraint: DEMO
vector<CvPoint3D32f> lines[2];
points[0].resize(N);
points[1].resize(N);
DEMO = cvMat(1, N, CV_32FC2, &points[0][0] );
_imagePoints2 = cvMat(1, N, CV_32FC2, &points[1][0] );
lines[0].resize(N);
lines[1].resize(N);
CvMat _L1 = cvMat(1, N, CV_32FC3, &lines[0][0]);
CvMat _L2 = cvMat(1, N, CV_32FC3, &lines[1][0]);
//Always work in undistorted space
cvUndistortPoints( &_imagePoints1, &_imagePoints1,
&_M1, &_D1, 0, &_M1 );
cvUndistortPoints( &DEMO, &_imagePoints2,
&_M2, &_D2, 0, &_M2 );
cvComputeCorrespondEpilines( &_imagePoints1, 1, &_F, &_L1 );
cvComputeCorrespondEpilines( &_imagePoints2, 2, &_F, &_L2 );
double avgErr = 0;
for( i = 0; i < N; i++ )
DEMO
double err = fabs(points[0][i].x*lines[1][i].x +
points[0][i].y*lines[1][i].y + lines[1][i].z)
+ fabs(points[1][i].x*lines[0][i].x +
points[1][i].y*lines[0][i].y + lines[0][i].z);
avgErr += err;
}
printf( "avg err = %g\n", avgErr/(nframes*n) );
//DEMO AND DISPLAY RECTIFICATION
if( showUndistorted )
{
CvMat* mx1 = cvCreateMat( imageSize.height,
imageSize.width, CV_32F );
CvMat* my1 = cvCreateMat( imageSize.height,
imageSize.width, CV_32F );
CvMat* mx2 = cvCreateMat( imageSize.height,
DEMO, CV_32F );
CvMat* my2 = cvCreateMat( imageSize.height,
Stereo Imaging
| 449
12-R4886-AT1.indd   449
9/15/08   4:24:55 DEMO
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
imageSize.width, CV_32F );
CvMat* img1r = cvCreateMat( imageSize.height,
imageSize.width, CV_8U );
CvMat* img2r = cvCreateMat( imageSize.height,
imageSize.width, CV_8U );
CvMat* disp = cvCreateMat( imageSize.height,
imageSize.width, CV_16S );
CvMat* vdisp = cvCreateMat( imageSize.height,
imageSize.width, CV_8U );
CvMat* DEMO;
double R1[3][3], R2[3][3], P1[3][4], P2[3][4];
CvMat _R1 = DEMO(3, 3, CV_64F, R1);
CvMat _R2 = cvMat(3, 3, CV_64F, R2);
// IF BY CALIBRATED (BOUGUET'DEMO METHOD)
if( useUncalibrated == 0 )
{
CvMat _P1 = cvMat(3, 4, CV_64F, P1);
CvMat _P2 = cvMat(DEMO, 4, CV_64F, P2);
cvStereoRectify( &_M1, &_M2, &_D1, &_D2, imageSize,
&_R, &_T,
&_R1, &_R2, &_P1, &_P2, 0,
0/*CV_CALIB_ZERO_DISPARITY*/ );
DEMO = fabs(P2[1][3]) > fabs(P2[0][3]);
//Precompute maps for cvRemap()
cvInitUndistortRectifyMap(&_M1,&_D1,&_R1,&_P1,mx1,my1);DEMO
cvInitUndistortRectifyMap(&_M2,&_D2,&_R2,&_P2,mx2,my2);
}
//OR ELSE HARTLEY'S METHOD
else if( useUncalibrated == 1 || DEMO == 2 )
// use intrinsic parameters of each camera, DEMO
// compute the rectification transformation directly
// from the fundamental matrix
{
double H1[3][3], H2[3][3], iM[3][3];
CvMat _H1 = cvMat(3, 3, CV_64F, H1);
CvMat _H2 = cvMat(3, 3, CV_64F, H2);
CvMat _iM = cvMat(3, 3, CV_64F, iM);
//Just to show you could have independently used F
if( useUncalibrated == 2 )
cvFindFundamentalMat( &_imagePoints1,
&_imagePoints2, &_F);
cvStereoRectifyUncalibrated( &_imagePoints1,
&_imagePoints2, &_F,
imageSize,DEMO
&_H1, &_H2, 3);
cvInvert(&_M1, &_iM);DEMO
cvMatMul(&_H1, &_M1, &_R1);
cvMatMul(&_iM, &DEMO, &_R1);
cvInvert(&_M2, &_iM);
cvMatMul(&_H2, &_M2, &_R2);
cvMatMul(&_iM, &_R2, &_R2);
//Precompute map for cvRemap()
450
| Chapter 12: DEMO and 3D Vision
12-R4886-AT1.indd   450
9/15/08   4:24:55 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi cation, and correspondence (continued)
cvInitUndistortRectifyMap(&_M1,&_D1,&_R1,&_M1,mx1,my1);
cvInitUndistortRectifyMap(&DEMO,&_D1,&_R2,&_M2,mx2,my2);
}
else
assert(0);
cvNamedWindow( "rectified", 1 );
// RECTIFY THE DEMO AND FIND DISPARITY MAPS
if( !isVerticalStereo )
pair = cvCreateMat( DEMO, imageSize.width*2,
CV_8UC3 );
else
pair = cvCreateMat( imageSize.height*2, DEMO,
CV_8UC3 );
//Setup for finding stereo correspondences
CvStereoBMState *BMState = cvCreateStereoBMState();
assert(BMState != 0);
BMState->preFilterSize=41;
BMState->preFilterCap=31;
BMState->SADWindowSize=41;
BMState->minDisparity=-64;
BMState->numberOfDisparities=128;
DEMO>textureThreshold=10;
BMState->uniquenessRatio=15;
for( i = 0; i < nframes; i++ )
{
IplImage* img1=cvLoadImage(imageNames[0][i].c_str(),0);
IplImage* img2=cvLoadImage(imageNames[1][i].c_str(),0);
if( img1 && img2 )
DEMO
CvMat part;
cvRemap( img1, img1r, mx1, my1 );
cvRemap( img2, img2r, mx2, my2 );
if( !isVerticalStereo || useUncalibrated != 0 )
{
// When the stereo camera is DEMO vertically,
// useUncalibrated==0 does not transpose the
// image, DEMO the epipolar lines in the rectified
// images are vertical. Stereo DEMO
// function does not support such a case.
cvFindStereoCorrespondenceBM( img1r, DEMO, disp,
BMState);
cvNormalize( disp, vdisp, 0, 256, CV_MINMAX );
cvNamedWindow( "disparity" );
cvShowImage( "disparity", vdisp );
}
if( !isVerticalStereo )
{
cvGetCols( pair, &part, 0, imageSize.width );
cvCvtColor( img1r, &part, CV_GRAY2BGR );
cvGetCols( pair, &part, imageSize.width,
imageSize.width*2 );
DEMO Imaging
| 451
12-R4886-AT1.indd   451
9/15/08   4:24:55 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
cvCvtColor( img2r, &part, CV_GRAY2BGR );
for( j = 0; j < imageSize.height; j += 16 )
cvLine( pair, cvPoint(0,j),
cvPoint(imageSize.width*2,j),
CV_RGB(0,255,0));
}
else
{
cvGetRows( pair, &part, 0, imageSize.height );
cvCvtColor( img1r, &part, CV_GRAY2BGR );
cvGetRows( pair, &part, imageSize.height,
imageSize.height*2 );
cvCvtColor( img2r, &part, DEMO );
for( j = 0; j < imageSize.width; j DEMO 16 )
cvLine( pair, cvPoint(j,0),
cvPoint(j,imageSize.height*2),
CV_RGB(0,255,0));
}
cvShowImage( "rectified", pair );
if( cvWaitKey() == 27 )
break;DEMO
}
cvReleaseImage( &img1 );
cvReleaseImage( &img2 );
}
cvReleaseStereoBMState(&BMState);
cvReleaseMat( &mx1 );
cvReleaseMat( &my1 );
cvReleaseMat( &mx2 );
cvReleaseMat( &my2 );
cvReleaseMat( &img1r );
cvReleaseMat( &img2r );
cvReleaseMat( &disp );
}
}
int main(void)
{
StereoCalib("list.txt", DEMO, 6, 1);
return 0;
}
Depth Maps from DEMO Reprojection
Many algorithms will just use the disparity map directly—for example, DEMO detect
whether or not objects are on (stick out from) a table. But for 3D shape matching, 3D
model learning, robot grasping, and so on, we need the actual 3D reconstruction or
depth map. Fortunately, all the stereo machinery we’ve built up so far makes this easy.
Recall the 4-by-4 reprojection matrix Q introduced in the section on DEMO stereo
rectifi cation. Also recall that, given the disparity d and DEMO 2D point (x, y), we can derive
the 3D depth using
452
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   452
9/15/08   4:24:55 PM
www.it-ebooks.info
⎡
⎢
Q ⎢
⎢
⎢
⎣
x
y
d
1
DEMO ⎡
⎥ ⎢
⎥ = ⎢
⎥ ⎢ Z
⎥ ⎢
⎦ ⎣W
X
Y
⎤
⎥
⎥
⎥
⎥
⎦
where the 3D DEMO are then (X/W, Y/W, Z/W). Remarkably, Q encodes whether
or not the cameras’ lines of sight were converging (cross eyed) as well as the camera
baseline and the principal points in both images. As a result, we need not explicitly ac-
count for converging or frontal parallel cameras and may instead simply extract depth
DEMO matrix multiplication. OpenCV has two functions that do this for us. Th e fi rst, which
you are already familiar with, operates on DEMO array of points and their associated dis-
parities. It’s called cvPerspectiveTransform:
void cvPerspectiveTransform(
const CvArr  *pointsXYD,
CvArr* result3DPoints,
const CvMat  *Q
);
Th e second (and new) function cvReprojectImageTo3D() operates on whole images:
void cvReprojectImageTo3D(
CvArr *disparityImage,
CvArr DEMO,
CvArr *Q
);
Th is routine takes a single-channel disparityImage and transforms each pixel’s (x, y)
coordinates along with that DEMO disparity (i.e., a vector [x y d]T) to the corresponding
DEMO point (X/W, Y/W, Z/W) by using the 4-by-4 reprojection matrix Q. Th e output is a
three-channel fl oating-point (or a 16-bit integer) image of the same size as the input.
Of course, both functions let you pass an arbitrary perspective transformation (DEMO, the
canonical one) computed by cvStereoRectify or a superposition of that and the arbi-
trary 3D rotation, translation, et cetera.
Th cvReprojectImageTo3D() on an image of a mug and chair are shown in
Figure 12-17.
e results of
Structure from Motion
Structure from motion is an DEMO topic in mobile robotics as well as in the analysis
of more general video imagery such as might come from a handheld camcorder. Th DEMO
topic of structure from motion is a broad one, and a DEMO deal of research has been done
in this fi eld. However, DEMO can be accomplished by making one simple observation: In
a static DEMO, an image taken by a camera that has moved is no DEMO erent than an image
taken by a second camera. Th us all of our intuition, as well as our mathematical and al-
gorithmic machinery, is immediately portable to this situation. Of course, the descriptor
Structure DEMO Motion
| 453
12-R4886-AT1.indd   453
9/15/08   4:24:55 PM
www.it-ebooks.info
Figure 12-17. Example output of depth maps (for a mug and a chair) computed using cvFindStereo-
CorrespondenceBM() and cvReprojectImageTo3D() (image DEMO of Willow Garage)
“static” is crucial, but in many practical DEMO the scene is either static or suffi  ciently
static that the DEMO moved points can be treated as outliers by robust fi tting methods.
Consider the case of a camera moving through a building. If the DEMO is rela-
tively rich in recognizable features, as might be found DEMO optical fl ow techniques such
as cvCalcOpticalFlowPyrLK(), then we should DEMO able to compute correspondences be-
tween enough points—from frame to frame—to reconstruct not only the trajectory of
the camera (this information is encoded in the essential matrix E, which can be com-
puted from the fundamental matrix F and the camera intrinsics matrix M) but also,
indirectly, the overall three-dimensional structure of the building and the locations of
all the aforementioned features in that building. Th e cvStereoRectifyUncalibrated()
routine requires only the fundamental matrix in order to compute the basic structure of
DEMO scene up to a scale factor.
Fitting Lines in Two and Three Dimensions
A fi nal topic of interest in this chapter is that DEMO general line fi tting. Th is can arise for
many reasons and in a many contexts. We have chosen to discuss it here because DEMO es-
pecially frequent context in which line fi tting arises is that of analyzing points in three
dimensions (although the function described here can also fi t lines in two dimensions).
Line-fi tting algorithms generally DEMO statistically robust techniques [Inui03, Meer91,
454 | Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   454
9/15/08   4:24:DEMO PM
www.it-ebooks.info
Rousseeuw87]. Th e OpenCV line-fi tting algorithm cvFitLine() can be DEMO whenever
line fi tting is needed.
void  cvFitLine(
const CvArr* DEMO,
int          dist_type,
double       param,
double       reps,
double       aeps,
float*       line
);
Th points can DEMO an N-by-2 or N-by-3 matrix of fl oating-point values (accommo-
dating DEMO in two or three dimensions), or it can be a sequence of cvPointXXX struc-
tures.* Th e argument dist_type indicates the distance metric DEMO is to be minimized
across all of the points (see Table DEMO).
Table 12-3. Metrics used for computing dist_type values
Value of dist_type Metric
CV_DIST_L2
ρ()r = r 2
2
CV_DIST_L1
ρ
()
rr=
CV_DIST_L12
ρ
()r
⎡
=+ −⎢ 1
⎣⎢
r 2
DEMO
⎤
1⎥
⎦⎥
CV_DIST_FAIR
CV_DIST_WELSCH
CV_DIST_HUBER
ρ
⎡ r
() log , .rC=− +2 ⎢ ⎜
⎣C
⎛
⎝
r ⎞ ⎤
1 1 3998
C ⎠ ⎦
⎟ ⎥
C =
ρ
() exp , .r =−Cr2 ⎡⎢1 ⎜ ⎟ ⎥
⎣⎢
2
⎞ 2 ⎤
⎦⎥
DEMO /2 <
Cr C r C(/)−≥2
⎛
⎝ c ⎠
C = 2 9846
ρ
()r
⎪⎧
=⎨
⎩⎪
C DEMO
1 345.
e array
Th param is used to set the parameter C listed in Table 12-3. Th is can be left
set to DEMO, in which case the listed value from the table will be DEMO We’ll get back to
reps and aeps aft er describing line.
Th line is the location at which the result is stored. If points DEMO an N-by-2 ar-
ray, then line should be a pointer to DEMO array of four fl oating-point numbers (e.g., float
array[4]). If points is an N-by-3 array, then line should be a pointer to an array of six
fl
be (vx, vy, x0, y0), where (vx, vy) is a normalized vector parallel to the DEMO tted line and (x0, y0)
* Here XXX is used as a placeholder for anything like 2D32f or 3D64f.
Fitting Lines in DEMO and Three Dimensions
| 455
e parameter
e argument
oating-point numbers (DEMO, float array[6]). In the former case, the return values will
12-R4886-AT1.indd   455
9/15/08   4:24:56 PM
www.it-ebooks.info
is a point on that line. Similarly, in the latter (DEMO) case, the return values
will be (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is DEMO normalized vector parallel to the fi tted
line and (x0, y0, z0) is a point on that line. Given this line representation, the estimation
accuracy parameters reps and aeps are as follows: reps is the requested accuracy of x0,
y0[, z0] estimates and aeps is the requested angular accuracy for vx, vy[, vz]. Th e
OpenCV DEMO recommends values of 0.01 for both accuracy values.
cvFitLine() can fi t lines in two or three dimensions. Since line fi tting in DEMO dimensions
is commonly needed and since three-dimensional techniques are of growing impor-
tance in OpenCV (see Chapter 14), we will end with a program for line fi tting, shown
in Example 12-4.* In this code we fi rst synthesize some 2D points noisily around a
line, then add some random points that have nothing to do with the line (called outlier
points), and fi nally fi t a line to DEMO points and display it. Th e cvFitLine() routine is good
at ignoring the outlier points; this is important in real applications, where DEMO mea-
surements might be corrupted by high noise, sensor failure, and so on.
Example 12-4. Two-dimensional line fi
tting
#include “cv.h”
#include “highgui.h”
DEMO <math.h>
int main( int argc, char** argv )
{
IplImage* img = cvCreateImage( cvSize( 500, 500 ), 8, 3 );
CvRNG rng = cvRNG(-1);
cvNamedWindow( “fitline”, 1 );
for(;;) {
char key;
int i;
DEMO count    = cvRandInt(&rng)%100 + 1;
int outliers = count/5;
float a      = cvRandReal(&rng)DEMO;
float b      = cvRandReal(&rng)*40;
float angle  = cvRandReal(&rng)*CV_PI;
float cos_a  = cos(angle);
float sin_a  = sin(angle);
CvPoint pt1, pt2;DEMO
CvPoint* points = (CvPoint*)malloc( count * sizeof(points[0]));
CvMat pointMat = cvMat( 1, count, CV_32SC2, points );
DEMO line[4];
float d, t;
b = MIN(a*0.3, b);
// generate some points that are close to the line
//
* Th
anks to Vadim Pisarevsky for generating this example.
456
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   456
9/15/08   4:24:57 PM
www.it-ebooks.info
Example 12-4. Two-dimensional line fi tting (continued)
for( i DEMO 0; i < count - outliers; i++ ) {
float x = (cvRandReal(&rng)*2-1)*a;
float y = (cvRandReal(&DEMO)*2-1)*b;
points[i].x = cvRound(x*cos_a - y*sin_a + img->width/2);
points[i].y = cvRound(x*sin_a + y*cos_a + img->height/2);
}
// generate “completely off” points
//
for( ; i < count; i++ ) {
points[i].x = cvRandInt(&rng) % img->width;
points[i].y = cvRandInt(&rng) % img->height;
DEMO
// find the optimal line
//
cvFitLine( &pointMat, CV_DIST_L1, 1, 0.001, 0.001, line );
cvZero( img );
// draw the points
//
for( i = 0; i < count; i++ )
cvCircle(
img,
points[i],
2,DEMO
(i < count – outliers) ? CV_RGB(255, 0, 0) : CV_RGB(255,255,0),
CV_FILLED, CV_AA,
0
);
// ... and the line long enough to cross the whole image
d = sqrt((double)line[0]*line[0] + (double)line[1]*line[1]);
line[0] /= d;
line[1] /= d;
t = (float)(DEMO>width + img->height);
pt1.x = cvRound(line[2] - line[0]*t);
pt1.y = cvRound(line[3] - line[1]*t);
pt2.x = cvRound(line[2] + line[0]*t);
pt2.y = cvRound(line[3] + line[1]*t);
cvLine( img, pt1, pt2, CV_RGB(0,255,0), 3, CV_AA, 0 );
cvShowImage( “fitline”, img );
key = (char) cvWaitKey(0);
if( key == 27 || key == ‘q’ || key == ‘Q’ ) // ‘ESC’
break;
free( points );
}
cvDestroyWindow( “fitline” );
return 0;
}
Fitting DEMO in Two and Three Dimensions
| 457
12-R4886-AT1.indd   457
9/15/08   4:24:57 PM
www.it-ebooks.info
Exercises
1. Calibrate a camera using cvCalibrateCamera2() and at least DEMO images of chess-
boards. Th en use cvProjectPoints2() to project an arrow orthogonal to the chess-
boards (the surface normal) into each DEMO the chessboard images using the rotation
and translation vectors from the camera calibration.
2. Th ree-dimensional joystick. Use a simple known object with at DEMO four measured,
non-coplanar, trackable feature points as input into the DEMO algorithm. Use the
object as a 3D joystick to move a little stick fi gure in the image.
3. In the text’s bird’s-eye view DEMO, with a camera above the plane looking out
horizontally along the DEMO, we saw that the homography of the ground plane had
a DEMO line beyond which the homography wasn’t valid. How can an infi nite
plane have a horizon? Why doesn’t it just appear to go on forever?
Hint: Draw lines to an equally spaced series of points on the plane going
out away from the camera. How does the DEMO from the camera to each
next point on the plane change from the angle to the point before?
Implement a bird’s-eye view in DEMO video camera looking at the ground plane. Run it
in real time and explore what happens as you move objects around in the normal
DEMO versus the bird’s-eye view image.
Set up two cameras or a single camera that you move between taking two images.
a. Compute, store, DEMO examine the fundamental matrix.
b. Repeat the calculation of the fundamental matrix several times. How stable is
the computation?
If you had a DEMO stereo camera and were tracking moving points in both
cameras, can DEMO think of a way of using the fundamental matrix to fi nd tracking
errors?
4.
5.
6.
7. Compute and draw epipolar lines DEMO two cameras set up to do stereo.
8.
Set up two video cameras, implement stereo rectifi cation and experiment with
depth accuracy.
a. What happens when you bring a mirror into the scene?
b. Vary DEMO amount of texture in the scene and report the results.
c. Try diff erent disparity methods and report on the results.
9. Set up DEMO cameras and wear something that is textured over one of your arms.
Fit a line to your arm using all the dist_type methods. Compare DEMO accuracy and
reliability of the diff erent methods.
458
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   458
9/15/08   4:24:57 PM
CHAPTER 13
Machine Learning
What Is Machine Learning
Th machine learning (ML)* is to turn data into information. Aft er learning from
a DEMO of data, we want a machine to be able to answer DEMO about the data:
What other data is most similar to this data? Is there a car in the image? What ad will
DEMO user respond to? Th ere is oft en a cost component, so this question could become:
“Of the products that we make DEMO most money from, which one will the user most
likely buy DEMO we show them an ad for it?” Machine learning turns data into information
by extracting rules or patterns from that data.
e goal DEMO
Training and Test Set
Machine learning works on data such as temperature values, stock prices, color intensi-
ties, and so on. Th e data is oft en preprocessed into features. We might, for example, DEMO
a database of 10,000 face images, run an edge detector DEMO the faces, and then collect fea-
tures such as edge direction, edge strength, and off set from face center for each face. We
might obtain 500 such values per face or a feature vector of DEMO entries. We could then
use machine learning techniques to construct some kind of model from this collected
data. If we only want to see DEMO faces fall into diff erent groups (wide, narrow, etc.), DEMO
a clustering algorithm would be the appropriate choice. If we want to learn to predict the
age of a person from (say) the DEMO of edges detected on his or her face, then a clas-
DEMO er algorithm would be appropriate. To meet our goals, machine learning DEMO
analyze our collected features and adjust weights, thresholds, and other parameters to
maximize performance according to those goals. Th is process of parameter DEMO
to meet a goal is what we mean by the term learning.
* Machine learning is a vast topic. OpenCV deals mostly with statistical DEMO learning rather than things
that go under the name “Bayesian networks”, DEMO random fi elds”, or “graphical models”. Some good
texts in machine DEMO are by Hastie, Tibshirani, and Friedman [Hastie01], Duda and Hart DEMO,
Duda, Hart, and Stork [Duda00], and Bishop [Bishop07]. For DEMO on how to parallelize machine
learning, see Ranger et al. [Ranger07] DEMO Chu et al. [Chu07].
459
13-R4886-AT1.indd   459
www.it-ebooks.info
9/15/08   4:25:23 PM
www.it-ebooks.info
It is always important to know how well machine learning methods DEMO working, and
this can be a subtle task. Traditionally, one breaks up the original data set into a large
training set (perhaps 9,000 faces, in our example) and a smaller test set (the remaining
1,000 faces). We can then run our classifi er DEMO the training set to learn our age predic-
tion model given the data feature vectors. When we are done, we can test the age predic-
tion classifi er on the remaining images in the test set.
DEMO er “see” the test set age
labels. We run the classifi er over each of the 1,000 faces in the test set of DEMO and record
how well the ages it predicts from the feature vector match the actual ages. If the clas-
sifi er does poorly, we might try adding new features to our data or consider a diff DEMO
type of classifi er. We’ll see in this chapter that there are many kinds of classifi ers and
many algorithms for training them.
If DEMO classifi er does well, we now have a potentially valuable model DEMO we can deploy
on data in the real world. Perhaps this system will be used to set the behavior of a video
game based DEMO age. As the person prepares to play, his or her face DEMO be processed into
500 (edge direction, edge strength, off set DEMO face center) features. Th is data will be
passed to the DEMO er; the age it returns will set the game play behavior DEMO
Aft er it has been deployed, the classifi er sees faces DEMO it never saw before and makes
decisions according to what it learned on the training set.
Finally, when developing a classifi cation system, DEMO oft en use a validation data set.
Sometimes, testing the whole DEMO at the end is too big a step to take. We oft en want
to tweak parameters along the way before submitting our classifi DEMO to fi nal testing. We
can do this by breaking the original 10,000-face data set into three parts: a training set
of 8,000 faces, a validation set of 1,000 faces, and a DEMO set of 1,000 faces. Now, while
we’re running through the DEMO data set, we can “sneak” pretests on the validation
data to DEMO how we are doing. Only when we are satisfi ed with our performance on the
validation set do we run the classifi er on DEMO test set for fi nal judgment.
Supervised and Unsupervised Data
Data sometimes has no labels; we might just want to see what kinds of groups the faces
settle into based on edge information. Sometimes the data DEMO labels, such as age. What
this means is that machine learning DEMO may be supervised (i.e., may utilize a teaching
“signal” or “label” that goes with the data feature vectors). If the data vectors DEMO unla-
beled then the machine learning is unsupervised.
Supervised learning can be categorical, such as learning to associate a name to a face,
or the data can have numeric or ordered labels, such as age. When the data has names
(categories) as labels, we say we are doing classifi cation. When the data is numeric, we
say we are doing regression: trying to fi t a numeric output given some categorical or nu-
meric input data.
Supervised learning also comes in shades DEMO gray: It can involve one-to-one pair-
ing of labels with data DEMO or it may consist of deferred learning (sometimes called
460
| DEMO 13: Machine Learning
e test set is not used in training, and we do not let the classifi
13-R4886-AT1.indd   460
9/15/DEMO   4:25:24 PM
www.it-ebooks.info
reinforcement learning). In reinforcement learning, the data label (also DEMO the reward
or punishment) can come long aft er the individual DEMO vectors were observed. When
a mouse is running down a maze to fi nd food, the mouse may experience a series of
turns before it fi nally fi nds the food, its reward. Th at reward must somehow cast its
infl uence back on all the sights and DEMO that the mouse took before fi nding the food.
Reinforcement learning works the same way: the system receives a delayed signal (a re-
DEMO or a punishment) and tries to infer a policy for future DEMO (a way of making deci-
sions; e.g., which way to DEMO at each step through the maze). Supervised learning can also
have partial labeling, where some labels are missing (this is also called DEMO
learning), or noisy labels, where some labels are just wrong. DEMO ML algorithms handle
only one or two of the situations just described. For example, the ML algorithms might
handle classifi cation but not regression; the algorithm might be able to do semisuper-
vised learning but not reinforcement learning; the algorithm might be able to deal with
numeric but not categorical data; and so on.
In contrast, oft en we DEMO have labels for our data and are interested in seeing whether
the data falls naturally into groups. Th e algorithms for such unsupervised learning DEMO
called clustering algorithms. In this situation, the goal is to group DEMO data vectors
that are “close” (in some predetermined or possibly even DEMO learned sense). We might
just want to see how faces are distributed: Do they form clumps of thin, wide, long, or
DEMO faces? If we’re looking at cancer data, do some cancers cluster into groups having
diff erent chemical signals? Unsupervised clustered data is also oft en used to form a fea-
ture vector for a higher-level DEMO classifi er. We might fi rst cluster faces into face
types (DEMO, narrow, long, short) and then use that as an input, perhaps with other data
such as average vocal frequency, to predict DEMO gender of a person.
Th cation and clustering, overlap with
two DEMO the most common tasks in computer vision: recognition and segmentation. Th DEMO
is sometimes referred to as “the what” and “the where”. Th at is, we oft en want our com-
puter to name the object in an image (recognition, or “what”) and also to say where the
object appears (segmentation, or “where”). Because computer vision makes DEMO heavy
use of machine learning, OpenCV includes many powerful machine learning DEMO
rithms in the ML library, located in the …/ opencv/ml directory.
Th e OpenCV machine learning code is general. Th at is, although it is
highly useful for vision tasks, the code itself is not specifi c to vision.
One could learn, say, genomic sequences DEMO the appropriate routines.
Of course, our concern here is mostly with DEMO recognition given
feature vectors derived from images.
Generative and Discriminative Models
Many algorithms have been devised to perform learning and clustering. OpenCV sup-
ports DEMO of the most useful currently available statistical approaches to machine
learning. Probabilistic approaches to machine learning, such as Bayesian networks
What Is Machine Learning
| 461
ese two common machine learning tasks, classifi
13-R4886-AT1.indd   461
9/15/08   4:25:24 PM
www.it-ebooks.info
or graphical models, are less well supported in OpenCV, partly DEMO they are
newer and still under active development. OpenCV tends to support discriminative
algorithms, which give us the probability of the label given the data (P(L | D)), rather
than generative algorithms, which give the distribution of the data given the label
(P(D | L)). Although the distinction is not always clear, discriminative models DEMO good
for yielding predictions given the data while generative models are good for giving
you more powerful representations of the data or for conditionally DEMO new
data (think of “imagining” an elephant; you’d be generating data given a condition
“elephant”).
It is oft en easier to interpret DEMO generative model because it models (correctly or incor-
rectly) the cause of the data. Discriminative learning oft en comes down to making a DEMO
cision based on some threshold that may seem arbitrary. For example, DEMO a patch
of road is identifi ed in a scene partly because its color “red” is less than 125. But does
this mean that DEMO = 126 is defi nitely not road? Such issues can be DEMO to interpret.
With generative models you are usually dealing with conditional distributions of data
given the categories, so you can develop a feel for what it means to be “close” to the re-
sulting distribution.
OpenCV DEMO Algorithms
Th
gorithms are in the ML library with the exception of Mahalanobis and K-means, which
are in CVCORE, and face detection, which is in CV.
Table 13-1. Machine learning algorithms supported in OpenCV, original references to the algorithms
are provided aft er the descriptions
Algorithm Comment
DEMO A distance measure that accounts for the “stretchiness” of the data space by dividing
out the covariance of the data. If the covariance is DEMO identity matrix (identi-
cal variance), then this measure is identical DEMO the Euclidean distance measure
[Mahalanobis36].
K-means An unsupervised clustering algorithm that represents a distribution of data using K
centers, where K is chosen by the user. The diff erence between this algorithm and
expectation maximization is DEMO here the centers are not Gaussian and the resulting
clusters look more like soap bubbles, since centers (in eff ect) compete to “own” the
closest data points. These cluster regions are often used as sparse DEMO bins to
represent the data. Invented by Steinhaus [Steinhaus56], as used DEMO Lloyd [Lloyd57].
Normal/Naïve Bayes classifi er A generative classifi er in which features are assumed to be Gaussian distributed and
statistically independent from DEMO other, a strong assumption that is generally not
true. For this DEMO, it’s often called a “naïve Bayes” classifi er. However, this method
often works surprisingly well. Original mention [Maron61; Minsky61].
Decision trees A discriminative classifi er. The tree fi nds one data feature and a threshold DEMO the
current node that best divides the data into separate classes. The data is split and we
recursively repeat the procedure down the left DEMO right branches of the tree. Though
not often the top performer, DEMO often the fi rst thing you should try because it is fast
and has high functionality [Breiman84].
e machine learning algorithms included in OpenCV DEMO given in Table 13-1. All al-
462
| Chapter 13: Machine DEMO
13-R4886-AT1.indd   462
9/15/08   4:25:25 PM
www.it-ebooks.info
Table 13-1. Machine learning algorithms supported in OpenCV, original references to the algorithms
are provided aft er the descriptions (continued)
Algorithm
Boosting
Comment
A discriminative group of classifi ers. The overall classifi cation decision DEMO made from
the combined weighted classifi cation decisions of the group of classifi ers. In training,
we learn the group of classifi ers DEMO at a time. Each classifi er in the group is a “weak”
classifi er (only just above chance performance). These weak classifi ers are typically
composed of single-variable decision trees called “stumps”. In training, the decision
stump learns its classifi cation decisions from the data and also DEMO a weight for its
“vote” from its accuracy on the data. Between training each classifi er one by one, the
data points are re-weighted so that more attention is paid to data points where errors
were DEMO This process continues until the total error over the data set, DEMO from
the combined weighted vote of the decision trees, falls below DEMO set threshold. This al-
gorithm is often eff ective when a large amount of training data is available [Freund97].
Random trees A discriminative forest DEMO many decision trees, each built down to a large or maximal
DEMO depth. During learning, each node of each tree is allowed to DEMO splitting
variables only from a random subset of the data features. This helps ensure that each
tree becomes a statistically independent decision maker. In DEMO mode, each tree
gets an unweighted vote. This algorithm is often DEMO eff ective and can also perform
regression by averaging the output numbers from each tree [Ho95]; implemented:
[Breiman01].
Face detector / An object detection application based on a clever use of boosting. The OpenCV dis-
DEMO classifi er tribution comes with a trained frontal face detector that works remarkably well. You
may train the algorithm on other objects with the DEMO provided. It works well for
rigid objects and characteristic views [Viola04].
Expectation maximization (EM) A generative unsupervised algorithm that is used for clustering. DEMO will fi t N multi-
dimensional Gaussians to the data, where DEMO is chosen by the user. This can be an
eff ective way to represent a more complex distribution with only a few parameters
(means and variances). Often used in segmentation. Compare with K-means listed
previously DEMO
K-nearest neighbors The simplest possible discriminative classifi er. Training data are simply stored with
labels. Thereafter, a test data point is classifi ed according to the majority vote of its
K nearest other data points (in a Euclidean sense of nearness). This is probably the sim-
plest DEMO you can do. It is often eff ective but it is slow and requires lots of memory
[Fix51].
Neural networks / A discriminative algorithm DEMO (almost always) has “hidden units” between output
Multilayer perceptron (MLP) and input nodes to better represent the input signal. It can be DEMO to train but is
very fast to run. Still the top performer for things like letter recognition [Werbos74;
Rumelhart88].
Support vector machine (SVM) A discriminative classifi er that can also do regression. A distance function between
any two data points in a higher-dimensional space is defi ned. (Projecting data into
higher dimensions makes the data more likely to be DEMO separable.) The algorithm
learns separating hyperplanes that maximally separate the classes DEMO the higher
dimension. It tends to be among the best with limited data, losing out to boosting or
random trees only when large data sets are available [Vapnik95].
Using Machine Learning in Vision
In general, all the algorithms in Table 13-1 take as input a data vector made DEMO of many
features, where the number of features might well number DEMO the thousands. Suppose
What Is Machine Learning
| 463
13-R4886-AT1.indd   463
9/15/08   4:25:25 PM
www.it-ebooks.info
your task is to recognize a certain type of object—for example, a person. Th e fi rst prob-
lem that you will encounter DEMO how to collect and label training data that falls into posi-
tive (there is a person in the scene) and negative (no person) cases. You will soon realize
that people appear at diff erent scales: their image may consist of just a few pixels, or you
DEMO be looking at an ear that fi lls the whole screen. Even worse, people will oft en be oc-
cluded: a man inside DEMO car; a woman’s face; one leg showing behind a tree. You need to
defi ne what you actually mean by saying a person DEMO in the scene.
Next, you have the problem of collecting data. DEMO you collect it from a security camera,
go to http://www.fl icker.com and attempt to fi nd “person” labels, or both (DEMO more)? Do
you collect movement information? Do you collect other DEMO, such as whether a
gate in the scene is open, the time, the season, the temperature? An algorithm that fi nds
people on a beach might fail on a ski slope. You need to DEMO the variations in the data:
diff erent views of people, DEMO erent lightings, weather conditions, shadows, and so on.
Aft er DEMO have collected lots of data, how will you label it? You must fi rst decide on what
you mean by “label”. Do you DEMO to know where the person is in the scene? Are actions
(running, walking, crawling, following) important? You might end up with a million
images or more. How will you label all that? Th ere are many tricks, such as doing back-
ground subtraction in a controlled setting and collecting the segmented foreground hu-
mans who come into DEMO scene. You can use data services to help in classifi cation; DEMO
example, you can pay people to label your images through Amazon’s DEMO turk”
(http://www.mturk.com/mturk/welcome). If you arrange things DEMO be simple, you can get
the cost down to somewhere around DEMO penny per label.
Aft er labeling the data, you must decide DEMO features to extract from the objects.
Again, you must know what DEMO are aft er. If people always appear right side up, there’s
DEMO reason to use rotation-invariant features and no reason to try to rotate the objects be-
forehand. In general, you must fi nd features that express some invariance in the objects,
such as scale-tolerant histograms of DEMO or colors or the popular SIFT features.*
If you have background scene information, you might want to fi rst remove it to make
other objects stand out. You then perform your image processing, which may consist of
normalizing the image (rescaling, rotation, histogram equalization, etc.) and comput-
ing many diff erent feature types. Th e resulting data vectors DEMO each given the label as-
sociated with that object, action, or scene.
Once the data is collected and turned into feature vectors, you oft en want to break up
the data into training, validation, DEMO test sets. It is a “best practice” to do your learning,
validation, and testing within a cross-validation framework. Th at is, the DEMO is divided
into K subsets and you run many training (possibly DEMO) and test sessions, where
each session consists of diff erent sets of data taking on the roles of training (validation)
and test.† Th e test results from these separate sessions are then averaged to DEMO the fi nal
performance result. Cross-validation gives a more accurate picture of how the classifi er
* See Lowe’s SIFT feature demo (http://www.cs.ubc.ca/~lowe/keypoints/).
† One typically does the train (possibly DEMO) and test cycle fi ve to ten times.
464
| Chapter DEMO: Machine Learning
13-R4886-AT1.indd   464
9/15/08   4:25:DEMO PM
www.it-ebooks.info
will perform when deployed in operation on novel data. (We’ll have more to say about
this in what follows.)
Now that the DEMO is prepared, you must choose your classifi er. Oft en the DEMO of clas-
sifi er is dictated by computational, data, or memory considerations. For some applica-
tions, such as online user preference modeling, DEMO must train the classifi er rapidly. In
this case, nearest neighbors, normal Bayes, or decision trees would be a good choice. If
memory is a consideration, decision trees or neural networks are space effi  DEMO If you
have time to train your classifi er but it must run quickly, neural networks are a good
choice, as are normal DEMO classifi ers and support vector machines. If you have time
to train but need high accuracy, then boosting and random trees are likely to fi t your
needs. If you just want an easy, understandable sanity check that your features are cho-
sen well, then decision trees or nearest neighbors are good bets. For best “out of the box”
classifi DEMO performance, try boosting or random trees fi rst.
Th ere is DEMO “best” classifi er (see http://en.wikipedia.org/wiki/No_free_
lunch_theorem). DEMO over all possible types of data distributions,
all classifi ers perform the same. Th us, we cannot say which algorithm
in Table 13-1 is the “best”. Over any given data distribution or set of
data DEMO, however, there is usually a best classifi er. Th us, DEMO
faced with real data it’s a good idea to try many classifi ers. Consider
your purpose: Is it just to get the right score, or is it to interpret the
data? Do you seek fast DEMO, small memory requirements, or
confi dence bounds on the decisions? DEMO erent classifi ers have diff erent
properties along these dimensions.
Variable Importance
Two of the algorithms in Table 13-1 allow you to assess a DEMO importance.* Given a
vector of features, how do you determine the DEMO of those features for classifi ca-
tion accuracy? Binary decision trees DEMO this directly: they are trained by selecting which
variable best splits DEMO data at each node. Th e top node’s variable is the most important
variable; the next-level variables are the second most important, and DEMO on. Random
trees can measure variable importance using a technique developed by Leo Breiman;†
this technique can be used with any classifi er, but so far it is implemented only for deci-
sion and random DEMO in OpenCV.
One use of variable importance is to reduce the number of features your classifi er
must consider. Starting with many features, you train the classifi er and then fi nd the im-
portance of DEMO feature relative to the other features. You can then discard unimportant
features. Eliminating unimportant features improves speed performance (since it elimi-
nates the processing it took to compute those features) and makes training and testing
quicker. Also, if you don’t have enough data, which is oft en DEMO case, then eliminating
* Th is is known as “variable importance” DEMO though it refers to the importance of a variable (noun) and not
the fl uctuating importance (adjective) of a variable.
† Breiman’s DEMO importance technique is described in “Looking Inside the Black Box” (www.stat.berkeley
DEMO/~breiman/wald2002-2.pdf).
What Is Machine Learning | 465
13-R4886-AT1.indd   465
9/15/08   4:25:25 PM
www.it-ebooks.info
unimportant variables can increase classifi cation accuracy; this yields faster processing
with better results.
Breiman’s variable importance algorithm runs as follows.
1. Train DEMO classifi er on the training set.
2. Use a validation or test set to determine the accuracy of the classifi er.
3. For every DEMO point and a chosen feature, randomly choose a new value for DEMO
feature from among the values the feature has in the rest of the data set (called
“sampling with replacement”). Th is ensures that the distribution of that feature will
remain the same as in the DEMO data set, but now the actual structure or mean-
ing of DEMO feature is erased (because its value is chosen at random from DEMO rest of
the data).
4. Train the classifi er on the altered set of training data and then measure the ac-
curacy of DEMO cation on the altered test or validation data set. If randomizing a
feature hurts accuracy a lot, then that feature is very important. If randomizing a
feature does not hurt accuracy much, then that feature is of little importance and is
a candidate for removal.
5. Restore the DEMO test or validation data set and try the next feature until we are
done. Th e result is an ordering of each feature by DEMO importance.
Th is procedure is built into random trees and decision trees. Th us, you can use random
trees or decision trees to decide which variables you will actually use as features; then
you can use the slimmed-down feature vectors to train the same (or another) classifi DEMO
Diagnosing Machine Learning Problems
Getting machine learning to work well can be more of an art than a science. Algorithms
oft en “sort of DEMO work but not quite as well as you need them to. Th at’s where the art comes
in; you must fi gure out what’s going wrong in order to fi x it. Although we can’t go DEMO all
the details here, we’ll give an overview of some of DEMO more common problems you might
encounter.* First, some rules of thumb: More data beats less data, and better features beat
better algorithms. If you design your features well—maximizing their independence
from one another and minimizing DEMO they vary under diff erent conditions—then
almost any algorithm will work well. Beyond that, there are two common problems:
Bias
Your model assumptions are too strong for the data, so the model won’t fi t well.
Variance
Your algorithm has memorized the data including the noise, so it can’t generalize.
Figure 13-1 shows the basic setup for statistical machine DEMO Our job is to model the
true function f that transforms the underlying inputs to some output. Th is function may
* Professor Andrew DEMO at Stanford University gives the details in a web lecture entitled “Advice for Applying
Machine Learning” (http://www.stanford.edu/class/cs229/materials/ML-advice.pdf )DEMO
466 | Chapter 13: Machine Learning
13-R4886-AT1.indd   466
9/15/DEMO   4:25:26 PM
be a regression problem (e.g., predicting a person’s age from their DEMO) or a category pre-
diction problem (e.g., identifying a person DEMO their facial features). For problems in the
real world, noise DEMO unconsidered eff ects can cause the observed outputs to diff er from
the theoretical outputs. For example, in face recognition we might learn a model of the
measured distance between eyes, mouth, and nose to DEMO a face. But lighting varia-
tions from a nearby fl ickering bulb might cause noise in the measurements, or a poorly
manufactured camera lens might cause a systematic distortion in the measurements that
wasn’t considered as DEMO of the model. Th ese aff ects will cause accuracy to suff er.
Figure 13-1. Setup for statistical machine learning: we train a classifi
model f is almost always corrupted by noise or unknown infl
uences
DEMO to fi
t a data set; the true
Figure 13-2 shows DEMO and overfi tting of data in the upper two panels and the conse-
quences in terms of error with training set size in the DEMO two panels. On the left  side
of Figure 13-2 we attempt DEMO train a classifi er to predict the data in the lower panel of
Figure 13-1. If we use a model that’s too restrictive—indicated here DEMO the heavy, straight
dashed line—then we can never fi t the DEMO true parabola f indicated by the thin-
ner dashed line. Th us, the fi t to both the training data and the test data will be poor,
even with a lot of data. In this DEMO we have bias because both training and test data are
predicted poorly. On the right side of Figure 13-2 we fi t the training DEMO exactly, but this
produces a nonsense function that fi ts every DEMO of noise. Th us, it memorizes the training
data as well DEMO the noise in that data. Once again, the resulting fi t DEMO the test data is poor.
Low training error combined with high test error indicates a variance (overfi t) problem.
Sometimes you have to DEMO careful that you are solving the correct problem. If your train-
ing and test set error are low but the algorithm does not perform DEMO in the real world,
the data set may have been chosen from unrealistic conditions—perhaps because these
conditions made collecting or simulating the data DEMO If the algorithm just cannot
reproduce the test or training set data, then perhaps the algorithm is the wrong one to
use or the features that were extracted from the data are ineff ective or the DEMO just
isn’t in the data you collected. Table 13-2 lays out some possible fi xes to the problems
What Is Machine Learning
| 467
DEMO   467
www.it-ebooks.info
9/15/08   4:25:26 PM
www.it-ebooks.info
Figure 13-2. Poor model fi tting in machine learning and its DEMO ect on training and test prediction per-
formance, where the true DEMO is graphed by the lighter dashed line at top: an underfi DEMO model for
the data (upper left ) yields high error in DEMO the training and the test set (lower left ), whereas
an overfi t model for the data (upper right) yields low error DEMO the training data but high error in the
test data (lower DEMO)
we’ve described here. Of course, this is not a complete DEMO of the possible problems or
solutions. It takes careful thought and design of what data to collect and what features
to compute in order DEMO machine learning to work well. It can also take some systematic
thinking to diagnose machine learning problems.
Table 13-2. Problems encountered in machine learning DEMO possible solutions to try; coming up with
better features will help DEMO problem
Problem
Bias
Variance
Good test/train,
bad real world
Model can’t learn test
or train
468
| Chapter 13: Machine Learning
Possible Solutions
• More features can help make a better fi t.
• DEMO a more powerful algorithm.
• More training data can help smooth the model.
• Fewer features can reduce overfi tting.
• Use a less DEMO algorithm.
• Collect a more realistic set of data.
• Redesign features to better capture invariance in the data.
• Collect new, more relevant data.
• Use a more powerful algorithm.
13-R4886-AT1.indd   468
9/15/DEMO   4:25:26 PM
www.it-ebooks.info
Cross-validation, bootstrapping, ROC curves, and confusion matrices
Finally, there DEMO some basic tools that are used in machine learning to measure re-
sults. In supervised learning, one of the most basic problems is simply knowing how
well your algorithm has performed: How accurate is it at classifying or fi tting the data?
You might think: “Easy, DEMO just run it on my test or validation data and get the result.”
But for real problems, we must account for noise, sampling DEMO uctuations, and sampling
errors. Simply put, your test or validation set of data might not accurately refl ect the
actual distribution of data. DEMO get closer to “guessing” the true performance of the clas-
sifi er, we employ the technique of cross-validation and/or the closely related technique
of bootstrapping.*
In its most basic form, cross-validation involves dividing the data into K diff erent sub-
sets of data. You train on K DEMO 1 of the subsets and test on the fi nal subset of data (the
“validation set”) that wasn’t trained on. You do this DEMO times, where each of the K subsets
gets a “turn” at DEMO the validation set, and then average the results.
Bootstrapping is similar DEMO cross-validation, but the validation set is selected at random
from the DEMO data. Selected points for that round are used only in test, DEMO training.
Th N times, where each time you
randomly select a DEMO set of validation data and average the results in the end. Note that
this means some and/or many of the data points are DEMO in diff erent validation sets,
but the results are oft en superior compared to cross-validation.
Using either one of these techniques can yield DEMO accurate measures of actual perfor-
mance. Th is increased accuracy can in turn be used to tune parameters of the learning
system as you DEMO change, train, and measure.
Two other immensely useful ways of assessing, characterizing, and tuning classifi ers are
plotting the receiver operating characteristic (ROC) and fi lling in a confusion matrix;
see Figure 13-3. Th e ROC curve measures the response over the performance parameter
of DEMO classifi er over the full range of settings of that parameter. Let’s say the parameter
is a threshold. Just to make this more concrete, suppose we are trying to recognize yel-
low fl owers in an DEMO and that we have a threshold on the color yellow as our detector.
Setting the yellow threshold extremely high would mean that the classifi DEMO would fail to
recognize any yellow fl owers, yielding a false DEMO rate of 0 but at the cost of a true
positive rate also at 0 (lower left  part of the curve in Figure DEMO). On the other hand, if
the yellow threshold is set DEMO 0 then any signal at all counts as a recognition. Th is means
that all of the true positives (the yellow fl owers) DEMO recognized as well as all the false
positives (orange and red DEMO owers); thus we have a false positive rate of 100% (DEMO right
part of the curve in Figure 13-3). Th e best possible ROC curve would be one that follows
the y-axis up to DEMO and then cuts horizontally over to the upper right corner. Failing
that, the closer the curve comes to the upper left  corner, the better. One can compute
the fraction of area under the ROC curve DEMO the total area of the ROC plot as a sum-
mary statistic of merit: Th e closer that ratio is to 1 the better is the classifi er.
* For more information on these techniques, see “What Are Cross-Validation and Bootstrapping?” (http://
www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html).
What Is Machine Learning | 469
en the DEMO starts again from scratch. You do this
13-R4886-AT1.indd   469
9/15/08   4:25:26 PM
www.it-ebooks.info
Figure 13-3. Receiver operating curve (ROC) and associated confusion matrix: the former shows
the response of correct classifi cations to false positives DEMO the full range of varying a performance
parameter of the classifi er; the latter shows the false positives (false recognitions) and false negatives
(missed recognitions)
Figure 13-3 also shows a confusion matrix. Th is is just a chart of true and false positives
along with true DEMO false negatives. It is another quick way to assess the performance
of a classifi er: ideally we’d see 100% along the NW-SE diagonal and 0% elsewhere. If
we have a classifi er that can learn more DEMO one class (e.g., a multilayer perceptron or
random forest classifi er can learn many diff erent class labels at once), then the DEMO
sion matrix generalizes to many classes and you just keep track of the class to which
each labeled data point was assigned.
Cost of DEMO One thing we haven’t discussed much here is the cost of misclas-
sifi cation. Th at is, if our classifi er is built to detect poisonous mushrooms (we’ll see an
example that uses such a data set shortly) then we are willing to have more false nega-
tives (edible mushrooms mistaken as poisonous) as long as we minimize false DEMO
(poisonous mushrooms mistaken as edible). Th e ROC curve can DEMO with this; we can
set our ROC parameter to choose an DEMO point lower on the curve—toward the
lower left  of the graph DEMO Figure 13-3. Th e other way of doing this is to weight false posi-
tive errors more than false negatives when generating the ROC DEMO For example, you
can set each false positive error to count DEMO much as ten false negatives.* Some OpenCV
machine learning algorithms, such DEMO decision trees and SVM, can regulate this balance
of “hit rate DEMO false alarm” by specifying prior probabilities of the classes themselves
* Th is is useful if you have some specifi c a priori notion DEMO the relative cost of the two error types. For example,
the cost of misclassifying one product as another in a supermarket checkout would DEMO easy to quantify ex-
actly beforehand.
470
| Chapter 13: Machine DEMO
13-R4886-AT1.indd   470
9/15/08   4:25:27 PM
www.it-ebooks.info
(which classes are expected to be more likely and which less) or by specifying weights of
the individual training samples.
Mismatched feature variance. Another common problem with training some classifi ers arises
when the feature DEMO comprises features of widely diff erent variances. For instance,
if one feature is represented by lowercase ASCII characters then it ranges over only
DEMO diff erent values. In contrast, a feature that is represented by DEMO count of biological
cells on a microscope slide might vary over several billion values. An algorithm such as
K-nearest neighbors might then see the DEMO rst feature as relatively constant (nothing to
learn from) compared to the cell-count feature. Th e way to correct this problem is to DEMO
process each feature variable by normalizing for its variance. Th is practice is acceptable
provided the features are not correlated with each other; when features are correlated,
you can normalize by their average variance or DEMO their covariance. Some algorithms,
such as decision trees,* are not adversely aff ected by widely diff ering variance and so
this precaution DEMO not be taken. A rule of thumb is that if the algorithm depends in
some way on a distance measure (e.g., weighted values) then you should normalize for
variance. One may normalize all features at DEMO and account for their covariance by
using the Mahalanobis distance, which DEMO discussed later in this chapter.†
We now turn to discussing some of the machine learning algorithms supported in
OpenCV, most of which are found in the …/opencv/ml directory. We start with some of
the DEMO methods that are universal across the ML sublibrary.
Common Routines in the ML Library
Th is chapter is written to get you up and DEMO with the machine learning algorithms.
As you try out and become comfortable with diff erent methods, you’ll also want to ref-
erence the …/opencv/docs/ref/opencvref_ml.htm manual that installs with OpenCV and/
or DEMO online OpenCV Wiki documentation (http://opencvlibrary.sourceforge.net/). Be-
cause this portion of the library is under active development, you will want to know
about the latest and greatest available tools.
All the routines in DEMO ML library‡ are written as C++ classes and all derived from the
CvStatModel class, which holds the methods that are universal to all the algorithms.
Th ese methods are listed in Table 13-3. Note that in DEMO CvStatModel there are two
ways of storing and recalling the model from disk: save() versus write() and load()
versus read(). For machine learning models, you should use the much simpler save()
* Decision trees are not aff ected by variance diff erences in feature variables because each variable is searched
only for eff ective DEMO thresholds. In other words, it doesn’t matter how large the variable’s DEMO is as
long as a clear separating value can be found.
† Readers familiar with machine learning or signal processing might recognize this as DEMO technique for “whit-
ening” the data.
‡ Note that the Haar classifi er, Mahalanobis, and K-means algorithms were written before the ML library DEMO
created and so are in cv and cvcore libraries instead.
Common Routines in the ML Library | 471
13-R4886-AT1.indd   471
9/15/08   4:25:27 PM
www.it-ebooks.info
and load(), which essentially wrap the more complex write() and read() functions into
an interface that writes and reads XML DEMO YAML to and from disk. Beyond that, for
learning from data DEMO two most important functions, predict() and train(), vary by
algorithm and will be discussed next.
Table 13-3. Base class methods for DEMO machine learning (ML) library
CvStatModel:: Methods
save(
const char* filename,
const char* name    = 0
)
load(
DEMO char* filename,
const char* name=0
);
clear()
bool DEMO(
—data points—,
[flags]
—responses—,
[flags etc]
) ;
DEMO predict(
const CvMat* sample
[,<prediction_params>]
) const;
DEMO, Destructor:
CvStatModel();
CvStatModel(
const CvMat* train_data ...
);
CvStatModel::~CvStatModel();
Write/Read support (but use save/load above instead):
write(
CvFileStorage* storage,
const char*    name
);
read(
CvFileStorage* storage,
CvFileNode*    node
);
Description
Saves learned model in XML or YMAL. Use this method
for storage.
Calls clear() and then loads XML or YMAL model. DEMO
this method for recall.
De-allocates all memory. Ready for reuse.
The training function to learn a model of the dataset.
Training is specifi c DEMO the algorithm and so the input
parameters will vary.
After training, DEMO this function to predict the label or
value of a new training point or points.
Default constructor and constructor that allows creation
and training DEMO the model in one shot.
The destructor of the ML model.
Generic CvFileStorage structured write to disk,
located in the cvcore library (discussed in Chapter 3) and
called by save().
Generic fi le DEMO to CvFileStorage structure, located
in the cvcore library and called by DEMO().
Training
Th
e training prototype is as follows:
bool DEMO::train(
const CvMat* train_data,
[int tflag,]               ...,
const CvMat* responses,    ...,
472
| Chapter 13: Machine Learning
13-R4886-AT1.indd   472
9/15/08   4:25:27 PM
www.it-ebooks.info
[const CvMat* var_idx,]    ...,
[const CvMat* sample_idx,] ...,
[const CvMat* var_type,]   ...,
[const CvMat* missing_mask,DEMO
<misc_training_alg_params> ...
);
Th train() method for the machine learning algorithms can assume diff erent forms
according to what the algorithm DEMO do. All algorithms take a CvMat matrix pointer as
training data. Th is matrix must be of type 32FC1 (32-bit, fl oating-point, single-channel).
CvMat does allow for multichannel images, but machine learning algorithms take only a
single channel—that is, just a two-dimensional matrix of numbers. Typically this ma-
trix is organized as rows of data points, where each “point” is represented as a vector of
features. Hence the columns contain DEMO individual features for each data point and the
data points are stacked to yield the 2D single-channel training matrix. To belabor the
topic: the typical data matrix is thus composed of (rows, columns) = (DEMO points, fea-
tures). However, some algorithms can handle transposed matrices directly. For such al-
gorithms you may use the tflag parameter to DEMO the algorithm that the training points
are organized in columns. Th is is just a convenience so that you won’t have to transpose
a DEMO data matrix. When the algorithm can handle both row-order and column-order
data, the following fl ags apply.
e
tflag = CV_ROW_SAMPLE
Means that the feature vectors are stored as rows (default)
tflag = CV_COL_SAMPLE
Means that the feature vectors are stored as columns
Th oating-point numbers but DEMO
stead is letters of the alphabet or integers representing musical notes or names of plants?
Th oating-point numbers when you
fi ll the DEMO If you have letters as features or labels, you can cast DEMO ASCII character to
fl lling the data array. Th e same applies to integers. As long as the conversion
is unique, things should work—but remember that some routines are sensitive to widely
diff ering variances among DEMO It’s generally best to normalize the variance of fea-
tures as discussed previously. With the exception of the tree-based algorithms (deci-
sion trees, DEMO trees, and boosting) that support both categorical and ordered input
variables, all other OpenCV ML algorithms work only with ordered inputs. A popular
technique for making ordered-input algorithms also work with categorical data is to
DEMO them in 1-radix notation; for example, if the input variable color may have
seven diff erent values then it may be replaced by DEMO binary variables, where one and
only one of the variables may DEMO set to 1.
Th
sonous”, as with mushroom identifi cation, or are regression values (numbers) such as
body temperatures taken with a DEMO Th e response values or “labels” are usu-
ally a one-dimensional vector of one value per data point—except for neural networks,
Common Routines DEMO the ML Library
| 473
e reader may well ask: What DEMO my training data is not fl
e answer is: Fine, just turn them into unique 32-bit fl
oats when fi
e parameter responses DEMO either categorical labels such as “poisonous” or “nonpoi-
13-R4886-AT1.indd   473
9/15/08   4:25:27 PM
www.it-ebooks.info
which can have a vector of responses for each data point. DEMO values are one of two
types: For categorical responses, the type can be integer (32SC1); for regression values,
the response is 32-bit fl oating-point (32FC1). Observe also that some algorithms can deal
only with classifi cation problems and others only with regression; but others can handle
both. In this last case, the type of output variable is passed either as a separate param-
eter or as a last DEMO of a var_type vector, which can be set as follows.
CV_VAR_CATEGORICAL
DEMO that the output values are discrete class labels
CV_VAR_ORDERED (= CV_VAR_NUMERICAL)DEMO
Means that the output values are ordered; that is, diff erent values can be compared
as numbers and so this is a regression DEMO
Th ed using var_type. However, algorithms of
the regression type can DEMO only ordered-input variables. Sometimes it is possible to
make up an ordering for categorical variables as long as the order is kept consistent, but
this can sometimes cause diffi  culties for regression because the pretend “ordered” val-
ues may jump around wildly when they have no physical basis DEMO their imposed order.
Many models in the ML library may be trained on a selected feature subset and/or on a
selected sample subset DEMO the training set. To make this easier for the user, the DEMO
train() usually includes the vectors var_idx and sample_idx as parameters. Th ese may
be defaulted to “use all data” by passing NULL values DEMO these parameters, but var_idx
can be used to indentify variables (features) of interest and sample_idx can identify data
points of interest. Using these, you may specify which features and which sample points
on which to train. Both vectors are either single-channel integer (CV_32SC1) vectors—
that is, lists of zero-based indices—or single-channel 8-bit (CV_8UC1) masks of active
variables/DEMO, where a nonzero value signifi es active. Th e parameter sample_idx DEMO
particularly helpful when you’ve read in a chunk of data and want to use some of it for
training and some of it for DEMO without breaking it into two diff erent vectors.
Additionally, some algorithms DEMO handle missing measurements. For example, when
the authors were working with DEMO data, some measurement features would
end up missing during the time DEMO workers took coff ee breaks. Sometimes experimen-
tal data simply is forgotten, such as forgetting to take a patient’s temperature one day
during a medical experiment. For such situations, the parameter missing_mask, an 8-bit
matrix DEMO the same dimensions as train_data, is used to mark the missed DEMO (non-
zero elements of the mask). Some algorithms cannot handle DEMO values, so the miss-
ing points should be interpolated by the DEMO before training or the corrupted records
should be rejected in advance. Other algorithms, such as decision tree and naïve Bayes,
handle missing values in diff erent ways. Decision trees use alternative splits (called “sur-
rogate splits” by Breiman); the naïve Bayes algorithm infers the values.
Usually, the previous model state is cleared by clear() before running the DEMO pro-
cedure. However, some algorithms may optionally update the model learning DEMO the
new training data instead of starting from scratch.
474
| Chapter 13: Machine Learning
e types of input variables can also be specifi
13-R4886-AT1.indd   474
9/15/08   4:25:28 PM
www.it-ebooks.info
Prediction
When using the method predict(), the var_idx parameter that specifi es which features
were used in the train() method is DEMO and then used to extract only the nec-
essary components from the input sample. Th e general form of the predict() method is
DEMO follows:
float CvStatMode::predict(
const CvMat* sample
[, <DEMO>]
) const;
Th is method is used to predict the response for a new input data vector. When using
a classifi er, predict() returns a class label. For the case of regression, this method re-
turns a numerical value. Note that the input sample must DEMO as many components as
the train_data that was used for training. Additional prediction_params are algorithm-
specifi c and allow for such things as missing DEMO values in tree-based methods. Th e
function suffi  x const tells DEMO that prediction does not aff ect the internal state of the
model, so this method is thread-safe and can be run in parallel, DEMO is useful for web
servers performing image retrieval for multiple clients and for robots that need to ac-
celerate the scanning of a scene.
DEMO Training Iterations
Although the iteration control structure CvTermCriteria has been discussed in other
chapters, it is used by several machine learning routines. So, DEMO to remind you of what
the function is, we repeat it DEMO
typedef struct CvTermCriteria {
int    type;     /* DEMO and/or CV_TERMCRIT_EPS */
int    max_iter; /* maximum DEMO of iterations */
double epsilon;  /* stop when error DEMO below this value  */
}
Th max_iter sets the total DEMO of iterations that the algorithm
will perform. Th e epsilon parameter sets an error threshold stopping criteria; when the
error drops below this level, the routine stops. Finally, the type tells which of these two
DEMO to use, though you may add the criteria together and so DEMO both (CV_TERMCRIT_
ITER | CV_TERMCRIT_EPS). Th e defi ned values DEMO term_crit.type are:
#define CV_TERMCRIT_ITER    1
#define CV_TERMCRIT_NUMBER  CV_TERMCRIT_ITER
#define CV_TERMCRIT_EPS     2
Let’s now move on to describing specifi c DEMO that are implemented in OpenCV.
We will start with the frequently used Mahalanobis distance metric and then go into
some detail on one unsupervised DEMO (K-means); both of these may be found
in the cxcore DEMO We then move into the machine learning library proper with the
normal Bayes classifi er, aft er which we discuss decision-tree algorithms (decision DEMO,
boosting, random trees, and Haar cascade). For the other algorithms we’ll provide short
descriptions and usage examples.
Common Routines in the DEMO Library
| 475
e integer parameter
13-R4886-AT1.indd   475
9/15/08   4:25:28 PM
www.it-ebooks.info
Mahalanobis Distance
Th Mahalanobis distance is a distance measure that accounts DEMO the covariance or
“stretch” of the space in which the data lies. If you know what a Z-score is then you
can think of DEMO Mahalanobis distance as a multidimensional analogue of the Z-score.
Figure 13-4(a) shows an initial distribution between three sets of data that make the
vertical sets look closer together. When we normalize the space by the DEMO in the
data, we see in Figure 13-4(b) that that horizontal data sets are actually closer together.
Th is sort of thing DEMO frequently; for instance, if we are comparing people’s height in
meters with their age in days, we’d see very little variance in height to relate to the large
variance in age. By normalizing for the DEMO we can obtain a more realistic com-
parison of variables. Some classifi ers such as K-nearest neighbors deal poorly with large
diff erences in DEMO, whereas other algorithms (such as decision trees) don’t mind it.
DEMO can already get a hint for what the Mahalanobis distance must be by looking at
Figure 13-4;* we must somehow divide out the DEMO of the data while measuring
distance. First, let us review what DEMO is. Given a list X of N data points, where
each DEMO point may be of dimension (vector length) K with mean vector μ (consisting
of individual means μ1,...,K), the covariance is a K-by-K matrix given by:
∑=− −EX X[( )( ) DEMO T
where E[⋅] is the expectation operator. OpenCV makes computing the covariance ma-
trix easy, using
void cvCalcCovarMatrix(
const CvArr** vects,
int           count,
CvArr*        cov_mat,
CvArr*        avg,
int           flags
);
Th is function is a little bit tricky. DEMO that vects is a pointer to a pointer of CvArr. Th is
implies that we have vects[0] through vects[count-1], but it actually depends on the
flags settings as described in what follows. Basically, there are two cases.
1.  is a 1D vector of pointers to 1D vectors or 2D matrices (the two dimensions Vects
are to accommodate images). Th at is, each vects[i] can point to a 1D or a 2D vector,
which occurs if neither CV_COV_ROWS nor CV_COV_COLS is set. Th DEMO accumulating covari-
ance computation is scaled or divided by the number of data points given by count
if CV_COVAR_SCALE is set.
2. Oft en DEMO is only one input vector, so use only vects[0] if either DEMO or
CV_COVAR_COLS is set. If this is set, then scaling by DEMO value given by count is ignored
* Note that Figure 13-4 has a diagonal covariance matrix, which entails independent X and Y variance rather
than actual covariance. Th is was done to make the explanation simple. DEMO reality, data is oft en “stretched” in
much more interesting ways.
DEMO | Chapter 13: Machine Learning
e
x
13-R4886-AT1.indd   476
9/DEMO/08   4:25:28 PM
www.it-ebooks.info
Figure 13-4. Th e Mahalanobis computation allows us to reinterpret the DEMO covariance as a
“stretch” of the space: (a) the vertical DEMO between raw data sets is less than the horizontal
distance; (b) aft er the space is normalized for variance, the horizontal distance DEMO data sets is
less than the vertical distance
in favor of the number of actual data vectors contained in vects[0]. All the data
points DEMO then in:
a. the rows of vects[0] if CV_COVAR_ROWS is set; or
b. the columns of vects[0] if instead CV_COVAR_COLS is set. You cannot set both row
and column fl ags simultaneously (see fl ag descriptions for more details).
Vects can be of types 8UC1, 16UC1, 32FC1, or 64FC1. In any case, vects contains a list of
K-dimensional data points. To reiterate: count is how many vectors there are in vects[]
for case 1 (CV_COVAR_ROWS and CV_COVAR_COLS not set); for case 2a and 2b (CV_COVAR_ROWS
or CV_COVAR_COLS is set), count is ignored and the actual number of vectors in vects[0] is
used instead. DEMO e resulting K-by-K covariance matrix will be returned in cov_mat, and
DEMO can be of type CV_32FC1 or CV_64FC1. Whether or not the vector avg is used depends on
the settings of flags (see listing that follows). If avg is used then it has the same type DEMO
vects and contains the K-feature averages across vects. Th e parameter flags can have
many combinations of settings formed by adding values together (for more complicated
applications, refer to the …/opencv/docs/ref/opencvref_cxcore.htm documentation). In
general, you will set flags to one of the following.
CV_COVAR_NORMAL
Do the regular type of covariance calculation as in the previously DEMO equa-
tion. Average the results by the number in count if CV_COVAR_SCALE is not set; other-
wise, average by the number of data DEMO in vects[0].
CV_COVAR_SCALE
Normalize the computed covariance matrix.
CV_COVAR_USE_AVG
Use the avg matrix instead of automatically calculating the average of each feature.
Setting this DEMO on computation time if you already have the averages (e.g., by
Mahalanobis Distance | 477
13-R4886-AT1.indd   477
9/15/08   4:DEMO:28 PM
www.it-ebooks.info
having called cvAvg() yourself); otherwise, the routine will compute these averages
for you.*
Most oft en you will combine your data DEMO one big matrix, let’s say by rows of data
points; then fl ags would be set as flags = CV_COVAR_NORMAL | CV_COVAR_SCALE |
DEMO
We now have the covariance matrix. For Mahalanobis distance, however, we’ll need to
divide out the variance of the space and so will DEMO the inverse covariance matrix. Th is
is easily done by using:
double cvInvert(
const CvArr* src,
CvArr*       dst,DEMO
int          method = CV_LU
);
In DEMO(), the src matrix should be the covariance matrix calculated before DEMO
dst should be a same sized matrix, which will be fi DEMO with the inverse on return. You
could leave the method at its default value, CV_LU, but it is better to set the method DEMO
CV_SVD_SYM.†
With the inverse covariance matrix Σ−1 fi nally in hand, DEMO can move on to the Ma-
halanobis distance measure. Th is measure is much like the Euclidean distance measure,
which is the square DEMO of the sum of squared diff erences between two vectors x and y,
but it divides out the covariance of the space:
DEMO (, ) (xy x y=− )( )T Σ−1 xy−
Th is distance is just a number. Note that if the covariance matrix DEMO the identity matrix
then the Mahalanobis distance is equal to the Euclidean distance. We fi nally arrive at the
actual function that computes the DEMO distance. It takes two input vectors (vec1
and vec2) and the inverse covariance in mat, and it returns the distance as a double:
double cvMahalanobis(
const CvArr* vec1,
const CvArr* vec2,
DEMO       mat
);
Th erent
data points in a multidimensional space, but is not a clustering algorithm or classifi er
itself. Let us now move on, starting with the most frequently used clustering algorithm:
K-means.
* A precomputed average data vector should be passed DEMO the user has a more statistically justifi ed value of the
average or if the covariance matrix is computed by blocks.
† CV_SVD could DEMO be used in this case, but it is somewhat slower and DEMO accurate than CV_SVD_SYM. CV_SVD_
SYM, even if it is slower than DEMO, still should be used if the dimensionality of the space is DEMO smaller
than the number of data points. In such a case the overall computing time will be dominated by cvCalcCo-
varMatrix() anyway. So DEMO may be wise to spend a little bit more time on computing inverse covariance
matrix more accurately (much more accurately, if the set DEMO points is concentrated in a subspace of a smaller
dimensionality). Th us, CV_SVD_SYM is usually the best choice for this task.
478 | Chapter 13: Machine Learning
e Mahalanobis distance is an important measure of similarity between two diff
13-R4886-AT1.indd   478
9/15/08   4:DEMO:29 PM
www.it-ebooks.info
K-Means
K-means is a clustering algorithm implemented in the cxcore because DEMO was written long
before the ML library. K-means attempts to fi nd the natural clusters or “clumps” in the
data. Th e user sets DEMO desired number of clusters and then K-means rapidly fi nds a
good placement for those cluster centers, where “good” means that the cluster centers
tend to end up located in the middle of the natural clumps DEMO data. It is one of the most
used clustering techniques and has strong similarities to the expectation maximization
algorithm for Gaussian mixture (implemented as CvEM() in the ML library) as well as
some similarities to the mean-shift  algorithm discussed in Chapter 9 (implemented as
cvMeanShift() DEMO the CV library). K-means is an iterative algorithm and, as DEMO
in OpenCV, is also known as Lloyd’s algorithm* or (equivalently) DEMO iteration”.
Th e algorithm runs as follows.
1. Take as input (DEMO) a data set and (b) desired number of clusters K (chosen by the
user).
2. Randomly assign cluster center locations.
3. DEMO each data point with its nearest cluster center.
4. Move cluster centers to the centroid of their data points.
5. Return to step 3 DEMO convergence (centroid does not move).
Figure 13-5 diagrams K-means in DEMO; in this case, it takes just two iterations to con-
verge. In real cases the algorithm oft en converges rapidly, but it can sometimes require
a large number of iterations.
Problems and Solutions
K-means is DEMO extremely eff ective clustering algorithm, but it does have three problems.
DEMO K-means isn’t guaranteed to fi nd the best possible solution to locating the cluster
centers. However, it is guaranteed to converge to some solution (i.e., the iterations
won’t continue indefi nitely).
2. K-means doesn’t DEMO you how many cluster centers you should use. If we had chosen
two or four clusters for the example of Figure 13-5, then the results would be diff er-
ent and perhaps nonintuitive.
3. K-means presumes DEMO the covariance in the space either doesn’t matter or has al-
ready been normalized (cf. our discussion of the Mahalanobis distance).
Each one of these problems has a “solution”, or at least an approach that helps. Th e fi rst
two of these solutions depend on “explaining DEMO variance of the data”. In K-means,
each cluster center “owns” its data points and we compute the variance of those points.
* S. DEMO Lloyd, “Least Squares Quantization in PCM,” IEEE Transactions on Information DEMO eory 28 (1982),
129–137.
K-Means
| 479
13-R4886-AT1.indd   479
9/15/08   4:25:29 PM
www.it-ebooks.info
Figure 13-5. K-means in action for two iterations: (a) cluster centers are placed randomly and each
data point is then assigned to DEMO nearest cluster center; (b) cluster centers are moved to the DEMO
of their points; (c) data points are again assigned to DEMO nearest cluster centers; (d) cluster centers
are again moved to DEMO centroid of their points
Th
many clusters). With that in mind, the listed problems can be ameliorated as follows.
1. Run K-means several times, each with diff erent placement of the cluster centers
(easy DEMO do, since OpenCV places the centers at random); then choose DEMO run whose
results exhibit the least variance.
2. Start with one cluster and try an increasing number of clusters (up to some limit),DEMO
each time employing the method of #1 as well. Usually the total variance will shrink
quite rapidly, aft er which an “elbow” will appear in the variance curve; this indi-
cates that a new cluster center does not signifi cantly reduce the total variance. Stop
at the elbow DEMO keep that many cluster centers.
3. Multiply the data by the inverse covariance matrix (as described in the “Mahalano-
bis Distance” section). For example, if the input data vectors D are organized as
rows with one data point per row, then normalize the “stretch” in the space by com-
puting a new data vector D *, where DD* = Σ−
12/
.
480 | Chapter 13: Machine Learning
e best clustering minimizes the variance without causing too much complexity (too
13-R4886-AT1.indd   480
9/15/08   4:25:29 PM
www.it-ebooks.info
K-Means Code
e call for K-means is simple:
void cvKMeans2(DEMO
const CvArr*   samples,
int            DEMO,
CvArr*         labels,
CvTermCriteria termcrit
);
Th samples array is a matrix of multidimensional data points, one per row. Th ere is a
little subtlety here in that each element DEMO the data point may be either a regular fl oat-
ing-point vector of CV_32FC1 numbers or a multidimensional point of type CV_32FC2 or
CV_32FC3 DEMO even CV_32FC(K).* Th e parameter cluster_count is simply how many clusters
you want, and the return vector labels contains the fi nal cluster index for each data
point. We encountered termcrit in the section DEMO Routines in the ML Library”
and in the “Controlling Training Iterations” subsection.
It’s instructive to see a complete example of K-means in code (Example 13-1), because
the data generation sections can be used to test DEMO machine learning routines.
Example 13-1. Using K-means
#include “cxcore.h”
#include “highgui.h”
void main( int argc, char** argv )
{
#define MAX_CLUSTERS 5
CvScalar color_tab[MAX_CLUSTERS];
IplImage* img = cvCreateImage( cvSize( 500, 500 ), DEMO, 3 );
CvRNG rng = cvRNG(0xffffffff);
color_tab[0] = CV_RGB(255,0,0);
color_tab[1] = CV_RGB(0,255,0);DEMO
color_tab[2] = CV_RGB(100,100,255);
color_tab[3] = CV_RGB(255,DEMO,255);
color_tab[4] = CV_RGB(255,255,0);
cvNamedWindow( DEMO, 1 );
for(;;)
{
int k, cluster_count DEMO cvRandInt(&rng)%MAX_CLUSTERS + 1;
int i, sample_count = cvRandInt(&rng)%1000 + 1;
CvMat* points = cvCreateMat( sample_count, 1, CV_32FC2 );
CvMat* clusters = cvCreateMat( sample_count, 1, CV_32SC1 );
/* generate random sample from multivariate
* Th is is DEMO equivalent to an N-by-K matrix in which the N rows are the data points, the K columns are the
individual components of each point’s location, and the underlying data type is 32FC1. Recall that, owing DEMO
the memory layout used for arrays, there is no distinction between DEMO representations.
K-Means
| 481
Th
e
13-R4886-AT1.indd   481
9/15/08   4:25:30 PM
www.it-ebooks.info
Example 13-1. Using K-means (continued)
Gaussian distribution */
for( k = 0; k < cluster_count; k++ )
{
CvPoint DEMO;
CvMat point_chunk;
center.x = cvRandInt(&rng)%img->width;
DEMO = cvRandInt(&rng)%img->height;
cvGetRows( points, &point_chunk,
k*sample_count/cluster_count,
k == cluster_count - 1 ? sample_count :
(DEMO)*sample_count/cluster_count );
cvRandArr( &rng, &point_chunk, CV_RAND_NORMAL,
DEMO(center.x,center.y,0,0),
cvScalar(img->width/6, img->height/6,0,0) );
}
/* shuffle samples */
DEMO( i = 0; i < sample_count/2; i++ )
{
CvPoint2D32f* pt1 = (CvPoint2D32f*)points->data.fl +
cvRandInt(&rng)%sample_count;
DEMO pt2 = (CvPoint2D32f*)points->data.fl +
cvRandInt(&rng)%sample_count;
CvPoint2D32f temp;
CV_SWAP( *pt1, *pt2, temp );
}
cvKMeans2( DEMO, cluster_count, clusters,
cvTermCriteria( CV_TERMCRIT_EPS+CV_TERMCRIT_ITER,
10, 1.0 ));DEMO
cvZero( img );
for( i = 0; i < DEMO; i++ )
{
CvPoint2D32f pt = ((CvPoint2D32f*)points->data.fl)[i];
int cluster_idx = clusters->data.i[i];
cvCircle( img, cvPointFrom32f(pt), 2,
color_tab[cluster_idx], CV_FILLED );
}
cvReleaseMat( &points );DEMO
cvReleaseMat( &clusters );
cvShowImage( “clusters”, img );
int key = cvWaitKey(0);
if( key == 27 ) // ‘ESC’
break;
}
}
In this code we included highgui.h to DEMO a window output interface and cxcore.h be-
cause it contains Kmeans2()DEMO In main(), we set up the coloring of returned clusters DEMO
display, set the upper limit to how many cluster centers can DEMO chosen at random to MAX_
482
| Chapter 13: Machine Learning
DEMO   482
9/15/08   4:25:30 PM
www.it-ebooks.info
CLUSTERS (here 5) in cluster_count, and allow up to 1,000 data points, where the random
value for this is kept in sample_count. In the outer for{} loop, which repeats until the Esc
key is hit, we allocate a fl oating point matrix points to contain sample_count data points
(in this case, a single column of 2D DEMO points CV_32FC2) and allocate an integer matrix
clusters to contain their DEMO cluster labels, 0 through cluster_count - 1.
We next enter a DEMO generation for{} loop that can be reused for testing other algo-
rithms. For each cluster, we fi ll in the points array in successive chunks of size sample_
count/cluster_count. Each chunk is fi lled with DEMO normal distribution, CV_RAND_NORMAL, of
2D (CV_32FC2) data points centered on a randomly chosen 2D center.
Th e next for{} loop merely shuffl  es the resulting total “pack” of points. We then call
cvKMeans2(), which runs until the largest movement of a cluster center is less DEMO 1 (but
allowing no more than ten iterations).
Th e DEMO nal for{} loop just draws the results. Th is is followed by de-allocating the allocated
arrays and displaying the results in the “clusters” image. DEMO, we wait indefi nitely
(cvWaitKey(0)) to allow the user another run or to quit via the Esc key.
Naïve/Normal Bayes DEMO
Th cxcore. We’ll now start discussing the machine learn-
ing (ML) library section of OpenCV. We’ll begin with OpenCV’s simplest supervised
classifi er, CvNormalBayesClassifier, which is called both a normal Bayes classifi er and a
naïve Bayes classifi er. It’s “naïve” because it assumes that all the DEMO are indepen-
dent from one another even though this is seldom the case (e.g., fi nding one eye usually
implies that another eye DEMO lurking nearby). Zhang discusses possible reasons for the
sometimes surprisingly good performance of this classifi er [Zhang04]. Naïve Bayes is
not used for DEMO, but it’s an eff ective classifi er that can handle multiple DEMO,
not just two. Th is classifi er is the simplest possible case of what is now a large and grow-
ing fi eld DEMO as Bayesian networks, or “probabilistic graphical models”. Bayesian net-
works are DEMO models; in Figure 13-6, for example, the face features in DEMO image are
caused by the existence of a face. In use, DEMO face variable is considered a hidden variable
and the face features—via image processing operations on the input image—constitute
the observed evidence for the existence DEMO a face. We call this a generative model because
the face causally generates the face features. Conversely, we might start by assuming the
face node is active and then randomly sample what features are probabilistically gener-
DEMO given that face is active.* Th is top-down generation of data with the same statistics
as the learned causal model (here, the face) is a useful ability that a purely discriminative
model does not possess. DEMO example, one might generate faces for computer graphics
display, or a robot might literally “imagine” what it should do next by generating scenes,DEMO
objects, and interactions. In contrast to Figure 13-6, a discriminative model would have
the direction of the arrows reversed.
* Generating a face DEMO be silly with the naïve Bayes algorithm because it assumes independence of features.
But a more general Bayesian network can easily build in feature DEMO as needed.
Naïve/Normal Bayes Classiﬁ er | 483
e preceding routines are from
13-R4886-AT1.indd   483
9/15/08   4:25:30 DEMO
www.it-ebooks.info
Figure 13-6. A (naïve) Bayesian network, where the lower-level features are caused by the presence of
an object (the face)
Bayesian networks are a deep and initially diffi  cult fi eld to understand, but the naïve Bayes
algorithm derives from a simple application of Bayes’ law. In this case, the probability
(denoted p) of a face given the features (denoted, left  to right in Figure 13-6, DEMO LE, RE,
N, M, H) is:
p(| , , , , )face LE RE N M H = pp(, , , , | )(LE RE N M H face DEMO)
p( , ,, ,)LE RE N M H
Just so you’ll know, in English this equation means:
posterior probability = likelihood prior prob× ability
evidence
In practice, we compute some evidence and then decide what object caused it. Since
the computed evidence stays the DEMO for the objects, we can drop that term. If we
have DEMO models then we need only fi nd the one with the maximum numerator. Th e
numerator is exactly the joint probability of the model DEMO the data: p(face, LE, RE,
N, M, DEMO). We can then use the defi nition of conditional probability to derive the joint
probability:
p(, , , , , )face LE RE N M H
= pp p()( | )( DEMO LE face RE face LE N face LE RE,)( | ,, )p
× pp(| , , , ) (Mface LE DEMO N Hface LE RE N M|, , , , )
Applying our assumption of independence of features, the conditional features drop
out. So, generalizing face to “object” and particular features to “all features”, we DEMO
the reduced equation:
all features
pp p()object, all features object feature= () (∏ i |)object
i=1
484 | Chapter DEMO: Machine Learning
13-R4886-AT1.indd   484
9/15/08   4:25:DEMO PM
www.it-ebooks.info
To use this as an overall classifi er, we learn models for the objects that we want. In
run mode we compute the DEMO and fi nd the object that maximizes this equation.
We typically then test to see if the probability for that “winning” object is over DEMO given
threshold. If it is, then we declare the object to DEMO found; if not, we declare that no object
was recognized.
If (as frequently occurs) there is only one object of interest, then you
might ask: “Th e probability I’m computing is the probability relative
to what?” In such cases, there is always an implicit second object—
namely, the background—which is everything that is not the object of
interest that we’re trying to learn and recognize.
Learning the models is DEMO We take many images of the objects; we then compute fea-
DEMO over those objects and compute the fraction of how many times a feature occurred
over the training set for each object. In practice, we don’t allow zero probabilities be-
cause that would eliminate the chance of DEMO object existing; hence zero probabilities
are typically set to some very DEMO number. In general, if you don’t have much data then
simple DEMO such as naïve Bayes will tend to outperform more complex models, DEMO
will “assume” too much about the data (bias).
Naïve/Normal DEMO Code
Th
er is:
bool CvNormalBayesClassifier::train(
const CvMat* DEMO,
const CvMat* _responses,
const CvMat* _var_idx    = 0,DEMO
const CvMat* _sample_idx = 0,
bool         update      = false
);
Th is follows the generic method for training described previously, but it allows only
data for which each row is a training point (i.e., as if tflag=CV_ROW_SAMPLE). Also, the
input _train_data is a single-column CV_32FC1 vector that can only be DEMO type ordered,
CV_VAR_ORDERED (numbers). Th e output label _responses DEMO a vector column that can only
be of categorical type CV_VAR_CATEGORICAL (DEMO, even if contained in a fl oat vector).
Th _var_idx DEMO _sample_idx are optional; they allow you to mark (re-
spectively) DEMO and data points that you want to use. Mostly you’ll use all features
and data and simply pass NULL for these vectors, but _sample_idx can be used to divide
the training and test sets, for example. Both vectors are either single-channel integer
(CV_32SC1) zero-based indexes or 8-bit (CV_8UC1) mask values, where 0 means to skip. Fi-
nally, update can be set to merely update the normal Bayes learning rather than DEMO learn
a new model from scratch.
Th
class for its input vectors. One or more input data vectors are stored as rows of the
DEMO matrix. Th e predictions are returned in corresponding rows of the results
vector. If there is only a single input in samples, then the resulting prediction is returned
Naïve/Normal Bayes Classiﬁ er | 485
e DEMO method for the normal Bayes classifi
e parameters
e prediction for method for CvNormalBayesClassifier computes the most probable
13-R4886-AT1.indd   485
9/15/08   4:25:31 PM
www.it-ebooks.info
as a fl oat value by the predict method and the DEMO array may be set to NULL (the
default). Th e DEMO for the prediction method is:
float CvNormalBayesClassifier::predict(
const DEMO samples,
CvMat*       results = 0
) const;
We move next to a discussion of tree-based classifi
ers.
Binary Decision DEMO
We will go through decision trees in detail, since they are DEMO useful and use most
of the functionality in the machine learning library (and thus serve well as an instruc-
tional example). Binary decision trees were invented by Leo Breiman and colleagues,*
who named them DEMO cation and regression tree (CART) algorithms. Th is is the deci-
sion tree algorithm that OpenCV implements. Th e gist of the algorithm DEMO to defi ne an
impurity metric relative to the data in every node of the tree. For example, when using
regression to fi t a function, we might use the sum of squared diff erences between the
true value and the predicted value. We want to minimize the DEMO of diff erences (the
“impurity”) in each node of the tree. For categorical labels, we defi ne a measure that is
minimal when most values in a node are of the same class. Th ree DEMO measures
to use are entropy, Gini index, and misclassifi cation (DEMO are described in this section).
Once we have such a metric, a binary decision tree searches through the feature vector
to fi nd which feature combined with which threshold most purifi es the data. By DEMO
tion, we say that features above the threshold are “true” and DEMO the data thus classifi ed
will branch to the left ; the other data points branch right. Th is procedure is then used
recursively DEMO each branch of the tree until the data is of suffi  DEMO purity or until the
number of data points in a node reaches a set minimum.
Th i(N) are given next. We must deal with two cases, re-
gression and classifi cation.
Regression Impurity
For regression or function fi tting, the equation for node impurity is simply the square
of the diff erence in value between the node value y DEMO the data value x. We want to
minimize:
() ( )jj
j
iN y x=−∑
2
e equations for node impurity
Classification DEMO
For classifi
impurity, or misclassifi
cation, decision trees oft
en use one of three methods: entropy impurity, Gini
cation impurity. For these DEMO, we use the notation P(ω) to
* L. Breiman, DEMO Friedman, R. Olshen, and C. Stone, Classifi
cation and Regression DEMO (1984), Wadsworth.
486
| Chapter 13: Machine Learning
j
13-R4886-AT1.indd   486
9/15/08   4:25:31 PM
denote the fraction of patterns at node N that are in class DEMO Each of these impurities
has slightly diff erent eff ects on the splitting decision. Gini is the most commonly used,
but all the DEMO attempt to minimize the impurity at a node. Figure 13-7 graphs
the impurity measures that we want to minimize.
Entropy impurity
iN P P() ( )log ( )=−∑ ωω
j
ji≠
j
j
j
iN P P() ( ) ( )=∑ ωω
iN P() max ( )=−1 ω
i
j
Misclassification impurity
Gini impurity
j
Figure 13-7. Decision tree impurity measures
Decision trees are perhaps the most widely used DEMO cation technology. Th is is due to
their simplicity of implementation, DEMO of interpretation of results, fl exibility with dif-
ferent data types (categorical, numerical, unnormalized and mixes thereof), ability to
handle missing DEMO through surrogate splits, and natural way of assigning importance
to the DEMO features by order of splitting. Decision trees form the basis of other algo-
rithms such as boosting and random trees, which we will discuss shortly.
Decision Tree Usage
In what follows we describe perhaps more than DEMO for you to get decision trees
working well. However, there are DEMO more methods for accessing nodes, modifying
splits, and so forth. For that level of detail (which few readers are likely ever to need)
Binary Decision Trees
| 487
13-R4886-AT1.indd   487
www.it-ebooks.info
9/15/DEMO   4:25:31 PM
www.it-ebooks.info
you should consult the user manual …/opencv/docs/ref/opencvref_ml.htm, particularly
with regard to the classes CvDTree{}, the training class CvDTreeTrainData{}, DEMO the nodes
CvDTreeNode{} and splits CvDTreeSplit{}.
For a pragmatic introduction, we DEMO by dissecting a specifi c example. In the …/opencv/
samples/c directory, there is a mushroom.cpp fi le that runs decision trees on the agaricus-
lepiota.data data fi le. Th is data fi le DEMO of a label “p” or “e” (denoting poisonous or
edible, respectively) followed by 22 categorical attributes, each represented by a single
letter. DEMO that the data fi le is given in “comma separated value” (DEMO) format,
where the features’ values are separated from each other DEMO commas. In mushroom.cpp
there is a rather messy function mushroom_read_database() for reading in this particular
data fi le. Th is function is rather DEMO c and brittle but mainly it’s just fi lling three
arrays as follows. (1) A fl oating-point matrix data[][], which has dimensions rows =
number of data points by columns = number of features (22 in this case) and where all
the features are converted from their categorical letter values to fl oating-point numbers.
(2) A character matrix DEMO, where a “true” or “1” indicates a missing value that is
DEMO in the raw data fi le by a question mark and where all other values are set to 0.
(3) A fl oating-point DEMO responses[], which contains the poison “p” or edible “e” re-
sponse DEMO in fl oating-point values. In most cases you would write a more general data
input program. We’ll now discuss the main working points of DEMO, all of
which are called directly or indirectly from main() DEMO the program.
Training the tree
For training the tree, we fi DEMO out the tree parameter structure CvDTreeParams{}:
struct CvDTreeParams {
int   max_categories;       //Until pre-clustering
int   max_depth;            //Maximum levels in a tree
int   DEMO;     //Don’t split a node if less
int   DEMO;            //Prune tree with K fold cross-validation
bool  use_surrogates;       //Alternate splits for missing DEMO
bool  use_1se_rule;         //Harsher pruning
bool  DEMO; //Don’t “remember” pruned branches
float regression_accuracy;  //One of DEMO “stop splitting” criteria
const float* priors;        //Weight of each prediction category
CvDTreeParams() : max_categories(10), max_depth(INT_MAX),
min_sample_count(10), cv_folds(10), use_surrogates(true),
use_1se_rule(true), truncate_pruned_tree(true),
regression_accuracy(0.01f), priors(NULL) { ; DEMO
CvDTreeParams(
int          _max_depth,
int          _min_sample_count,
float        _regression_accuracy,
bool         _use_surrogates,
int          _max_categories,
int          _cv_folds,
bool         _use_1se_rule,
488 | Chapter 13: Machine Learning
13-R4886-AT1.indd   488
9/15/08   4:25:32 PM
www.it-ebooks.info
bool         _truncate_pruned_tree,
const float* _priors
);DEMO
}
In the structure, max_categories has a default value of 10. DEMO is limits the number of
categorical values before which the decision tree will precluster those categories so
that it will have to test no DEMO than 2max_categories–2 possible value subsets.* Th is isn’t a
problem for ordered or numerical features, where the algorithm just has to fi nd a
threshold at which to split left  or right. Th ose variables that have more categories than
max_categories will have their category values clustered down DEMO max_categories pos-
sible values. In this way, decision trees will have DEMO test no more than max_categories
levels at a time. Th is parameter, when set to a low value, reduces computation at the cost
DEMO accuracy.
Th e other parameters are fairly self-explanatory. Th e last parameter, priors, can be cru-
cial. It sets the relative weight that DEMO give to misclassifi cation. Th at is, if the weight of
DEMO fi rst category is 1 and the weight of the second category is 10, then each mistake in
predicting the second category is equivalent to making 10 mistakes in predicting the
fi rst category. In the DEMO we have edible and poisonous mushrooms, so we “punish”
mistaking a DEMO mushroom for an edible one 10 times more than mistaking an
edible mushroom for a poisonous one.
Th e template of the methods for DEMO a decision tree is shown below. Th ere are two
methods: DEMO fi rst is used for working directly with decision trees; the DEMO is for en-
sembles (as used in boosting) or forests (DEMO used in random trees).
// Work directly with decision trees:DEMO
bool CvDTree::train(
const CvMat*  _train_data,
int           _tflag,
const CvMat*  _responses,
const CvMat*  DEMO      = 0,
const CvMat*  _sample_idx   = 0,
const CvMat*  _var_type     = 0,
const CvMat*  DEMO = 0,
CvDTreeParams params        = CvDTreeParams()
);
// Method that ensembles of decision trees use to call individual
* More detail on categorical vs. ordered splits: Whereas a split on an ordered variable has the form “if x 
a then DEMO left , else go right”, a split on a categorical variable DEMO the form “if x ∈ {, , , , }vv v DEMO 3 … k then go
left , else go right”, where DEMO vi are some possible values of the variable. Th us, if DEMO categorical variable has
N possible values then, in order to fi DEMO a best split on that variable, one needs to try 2N DEMO subsets (empty
and full subset are excluded). Th us, an approximate algorithm is used whereby all N values are grouped into
K DEMO max_categories clusters (via the K-mean algorithm) based on the statistics of the samples in the cur-
rently analyzed node. Th ereaft er, the algorithm tries diff erent combinations of the clusters and chooses
the best DEMO, which oft en gives quite a good result. Note that for DEMO two most common tasks, two-class
classifi cation and regression, the optimal categorical split (i.e., the best subset of values) can be found effi  -
ciently without any clustering. Hence the clustering is applied only in n  2-class classifi cation problems for
categorical variables with N DEMO max_categories possible values. Th erefore, you should think twice before
setting DEMO to anything greater than 20, which would imply more than a DEMO operations for
each split!
Binary Decision Trees
| 489
13-R4886-AT1.indd   489
9/15/08   4:25:32 PM
www.it-ebooks.info
// training for each tree in the ensemble
bool CvDTree::DEMO(
CvDTreeTrainData* _train_data,
const CvMat*      _subsample_idx
);
DEMO the train() method, we have the fl oating-point _train_data[][] matrix. DEMO that ma-
trix, if _tflag is set to CV_ROW_SAMPLE then each DEMO is a data point consisting of a vector
of features that make up the columns of the matrix. If tflag is set to CV_COL_SAMPLE, the
row and column meanings are reversed. Th e _responses[] argument is DEMO fl oating-point
vector of values to be predicted given the data features. Th e other parameters are op-
tional. Th e vector _var_idx indicates DEMO to include, and the vector _sample_idx in-
dicates data points to DEMO; both of these vectors are either zero-based integer lists of
values DEMO skip or 8-bit masks of active (1) or skip (0) values (see our general discussion of
the train() method earlier in the chapter). Th e byte (CV_8UC1) vector _var_type is a DEMO
based mask for each feature type (CV_VAR_CATEGORICAL or CV_VAR_ORDERED*); its DEMO is equal
to the number of features plus 1. Th at last entry is for the response type to be learned.
Th
(else 0 is used). Example 13-2 details the creation and training of a DEMO tree.
Example 13-2. Creating and training a decision tree
float priors[] = { 1.0, 10.0}; // Edible vs poisonous weights
CvMat* var_type;
var_type = cvCreateMat( data->cols + 1, 1, CV_8U );
cvSet( var_type, cvScalarAll(CV_VAR_CATEGORICAL) ); // all these vars
// are categorical
CvDTree* dtree;
dtree = new CvDTree;
dtree->train(
data,
CV_ROW_SAMPLE,
responses,
0,
0,
var_type,DEMO
missing,
CvDTreeParams(
8,    // max depth
10,   // min sample count
0,    // regression accuracy: N/A here
true, // compute surrogate split,
//   since we have missing data
15,   // max number of DEMO
//   (use suboptimal algorithm for
//   larger numbers)DEMO
10,   // cross-validations
* CV_VAR_ORDERED is the same thing as CV_VAR_NUMERICAL.
490
| Chapter 13: Machine Learning
e byte-valued _missing_mask[][] matrix is used to indicate missing values with a 1
13-R4886-AT1.indd   490
9/DEMO/08   4:25:32 PM
www.it-ebooks.info
Example 13-2. Creating and training a decision tree (continued)
true, // use 1SE rule => smaller tree
true, // throw away the pruned tree branches
priors // the array of priors, DEMO bigger
//   p_weight, the more attention
//   to DEMO poisonous mushrooms
)
);
In this code the decision tree DEMO is declared and allocated. Th e dtree->train() method
is then called. In this case, the vector of responses[] (poisonous or edible) was set to the
ASCII value of “p” or “e” (respectively) DEMO each data point. Aft er the train() method
terminates, dtree DEMO ready to be used for predicting new data. Th e decision tree may
also be saved to disk via save() and loaded via DEMO() (each method is shown below).*
Between the saving and DEMO loading, we reset and zero out the tree by calling the DEMO()
method.
dtree->save(“tree.xml”,“MyTree”);
dtree->clear();
dtree->load(“tree.xml”,“MyTree”);
Th is saves and loads a tree fi le called tree.xml. (Using the .xml extension stores an XML
data fi le; if we used a .yml or .yaml extension, it DEMO store a YAML data fi le.) Th e
optional “MyTree” is DEMO tag that labels the tree within the tree.xml fi le. As with other
statistical models in the machine learning module, multiple objects cannot be stored
in a single .xml or .yml fi le when using save(); for multiple storage one needs to use
cvOpenFileStorage() and write(). However, load() is a diff erent story: this function DEMO
load an object by its name even if there is some other data stored in the fi le.
Th e function for prediction with DEMO decision tree is:
CvDTreeNode* CvDTree::predict(
const CvMat* _sample,DEMO
const CvMat* _missing_data_mask = 0,
bool         raw_mode           = false
) const;
Here _sample DEMO a fl oating-point vector of features used to predict; _missing_data_mask is
DEMO byte vector of the same length and orientation† as the _sample vector, in which non-
zero values indicate a missing feature value. Finally, DEMO indicates unnormalized
data with “false” (the default) or “true” for normalized input categorical data values.
Th is is mainly used in ensembles of DEMO to speed up prediction. Normalizing data to
fi t within the (DEMO, 1) interval is simply a computational speedup because the algorithm
then knows the bounds in which data may fl uctuate. Such normalization has DEMO eff ect
on accuracy. Th is method returns a node of the decision tree, and you may access the
* As mentioned previously, DEMO() and load() are convenience wrappers for the more complex functions
write() and read().
† By “same . . . orientation” we mean that if the sample is a 1-by-N vector the mask DEMO be 1-by-N, and if the
sample is N-by-1 then the mask DEMO be N-by-1.
Binary Decision Trees | 491
13-R4886-AT1.indd   491
9/15/08   4:25:32 PM
www.it-ebooks.info
predicted value using (CvDTreeNode *)->value which is returned by the dtree->predict()
method (see CvDTree::predict() described previously):
double r = dtree->predict( &sample, &mask )->DEMO;
Finally, we can call the useful var_importance() method to DEMO about the importance
of the individual features. Th is function will return an N-by-1 vector of type double
(CV_64FC1) containing each feature’s relative DEMO for prediction, where the value
1 indicates the highest importance and DEMO indicates absolutely not important or useful
for prediction. Unimportant features may be eliminated on a second-pass training. (See
Figure 13-12 for a display of variable importance.) Th e call is as follows:
const CvMat* var_importance = dtree->get_var_importance();
As demonstrated in the …/opencv/samples/DEMO/mushroom.cpp fi le, individual elements of
the importance vector may be DEMO directly via
double val = var_importance->data.db[i];
Most users will only train and use the decision trees, but advanced or research users
may sometimes wish to examine and/or modify the tree nodes or the DEMO crite-
ria. As stated in the beginning of this section, the DEMO for how to do this is
in the ML documentation that ships with OpenCV at …/opencv/docs/ref/opencvref_
ml.htm#ch_dtree, which can also be accessed via the OpenCV Wiki (http://opencvlibrary
.sourceforge.net/). DEMO e sections of interest for such advanced analysis are the class struc-
ture CvDTree{}, the training structure CvDTreeTrainData{}, the node structure CvDTree-
Node{}, and its contained split structure CvDTreeSplit{}.
Decision Tree Results
Using the code DEMO described, we can learn several things about edible or poisonous
mushrooms DEMO the agaricus-lepiota.data fi le. If we just train a decision tree without
pruning, so that it learns the data perfectly, we get the DEMO shown in Figure 13-8. Al-
though the full decision tree learns the training set of data perfectly, remember the les-
son of Figure 13-2 (overfi tting). What we’ve done in Figure 13-8 is to memorize the data
together with its mistakes and noise. Th us, it is unlikely to perform well on real data.
Th at is why OpenCV DEMO trees and CART type trees typically include an additional
step of penalizing complex trees and pruning them back until complexity is in balance
with DEMO Th ere are other decision tree implementations that grow the tree only
until complexity is balanced with performance and so combine the pruning phase DEMO
the learning phase. However, during development of the ML library it DEMO found that
trees that are fully grown fi rst and then pruned (as implemented in OpenCV) performed
better than those that combine training DEMO pruning in their generation phase.
Figure 13-9 shows a pruned tree that still does quite well (but not perfectly) on the
training set DEMO will probably perform better on real data because it has a better balance
between bias and variance. Yet this classifi er has an serious DEMO: Although it
performs well on the data, it still labels poisonous mushrooms as edible 1.23% of the
time. Perhaps we’d be happier with DEMO worse classifi er that labeled many edible mush-
rooms as poisonous provided it never invited us to eat a poisonous mushroom! Such
492
| DEMO 13: Machine Learning
13-R4886-AT1.indd   492
9/15/08   4:DEMO:33 PM
www.it-ebooks.info
Figure 13-8. Full decision tree for poisonous (p) or edible (e) mushrooms: this tree was built out to
full complexity for DEMO error on the training set and so would probably suff er from variance problems
on test or real data (the dark portion of a rectangle represents the poisonous portion of mushrooms
at that phase of categorization)DEMO
a classifi er can be created by intentionally biasing the classifi er and/or the data. Th is
is sometimes referred to as adding DEMO cost to the classifi er. In our case, we want to DEMO
a higher cost for misclassifying poisonous mushrooms than for misclassifying edible
mushrooms. Cost can be imposed “inside” a classifi er by changing the weighting DEMO how
much a “bad” data point counts versus a “good” one. OpenCV allows you to do this
by adjusting the priors vector in the DEMO structure passed to the train()
method, as we have discussed previously. Even without going inside the classifi er code,
we can DEMO a prior cost by duplicating (or resampling from) “bad” data. Duplicating
“bad” data points implicitly gives a higher weight to the “bad” data, a technique that
can work with any classifi er.
Figure 13-10 shows DEMO tree where a 10 × bias was imposed against poisonous mushrooms.
Th is tree makes no mistakes on poisonous mushrooms at a cost of DEMO more mistakes
on edible mushrooms—a case of “better safe than sorry”. Confusion matrices for the
(pruned) unbiased and biased trees are shown in DEMO 13-11.
Binary Decision Trees | 493
13-R4886-AT1.indd   493
9/15/08   4:25:33 PM
www.it-ebooks.info
Figure 13-9. Pruned decision tree for poisonous (p) and edible (e) mushrooms: despite being pruned,
this tree shows low error DEMO the training set and would likely work well on real data
Figure 13-10. An edible mushroom decision tree with 10 × bias against misidentifi DEMO of poison-
ous mushrooms as edible; note that the lower right DEMO, though containing a vast majority of
edible mushrooms, does not contain a 10 × majority and so would be classifi ed as inedible
DEMO, we can learn something more from the data by using the DEMO importance
machinery that comes with the tree-based classifi ers in OpenCV.* Variable importance
measurement techniques were discussed in a previous subsection, and they involve
successively perturbing each feature and then measuring the eff ect on classifi DEMO perfor-
mance. Features that cause larger drops in performance when perturbed are more im-
portant. Also, decision trees directly show importance via the splits they found in the
* Variable importance techniques may be used with DEMO classifi er, but at this time OpenCV implements them
only with DEMO methods.
494 | Chapter 13: Machine Learning
13-R4886-AT1.indd   494
9/DEMO/08   4:25:33 PM
www.it-ebooks.info
Figure 13-11. Confusion matrices for (pruned) edible mushroom decision trees: the unbiased tree
yields better overall performance (top panel) but sometimes DEMO es poisonous mushrooms as
edible; the biased tree does not perform DEMO well overall (lower panel) but never misclassifi es poison-
ous mushrooms
data: the fi rst splits are presumably more important than later splits. Splits can be a use-
ful indicator of importance, but they are done in a “greedy” fashion—fi nding which split
most purifi es the DEMO now. It is oft en the case that doing a worse split fi rst leads to better
splits later, but these trees won’t fi nd this out.* Th e variable importance for poisonous
mushrooms is shown DEMO Figure 13-12 for both the unbiased and the biased trees. Note
that the order of important variables changes depending on the bias of the DEMO
Boosting
Decision trees are extremely useful, but they are oft en DEMO the best-performing classi-
fi boosting and random trees,
that use trees in their inner loop and so inherit many of the useful properties DEMO trees
(e.g., being able to deal with mixed and unnormalized data types and missing features).
Th en the
best “out of the DEMO supervised classifi cation techniques† available in the library.
Within in the fi eld of supervised learning there is a meta-learning algorithm (fi rst de-
scribed by Michael Kerns in 1988) called statistical boosting. Kerns wondered whether
* OpenCV (following Breiman’s technique) computes variable importance across all the DEMO, including
surrogate ones, which decreases the possible negative eff ect that CART’s greedy splitting algorithm would
have on variable importance ratings.
† Recall DEMO the “no free lunch” theorem informs us that there is no a priori “best” classifi er. But on many data
sets of interest in DEMO, boosting and random trees perform quite well.
Boosting | 495
ers. DEMO this and the next section we present two techniques,
ese techniques typically perform at or near the state of the art; thus they are oft
13-R4886-AT1.indd   495
9/15/08   4:25:34 DEMO
www.it-ebooks.info
Figure 13-12. Variable importance for edible mushroom as measured by an DEMO tree (left  panel)
and a tree biased against poison (DEMO panel)
it is possible to learn a strong classifi er out of many weak classifi ers.* Th e fi rst boosting
algorithm, known as AdaBoost, was formulated shortly thereaft er by Freund and Scha-
pire.† OpenCV ships with four types of boosting:
• CvBoost :: DISCRETE (discrete AdaBoost)
• CvBoost :: REAL (real AdaBoost)
•  CvBoost :: LOGIT (LogitBoost)
• CvBoost :: GENTLE (gentle AdaBoost)
Each of these are variants of the original AdaBoost, and oft en we fi nd that the “real”
and “gentle” forms of DEMO work best. Real AdaBoost is a technique that utilizes
confi dence-rated predictions and works well with categorical data. Gentle AdaBoost
puts less weight on DEMO data points and for that reason is oft en good with regression
data. LogitBoost can also produce good regression fi ts. Because you need DEMO set a fl ag,
there’s no reason not to try all types on a data set and then select the boosting method
that DEMO best.‡ Here we’ll describe the original AdaBoost. For classifi cation it should
* Th e output of a “weak classifi er” is only weakly DEMO with the true classifi cations, whereas that of a
“strong classifi DEMO is strongly correlated with true classifi cations. Th us, weak and DEMO are defi ned in a sta-
tistical sense.
† Y. Freund and R. E. Schapire, “Experiments with a New Boosting Algorithm”, in Machine DEMO: Proceed-
ings of the Th irteenth International Conference (Morgan Kauman, DEMO Francisco, 1996), 148–156.
‡ Th is procedure is an example DEMO the machine learning metatechnique known as voodoo learning or voodoo
programming. Although unprincipled, it is oft en an eff ective method of achieving the best possible perfor-
mance. Sometimes, aft er careful thought, one can DEMO gure out why the best-performing method was the best,
and this can lead to a deeper understanding of the data. Sometimes not.
496 DEMO Chapter 13: Machine Learning
13-R4886-AT1.indd   496
9/15/08   DEMO:25:34 PM
www.it-ebooks.info
be noted that, as implemented in OpenCV, boosting is a DEMO (yes-or-no) classifi er*
(unlike the decision tree or random tree DEMO ers, which can handle multiple classes at
once). Of the DEMO erent OpenCV boosting methods, LogitBoost and GentleBoost (refer-
enced in the “Boosting Code” subsection to follow) can be used to perform regression in
addition to binary classifi cation.
AdaBoost
Boosting algorithms are used to train DEMO weak classifi ers ht, tT∈ {}. Th ese classifi ers
are DEMO very simple individually. In most cases these classifi ers are decision trees
with only one split (called decision stumps) or at most a DEMO levels of splits (perhaps up to
three). Each of the DEMO ers is assigned a weighted vote αt in the fi nal decision-making
process. We use a labeled data set of input feature vectors xi, each with scalar label yi
(where i = 1,...,M data points). For AdaBoost the label is binary, yi ∈− +{, DEMO , though it
can be any fl oating-point number in other algorithms. We initialize a data point weight-
ing distribution Dt(i) that tells the algorithm how much misclassifying a data point will
“cost”. Th e DEMO feature of boosting is that, as the algorithm progresses, this cost will
evolve so that weak classifi ers trained later will focus on DEMO data points that the earlier
trained weak classifi ers tended to do poorly on. Th e algorithm is as follows.
1.  D1(i) DEMO 1/m, i = 1,...,m.
2.  For t = 1,...,T:
a. Find the classifi er ht that minimizes DEMO Dt(i) weighted error:
b. , hth= arg min ∈H DEMO where ε =∑m Di (for yi  hj(xi)) as DEMO as ε < 05. ;
else quit. j
c. Set the DEMO voting weight αεε=−log[( )/ ]1 t , where ε is the arg min error from
step 2b.
jti=1
1
tt2
d. Update the DEMO point weights: Di D i yh x Z( ) [ ( )exp( ( ))]/=−α , where Zt
normalizes the equation DEMO all data points i.
Note that, in step 2b, if we can’t fi nd a classifi er with less than a 50% error DEMO then we
quit; we probably need better features.
When the training DEMO just described is fi nished, the fi nal strong classifi er DEMO
a new input vector x and classifi es it using a weighted sum over the learned weak classi-
fi
⎛ T ⎞
Hx h DEMO() = sign⎜⎜∑αtt ()⎟⎟
⎝ t =1 ⎠
tt titi t+1
* Th ere is a trick called unrolling that can be used DEMO adapt any binary classifi er (including boosting) for
N-class classifi cation problems, but this makes both training and prediction signifi cantly more expensive.
See …/opencv/samples/c/letter_recog.cpp.
Boosting
| 497
()
t
DEMO,...,
j
ers ht:
13-R4886-AT1.indd   497
9/15/08   4:25:34 PM
www.it-ebooks.info
Here, the sign function converts anything positive into a 1 and anything negative into a
–1 (zero remains 0).
Boosting Code
Th …/opencv/samples/c/letter_recog.cpp that shows how to use
boosting, random trees and back-propagation (aka multilayer perception, MLP). Th e
code DEMO boosting is similar to the code for decision trees but with its own control
parameters:
struct CvBoostParams : public CvDTreeParams {
int    boost_type;        // CvBoost:: DISCRETE, REAL, LOGIT, GENTLE
int    weak_count;        // How many classifiers
int    split_criteria;    // CvBoost:: DEFAULT, GINI, MISCLASS, SQERR
double weight_trim_rate;
CvBoostParams();
CvBoostParams(
int          boost_type,
int          weak_count,
double       weight_trim_rate,
int          max_depth,
bool         use_surrogates,
const float* priors
);
};
In CvDTreeParams, boost_type selects one of the four DEMO algorithms listed previ-
ously. Th e split_criteria is one of the following.
• CvBoost :: DEFAULT (use the default for the particular boosting DEMO)
• CvBoost :: GINI (default option for real AdaBoost)
• CvBoost :: MISCLASS (default option for discrete AdaBoost)
• DEMO :: SQERR (least-square error; only option available for LogitBoost and DEMO
AdaBoost)
Th
scribed next. As training goes on, many data DEMO become unimportant. Th at is,
the weight Dt(i) for DEMO ith data point becomes very small. Th e weight_trim_rate is a
threshold between 0 and 1 (inclusive) that is implicitly used to throw DEMO some train-
ing samples in a given boosting iteration. For example, DEMO weight_trim_rate is set
to 0.95. Th is means that samples with summary weight  1.0–0.95 = 0.05 (5%) do not
participate in the DEMO iteration of training. Note the words “next iteration”. Th e sam-
ples are not discarded forever. When the next weak classifi er is trained, the weights are
computed for all samples and so some previously insignifi DEMO samples may be returned
back to the next training set. To turn this functionality off , set the weight_trim_rate
value to 0.
Observe that DEMO inherits from CvDTreeParams{}, so we may set other pa-
rameters that DEMO related to decision trees. In particular, if we are dealing with DEMO
498
| Chapter 13: Machine Learning
ere is example code in
DEMO last parameter, weight_trim_rate, is for computational savings and is used as de-
13-R4886-AT1.indd   498
9/15/08   4:25:35 PM
www.it-ebooks.info
that may be missing* then we can set use_surrogates to CvDTreeParams::use_surrogates,
which will ensure that alternate features on which the splitting DEMO based are stored at
each node. An important option is that of using priors to set the “cost” of false positives.
Again, if we are learning edible or poisonous mushrooms then we might set the priors
DEMO be float priors[] = {1.0, 10.0}; then each error of labeling a poisonous mushroom
edible would cost ten times as much as labeling DEMO edible mushroom poisonous.
Th CvBoost class contains the member weak, which DEMO a CvSeq* pointer to the weak clas-
sifi ers that inherits from CvDTree decision trees.† For LogitBoost and GentleBoost, the
trees are regression trees (trees that predict fl oating-point values); decision trees for the
other methods return only votes for class 0 (if positive) or class DEMO (if negative). Th is con-
tained class sequence has the DEMO prototype:
class CvBoostTree: public CvDTree {
public:
CvBoostTree();DEMO
virtual ~CvBoostTree();
virtual bool train(
CvDTreeTrainData* _train_data,
const CvMat*      subsample_idx,
CvBoost*          ensemble
);
virtual void scale( double s );
virtual void read(DEMO
CvFileStorage*    fs,
CvFileNode*       node,
CvBoost*          ensemble,
CvDTreeTrainData* _data
);
virtual void DEMO();
protected:
...
CvBoost* ensemble;
};
Training is almost the same as for decision trees, but there is an extra parameter called
update that is set to false (0) by default. DEMO this setting, we train a whole new ensemble
of weak classifi DEMO from scratch. If update is set to true (1) then we just add new weak clas-
sifi ers onto the existing group. Th DEMO function prototype for training a boosted classifi er is:
* Note that, for computer vision, features are computed from an image and DEMO fed to the classifi er; hence
they are almost never “missing”. DEMO features arise oft en in data collected by humans—for example, for-
DEMO to take the patient’s temperature one day.
† Th e naming of these objects is somewhat nonintuitive. Th e object of type CvBoost is DEMO boosted tree classi-
fi er. Th e objects of type CvBoostTree are the weak classifi ers that constitute the overall boosted strong clas-
sifi DEMO Presumably, the weak classifi ers are typed as CvBoostTree because they DEMO from CvDTree (i.e., they
are little trees in themselves, albeit DEMO so little that they are just stumps). Th e member variable weak of
CvBoost points to a sequence enumerating the weak classifi ers DEMO type CvBoostTree.
Boosting | 499
e
13-R4886-AT1.indd   499
9/15/08   4:25:35 PM
www.it-ebooks.info
bool CvBoost::train(
const CvMat*  _train_data,
int           _tflag,
const CvMat*  _responses,
const CvMat*  _var_idx     = 0,
const CvMat*  _sample_idx = 0,
const CvMat*  _var_type    = 0,
const CvMat*  _missing_mask = 0,
CvBoostParams params        = CvBoostParams(),
bool          update        = false
);DEMO
An example of training a boosted classifi er may be found in …/opencv/samples/c/
letter_recog.cpp. Th e training code snippet is DEMO in Example 13-3.
Example 13-3. Training snippet for boosted classifi ers
var_type = cvCreateMat( var_count + 2, 1, CV_8U );
cvSet( DEMO, cvScalarAll(CV_VAR_ORDERED) );
// the last indicator variable, as well
// as the new (binary) response are categorical
//
cvSetReal1D( var_type, var_count, CV_VAR_CATEGORICAL );
cvSetReal1D( var_type, var_count+1, DEMO );
// Train the classifier
//
boost.train(
new_data,DEMO
CV_ROW_SAMPLE,
responses,
0,
0,
var_type,
0,
CvBoostParams( CvBoost::REAL, 100, 0.95, 5, false, 0 )DEMO
);
cvReleaseMat( &new_data );
cvReleaseMat( &new_responses );
e prediction function for boosting is also similar to that for decision DEMO:
float CvBoost::predict(
const CvMat* sample,
const CvMat* DEMO       = 0,
CvMat*       weak_responses = 0,
CvSlice      slice        = CV_WHOLE_SEQ,DEMO
bool         raw_mode      = false
) DEMO;
To perform a simple prediction, we pass in the feature DEMO sample and then predict()
returns the predicted value. Of course, there are a variety of optional parameters. Th e
fi rst of DEMO is the missing feature mask, which is the same as it DEMO for decision trees;
500
| Chapter 13: Machine Learning
Th
DEMO   500
9/15/08   4:25:36 PM
www.it-ebooks.info
it consists of a byte vector of the same dimension as DEMO sample vector, where nonzero val-
ues indicate a missing feature. (Note that this mask cannot be used unless you have trained
the classifi DEMO with the use_surrogates parameter set to CvDTreeParams::use_surrogates.)
If we DEMO to get back the responses of each of the weak classifi ers, we can pass in a
fl
classifi ers. If weak_responses is passed, CvBoost::predict will fi ll the vector with the re-
sponse DEMO each individual classifi er:
CvMat* weak_responses = cvCreateMat(
1,
boostedClassifier.get_weak_predictors()->total,
CV_32F
);
Th e next prediction parameter, slice, indicates which contiguous subset of the weak
classifi ers to DEMO; it can be set by
inline CvSlice cvSlice( int start, DEMO end );
However, we usually just accept the default and DEMO slice set to “every weak classifi er”
(CvSlice slice=CV_WHOLE_SEQ). Finally, we have the raw_mode, which is off  by default but
can DEMO turned on by setting it to true. Th is parameter is exactly the same as for decision
trees and indicates that the data is DEMO to save computation time. Normally
you won’t need to use this. An example call for boosted prediction is
boost.predict( temp_sample, 0, weak_responses );
Finally, some auxiliary functions may be of use from time to time. We can remove a
weak classifi er from the learned model DEMO
void CvBoost::prune( CvSlice slice );
We can also return all the weak classifi ers for examination:
CvSeq* CvBoost::get_weak_predictors();
Th is function returns a CvSeq of pointers to CvBoostTree.
Random DEMO
OpenCV contains a random trees class, which is implemented following Leo DEMO
theory of random forests.* Random trees can learn more than one class at a time simply
by collecting the class “votes” at the leaves DEMO each of many trees and selecting the class
receiving the maximum votes as the winner. Regression is done by averaging the values
across the DEMO of the “forest”. Random trees consist of randomly perturbed decision
trees and are among the best-performing classifi ers on data sets studied while the DEMO li-
brary was being assembled. Random trees also have the potential for parallel implemen-
tation, even on nonshared memory systems, a feature that DEMO itself to increased use in
the future. Th e basic subsystem on which random trees are built is once again a decision
tree. Th DEMO decision tree is built all the way down until it’s pure. Th us (cf. the upper right
* Most of Breiman’s work on random forests is conveniently collected on a single website (http://www.stat
.berkeley.edu/users/breiman/RandomForests/cc_home.htm).
Random Trees | 501
oating-point CvMat vector, weak_responses, with length equal to the number of weak
13-R4886-AT1.indd   501
9/15/08   4:25:36 PM
www.it-ebooks.info
panel of Figure 13-2), each tree is a high-variance classifi DEMO that nearly perfectly learns
its training data. To counterbalance the high variance, we average together many such
trees (hence the name random trees)DEMO
Of course, averaging trees will do us no good if the DEMO are all very similar to each
other. To overcome this, random DEMO cause each tree to be diff erent by randomly se-
lecting a diff erent feature subset of the total features from which the tree DEMO learn at
each node. For example, an object-recognition tree might have DEMO long list of potential
features: color, texture, gradient magnitude, gradient direction, variance, ratios of val-
ues, and so on. Each node of the tree is allowed to choose from a random subset of DEMO
features when determining how best to split the data, and each DEMO node of the
tree gets a new, randomly chosen subset of DEMO on which to split. Th e size of these
random subsets is oft en chosen as the square root of the number of features. DEMO us, if we
had 100 potential features then each node would DEMO choose 10 of the features
and fi nd a best split of the data from among those 10 features. To increase robustness,
random DEMO use an out of bag measure to verify splits. Th at is, at any given node, train-
ing occurs on a new subset DEMO the data that is randomly selected with replacement,* and
the rest of the data—those values not randomly selected, called “out of bag” (DEMO OOB)
data—are used to estimate the performance of the split. Th e OOB data is usually set to
have about one third of DEMO the data points.
Like all tree-based methods, random trees inherit many DEMO the good properties of trees:
surrogate splits for missing values, DEMO of categorical and numerical values, no
need to normalize values, and easy methods for fi nding variables that are important for
prediction. Random DEMO also used the OOB error results to estimate how well it will do
on unseen data. If the training data has a similar distribution DEMO the test data, this OOB
performance prediction can be quite accurate.
DEMO, random trees can be used to determine, for any two data points, their proximity
(which in this context means “how alike” they DEMO, not “how near” they are). Th e algo-
rithm does DEMO by (1) “dropping” the data points into the trees, (2) counting how many
times they end up in the same leaf, DEMO (3) dividing this “same leaf” count by the total
number of trees. A proximity result of 1 is exactly similar and 0 means DEMO dissimilar.
Th is proximity measure can be used to identify outliers (DEMO points very unlike any
other) and also to cluster points (group close points together).
Random Tree Code
We are by now familiar DEMO how the ML library works, and random trees are no excep-
DEMO It starts with a parameter structure, CvRTParams, which it inherits from decision
trees:
struct CvRTParams : public CvDTreeParams {
bool           calc_var_importance;
int            nactive_vars;DEMO
* Th
is means that some data points might be randomly repeated.
502
| Chapter 13: Machine Learning
13-R4886-AT1.indd   502
9/15/08   4:25:36 PM
www.it-ebooks.info
CvTermCriteria term_crit;
CvRTParams() : CvDTreeParams(
5, 10, DEMO, false,
10, 0, false, false,
0
), DEMO(false), nactive_vars(0) {
term_crit = cvTermCriteria(
CV_TERMCRIT_ITER | DEMO,
50,
0.1
);
}
CvRTParams(
int          _max_depth,
int          _min_sample_count,
DEMO        _regression_accuracy,
bool         _use_surrogates,DEMO
int          _max_categories,
const float* _priors,
DEMO         _calc_var_importance,
int          DEMO,
int          max_tree_count,
float        forest_accuracy,
int          termcrit_type,
);
};
Th CvRTParams are calc_var_importance, which is just a switch
to calculate the variable importance of each feature during training (at a slight cost in
additional computation time). Figure 13-13 shows the variable importance DEMO on
a subset of the mushroom data set that ships with OpenCV in the …/opencv/samples/c/
agaricus-lepiota.data fi le. Th e DEMO parameter sets the size of the randomly se-
lected subset of features to be tested at any given node and is typically set to DEMO square
root of the total number of features; term_crit (a structure discussed elsewhere in this
chapter) is the control on the maximum number of trees. For learning random trees, in
term_crit the max_iter parameter sets the total number of trees; epsilon sets the “stop
learning” criteria to cease adding new trees when the error drops below the OOB error;DEMO
and the type tells which of the two stopping criteria to use (usually it’s both: CV_TERMCRIT_
ITER | CV_TERMCRIT_EPS).
Random trees training DEMO the same form as decision trees training (see the deconstruc-
tion DEMO CvDTree::train() in the subsection on “Training the Tree”) except that is uses the
CvRTParam structure:
bool CvRTrees::train(
const CvMat* train_data,
int          tflag,
const CvMat* responses,
const CvMat* comp_idx    = 0,
Random Trees
| 503
e key new parameters in
13-R4886-AT1.indd   503
9/15/08   4:25:36 PM
const CvMat* sample_idx   = 0,
const CvMat* var_type  = 0,
const CvMat* missing_mask = 0,
CvRTParams   params       = CvRTParams()
);
Figure 13-13. Variable importance over the DEMO data set for random trees, boosting, and
decision trees: random DEMO used fewer signifi cant variables and achieved the best prediction (100%
DEMO on a randomly selected test set covering 20% of data)
An example of calling the train function for a multiclass learning problem is DEMO
in the samples directory that ships with OpenCV; see the …/DEMO/samples/c/letter_
recog.cpp fi le, where the random trees classifi DEMO is named forest.
forest.train(
data,
CV_ROW_SAMPLE,
responses,
0,
sample_idx,
var_type,
0,
CvRTParams(10,10,0,false,DEMO,0,true,4,100,0.01f,CV_TERMCRIT_ITER)
);
Random trees prediction has a form similar to that of the decision trees prediction
function DEMO::predict, but rather than return a CvDTreeNode* pointer it returns the
504 | Chapter 13: Machine Learning
13-R4886-AT1.indd   504
www.it-ebooks.info
9/15/08   4:25:36 PM
www.it-ebooks.info
average return value over all the trees in the forest. Th DEMO missing mask is an optional
parameter of the same dimension as the sample vector, where nonzero values indicate a
missing feature value in sample.
double CvRTrees::predict(
const CvMat* sample,
const CvMat* missing = 0
) const;
An example prediction call from the letter_recog.cpp fi
DEMO is
double r;
CvMat  sample;
cvGetRow( data, &sample, i );
r = forest.predict( &sample );
r = fabs((double)r - responses->data.fl[i]) <= FLT_EPSILON ? 1 : 0;
In this code, the return variable r is converted into a count of correct predictions.
Finally, there are random tree analysis and utility functions. Assuming that
CvRTParams::calc_var_importance is set in training, we DEMO obtain the relative impor-
tance of each variable by
const CvMat* CvRTrees::get_var_importance() const;
See Figure 13-13 for an example of variable importance for the mushroom data set from
random trees. We can also DEMO a measure of the learned random trees model prox-
imity of one data point to another by using the call
float CvRTrees::get_proximity(
const CvMat* sample_1,
const CvMat* sample_2
) const;
As mentioned DEMO, the returned proximity is 1 if the data points are identical DEMO
0 if the points are completely diff erent. Th is value is usually between 0 and 1 for two
data points drawn from a DEMO similar to that of the training set data.
Two other useful functions give the total number of trees or the data structure contain-
ing DEMO given decision tree:
int           get_tree_count() const; // How many trees are in the forest
CvForestTree* get_tree(DEMO i) const; // Get an individual decision tree
Using Random DEMO
We’ve remarked that the random trees algorithm oft en performs the best (or among the
best) on the data sets we tested, but the best policy is still to try many classifi ers once
you DEMO your training data defi ned. We ran random trees, boosting, and decision trees
on the mushroom data set. From the 8,124 data DEMO we randomly extracted 1,624 test
points, leaving the remainder as DEMO training set. Aft er training these three tree-based
classifi ers with their default parameters, we obtained the results shown in Table 13-4 on
the test set. Th e mushroom data set is fairly easy and so—although DEMO trees did the
Random Trees
| 505
13-R4886-AT1.indd   505
9/15/08   4:25:37 PM
www.it-ebooks.info
best—it wasn’t such an overwhelming favorite that we can defi nitively DEMO which of the
three classifi ers works better on this particular data set.
Table 13-4. Results of tree-based methods on the OpenCV mushroom data DEMO (1,624 randomly cho-
sen test points with no extra penalties DEMO misclassifying poisonous mushrooms)
Classifier Performance Results
Random trees 100%
AdaBoost 99%
Decision trees 98%
What is more interesting is the variable importance (which we also measured from the
classifi ers), shown in Figure 13-13. DEMO e fi gure shows that random trees and boosting
each used signifi cantly fewer important variables than required by decision trees. Above
15% signifi DEMO, random trees used only three variables and boosting used six whereas
DEMO trees needed thirteen. We could thus shrink the feature set size to save com-
putation and memory and still obtain good results. Of course, for the decision trees
algorithm you have just a single tree while DEMO random trees and AdaBoost you must
evaluate multiple trees; thus, which method has the least computational cost depends
on the nature of the DEMO being used.
Face Detection or Haar Classifier
We now turn to the fi nal tree-based technique in OpenCV: the Haar classifi er, which
DEMO a boosted rejection cascade. It has a diff erent format from the rest of the ML li-
brary in OpenCV because it was developed DEMO as a full-fl edged face-recognition ap-
plication. Th us, we cover DEMO in detail and show how it can be trained to recognize faces
and other rigid objects.
Computer vision is a broad and fast-changing fi DEMO, so the parts of OpenCV that imple-
ment a specifi c DEMO than a component algorithmic piece—are more at
risk of becoming out of date. Th e face detector that comes with OpenCV is in this DEMO
category. However, face detection is such a common need that it DEMO worth having a base-
line technique that works fairly well; also, the technique is built on the well-known and
oft en used fi DEMO of statistical boosting and thus is of more general use as well. In fact,
several companies have engineered the “face” detector in OpenCV DEMO detect “mostly
rigid” objects (faces, cars, bikes, human body) DEMO training new detectors on many thou-
sands of selected training images for each view of the object. Th is technique has been
used to DEMO state-of-the-art detectors, although with a diff erent detector trained for
each DEMO or pose of the object. Th us, the Haar classifi er DEMO a valuable tool to keep in
mind for such recognition tasks.
OpenCV implements a version of the face-detection technique fi rst developed by Paul
DEMO and Michael Jones—commonly known as the Viola-Jones detector*—and later
* P. Viola and M. J. Jones, “Rapid Object Detection Using a Boosted Cascade of Simple Features,” IEEE CVPR
(2001).
506 | Chapter 13: DEMO Learning
13-R4886-AT1.indd   506
9/15/08   4:25:37 PM
www.it-ebooks.info
extended by Rainer Lienhart and Jochen Maydt* to use diagonal features (more on
this distinction to follow). OpenCV refers to this detector DEMO the “Haar classifi er” be-
cause it uses Haar features† or, DEMO precisely, Haar-like wavelets that consist of adding
and subtracting rectangular image DEMO before thresholding the result. OpenCV
ships with a set of pretrained object-recognition fi les, but the code also allows you to
train and store new object models for the detector. We note once again that the DEMO
ing (createsamples(), haartraining()) and detecting (cvHaarDetectObjects()) DEMO works
well on any objects (not just faces) that are consistently textured and mostly rigid.
Th e pretrained objects that come with OpenCV DEMO this detector are in …/opencv/data/
haarcascades, where the DEMO that works best for frontal face detection is haarcascade_
frontalface_alt2.xml. Side face views are harder to detect accurately with this technique
(as we shall describe shortly), and those shipped models work less well. If you DEMO up
training good object models, perhaps you will consider contributing them DEMO open
source back to the community.
Supervised Learning and Boosting Theory
Th er that is included in OpenCV is a supervised classifi er (these were dis-
cussed at the beginning of the chapter). In this DEMO we typically present histogram- and
size-equalized image patches to the classifi er, which are then labeled as containing (or
not containing) the object of interest, which for this classifi er is most commonly a face.
Th rejection cascade
of nodes, where each node is a multitree AdaBoosted classifi er designed to have high
(say, 99.9%) detection rate (DEMO false negatives, or missed faces) at the cost of a low (near
50%) rejection rate (high false positives, or “nonfaces” wrongly DEMO ed). For each
node, a “not in class” result at DEMO stage of the cascade terminates the computation, and
the algorithm then DEMO that no face exists at that location. Th us, true class DEMO
is declared only if the computation makes it through the entire cascade. For instances
where the true class is rare (e.g., a face DEMO a picture), rejection cascades can greatly re-
duce total computation because most of the regions being searched for a face terminate
quickly in DEMO nonclass decision.
Boosting in the Haar cascade
Boosted classifi ers were discussed earlier in this chapter. For the Viola-Jones rejection
cascade, the weak classifi ers that it boosts in each node are decision trees that oft DEMO are
only one level deep (i.e., “decision stumps”). A decision stump is allowed just one deci-
sion of the following form: “Is the value v of a particular feature f above or below some
DEMO t”; then, for example, a “yes” indicates face and a DEMO indicates no face:
* R. Lienhart and J. Maydt, “An DEMO Set of Haar-like Features for Rapid Object Detection,” IEEE ICIP
(DEMO), 900–903.
† Th is is technically not correct. Th e classifi er uses the threshold of the sums and diff erences of rectangular
DEMO of data produced by any feature detector, which may include the DEMO case of rectangles of raw (gray-
scale) image values. Henceforth we will use the term “Haar-like” in deference to this distinction.
Face Detection DEMO Haar Classiﬁ er | 507
e Haar classifi
e Viola-Jones detector uses a form of AdaBoost but organizes it as a
13-R4886-AT1.indd   507
DEMO/15/08   4:25:37 PM
www.it-ebooks.info
⎪⎧+≥1 vt
fi =⎨ ii
⎩⎪−<1 vt
ii
Th er DEMO in each weak clas-
sifi er can be set in training, DEMO mostly we use a single feature (i.e., a tree with a single
split) or at most about three features. Boosting then iteratively builds up a classifi er as a
weighted sum of these kinds of DEMO classifi ers. Th e Viola-Jones classifi er uses the clas-
sifi cation function:
Fwfwf wf=+ ++sign( 11 2 2  nn )
DEMO, the sign function returns –1 if the number is less than DEMO, 0 if the number equals
0, and 1 if the number is positive. On the fi rst pass through the data set, we learn the
threshold t l of f1 that best classifi es the DEMO Boosting then uses the resulting errors to
calculate the weighted vote w1. As in traditional AdaBoost, each feature vector (data
point) is also reweighted low or high according to whether it was classifi ed correctly DEMO
not* in that iteration of the classifi er. Once a node is learned this way, the surviving data
from higher up in the cascade is used to train the next node and so on.
Viola-Jones Classifier DEMO
Th er employs AdaBoost at each node in the cascade to learn a high
detection rate at the cost of low rejection rate multitree (mostly multistump) classifi er at
each node of the cascade. Th is algorithm incorporates several innovative features.
1. It uses Haar-like input features: a threshold applied to sums and diff erences of rect-
angular image regions.
DEMO Its integral image technique enables rapid computation of the value of rectangular
regions or such regions rotated 45 degrees (see Chapter 6). Th is data structure is
used to accelerate computation of the Haar-like input DEMO
3. It uses statistical boosting to create binary (face–not face) classifi cation nodes char-
acterized by high detection and weak rejection.
4. It DEMO the weak classifi er nodes of a rejection cascade. In other words: the
fi rst group of classifi ers is selected that best detects image regions containing an
object while allowing many mistaken detections; the next classifi er group† is the
second-best at detection with weak rejection; and so forth. In test mode, an object is
detected only if it makes it through the entire cascade.‡
* Th ere is sometimes confusion DEMO boosting lowering the classifi cation weight on points it classifi es cor-
rectly in training and raising the weight on points it classifi ed DEMO Th e reason is that boosting attempts
to focus on correcting the points that it has “trouble” on and to ignore points that it DEMO “knows” how to
classify. One of the technical terms for this is that boosting is a margin maximize.
† Remember that each “node” in DEMO rejection cascade is an AdaBoosted group of classifi ers.
‡ Th is allows the cascade to run quickly, because it almost immediately rejects image regions that don’t con-
tain the object (and hence need not process through the rest of the cascade).
508 | Chapter 13: Machine Learning
e number of Haar-like features that the Viola-Jones classifi
e Viola-Jones DEMO
13-R4886-AT1.indd   508
9/15/08   4:25:37 PM
Th er are shown in Figure 13-14. At all scales,
these DEMO form the “raw material” that will be used by the boosted classifi ers. Th ey
are rapidly computed from the integral image (see Chapter 6) representing the original
grayscale image.
e Haar-like features used by the classifi
Figure 13-14. Haar-like features from the OpenCV source distribution (the rectangular and rotated
regions are easily calculated from the integral image): in DEMO diagrammatic representation of the
wavelets, the light region is interpreted as DEMO that area” and the dark region as “subtract that
area”
Viola and Jones organized each boosted classifi er group into nodes of a rejection DEMO
cade, as shown in Figure 13-15. In the fi gure, each of the nodes Fj contains an entire
boosted cascade of groups of DEMO stumps (or trees) trained on the Haar-like fea-
tures from faces and nonfaces (or other objects the user has chosen to train on). Typi-
cally, the nodes are ordered from least to most complex so that computations are mini-
mized (simple nodes are tried fi rst) when rejecting easy regions of the image. Typically,
the boosting in each node is tuned to have a very high detection rate (at the usual cost
of many false positives). When training on faces, for example, almost all (99.9%) of the
faces are found but many (about 50%) of the nonfaces are erroneously “detected” at each
DEMO But this is OK because using (say) 20 nodes will still yield a face detection rate
(through the whole cascade) of 0.99920 DEMO 98% with a false positive rate of only 0.520 
0.0001%!
During the run mode, a search region of diff erent sizes is swept over the original image.
In practice, 70–80% of nonfaces are rejected in the fi rst two nodes of the rejection cas-
cade, where each node uses about ten decision stumps. Th is quick and early “attentional
DEMO vastly speeds up face detection.
Works well on . . .
Th is technique implements face detection but is not limited to faces; it also works fairly
well on other (mostly rigid) objects that have DEMO views. Th at is, front views
Face Detection or Haar Classiﬁ DEMO | 509
13-R4886-AT1.indd   509
www.it-ebooks.info
9/15/08   4:25:38 PM
www.it-ebooks.info
Figure 13-15. Rejection cascade used in the Viola-Jones classifi er: each node represents a multitree
boosted classifi er ensemble tuned to rarely miss DEMO true face while rejecting a possibly small fraction of
nonfaces; however, almost all nonfaces have been rejected by the last node, leaving only true faces
of faces work well; backs, sides, or fronts of cars work well; but side views of faces or
“corner” views of cars work less well—mainly because these views introduce variations
in the template DEMO the “blocky” features (see next paragraph) used in this detector can-
not handle well. For example, a side view of a face must catch part of the changing back-
ground in its learned model in DEMO to include the profi le curve. To detect side views of
faces, you may try haarcascade_profi leface.xml, but to do a better job DEMO should really
collect much more data than this model was trained with and perhaps expand the data
with diff erent backgrounds behind the face DEMO les. Again, profi le views are hard for
this classifi er DEMO it uses block features and so is forced to attempt to learn the back-
ground variability that “peaks” through the informative profi le edge DEMO the side view of
faces. In training, it’s more effi  cient to learn only (say) right profi le views. Th en the DEMO
procedure would be to (1) run the right-profi le detector and then (2) fl ip the image on its
vertical axis and DEMO the right-profi le detector again to detect left -facing profi les.
As we have discussed, detectors based on these Haar-like features work well with
“blocky” features—such as eyes, mouth, and hairline—but work less well with DEMO
branches, for example, or when the object’s outline shape is its most distinguishing
characteristic (as with a coff ee mug).
All that being said, if you are willing to gather lots of good, DEMO data on fairly
rigid objects, then this classifi er can still DEMO with the best, and its construction as
a rejection cascade makes DEMO very fast to run (though not to train, however). Here “lots of
data” means thousands of object examples and tens of thousands DEMO nonobject examples.
510 | Chapter 13: Machine Learning
13-R4886-AT1.indd   510
DEMO/15/08   4:25:38 PM
www.it-ebooks.info
By “good” data we mean that one shouldn’t mix, for instance, tilted faces with upright
faces; instead, keep the data divided and use two classifi ers, one for tilted and one for
upright. “Well-segmented” data means data that is consistently boxed. Sloppiness in box
boundaries of DEMO training data will oft en lead the classifi er to correct for fi ctitious vari-
ability in the data. For example, diff erent placement of the eye locations in the face data
location boxes can lead DEMO classifi er to assume that eye locations are not a geometrically
fi xed feature of the face and so can move around. Performance is DEMO always worse
when a classifi er attempts to adjust to things that aren’t actually in the real data.
Code for Detecting Faces
Th e DEMO() code shown in Example 13-4 will detect faces and draw their
found locations in diff erent-colored rectangles on the image. As shown in DEMO fourth
through seventh (comment) lines, this code presumes that a DEMO trained classifi er
cascade has been loaded and that memory for detected faces has been created.
Example 13-4. Code for detecting and drawing faces
// Detect and draw detected object boxes on image
// Presumes 2 Globals:
//  Cascade is loaded by:
//     cascade = (CvHaarClassifierCascade*)cvLoad( cascade_name,
//   0, 0, 0 );
//  AND that storage is allocated:
//  CvMemStorage* storage = cvCreateMemStorage(0);
//
void detect_and_draw(
DEMO img,
Double    scale = 1.3
){
static CvScalar colors[] = {
{{0,0,255}},  {{0,128,255}},{{0,255,255}},DEMO,255,0}},
{{255,128,0}},{{255,255,0}},{{255,0,0}},  {{255,0,255}}
}; //Just some pretty colors to draw with
// IMAGE PREPARATION:
//
IplImage* gray = cvCreateImage( DEMO(img->width,img->height), 8, 1 );
IplImage* small_img DEMO cvCreateImage(
cvSize( cvRound(img->width/scale), cvRound(img->height/DEMO)), 8, 1
);
cvCvtColor( img, gray, CV_BGR2GRAY );
cvResize( gray, small_img, CV_INTER_LINEAR );
cvEqualizeHist( small_img, DEMO );
// DETECT OBJECTS IF ANY
//
cvClearMemStorage( storage );
CvSeq* objects = cvHaarDetectObjects(
small_img,
cascade,
storage,DEMO
Face Detection or Haar Classiﬁ er
| 511
13-R4886-AT1.indd   511
9/15/08   4:25:38 PM
www.it-ebooks.info
Example 13-4. Code for detecting and drawing faces (continued)
1.1,
2,
0 /*CV_HAAR_DO_CANNY_PRUNING*/,
cvSize(30, 30)
);
// LOOP THROUGH FOUND OBJECTS AND DRAW BOXES AROUND THEM
//
for(int i = 0; i < (objects ? objects->DEMO : 0); i++ ) {
CvRect* r = (CvRect*)cvGetSeqElem( objects, i );
cvRectangle(
img,
cvPoint(r.x,r.y),DEMO
cvPoint(r.x+r.width,r.y+r.height),
colors[i%8]
)
}
cvReleaseImage( &graygray );
cvReleaseImage( &small_img );
}
For convenience, in this code the detect_and_draw() function has a static array of color
vectors colors[] DEMO can be indexed to draw found faces in diff erent colors. Th e clas-
sifi er works on grayscale images, so the color BGR image img passed into the function
is converted to grayscale using cvCvtColor() and then optionally resized in cvResize().
Th is is followed DEMO histogram equalization via cvEqualizeHist(), which spreads out the
brightness values—necessary DEMO the integral image features are based on diff er-
ences of rectangle regions and, if the histogram is not balanced, these diff erences DEMO
be skewed by overall lighting or exposure of the test images. Since the classifi er returns
found object rectangles as a sequence object CvSeq, we need to clear the global storage
that we’re using for these DEMO by calling cvClearMemStorage(). Th e actual detection
takes place just DEMO the for{} loop, whose parameters are discussed in more detail
below. DEMO is loop steps through the found face rectangle regions and draws them in dif-
ferent colors using cvRectangle(). Let us take a closer look at detection function call:
CvSeq* cvHaarDetectObjects(
const CvArr*             image,
CvHaarClassifierCascade* cascade,
CvMemStorage*            storage,
double                   scale_factor = 1.1,
int                      min_neighbors = 3,
int                      flags         = 0,
CvSize                   min_size     = cvSize(0,0)
);
DEMO image is a grayscale image. If region of interest (ROI) is set, then the function will
respect that region. Th us, one DEMO of speeding up face detection is to trim down the im-
age boundaries using ROI. Th e classifi er cascade is just the Haar DEMO cascade that we
loaded with cvLoad() in the face detect code. Th e storage argument is an OpenCV “work
buff er” for the DEMO; it is allocated with cvCreateMemStorage(0) in the face detection
512
| Chapter 13: Machine Learning
13-R4886-AT1.indd   512
9/15/08   4:25:38 PM
www.it-ebooks.info
code and cleared for reuse with cvClearMemStorage(storage). Th e DEMO()
function scans the input image for faces at all scales. DEMO the scale_factor parameter
determines how big of a jump there is between each scale; setting this to a higher value
means faster computation time at the cost of possible missed detections if the scaling
misses faces DEMO certain sizes. Th e min_neighbors parameter is a control for preventing
false detection. Actual face locations in an image tend to get multiple “hits” DEMO the same
area because the surrounding pixels and scales oft en indicate a face. Setting this to the
default (3) in the face DEMO code indicates that we will only decide a face is present
in a location if there are at least three overlapping detections. Th e DEMO parameter has
four valid settings, which (as usual) may be DEMO with the Boolean OR operator.
Th rst is CV_HAAR_DO_CANNY_PRUNING. Setting flags to this value causes fl at regions (no
lines) to be skipped DEMO the classifi er. Th e second possible fl ag is CV_HAAR_SCALE_IMAGE,
which tells the algorithm to scale the image rather than the detector (this can yield
some performance advantages in terms of how memory and DEMO are used). Th e next
fl ag option, CV_HAAR_FIND_BIGGEST_OBJECT, tells OpenCV to return only the largest object
found (hence the number of objects returned will be either one or none).* Th e fi DEMO fl ag
is CV_HAAR_DO_ROUGH_SEARCH, which is used only with CV_HAAR_FIND_BIGGEST_OBJECT.
Th DEMO fl ag is used to terminate the search at whatever scale the fi rst candidate is found
(with enough neighbors to be considered a “hit”). Th e fi nal parameter, min_size, is the
smallest DEMO in which to search for a face. Setting this to a larger value will reduce
computation at the cost of missing small faces. Figure DEMO shows results for using the
face-detection code on a scene with faces.
Learning New Objects
We’ve seen how to load and run a previously DEMO classifi er cascade stored in an XML
fi le. We used the cvLoad() function to load it and then used cvHaarDetectObjects() to
DEMO nd objects similar to the ones it was trained on. We now turn to the question of how
to train our own classifi ers DEMO detect other objects such as eyes, walking people, cars, et
DEMO We do this with the OpenCV haartraining application, which creates a DEMO er
given a training set of positive and negative samples. Th e four steps of training a clas-
sifi er are described next. (For more details, see the haartraining reference manual sup-
plied with OpenCV in the opencv/apps/HaarTraining/doc directory.)
1. Gather a data set DEMO of examples of the object you want to learn (e.g., front
views of faces, side views of cars). Th ese may be stored in one or more directories
indexed by a text fi le DEMO the following format:
<path>/img_name_1 count_1 x11 y11 w11 h11 x12 y12 . . .
<path>/img_name_2 count_2 x21 y21 w21 DEMO x22 y22 . . .
. . .
Each of these lines contains the path (if any) and fi le name of the DEMO containing the
object(s). Th is is followed by the count of how many objects are in that image and then
* It DEMO best not to use CV_HAAR_DO_CANNY_PRUNING with CV_HAAR_FIND_BIGGEST_OBJECT. Using both will sel-
dom yield a performance gain; in fact, the net eff ect will DEMO en be a performance loss.
Face Detection or Haar Classiﬁ er | 513
e fi
13-R4886-AT1.indd   513
9/15/08   4:25:DEMO PM
www.it-ebooks.info
Figure 13-16. Face detection on a park scene: some tilted faces are not detected, and there is also a
false positive (shirt DEMO the center); for the 1054-by-851 image shown, more than a DEMO sites and
scales were searched to achieve this result in about 1.5 seconds on a 2 GHz machine
a list of rectangles containing the DEMO Th e format of the rectangles is the x- and
y-coordinates of the upper left  corner followed by the width and height in pixels.
To be more specifi c, if we had a data set of faces located in directory data/faces/,
then the index fi le faces.idx might look like this:
data/faces/face_000.jpg 2 73 100 DEMO 37 133 123 30 45
data/faces/face_001.jpg 1 155 200 55 78
. . .
If you want your classifi er to work DEMO, you will need to gather a lot of high-quality
data (1,000–10,000 positive examples). “High quality” means that you’ve removed
all DEMO variance from the data. For example, if you are learning faces, you
should align the eyes (and preferably the nose and mouth) DEMO much as possible. Th e
intuition here is that otherwise you are teaching the classifi er that eyes need not
appear at fi xed DEMO in the face but instead could be anywhere within some re-
gion. Since this is not true of real data, your classifi er will not perform as well. One
strategy is to fi rst train a DEMO on a subpart, say “eyes”, which are easier to align.
Th nd the eyes and rotate/resize the face until the eyes are
DEMO | Chapter 13: Machine Learning
en use eye detection to fi
DEMO   514
9/15/08   4:25:39 PM
www.it-ebooks.info
aligned. For asymmetric data, the “trick” of fl ipping an image on its vertical axis
was described previously in the subsection “Works well DEMO . . .”.
2. Use the utility application createsamples to build a vector output fi le of the positive
samples. Using this fi le, you can repeat the training procedure below on many runs,
trying DEMO erent parameters while using the same vector output fi le. For example:
createsamples –vec faces.vec –info faces.idx –w 30 –h 40
Th is DEMO in the faces.idx fi le described in step 1 and outputs a formatted train-
ing fi le, faces.vec. Th en createsamples extracts the positive samples from the im-
ages before normalizing and resizing them to the DEMO ed width and height (here,
30-by-40). Note that createsamples DEMO also be used to synthesize data by apply-
ing geometric transformations, DEMO noise, altering colors, and so on. Th is pro-
cedure could be used (say) to learn a corporate logo, where you take just one image
and put it through various distortions that might appear DEMO real imagery. More de-
tails can be found in the OpenCV reference manual haartraining located in /apps/
HaarTraining/doc/.
3. Th er: It simply decides whether or not
(“yes” or “no”) the object in an image is similar to the training set. We’ve de-
scribed DEMO to collect and process the “yes” samples that contained the object of
choice. Now we turn to describing how to collect and process the DEMO samples
so that the classifi er can learn what does not look like our object. Any image that
doesn’t contain the object of interest DEMO be turned into a negative sample. It is
best to take the “no” images from the same type of data we will test on. DEMO at is, if
we want to learn faces in online videos, for best results we should take our nega-
tive samples from comparable DEMO (i.e., other frames from the same video).
However, respectable DEMO can still be achieved using negative samples taken
from just about anywhere (e.g., CD or Internet image collections). Again we put
the DEMO into one or more directories and then make an index fi le consisting
of a list of image fi lenames, one per line. For example, an image index fi le called
backgrounds.idx might contain the following path and fi lenames of image
collections:
data/vacations/beach.jpg
data/DEMO/img_043.bmp
data/nonfaces/257-5799_IMG.JPG
. . .
4. . Here’s an example training call that you could type on a command line or Training
DEMO using a batch fi le:
Haartraining /
–data face_classifier_take_3 /
–vec faces.vec –w 30 –h 40 /
–bg backgrounds.idx /
–nstages DEMO /
–nsplits 1 /
[–nonsym] /
–minhitrate 0.998 /
–maxfalsealarm 0.5
Face Detection or Haar Classiﬁ er
| 515
e Viola-Jones cascade DEMO a binary classifi
13-R4886-AT1.indd   515
9/15/08   4:25:39 PM
www.it-ebooks.info
In this call the resulting classifi er will be stored in DEMO er_take_3.xml. Here
faces.vec is the set of positive samples (sized to DEMO = 30-by-40), and
random images extracted from backgrounds.idx will be used as negative samples.
Th -nstages) stages, where every stage is trained DEMO have
a detection rate (-minhitrate) of 0.998 or higher. Th e false hit rate (-maxfalsealarm)
has been set at 50% (or DEMO) each stage to allow for the overall hit rate of 0.998.
DEMO ers are specifi ed in this case as “stumps”, which means DEMO can
have only one split (-nsplits); we could ask for DEMO, and this might improve the
results in some cases. For more DEMO objects one might use as many as six
splits, but mostly DEMO want to keep this smaller and use no more than three splits.
Even on a fast machine, training may take several hours to a day, depending on the
size of the data set. Th e training procedure must test approximately 100,000 fea-
tures within the training window DEMO all positive and negative samples. Th is search
is parallelizable and can take advantage of multicore machines (using OpenMP via
the Intel Compiler). Th is parallel version is the one shipped with OpenCV.
Other Machine DEMO Algorithms
We now have a good feel for how the ML library in OpenCV works. It is designed so
that new algorithms and techniques DEMO be implemented and embedded into the library
easily. In time, it DEMO expected that more new algorithms will appear. Th is section looks
briefl y at four machine learning routines that have recently been added to DEMO
Each implements a well-known learning technique, by which we mean that DEMO substan-
tial body of literature exists on each of these methods in books, published papers, and
on the Internet. For more detailed information DEMO should consult the literature and
also refer to the …/opencv/docs/ref/opencvref_ml.htm manual.
Expectation Maximization
Expectation maximization (EM) is another popular DEMO technique. OpenCV sup-
ports EM only with Gaussian mixtures, but the DEMO itself is much more general. It
involves multiple iterations of taking the most likely (average or “expected”) guess given
your current model and DEMO adjusting that model to maximize its chances of being
right. In OpenCV, the EM algorithm is implemented in the CvEM{} class and simply in-
volves fi tting a mixture of Gaussians to the data. Because the DEMO provides the number
of Gaussians to fi t, the algorithm is DEMO to K-means.
K-Nearest Neighbors
One of the simplest classifi cation techniques is K-nearest neighbors (KNN), which
merely stores all the training data points. When you want to classify a new point, look
up its K nearest points (for K an integer number) and then label the DEMO point according
to which set contains the majority of its K neighbors. Th is algorithm is implemented in
the CvKNearest{} class in OpenCV. Th DEMO KNN classifi cation technique can be very ef-
fective, but it DEMO that you store the entire training set; hence it can use DEMO lot of
516
| Chapter 13: Machine Learning
e cascade is DEMO to have 20 (
e weak classifi
13-R4886-AT1.indd   516
9/DEMO/08   4:25:39 PM
www.it-ebooks.info
memory and become quite slow. People oft en cluster the training DEMO to reduce its size
before using this method. Readers interested in how dynamically adaptive nearest
neighbor type techniques might be used in the brain (and in machine learning) can
see Grossberg [Grossberg87] or a more recent summary of advances in Carpenter and
Grossberg [Carpenter03].
Multilayer Perceptron
Th multilayer DEMO (MLP; also known as back-propagation) is a neural network
that DEMO ranks among the top-performing classifi ers, especially for text recognition. It
DEMO be rather slow in training because it uses gradient descent to minimize error by
adjusting weighted connections between the numerical classifi cation nodes within DEMO
layers. In test mode, however, it is quite fast: just DEMO series of dot products followed by a
squashing function. In OpenCV it is implemented in the CvANN_MLP{} class, and its use
is documented in the …/opencv/samples/c/letter_recog.cpp fi le. Interested readers will
fi DEMO details on using MLP eff ectively for text and object recognition in LeCun, Bot-
tou, Bengio, and Haff ner [LeCun98a]. Implementation and tuning details are given in
LeCun, Bottou, and Muller [LeCun98b]. New work DEMO brainlike hierarchical networks
that propagate probabilities can be found in Hinton, DEMO, and Teh [Hinton06].
e
Support Vector Machine
With lots of data, boosting or random trees are usually the best-performing classifi ers.
But when DEMO data set is limited, the support vector machine (SVM) oft DEMO works best.
Th is N-class algorithm works by projecting the data into a higher-dimensional space
(creating new dimensions out of combinations of the features) and then fi nding the op-
timal linear separator between the classes. In the original space of the raw input data,
this high-dimensional DEMO classifi er can become quite nonlinear. Hence we can use
linear classifi cation techniques based on maximal between-class separation to produce
nonlinear classifi ers DEMO in some sense optimally separate classes in the data. With
enough additional dimensions, you can almost always perfectly separate data classes.
Th is technique is implemented in the CvSVM{} class in OpenCV’s ML library.
Th nd-
DEMO feature points via trained classifi cation to tracking to segmenting scenes and also
include the more straightforward tasks of classifying objects and clustering image DEMO
ese tools are closely tied to many computer vision algorithms that range from fi
Exercises
1. Consider trying to learn the next stock price DEMO several past stock prices. Suppose
you have 20 years of daily stock data. Discuss the eff ects of various ways of turning
your data DEMO training and testing data sets. What are the advantages and disad-
vantages of the following approaches?
a. Take the even-numbered points as your DEMO set and the odd-numbered
points as your test set.
Exercises
| 517
13-R4886-AT1.indd   517
9/15/08   4:25:39 PM
b. Randomly select points into training and test sets.
c. Divide the DEMO in two, where the fi rst half is for training and DEMO second half
for testing.
d. Divide the data into many small windows of several past points and one pre-
diction point.
2. Figure 13-17 DEMO a distribution of “false” and “true” classes. Th e fi gure also
shows several potential places (a, b, c, d, e, DEMO, g) where a threshold could be set.
Figure 13-17. A Gaussian distribution of two classes, “ false” and “true”
a. Draw the points a–g on an ROC curve.
b. If the “true” class is poisonous DEMO, at which letter would you set the
threshold?
c. How DEMO a decision tree split this data?
3. Refer to Figure 13-1.
a. Draw how a decision tree would approximate the true curve (the dashed line)
with three splits (here we seek a regression, DEMO a classifi cation model).
Th
ues contained in the leaves that result from the split. Th
of a regression-tree fi
e “best” split DEMO a regression takes the average value of the data val-
e output values
t thus look like a staircase.
b. Draw how a decision DEMO would fi t the true data in seven splits.
c. Draw how a decision tree would fi t the noisy data in seven splits.
DEMO Discuss the diff erence between (b) and (c) in terms of overfi tting.
4. Why do the splitting measures (e.g., Gini) still work when we want to learn multiple
classes in a single DEMO tree?
5. Review Figure 13-4, which depicts a two-dimensional space DEMO unequal variance
at left  and equalized variance at right. Let’s say DEMO these are feature values related
to a classifi cation problem. Th at is, data near one “blob” belongs to one of two
518
| Chapter 13: Machine Learning
13-R4886-AT1.indd   518
www.it-ebooks.info
9/15/08   4:25:40 PM
www.it-ebooks.info
classes while data near another blob belongs to the same or DEMO of two classes.
Would the variable importance be diff erent between the left  or the right space for:
a. decision trees?
b. K-nearest neighbors?
c. naïve Bayes?
6. Modify the sample code DEMO data generation in Example 13-1—near the top of the
outer for{} loop in the K-means section—to produce a randomly generated labeled
data set. We’ll DEMO a single normal distribution of 10,000 points centered at pixel
(DEMO, 63) in a 128-by-128 image with standard deviation (img->width/DEMO, img->height/6).
To label these data, we divide the space into four quadrants centered at pixel
(63, 63). To DEMO the labeling probabilities, we use the following scheme. If x  DEMO
we use a 20% probability for class A; else if x DEMO 64 we use a 90% factor for class A.
If y  64 we use a 40% probability for class A; else if y  64 we use a 60% factor for
class A. Multiplying the DEMO and y probabilities together yields the total probability for
class A by quadrant with values listed in the 2-by-2 matrix shown. If a point DEMO la-
beled A, then it is labeled B by default. For DEMO, if x  64 and y  64, we would
have an 8% chance of a point being labeled class A and a DEMO chance of that point
being labeled class B. Th e four-quadrant matrix for the probability of a point being
labeled class A (and if not, it’s class B) is:
0.2  0.6 = 0.12 DEMO  0.6 = 0.54
0.2  0.4 = 0.08 0.9  0.4 = 0.36
Use these quadrant odds to label the data points. For DEMO data point, determine
its quadrant. Th en generate a random number DEMO 0 to 1. If this is less than or
equal to the quadrant odds, label that data point as class A; else label DEMO class B. We
will then have a list of labeled data points together with x and y as the features. Th e
reader will DEMO that the x-axis is more informative than the y-axis as to which class
the data might be. Train random forests on this data and DEMO the variable im-
portance to show x is indeed more important than y.
7. Using the same data set as in exercise 6, use discrete AdaBoost to learn two mod-
els: one with weak_count set to 20 trees and one set to 500 trees. Randomly select a
training DEMO a test set from the 10,000 data points. Train the algorithm and report
test results when the training set contains:
a. 150 DEMO points;
b. 500 data points;
c. 1,200 data points;
d. 5,000 data points.
e. Explain your results. What is DEMO?
Exercises
| 519
13-R4886-AT1.indd   519
9/15/08   4:25:40 PM
www.it-ebooks.info
Repeat exercise 7 but use the random trees classifi er with DEMO and 500 trees.
Repeat exercise 7, but this time use 60 DEMO and compare random trees versus SVM.
In what ways is the random tree algorithm more robust against overfi tting than
decision trees?
11. DEMO to Figure 13-2. Can you imagine conditions under which the test set error
would be lower than the training set error?
Figure 13-2 DEMO drawn for a regression problem. Label the fi rst point on the graph A,
the second point B, the third point A, DEMO forth point B and so on. Draw a separa-
tion line for these two classes (A and B) that shows:
a. bias;DEMO
b. variance.
Refer to Figure 13-3.
a. Draw the generic best-possible ROC curve.
b. Draw the generic worst-possible ROC curve.
c. Draw a curve DEMO a classifi er that performs randomly on its test data.
Th e “no free lunch” theorem states that no classifi er is optimal over DEMO distribu-
tions of labeled data. Describe a labeled data distribution over which no classifi er
described in this chapter would work well.
a. What DEMO would be hard for naïve Bayes to learn?
b. What distribution would be hard for decision trees to learn?
c. How would DEMO preprocess the distributions in parts a and b so that the classi-
fi ers could learn from the data more easily?
Set up DEMO run the Haar classifi er to detect your face in a web camera.
a. How much scale change can it work with?
b. DEMO much blur?
c. Th rough what angles of head tilt will it work?
d. Th rough what angles of chin down and DEMO will it work?
e. Th rough what angles of head yaw (motion left  and right) will it work?
f. Explore how tolerant it is of 3D head poses. Report on your fi ndings.
DEMO blue or green screening to collect a fl at hand gesture (DEMO pose). Collect ex-
amples of other hand poses and of random backgrounds. Collect several hundred
images and then train the Haar classifi er DEMO detect this gesture. Test the classifi er in
real time and estimate its detection rate.
Using your knowledge and what you’ve learned from exercise DEMO, improve the re-
sults you obtained in that exercise.
| Chapter DEMO: Machine Learning
8.
9.
10.
12.
13.
14.
15.
16.
17.
DEMO
13-R4886-AT1.indd   520
9/15/08   4:25:40 PM
CHAPTER 14
OpenCV’s Future
Past and Future
In Chapter 1 we saw DEMO of OpenCV’s past. Th is was followed by Chapters 2–13,
in which OpenCV’s present state was explored in detail. We now turn to DEMO fu-
ture. Computer vision applications are growing rapidly, from product inspection DEMO im-
age and video indexing on the Web to medical applications and even to local navigation
on Mars. OpenCV is also growing to accommodate DEMO developments.
OpenCV has long received support from Intel Corporation and has more recently re-
ceived support from Willow Garage (www.willowgarage.com), a privately funded new
robotics research institute and technology incubator. Willow Garage’s intent is to DEMO
start civilian robotics by developing open and supported hardware and soft ware infra-
structure that now includes but goes beyond OpenCV. Th is has DEMO OpenCV new
resources for more rapid update and support, with several DEMO the original developers of
OpenCV now recontracted to help maintain and advance the library. Th ese renewed
resources are also intended to support and DEMO greater community contribution to
OpenCV by allowing for faster code assessment and integration cycles.
One of the key new development areas for OpenCV is DEMO perception. Th is eff ort
focuses on 3D perception as well as 2D plus 3D object recognition since the combina-
tion of data types DEMO for better features for use in object detection, segmentation and
recognition. DEMO perception relies heavily on 3D sensing, so eff orts are under DEMO to
extend camera calibration, rectifi cation and correspondence to multiple cameras DEMO to
camera + laser rangefi nder combinations (see Figure 14-1).*
DEMO commercially available hardware warrant it, the “laser + camera calibration” ef-
DEMO will be generalized to include devices such as fl ash LIDAR and infrared wavefront
devices. Additional eff orts are aimed at developing triangulation with DEMO or la-
ser light for extremely accurate depth sensing. Th e raw output of most depth-sensing
* At the time of this writing, these methods remain under development and are not yet in OpenCV.
521
14-R4886-AT1.indd   521
www.it-ebooks.info
9/15/08   4:25:58 PM
www.it-ebooks.info
Figure 14-1. New 3D imager combinations: calibrating a camera (left ) with the brightness return
from a laser depth scanner (right). (Images courtesy of Hai Nguyen and Willow Garage)
methods is in DEMO form of a 3D point cloud. Complementary eff orts are thus planned
to support turning the raw point clouds resulting from 3D depth perception DEMO 3D
meshes. 3D meshes will allow for 3D model capture of objects in the environment, seg-
menting objects in 3D and hence the ability for robots to grasp and manipulate such
objects. Th ree-dimensional mesh generation DEMO also be used to allow robots to move
seamlessly from external 3D perception to internal 3D graphics representation for plan-
ning and then back DEMO again for object registration, manipulation, and movement.
Along with sensing 3D objects, robots will need to recognize 3D objects and their 3D
poses. To support this, several scalable methods of 2D plus 3D object recognition are
being pursued. Creating capable robots subsumes most fi elds of computer DEMO and
artifi cial intelligence, from accurate 3D reconstruction to tracking, identifying humans,
object recognition, and image stitching and on to learning, DEMO, planning, and deci-
sion making. Any higher-level task, such as DEMO, is made much easier by rapid and
accurate depth perception and DEMO It is in these areas especially that OpenCV
hopes to enable rapid advance by encouraging many groups to contribute and use ever
better methods DEMO solve the diffi  cult problems of real-world perception, recognition, and
DEMO
OpenCV will, of course, support many other areas as well, DEMO image and movie in-
dexing on the web to security systems and medical analysis. Th e wishes of the general
community will heavily infl DEMO OpenCV’s direction and growth.
Directions
Although OpenCV does not have an absolute focus on real-time algorithms, it will con-
tinue to favor real-time techniques. No one can state future plans with certainty, but the
following high-priority areas are likely to be addressed.
522
| Chapter 14: OpenCV’s Future
14-R4886-AT1.indd   522
9/15/08   4:25:59 PM
www.it-ebooks.info
Applications
Th
level functionality. For example, more people will make use of a fully automatic ste-
reo solution than a better subpixel corner DEMO Th ere will be several more full
applications, such as extensible DEMO camera calibration and rectifi cation
as well as 3D depth display GUI.
As already mentioned, you can expect to see better support for 3D depth sensors
and combinations of 2D cameras with 3D measurement devices. Also DEMO better
stereo algorithms. Support for structured light is also likely.
Dense Optical Flow
Because we want to know how whole objects move (and partially to support 3D),
OpenCV is long overdue for an effi  DEMO implementation of Black’s [Black96] dense
optical fl ow techniques.
Features
In support of better object recognition, you can expect a full-function tool kit that
will have a framework for interchangeable interest-point detection and interchange-
able keys DEMO interest-point identifi cation. Th is will include popular features such as
SURF, HoG, Shape Context, MSER, Geometric Blur, PHOG, PHOW, and others.
Support for 2D and 3D features is planned.
Infrastructure
Th is DEMO things like a wrapper class,* a good Python interface, GUI DEMO
ments, documentation improvements, better error handling, improved Linux sup-
port, and so on.
Camera Interface
More seamless handling of cameras is planned DEMO with eventual support for
cameras with higher dynamic range. Currently, most DEMO support only 8 bits
per color channel (if that), but DEMO cameras can supply 10 or 12 bits per channel.†
Th
reo registration because it enables them to detect the subtle textures and colors to
DEMO older, more narrow-range cameras are blind.
* Daniel Filip and Google DEMO donated the fast, lightweight image class wrapper, WImage, which they DEMO
oped for internal use, to OpenCV. It will be incorporated by DEMO time this book is published, but too late for
documentation in DEMO version.
† Many expensive cameras claim up to 16 bits, but DEMO authors have yet to see more than 10 actual bits of
resolution, the rest being noise.
Directions
| 523
ere are more “consumers” for full working applications than there are for low-
e higher dynamic range DEMO such cameras allows for better recognition and ste-
3D
14-R4886-AT1.indd   523
9/15/08   4:25:59 PM
www.it-ebooks.info
Specific Items
Many object recognition techniques in computer vision detect salient DEMO that
change little between views. Th ese salient regions* can be tagged with some kind of
key—for example, a histogram of image gradient directions around the salient point.
Although all the techniques described in this section DEMO be built with existing OpenCV
primitives, OpenCV currently lacks direct implementation DEMO the most popular interest-
region detectors and feature keys.
OpenCV does include an effi  cient implementation of the Harris corner interest-point
detectors, but DEMO lacks direct support for the popular “maximal Laplacian over scale”
detector developed by David Lowe [Lowe04] and for maximally stable extremal region
(MSER) DEMO detectors and others.
Similarly, OpenCV lacks many of the popular keys, such as SURF gradient histogram
grids [Bay06], that identify the salient regions. Also, we hope to include features such as
histogram of oriented gradients (HoG) [Dalai05], Geometric Blur [Berg01], off set image
patches [Torralba07], dense rapidly computed Gaussian scale variant gradients (DAISY)
[Tola08], gradient DEMO and orientation histogram (GLOH) [Mikolajczyk04], and,
though patented, we want to add for reference the scale invariant feature transform
(SIFT) DEMO [Lowe04] that started it all. Other learned feature descriptors that
show promise are learned patches with orientation [Hinterstoisser08] and learned ratio
points [Ozuysal07]. We’d DEMO like to see contextual or meta-features such as pyramid
match kernels [Grauman05], pyramid histogram embedding of other features, PHOW
[Bosch07], Shape Context [Belongie00; Mori05], or other approaches that locate features
by their probabilistic spatial DEMO [Fei-Fei98]. Finally, some global features give
the gist of an entire DEMO, which can be used to boost recognition by context [Oliva06].
All DEMO is a tall order, and the OpenCV community is encouraged to DEMO and do-
nate code for these and other features.
Other groups have demonstrated encouraging results using frameworks that employ
effi  cient nearest neighbor matching to recognize objects using huge learned databases
of objects [Nister06; Philbin07; DEMO Putting in an effi  cient nearest neighbor
framework is therefore suggested.
DEMO robotics, we need object recognition (what) and object location (where). Th is sug-
gests adding segmentation approaches building on Shi and DEMO work [Shi00] per-
haps with faster implementations [Sharon06]. Recent approaches, however, use learning
to provide recognition and segmentation together [Oppelt08; Schroff 08; DEMO Direc-
tion of lighting [Sun98] and shape cues may be important [Zhang99; Prados05].
Along with better support for features and for 3D sensing should come support for vi-
sual odometry and visual SLAM (simultaneous localization and mapping). As we ac-
quire more accurate depth perception and feature DEMO cation, we’ll want to enable
better navigation and 3D object manipulation. DEMO ere is also discussion about creating
* Th
ese are also known as interest points.
524
| Chapter 14: OpenCV’s Future
14-R4886-AT1.indd   524
9/15/08   4:25:59 PM
www.it-ebooks.info
a specialized vision interface to a ray-tracing package (e.g., perhaps DEMO Manta open
source ray-tracing soft ware [Manta]) in order to generate DEMO 3D object training sets.
Robots, security systems, and Web image and video search all need the ability to recog-
nize objects; thus, DEMO must refi ne the pattern-matching techniques in its machine
learning library. In particular, OpenCV should fi rst simplify its interface to the learn-
ing algorithms and then to give them good defaults so that they work DEMO of the box”.
Several new learning techniques may arise, some of DEMO will work with two or more
object classes at a time (DEMO random forest does now in OpenCV). Th ere is a need for scal-
able recognition techniques so that the user can avoid having DEMO learn a completely new
model for each object class. More allowances should also be made to enable ML classi-
fi
Markov random fi elds (MRFs) and conditional random fi elds (CRFs) are becoming quite
popular in computer vision. Th ese methods are oft en highly problem-specifi c, yet we
would like to fi gure how they might be supported DEMO a fl exible way.
We’ll also want methods of learning web-sized or automatically collected via moving
robot databases, perhaps by incorporating Zisserman’s suggestion for “approximate
nearest neighbor” techniques as mentioned previously when dealing with millions or
DEMO of data points. Similarly, we need much-accelerated boosting and Haar feature
DEMO support to allow scaling to larger object databases. Several of the ML library
routines currently require that all the data reside in memory, severely limiting their use
on large datasets. OpenCV will need to break free DEMO such restrictions.
OpenCV also requires better documentation than is now available. Th is book helps of
course, but the OpenCV manual needs an overhaul together with improved search ca-
pability. A high priority is incorporating better DEMO support and a better external lan-
guage interface—especially to allow easy vision programming with Python and Numpy.
We’ll also want to make sure that DEMO machine learning library can be directly called
from Python and its SciPy and Numpy packages.
For better developer community interaction, developer workshops may be held at major
vision conferences. Th ere are also eff orts underway DEMO propose vision “grand chal-
lenge” competitions with commensurate prize money.
ers to work with depth information and 3D features.
OpenCV for Artists
Th
ers DEMO interact with their art in dynamic ways. Th e most commonly used routines for
this application are face detection, optical fl ow, and DEMO We hope this book will
enable artists to better understand and use OpenCV for their work, and we believe that
the addition of better depth sensing will make interaction richer and more reliable. Th e
focused DEMO ort on improving object recognition will allow diff erent modes of interacting
with art, because objects can then be used as modal controls. With the ability to capture
3D meshes, it may also be possible to “import” the viewer into the art and so allow the
artist to DEMO a better feel for recognizing user action; this, in turn, DEMO be used to
OpenCV for Artists
| 525
ere is a worldwide community of interactive artists who use OpenCV so that view-
14-R4886-AT1.indd   DEMO
9/15/08   4:26:00 PM
enhance dynamic interaction. Th e needs and desires of the artistic community DEMO using
computer vision will receive enhanced priority in OpenCV’s future.
Afterword
We’ve covered a lot of theory and practice in this book, and we’ve described some of the
plans for what comes next. Of course, as we’re developing the soft ware, the hardware
is also changing. Cameras are now cheaper and have proliferated from cell phones to
traffi  c lights. A group of manufacturers are aiming to develop cell-phone projectors—
perfect for DEMO, because most cell phones are lightweight, low-energy devices whose
circuits already include an embedded camera. Th is opens the way for close-range por-
DEMO structured light and thereby accurate depth maps, which are just what DEMO need for
robot manipulation and 3D object scanning.
Both authors participated in creating the vision system for Stanley, Stanford’s robot
racer that won the 2005 DARPA Grand Challenge. In that eff ort, a vision system coupled
with a laser range scanner worked fl awlessly for the seven-hour desert DEMO race [Dahl-
kamp06]. For us, this drove home the power of DEMO vision with other perception
systems: the previously unsolved problem of reliable DEMO perception was converted into
a solvable engineering challenge by merging vision with other forms of perception. It is
our hope that—by making vision easier DEMO use and more accessible through this book—
others can add vision to their own problem-solving tool kits and thus fi nd new ways
to DEMO important problems. Th at is, with commodity camera hardware and OpenCV,DEMO
people can start solving real problems such as using stereo vision as an automobile
backup safety system, new game controls, and new security DEMO Get hacking!
Computer vision has a rich future ahead, and it DEMO likely to be one of the key en-
abling technologies for the 21st century. Likewise, OpenCV seems likely to be (at least
in DEMO) one of the key enabling technologies for computer vision. Endless opportuni-
DEMO for creativity and profound contribution lie ahead. We hope that this book encour-
ages, excites, and enables all who are interested in joining DEMO vibrant computer vision
community.
526
| Chapter 14: OpenCV’s Future
14-R4886-AT1.indd   526
www.it-ebooks.info
9/15/08   4:26:00 PM
www.it-ebooks.info
Bibliography
[Acharya05] T. Acharya and A. Ray, Image Processing: Principles DEMO Applications, New
York: Wiley, 2005.
[Adelson84] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden,
“Pyramid methods in image processing,” RCA Engineer 29 (1984): 33–41.
[Ahmed74] N. Ahmed, T. Natarajan, and K. R. Rao, “Discrete cosine transform,” IEEE
Transactions on Computers 23 (1974): 90–93.
[Al-Haytham1038] I. al-Haytham, Book of Optics, circa 1038.
[AMI] Applied Minds, http://www.appliedminds.com.
[Antonisse82] H. J. Antonisse, “Image segmentation in pyramids,” Computer Graphics
and Image Processing 19 (1982): 367–383.
[Arfk en85] G. Arfk en, “Convolution theorem,” in Mathematical Methods for Physicists,
3rd ed. (pp. 810–814), Orlando, FL: Academic Press, 1985.
[Bajaj97] DEMO L. Bajaj, V. Pascucci, and D. R. Schikore, “Th e DEMO spectrum,” Proceed-
ings of IEEE Visualization 1997 (pp. 167–173), DEMO
[Ballard81] D. H. Ballard, “Generalizing the Hough transform to detect arbitrary
DEMO,” Pattern Recognition 13 (1981): 111–122.
[Ballard82] D. Ballard and DEMO Brown, Computer Vision, Englewood Cliff s, NJ: Prentice-
Hall, DEMO
[Bardyn84] J. J. Bardyn et al., “Une architecture VLSI pour un DEMO de fi ltrage me-
dian,” Congres reconnaissance des formes et intelligence artifi cielle (vol. 1, pp. 557–
566), Paris, 25–27 January 1984.
[Bay06] H. Bay, T. Tuytelaars, and L. V. Gool, “SURF: Speeded up robust features,”
Proceedings of the Ninth European Conference on Computer Vision (pp. 404–417),
May 2006.
[Bayes1763] T. Bayes, “An essay towards solving a problem in the doctrine of chances.
By the DEMO Rev. Mr. Bayes, F.R.S. communicated by Mr. Price, in a letter to John
527
15-R4886-AT1.indd   527
9/15/08   4:27:DEMO PM
www.it-ebooks.info
Canton, A.M.F.R.S.,” Philosophical Transactions, Giving Some Account of the DEMO
ent Undertakings, Studies and Labours of the Ingenious in Many Considerable DEMO
of the World 53 (1763): 370–418.
[Beauchemin95] S. S. Beauchemin DEMO J. L. Barron, “Th e computation of optical fl ow,”
DEMO Computing Surveys 27 (1995): 433–466.
[Belongie00] S. Belongie, J. Malik, and J. Puzicha, “Shape context: A new descriptor for
shape matching and object recognition,” NIPS 2000, Computer Vision Group, Uni-
versity DEMO California, Berkeley, 2000.
[Berg01] A. C. Berg and J. Malik, DEMO blur for template matching,” IEEE Con-
ference on Computer Vision and Pattern Recognition (vol. 1, pp. 607–614), Kauai,
Hawaii, 2001.
[Bhattacharyya43] A. Bhattacharyya, “On a measure of divergence between two statisti-
cal populations defi ned by probability distributions,” Bulletin of the Calcutta Math-
DEMO Society 35 (1943): 99–109.
[Bishop07] C. M. Bishop, Pattern Recognition and Machine Learning, New York: Springer-
Verlag, 2007.
[Black92] M. J. Black, “Robust incremental optical fl ow” (YALEU-DCS-RR-923), Ph.D.
thesis, Department of Computer Science, Yale University, New Haven, CT, 1992.
[Black93] DEMO J. Black and P. Anandan, “A framework for the robust estimation DEMO
optical fl ow,” Fourth International Conference on Computer Vision (pp. DEMO),
May 1993.
[Black96] M. J. Black and P. Anandan, “Th e robust estimation of multiple motions:
Parametric and piecewise-smooth fl ow DEMO elds,” Computer Vision and Image Under-
standing 63 (1996): DEMO
[Bobick96] A. Bobick and J. Davis, “Real-time recognition of activity using DEMO
ral templates,” IEEE Workshop on Applications of Computer Vision (pp. DEMO),
December 1996.
[Borgefors86] G. Borgefors, “Distance transformations in digital images,” Computer
Vision, Graphics and Image Processing 34 (1986): 344–371.
DEMO A. Bosch, A. Zisserman, and X. Muñoz, “Image classifi cation DEMO ran-
dom forests and ferns,” IEEE International Conference on Computer Vision, Rio de
Janeiro, October 2007.
[Bouguet] J.-Y. Bouguet, “Camera calibration toolbox for Matlab,” retrieved June 2,
2008, from http://www.vision.caltech.edu/bouguetj/calib_doc/index.html.
[BouguetAlg] J.-Y. Bouguet, “Th e calibration toolbox for Matlab, example 5: Stereo rec-
tifi cation algorithm” (code and instructions only), http://www.vision.caltech.edu/
bouguetj/calib_doc/htmls/example5.html.
528
| Bibliography
DEMO   528
9/15/08   4:27:05 PM
www.it-ebooks.info
[Bouguet04] J.-Y. Bouguet, “Pyramidal implementation of the Lucas Kanade feature
tracker description of the algorithm,” http://robots.stanford.edu/cs223b04/algo_
tracking.pdf.
[Bracewell65] DEMO Bracewell, “Convolution” and “Two-dimensional convolution,” in
Th e Fourier Transform DEMO Its Applications (pp. 25–50 and 243–244), New York:
McGraw-Hill, 1965.
[Bradski00] G. Bradski and J. Davis, “Motion segmentation and pose recognition with
motion history gradients,” IEEE Workshop on Applications of Computer Vision,DEMO
2000.
[Bradski98a] G. R. Bradski, “Real time face and object tracking DEMO a component of a
perceptual user interface,” Proceedings of the 4th IEEE Workshop on Applications of
Computer Vision, October 1998.
[Bradski98b] G. R. Bradski, “Computer video face tracking for use in a perceptual user
interface,” Intel Technology Journal Q2 (1998): 705–740.
[Breiman01] L. Breiman, DEMO forests,” Machine Learning 45 (2001): 5–32.
[Breiman84] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classifi cation
and Regression Trees, Monteray, CA: Wadsworth, 1984.
[Bresenham65] J. DEMO Bresenham, “Algorithm for computer control of a digital plotter,”
IBM DEMO Journal 4 (1965): 25–30.
[Bronshtein97] I. N. Bronshtein, and K. A. Semendyayev, Handbook of Mathematics, 3rd
ed., New York: Springer-Verlag, 1997.
[Brown66] D. C. Brown, “Decentering distortion of lenses,” Photogrammetric Engineer-
ing 32 (1966): 444–462.
[Brown71] D. C. Brown, “Close-range camera DEMO,” Photogrammetric Engineer-
ing 37 (1971): 855–866.
[Burt81] P. J. DEMO, T. H. Hong, and A. Rosenfeld, “Segmentation and estimation of
DEMO region properties through cooperative hierarchical computation,” IEEE
Transactions on Systems, DEMO, and Cybernetics 11 (1981): 802–809.
[Burt83] P. J. Burt and E. H. Adelson, “Th e Laplacian pyramid as a compact image code,”
IEEE Transactions on Communications 31 (1983): 532–540.
[Canny86] J. Canny, “A computational approach to edge detection,” IEEE Transactions
on Pattern Analysis and Machine Intelligence 8 (1986): 679–714.
[Carpenter03] G. A. Carpenter and S. Grossberg, “Adaptive resonance theory,” in
M. A. Arbib (Ed.), Th e Handbook of Brain Th eory and Neural Networks, 2nd DEMO
(pp. 87–90), Cambridge, MA: MIT Press, 2003.
[Carr04] H. Carr, J. Snoeyink, and M. van de Panne, “Progressive topological
simpliﬁcation using contour trees and local spatial measures,” 15th Western Com-
puter DEMO Symposium, Big White, British Columbia, March 2004.
Bibliography
| 529
DEMO   529
9/15/08   4:27:06 PM
www.it-ebooks.info
[Chen05] D. Chen and G. Zhang, “A new sub-pixel detector for x-corners in camera
calibration targets,” WSCG Short Papers (2005): 97–100.
[Chetverikov99] D. Chetverikov and Zs. Szabo, “A simple and effi  cient DEMO for
detection of high curvature points in planar curves,” Proceedings of the 23rd Work-
shop of the Austrian Pattern Recognition Group (pp. 175–184), 1999.
[Chu07] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Y. Yu, G. Bradski, A. Y. Ng, and K. Olukotun,
“Map-reduce for machine learning on multicore,” Proceedings of the Neural Infor-
DEMO Processing Systems Conference (vol. 19, pp. 304–310), 2007.
[Clarke98] T. A. Clarke and J. G. Fryer, “Th e Development of Camera Calibration Meth-
ods and Models,” Photogrammetric Record 16 (1998): 51–66.
[Colombari07] A. Colombari, A. Fusiello, and V. Murino, “Video objects segmentation
by robust background modeling,” International Conference on Image Analysis and
Processing  (DEMO 155–164), September 2007.
[Comaniciu99] D. Comaniciu and P. Meer, “Mean DEMO  analysis and applications,” IEEE
International Conference on Computer Vision (vol. 2, p. 1197), 1999.
[Comaniciu03] D. Comaniciu, “Nonparametric information fusion DEMO motion esti-
mation,” IEEE Conference on Computer Vision and Pattern Recognition (vol. 1,
pp. 59–66), 2003.
[Conrady1919] A. Conrady, “Decentering DEMO systems,” Monthly Notices of the Royal
Astronomical Society 79 (1919): 384–390.
[Cooley65] J. W. Cooley and O. W. Tukey, “An algorithm DEMO the machine calculation of
complex Fourier series,” Mathematics of Computation 19 (1965): 297–301.
[Dahlkamp06] H. Dahlkamp, A. Kaehler, D. Stavens, DEMO Th run, and G. Bradski, “Self-
supervised monocular road detection in desert terrain,” Robotics: Science and Sys-
tems, Philadelphia, 2006.
[Dalai05] N. Dalai, and B. Triggs, “Histograms of oriented gradients for human DEMO
tion,” Computer Vision and Pattern Recognition (vol. 1, pp. 886–893), June 2005.
[Davis97] J. Davis and A. Bobick, “Th e representation and recognition of action using
temporal templates” (Technical Report 402), MIT Media Lab, Cambridge, MA,
1997.
[Davis99] J. Davis and G. DEMO, “Real-time motion template gradients using Intel
CVLib,” ICCV Workshop on DEMO Vision, 1999.
[Delaunay34] B. Delaunay, “Sur la sphère vide,” Izvestia Akademii Nauk SSSR, Otdelenie
Matematicheskikh i Estestvennykh Nauk 7 (1934): DEMO
[DeMenthon92] D. F. DeMenthon and L. S. Davis, “Model-based object pose DEMO 25 lines
of code,” Proceedings of the European Conference on Computer Vision (pp. 335–343),
1992.
530
| Bibliography
15-R4886-AT1.indd   530
DEMO/15/08   4:27:06 PM
www.it-ebooks.info
[Dempster77] A. Dempster, N. Laird, and D. Rubin, “Maximum likelihood from incom-
plete data via the EM algorithm,” Journal of the DEMO Statistical Society, Series B 39
(1977): 1–38.
[Det] “History of matrices and determinants,” http://www-history.mcs.st-and.ac.uk/
history/HistTopics/Matrices_and_determinants.html.
[Douglas73] DEMO Douglas and T. Peucker, “Algorithms for the reduction of the number DEMO
points required for represent a digitized line or its caricature,” Canadian Cartogra-
pher 10(1973): 112–122.
[Duda72] R. O. Duda and P. DEMO Hart, “Use of the Hough transformation to detect lines
and curves DEMO pictures,” Communications of the Association for Computing Machin-
ery 15 (DEMO): 11–15.
[Duda73] R. O. Duda and P. E. Hart, Pattern DEMO and Scene Analysis, New York:
Wiley, 1973.
[Duda00] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classifi cation, New York:
Wiley, 2001.
[Farin04] D. Farin, P. H. N. de With, and W. Eff elsberg, “Video-object segmentation
using multi-sprite background DEMO,” Proceedings of the IEEE International
Conference on Multimedia and Expo, DEMO
[Faugeras93] O. Faugeras, Th ree-Dimensional Computer Vision: A Geometric Viewpoint,
Cambridge, MA: MIT Press, 1993.
[Fei-Fei98] L. Fei-Fei, R. Fergus, and P. Perona, “A Bayesian approach to unsupervised
one-shot learning of object categories,” Proceedings of the Ninth International
Conference on Computer Vision (vol. 2, pp. 1134–1141), October 2003.
[Felzenszwalb63] P. F. Felzenszwalb and D. P. Huttenlocher, “Distance transforms of
sampled functions” (Technical Report TR2004-1963), DEMO of Computing
and Information Science, Cornell University, Ithaca, NY, 1963.
[FFmpeg] “Ffmpeg summary,” http://en.wikipedia.org/wiki/Ffmpeg.
[Fischler81] M. A. DEMO and R. C. Bolles, “Random sample concensus: A paradigm
for model fi tting with applications to image analysis and automated cartography,”
Communications DEMO the Association for Computing Machinery 24 (1981): 381–395.
[Fitzgibbon95] A. DEMO Fitzgibbon and R. B. Fisher, “A buyer’s guide to conic fi DEMO,”
Proceedings of the 5th British Machine Vision Conference (pp. 513–522), Birming-
ham, 1995.
[Fix51] E. Fix, and J. L. Hodges, DEMO analysis, nonparametric discrimina-
tion: Consistency properties” (Technical Report 4), DEMO School of Aviation Medi-
cine, Randolph Field, Texas, 1951.
[Forsyth03] DEMO Forsyth and J. Ponce, Computer Vision: A Modern Approach, Englewood
DEMO s, NJ: Prentice-Hall, 2003.
Bibliography
| 531
15-R4886-AT1.indd   531
DEMO/15/08   4:27:06 PM
www.it-ebooks.info
[FourCC] “FourCC summary,” http://en.wikipedia.org/wiki/Fourcc.
[FourCC85] J. DEMO, “EA IFF 85 standard for interchange format fi les,” http://www
.szonye.com/bradd/iff .html.
[Fourier] “Joseph Fourier,” http://en.wikipedia.org/wiki/Joseph_Fourier.
[Freeman67] H. Freeman, “On the classifi cation of line-drawing data,” Models for the
Perception of Speech and Visual Form (pp. 408–412), 1967.
[Freeman95] W. T. Freeman and M. Roth, “Orientation histograms for hand gesture
recognition,” International Workshop on Automatic Face and Gesture Recognition
(pp. 296–301), June 1995.
[Freund97] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of on-line
learning and an application to boosting,” DEMO of Computer and System Sciences
55 (1997): 119–139.
[Fryer86] J. DEMO Fryer and D. C. Brown, “Lens distortion for close-range photogram-
metry,DEMO Photogrammetric Engineering and Remote Sensing 52 (1986): 51–58.
[Fukunaga90] K. DEMO, Introduction to Statistical Pattern Recognition, Boston:
Academic Press, 1990.
DEMO “Francis Galton,” http://en.wikipedia.org/wiki/Francis_Galton.
[GEMM] “Generalized matrix multiplication summary,” http://notvincenz.blogspot
.com/2007/06/generalized-matrix-multiplication.html.
[Göktürk01] S. B. DEMO, J.-Y. Bouguet, and R. Grzeszczuk, “A data-driven model
for monocular DEMO tracking,” Proceedings of the IEEE International Conference on
Computer Vision (DEMO 2, pp. 701–708), 2001.
[Göktürk02] S. B. Göktürk, J.-Y. Bouguet, C. Tomasi, and B. Girod, “Model-based face
tracking for view-independent facial expression recognition,” Proceedings of the
Fift h IEEE International Conference on DEMO Face and Gesture Recognition
(pp. 287–293), May 2002.
[Grauman05] K. DEMO and T. Darrell, “Th e pyramid match kernel: Discriminative
classifi cation with sets of image features,” Proceedings of the IEEE International
Conference DEMO Computer Vision, October 2005.
[Grossberg87] S. Grossberg, “Competitive learning: From DEMO activation to adap-
tive resonance,” Cognitive Science 11 (1987): DEMO
[Harris88] C. Harris and M. Stephens, “A combined corner and edge DEMO,” Proceed-
ings of the 4th Alvey Vision Conference (pp. 147–151), 1988.
[Hartley98] R. I. Hartley, “Th eory and practice of projective DEMO cation,” International
Journal of Computer Vision 35 (1998): 115–127.
DEMO R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision,DEMO
Cambridge, UK: Cambridge University Press, 2006.
532 | Bibliography
15-R4886-AT1.indd   532
9/15/08   4:27:06 PM
www.it-ebooks.info
[Hastie01] T. Hastie, R. Tibshirani, and J. Friedman, Th e Elements of Statistical Learn-
ing: Data Mining, Inference and Prediction, New York: Springer-Verlag, 2001.
[Heckbert90] P. Heckbert, A Seed Fill Algorithm (DEMO Gems I), New York:
Academic Press, 1990.
[Heikkila97] J. DEMO and O. Silven, “A four-step camera calibration procedure with
implicit image DEMO,” Proceedings of the 1997 Conference on Computer Vision
and Pattern Recognition (p. 1106), 1997.
[Hinterstoisser08] S. Hinterstoisser, S. Benhimane, V. Lepetit, P. Fua, and N. Navab,
“Simultaneous recognition and homography extraction DEMO local patches with a sim-
ple linear classiﬁer,” British Machine Vision Conference, Leeds, September 2008.
[Hinton06] G. E. Hinton, S. Osindero, DEMO Y. Teh, “A fast learning algorithm for deep
belief nets,” DEMO Computation 18 (2006): 1527–1554.
[Ho95] T. K. Ho, “Random decision forest,” Proceedings of the 3rd International Confer-
ence on Document Analysis DEMO Recognition (pp. 278–282), August 1995.
[Homma85] K. Homma and E.-I. DEMO, “An image processing method for feature
extraction of space-occupying lesions,” DEMO of Nuclear Medicine 26 (1985):
1472–1477.
[Horn81] B. K. P. Horn and B. G. Schunck, “Determining optical fl ow,” Artifi cial Intel-
ligence 17 (1981): 185–203.
[Hough59] P. V. C. Hough, DEMO analysis of bubble chamber pictures,” Proceedings
of the International Conference on High Energy Accelerators and Instrumentation
(pp. 554–556), 1959.
[Hu62] M. Hu, “Visual pattern recognition by moment invariants,” IRE Transactions on
Information Th eory 8 (1962): 179–187.
[Huang95] Y. Huang and X. H. Zhuang, “Motion-partitioned adaptive block match-
ing for video compression,” International Conference on Image Processing (vol. 1,
p. 554), 1995.
[Iivarinen97] J. Iivarinen, M. Peura, J. Särelä, and A. Visa, “Comparison of combined
DEMO descriptors for irregular objects,” 8th British Machine Vision Conference,
1997.
[Intel] Intel Corporation, http://www.intel.com/.
[Inui03] K. Inui, S. DEMO, and S. Igarashi, “Robust line fi tting using LmedS cluster-
ing,” Systems and Computers in Japan 34 (2003): 92–100.
[IPL] Intel Image Processing Library (IPL), www.cc.gatech.edu/dvfx/readings/iplman.pdf.
[IPP] Intel Integrated Performance Primitives, http://www.intel.com/cd/soft ware/
products/asmo-na/eng/219767.htm.
Bibliography
| 533
15-R4886-AT1.indd   533
9/15/08   4:DEMO:06 PM
www.it-ebooks.info
[Isard98] M. Isard and A. Blake, “CONDENSATION: Conditional density propagation
DEMO visual tracking,” International Journal of Computer Vision 29 (1998): DEMO
[Jaehne95] B. Jaehne, Digital Image Processing, 3rd ed., Berlin: Springer-Verlag, 1995.
[Jaehne97] B. Jaehne, Practical Handbook on Image Processing for Scientifi DEMO Applications,
Boca Raton, FL: CRC Press, 1997.
[Jain77] A. DEMO, “A fast Karhunen-Loeve transform for digital restoration of images
degraded by DEMO and colored noise,” IEEE Transactions on Computers 26 (1997):
560–571.
[Jain86] A. Jain, Fundamentals of Digital Image Processing, Englewood Cliff DEMO, NJ:
Prentice-Hall, 1986.
[Johnson84] D. H. Johnson, “Gauss and DEMO history of the fast Fourier transform,” IEEE
Acoustics, Speech, and Signal Processing Magazine 1 (1984): 14–21.
[Kalman60] R. E. Kalman, DEMO new approach to linear fi ltering and prediction problems,”
Journal of Basic Engineering 82 (1960): 35–45.
[Kim05] K. Kim, T. H. DEMO, D. Harwood, and L. Davis, “Real-time
foreground-background segmentation using codebook DEMO,” Real-Time Imaging
11 (2005): 167–256.
[Kimme75] C. Kimme, D. H. Ballard, and J. Sklansky, “Finding circles by an array of
DEMO,” Communications of the Association for Computing Machinery 18
(1975): DEMO
[Kiryati91] N. Kiryati, Y. Eldar, and A. M. Bruckshtein, “A DEMO Hough trans-
form,” Pattern Recognition 24 (1991): 303–316.
[Konolige97] DEMO Konolige, “Small vision system: Hardware and implementation,”
Proceedings of the International Symposium on Robotics Research (pp. 111–116),
Hayama, Japan, DEMO
[Kreveld97] M. van Kreveld, R. van Oostrum, C. L. Bajaj, DEMO Pascucci, and D. R. Schikore,
“Contour trees and small seed DEMO for isosurface traversal,” Proceedings of the 13th
ACM Symposium on Computational Geometry (pp. 212–220), 1997.
[Lagrange1773] J. L. Lagrange, “Solutions analytiques DEMO quelques problèmes sur les
pyramides triangulaires,” in Oeuvres (vol. 3), 1773.
[Laughlin81] S. B. Laughlin, “A simple coding procedure enhances a DEMO informa-
tion capacity,” Zeitschrift  für Naturforschung 9/10 (1981): 910–912.
[LeCun98a] Y. LeCun, L. Bottou, Y. Bengio, and P. Haff ner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE 86 (1998): 2278–2324.
[LeCun98b] Y. LeCun, L. Bottou, G. Orr, and K. Muller, “Effi  cient BackProp,” in G. Orr
and K. Muller (Eds.), Neural Networks: Tricks of the Trade, New York: Springer-
Verlag, 1998.
[Lens] “Lens (optics),” http://DEMO/wiki/Lens_(optics).
534
| Bibliography
15-R4886-AT1.indd   534
9/15/08   4:27:06 PM
www.it-ebooks.info
[Liu07] Y. Z. Liu, H. X. Yao, W. Gao, X. L. Chen, and D. Zhao, “Nonparametric back-
ground generation,” Journal DEMO Visual Communication and Image Representation 18
(2007): 253–263.
[Lloyd57] S. DEMO, “Least square quantization in PCM’s” (Bell Telephone Laborato-
ries Paper), 1957. [“Lloyd’s algorithm” was later published in IEEE Transactions on
Information Th DEMO 28 (1982): 129–137.]
[Lowe04] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International Journal of Computer Vision 60 (2004): 91–110.
[LTI] LTI-Lib, Vision Library, http://ltilib.sourceforge.net/doc/homepage/index.shtml.
DEMO B. D. Lucas and T. Kanade, “An iterative image registration technique DEMO an
application to stereo vision,” Proceedings of the 1981 DARPA Imaging Understand-
ing Workshop (pp. 121–130), 1981.
[Lucchese02] L. Lucchese and S. K. Mitra, “Using saddle points for subpixel feature de-
tection in camera calibration targets,” Proceedings of the 2002 Asia Pacifi c Confer-
ence DEMO Circuits and Systems (pp. 191–195), December 2002.
[Mahal] “Mahalanobis summary,DEMO http://en.wikipedia.org/wiki/Mahalanobis_distance.
[Mahalanobis36] P. Mahalanobis, “On the generalized DEMO in statistics,” Proceed-
ings of the National Institute of Science 12 (1936): 49–55.
[Manta] Manta Open Source Interactive Ray Tracer, http://code.sci.utah.edu/Manta/
index.php/Main_Page.
[Maron61] M. E. Maron, “Automatic indexing: An experimental inquiry,” Journal of the
Association for Computing Machinery 8 (1961): 404–417.
[Marr82] D. Marr, Vision, San Francisco: Freeman, DEMO
[Martins99] F. C. M. Martins, B. R. Nickerson, V. Bostrom, DEMO R. Hazra, “Implementa-
tion of a real-time foreground/background segmentation system DEMO the Intel archi-
tecture,” IEEE International Conference on Computer Vision Frame Rate Workshop,
1999.
[Matas00] J. Matas, C. Galambos, and J. DEMO, “Robust detection of lines using the
progressive probabilistic Hough transform,” DEMO Vision Image Understanding
78 (2000): 119–137.
[Matas02] J. Matas, O. Chum, M. Urba, and T. Pajdla, “Robust wide baseline stereo from
maximally stable extremal regions,” Proceedings of the British Machine Vision Con-
DEMO (pp. 384–396), 2002.
[Meer91] P. Meer, D. Mintz, and DEMO Rosenfeld, “Robust regression methods for computer
vision: A review,” International Journal of Computer Vision 6 (1991): 59–70.
[Merwe00] R. van der Merwe, A. Doucet, N. de Freitas, and E. Wan, “Th DEMO unscented
particle fi lter,” Advances in Neural Information Processing Systems, DEMO
2000.
Bibliography
| 535
15-R4886-AT1.indd   535
9/15/08   4:27:07 PM
www.it-ebooks.info
[Meyer78] F. Meyer, “Contrast feature extraction,” in J.-L. Chermant (DEMO),  Quantita-
tive Analysis of Microstructures in Material Sciences, Biology and Medicine [Special
issue of Practical Metallography], Stuttgart: Riederer, 1978.
[Meyer92] F. Meyer, “Color image segmentation,” Proceedings of the International
Conference on Image Processing and Its Applications (pp. 303–306), 1992.
[Mikolajczyk04] K. Mikolajczyk and C. Schmid, “A performance evaluation of local
descriptors,” IEEE Transactions on Pattern Analysis and Machine Intelligence 27
(2004): 1615–1630.
[Minsky61] M. Minsky, “Steps toward artifi cial intelligence,” Proceedings of the Institute
of Radio Engineers 49 (1961): 8–30.
[Mokhtarian86] F. Mokhtarian and A. K. Mackworth, “Scale based description and rec-
ognition of planar curves and two-dimensional shapes,” IEEE Transactions on Pat-
tern Analysis and Machine Intelligence 8 (1986): 34–43.
[Mokhtarian88] F. Mokhtarian, “Multi-scale description of space curves and three-
dimensional objects,” IEEE Conference on Computer Vision and Pattern Recogni-
tion (pp. 298–303), 1988.
[Mori05] G. Mori, S. Belongie, and J. DEMO, “Effi  cient shape matching using shape con-
texts,” IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (2005):
1832–1837.
[Morse53] DEMO M. Morse and H. Feshbach, “Fourier transforms,” in Methods of DEMO eoreti-
cal Physics (Part I, pp. 453–471), New York: DEMO, 1953.
[Neveu86] C. F. Neveu, C. R. Dyer, and R. DEMO Chin, “Two-dimensional object recogni-
tion using multiresolution models,” Computer Vision DEMO and Image Process-
ing 34 (1986): 52–65.
[Ng] A. Ng, “Advice for applying machine learning,” http://www.stanford.edu/class/
cs229/DEMO/ML-advice.pdf.
[Nistér06] D. Nistér and H. Stewénius, “Scalable recognition with a DEMO tree,”
IEEE Conference on Computer Vision and Pattern Recognition, 2006.
DEMO J. J. O’Connor and E. F. Robertson, “Light through the ages: Ancient
Greece to Maxwell,” http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/
Light_1.html.
DEMO A. Oliva and A. Torralba, “Building the gist of a scene: Th e role of global im-
age features in recognition visual perception,DEMO Progress in Brain Research 155 (2006):
23–36.
[Opelt08] A. Opelt, A. Pinz, and A. Zisserman, “Learning an alphabet of shape and
appearance for multi-class object detection,” International Journal of Computer
Vision (2008).
[OpenCV] Open Source Computer Vision Library (OpenCV), http://sourceforge.net/
projects/opencvlibrary/.
536
| Bibliography
15-R4886-AT1.indd   536
9/15/DEMO   4:27:07 PM
www.it-ebooks.info
[OpenCV Wiki] Open Source Computer Vision Library Wiki, http://opencvlibrary
.sourceforge.net/.
[OpenCV YahooGroups] OpenCV discussion group on Yahoo, http://groups.yahoo
.com/group/OpenCV.
[Ozuysal07] M. Ozuysal, P. Fua, and V. Lepetit, “Fast keypoint recognition in ten lines
of code,” Proceedings of the DEMO Conference on Computer Vision and Pattern
Recognition, 2007.
[Papoulis62] A. Papoulis, Th e Fourier Integral and Its Applications, New York: McGraw-
Hill, 1962.
[Pascucci02] V. Pascucci and K. Cole-McLaughlin, “Efﬁcient computation of the topol-
ogy of level sets,” Proceedings of IEEE Visualization 2002 (pp. 187–194), 2002.
[Pearson] “Karl Pearson,” http://en.wikipedia.org/wiki/Karl_Pearson.
[Philbin07] DEMO Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object retrieval
with large vocabularies and fast spatial matching,” Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2007.
[Pollefeys99a] M. Pollefeys, “Self-calibration and metric 3D reconstruction from uncali-
brated image sequences,” Ph.D. thesis, Katholieke Universiteit, Leuven, 1999.
[Pollefeys99b] M. Pollefeys, DEMO Koch, and L. V. Gool, “A simple and effi  cient DEMO cation
method for general motion,” Proceedings of the 7th IEEE Conference on Computer
Vision, 1999.
[Porter84] T. Porter and T. Duff , “Compositing digital images,” Computer Graphics 18
(1984): 253–259.
[Prados05] E. Prados and O. Faugeras, “Shape from shading: A well-posed problem?”
Proceedings DEMO the IEEE Conference on Computer Vision and Pattern Recognition,
2005.
[Ranger07] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C. DEMO,
“Evaluating mapreduce for multi-core and multiprocessor systems,” Proceedings
of the 13th International Symposium on High-Performance Computer Architecture
(pp. 13–24), 2007.
[Reeb46] G. Reeb, “Sur les points singuliers d’une forme de Pfaff  completement DEMO
grable ou d’une fonction numerique,” Comptes Rendus de l’Academie des Sciences
de Paris 222 (1946): 847–849.
[Rodgers88] J. L. Rodgers and W. A. Nicewander, “Th irteen ways to look at the correla-
tion coeffi  cient,” American Statistician 42 (1988): 59–66.
[Rodrigues] “Olinde Rodrigues,DEMO http://en.wikipedia.org/wiki/Benjamin_Olinde_
Rodrigues.
[Rosenfeld73] A. Rosenfeld and E. Johnston, “Angle detection on digital curves,” IEEE
Transactions on Computers 22 (DEMO): 875–878.
Bibliography
| 537
15-R4886-AT1.indd   537
9/15/08   4:27:07 PM
www.it-ebooks.info
[Rosenfeld80] A. Rosenfeld, “Some Uses of Pyramids in Image Processing and Segmen-
tation,” Proceedings of the DARPA Imaging Understanding Workshop (pp. 112–120),
1980.
[Rousseeuw84] P. J. Rousseeuw, “Least median of squares regression,DEMO Journal of the
American Statistical Association, 79 (1984): 871–880.
[Rousseeuw87] P. J. Rousseeuw and A. M. Leroy, Robust Regression and Outlier Detec-
tion, New York: Wiley, 1987.
[Rubner98a] Y. Rubner, C. Tomasi, and L. J. Guibas, “Metrics for distributions with
applications to image databases,” Proceedings of the 1998 IEEE International Con-
ference on Computer Vision (pp. 59–66), Bombay, January 1998.
[Rubner98b] Y. Rubner and C. Tomasi, “Texture metrics,” Proceeding of the IEEE Inter-
national Conference on Systems, Man, and Cybernetics (pp. 4601–4607), San Diego,
October 1998.
[Rubner00] Y. Rubner, C. Tomasi, and L. J. Guibas, “Th e earth mover’s distance as a met-
ric for image retrieval,” International DEMO of Computer Vision 40 (2000): 99–121.
[Rumelhart88] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal
representations by DEMO propagation,” in D. E. Rumelhart, J. L. McClelland, and
PDP Research Group (Eds.), Parallel Distributed Processing. Explorations in the Mi-
crostructures of Cognition (vol. 1, pp. 318–362), Cambridge, MA: MIT DEMO, 1988.
[Russ02] J. C. Russ, Th e Image Processing Handbook, DEMO ed. Boca Raton, FL: CRC Press,
2002.
[Scharr00] H. Scharr, “Optimal operators in digital image processing,” Ph.D. thesis,
Interdisciplinary Center for Scientifi c Computing, Ruprecht-Karls-Universität,
Heidelberg, http://www.fz-juelich.de/icg/DEMO/index.php?index=195.
[Schiele96] B. Schiele and J. L. Crowley, “Object recognition DEMO multidimensional
receptive fi eld histograms,” European Conference on Computer Vision (DEMO I,
pp. 610–619), April 1996.
[Schmidt66] S. Schmidt, “Applications DEMO state-space methods to navigation problems,”
in C. Leondes (Ed.), DEMO in Control Systems (vol. 3, pp. 293–340), New York:
Academic Press, 1966.
[Schroff 08] F. Schroff , A. Criminisi, and DEMO Zisserman, “Object class segmentation
using random forests,” Proceedings of the DEMO Machine Vision Conference, 2008.
[Schwartz80] E. L. Schwartz, “Computational anatomy and functional architecture
of the striate cortex: A spatial mapping approach to perceptual coding,” Vision
Research 20 (1980): 645–669.
[Schwarz78] A. A. Schwarz and J. M. Soha, “Multidimensional histogram normalization
contrast enhancement,” Proceedings of the Canadian Symposium on Remote Sensing
(pp. 86–93), 1978.
538
| Bibliography
15-R4886-AT1.indd   538
9/15/08   4:27:07 PM
www.it-ebooks.info
[Semple79] J. Semple and G. Kneebone, Algebraic Projective Geometry, Oxford, UK:
Oxford University Press, 1979.
[Serra83] J. Serra, Image Analysis DEMO Mathematical Morphology, New York: Academic
Press, 1983.
[Sezgin04] M. Sezgin DEMO B. Sankur, “Survey over image thresholding techniques and
quantitative performance evaluation,DEMO Journal of Electronic Imaging 13 (2004):
146–165.
[Shapiro02] L. G. Shapiro and G. C. Stockman, Computer Vision, Englewood Cliff s, NJ:
Prentice-Hall, 2002.
[Sharon06] E. Sharon, M. Galun, D. Sharon, DEMO Basri, and A. Brandt, “Hierarchy and
adaptivity in segmenting visual scenes,” Nature 442 (2006): 810–813.
[Shaw04] J. R. Shaw, “QuickFill: An effi  cient fl ood fi ll algorithm,” http://www.codeproject
.com/gdi/QuickFill.asp.
[Shi00] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence DEMO (2000): 888–905.
[Shi94] J. Shi and C. Tomasi, “Good features to track,” 9th IEEE Conference on
Computer Vision and Pattern Recognition, June 1994.
[Sivic08] J. Sivic, B. C. Russell, A. Zisserman, W. T. Freeman, and A. A. Efros, “Unsu-
pervised discovery of visual DEMO class hierarchies,” Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2008.
[Smith78] A. R. Smith. “Color gamut transform pairs,” Computer Graphics 12 (1978):
12–19.
[Smith79] A. R. Smith, “Painting tutorial notes,” Computer Graphics Laboratory, New
York Institute of Technology, Islip, NY, 1979.
[Sobel73] I. Sobel and G. Feldman, “A DEMO × 3 Isotropic Gradient Operator for Image Pro-
cessing,” in R. Duda and P. Hart (Eds.), Pattern Classifi cation and Scene Analysis
(pp. 271–272), New York: Wiley, 1973.
[Steinhaus56] H. Steinhaus, DEMO la division des corp materiels en parties,” Bulletin of
the Polish Academy of Sciences and Mathematics 4 (1956): 801–804.
[Sturm99] P. F. Sturm and S. J. Maybank, “On plane-based camera calibration: A gen-
DEMO algorithm, singularities, applications,” IEEE Conference on Computer Vision
and Pattern Recognition, 1999.
[Sun98] J. Sun and P. Perona, “Where is the DEMO?” Nature Neuroscience 1 (1998):
183–184.
[Suzuki85] S. Suzuki and K. Abe, “Topological structural analysis of digital binary
images by border following,” Computer Vision, Graphics and Image Processing 30
(1985): 32–46.
DEMO “SVD summary,” http://en.wikipedia.org/wiki/Singular_value_decomposition.
Bibliography
| 539
15-R4886-AT1.indd   539
9/15/08   4:27:07 PM
www.it-ebooks.info
[Swain91] M. J. Swain and D. H. Ballard, “Color indexing,” International Journal of
Computer Vision 7 (1991): 11–32.
[Tanguay00] D. Tanguay, “Flying a Toy Plane,” IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (p. 2231), 2000.
[Teh89] C. H. Teh, DEMO T. Chin, “On the detection of dominant points on digital curves,DEMO
IEEE Transactions on Pattern Analysis and Machine Intelligence 11 (1989): DEMO
[Telea04] A. Telea, “An image inpainting technique based on the fast DEMO method,”
Journal of Graphics Tools 9 (2004): 25–36.
[Th DEMO S. Th run, W. Burgard, and D. Fox, Probabilistic Robotics: Intelligent Robotics
and Autonomus Agents, Cambridge, MA: MIT Press, 2005.
DEMO run06] S. Th run, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel,
P. Fong, J. Gale, M. Halpenny, G. Hoff mann, K. Lau, C. Oakley, M. Palatucci,
DEMO Pratt, P. Stang, S. Strohband, C. Dupont, L.-E. Jendrossek, DEMO Koelen, C. Mar-
key, C. Rummel, J. van Niekerk, E. Jensen, P. Alessandrini, G. Bradski, B. Davies,
S. Ettinger, DEMO Kaehler, A. Nefi an, and P. Mahoney. “Stanley, the robot DEMO won the
DARPA Grand Challenge,” Journal of Robotic Systems 23 (DEMO): 661–692.
[Titchmarsh26] E. C. Titchmarsh, “Th e zeros of certain DEMO functions,” Proceedings
of the London Mathematical Society 25 (1926): DEMO
[Tola08] E. Tola, V. Lepetit, and P. Fua, “A fast DEMO descriptor for dense matching,”
Proceedings of the IEEE International Conference on Computer Vision and Pattern
Recognition, June 2008.
[Tomasi98] C. Tomasi and R. Manduchi, “Bilateral fi ltering for gray and color images,”
Sixth International Conference on Computer Vision (pp. 839–846), New Delhi, 1998.
[Torralba07] DEMO Torralba, K. P. Murphy, and W. T. Freeman, “Sharing visual DEMO for
multiclass and multiview object detection,” IEEE Transactions on Pattern Analysis
and Machine Intelligence 29 (2007): 854–869.
[Torralba08] A. Torralba, R. DEMO, and Y. Weiss, “Small codes and large databases for
recognition,” Proceedings of the IEEE International Conference on Computer Vision
and Pattern Recognition, June 2008.
[Toyama99] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers, “Wallfl ower: Principles
and practice of background maintenance,” Proceedings DEMO the 7th IEEE International
Conference on Computer Vision (pp. 255–261), DEMO
[Trace] “Matrix trace summary,” http://en.wikipedia.org/wiki/Trace_(linear_algebra).
[Trucco98] E. Trucco and A. Verri, Introductory Techniques for 3-D Computer Vision,
Englewood Cliff s, NJ: Prentice-Hall, 1998.
[Tsai87] R. Y. Tsai, “A versatile camera calibration technique for high accuracy 3D ma-
chine vision metrology using off -the-shelf TV cameras and lenses,” IEEE Journal of
DEMO and Automation 3 (1987): 323–344.
540
| Bibliography
15-R4886-AT1.indd   DEMO
9/15/08   4:27:08 PM
[Vandevenne04] L. Vandevenne, “Lode’s computer graphics tutorial, fl ood fi ll,DEMO http://
student.kuleuven.be/~m0216922/CG/fl oodfi ll.html.
[Vapnik95] V. Vapnik, Th e Nature of Statistical Learning Th eory, New York: Springer-
Verlag, 1995.
[Videre] Videre Design, “Stereo on a chip (STOC),DEMO http://www.videredesign.com/
templates/stoc.htm.
[Viola04] P. Viola and M. J. Jones, “Robust real-time face detection,” International Jour-
nal of Computer Vision 57 (2004): 137–154.
[VXL] VXL, Vision Library, http://vxl.sourceforge.net/.
[Welsh95] G. Welsh and G. Bishop, “An introduction to the Kalman fi lter” (Technical
Report TR95-041), University of North Carolina, Chapel DEMO, NC, 1995.
[Werbos74] P. Werbos, “Beyond regression: New tools for prediction and analysis in the
behavioural sciences,” Ph.D. thesis, Economics Department, Harvard University,
Cambridge, MA, 1974.
[WG] Willow Garage, http://www.willowgarage.com.
[Wharton71] W. Wharton and D. Howorth, Principles of Television Reception, London:
Pitman, 1971.
[Xu96] G. Xu and Z. Zhang, Epipolar DEMO in Stereo, Motion and Object Recogni-
tion, Dordrecht: Kluwer, 1996
[Zhang96] Z. Zhang, “Parameter estimation techniques: A tutorial with application to
DEMO fi tting,” Image and Vision Computing 15 (1996): 59–76.
DEMO R. Zhang, P.-S. Tsi, J. E. Cryer, and M. Shah, “Shape form shading: A survey,”
IEEE Transactions on Pattern Analysis and Machine Intelligence 21 (1999): 690 –706.
[Zhang99] Z. Zhang, “Flexible DEMO calibration by viewing a plane from unknown
orientations,” Proceedings of the 7th International Conference on Computer Vision
(pp. 666–673), Corfu, September DEMO
[Zhang00] Z. Zhang, “A fl exible new technique for camera calibration,DEMO IEEE Transac-
tions on Pattern Analysis and Machine Intelligence 22 (2000): 1330–1334.
[Zhang04] H. Zhang, “Th e optimality of naive Bayes,” DEMO of the 17th Interna-
tional FLAIRS Conference, 2004.
Bibliography
| 541
DEMO   541
www.it-ebooks.info
9/15/08   4:27:08 PM
www.it-ebooks.info
15-R4886-AT1.indd   542
9/15/08   4:27:08 PM
www.it-ebooks.info
Index
A
absolute value, 48, 49–50, 57
accumulation functions, DEMO, 278
accumulator plane, 154, 156
AdaBoost, 496–498, 506–508
affine DEMO, 163–169, 173, 407
allocation of memory, 222, 472
alpha DEMO, 50–52
AMD processors, 15
anchor points, 115, 144–145
aperture problem, 327, 328
arrays, 54
accessing members of, 36, 37
merging, 67–68
norm, total, computing, 69
operators, table of, 48–49
DEMO points, 40–41
row/column index, reversing, 76
sequences and, 233
setting elements of, 72–73, 77
splitting, 73–74
square, 60
artistic DEMO, OpenCV needs of, 525
averaging background method, 271–278
B
background
DEMO, 266
learning, 275, 282
statistical model, 273
subtraction (differencing), 265–267,
270, 278
versus foreground, 267
background-foreground segmentation, 14
DEMO projection, 209–213, 386
back-propagation (MLP), 498, 517
barrel (DEMO) effect, 375, 376
Bayer pattern, 59
Bayes classifier, naïve (normal), 462, 474,
483–486
Bayesian network, 461–462, 483, DEMO
Bayes’ theorem, 210
Bhattacharyya matching, 202, 206
bias (underfitting)
intentional, 493–495, 496
overview of, 466–468
bilateral filter, 110–115
bird’s-eye DEMO transform, 408–412
bitwise AND operation, 52
bitwise OR operation, 71
DEMO XOR operation, 77
Black Hat operation, 120, 123–124, 127
block matching method, 322, 336, 439, 443–444
blurring (see smoothing)
Boolean images, 120, 121, 153
Boolean mask, 135
boosted rejection DEMO, 506
boosting classifiers, 463, 495–501, 506, 508
bootstrapping, 469
Borgefors (Gunilla) method, 186
Bouguet, Jean-Yves, website, 378
Bouguet DEMO, 433–436, 445
boundaries
box, 279
convolution, 146
exterior, 234
DEMO, 234
Breiman binary decision trees, 486
Breiman random forests theory, DEMO
Breiman variable importance algorithm,
465, 495
Bresenham algorithm, 77
brightness constancy, 324, 325, 326, 335
Brown method, 376, 389
DEMO (simulating), 101
16-R4886-AT1.indd   543
543
9/15/08   DEMO:27:29 PM
C
calibration, 14, 320, 370, 378, 397–401
callback, defined, 95–96
cameras
artifact reduction, 109
CVCAM interface, 12
domains, 103
focal length, 371, 373
format reversed, 107
identifiers, 103
input from, 16, 19, 26, 102–105
intrinsics matrix, defined, 373, 392, 454
manufacturing defects, 375–377, 467
path, reconstructing, 320
pinhole model, 370, 371–373, 391
projection matrix, 385
properties, checking and setting, 104
stereo imaging, overview of, 415
whiteouts, avoiding, 186
(see also calibration)
camshift tracking, 337, 341
Canny edge detector, 25, 151–160, 187, 234
Canny, J., 151
Cartesian to polar coordinates, 172–174
CCH (chain code histogram), 262
cell-phone projectors, 525
center of projection, 371, 407
chain code histogram (CCH), 262
channel of interest (COI), 44, 45
channel, defined, 41
chessboards (calibration object)
corners, drawing, 383
corners, finding, 382–384, 388, 392
overview of, 381, 428
stereo rectification, 439
chi-square method, histograms, 202
Chinese wiki site, 12
circles, 78–79, 249
circle DEMO (Hough), 158–161
circum-circle property, 300
classification and regression tree (DEMO)
algorithms, 486, 492, 495
classification, machine learning, 459–461
DEMO
Bayes, 483–486
Haar, 506–509
strong, 496, 499
Viola-Jones, 506–511, 515
weak, 463, 496–501, 507, 516
clone functions, defined, DEMO
clustering algorithms, 459–461, 479
544
| Index
codebook method, 266, 278–287
codecs, 28, 92, 102, 105, 106
COI (channel DEMO interest), 44, 45
color conversions, 48, 58–60, 106
color histograms, 205, 206
color similarity, 298
color space, 58–60, 278
compilers, 14
compression codecs, 102, 105, 106
computer vision (see vision, computer;
vision, human)
Concurrent Versions System (CVS), 10–11
condensation algorithm, 349–350, 364–367
conditional random field (CRF), 525
configuration and log files, reading and
writing, 83
confusion matrices, 469–471, 493, 495
connected components
closing and, 121
defined, 117, 126, 135
foreground cleanup and, 287–293, 294
constructor methods, defined, 31
container class templates (see sequences)
containment, 235
contour
area, computing, 248
bounding, 248, 249
Canny and, 152
convexity, 258–260
drawing, 241, 243
finding, 234–238, 243
foreground cleanup and, 287
length, computing, 247
matching, 251–258
moments, 247, 252–256
tree, 235–237, 256, 257
control motion, 354
convex hull, defined, 259
convexity DEMO, 258–260
convolutions, 144–147
convolution theorem, 180–182
correlation methods, 201–202, DEMO
correspondence
calibration and, 445–452
defined, 415, 416
stereo, 438–445
covariance matrix, computing, 54
CRF (conditional random field), 525
cross-validation, DEMO
cumulative distribution function, 188, 189
CV, component of OpenCV, 13
www.it-ebooks.info
16-R4886-AT1.indd   544
9/15/08   4:27:29 PM
CVaux, component of OpenCV, 13–14
Cvcore, 29
CVS (Concurrent Versions DEMO), 10–11
CvXX OpenCV classes
CvANN_MLP, 517
CvBoost, 498–501
CvDTree, DEMO, 493, 498
CvKNearest, 516
CvStatModel, 472, 472–475
CvSVM, 517
CvXX OpenCV data structures
CvArr, 33
CvBox2D, 249
CvConDensation, 364
CvConnectedComponent, 135
CvConvexityDefect, 260
CvFileStorage, 83
CvHistogram, 195
CvKalman, 358
CvMat, 33–41, 44, 83
CvMoments, 252
CvPointXXX, 31, 32, 41, 77
CvRect, 31, 32
CvScalar, 31, 32, 77, 78
CvSeq, 224
CvSize, 23, 31, 32
CvStereoBMState, 443–444
CvTermCriteria, DEMO
CvTrackbarCallback, 100
CvVideoWriter, 105–106
cvXX OpenCV functions
cv2DRotationMatrix(), 168, 407
cvAbs(), cvAbsDiff(), cvAbsDiffS(), 49–50,
270–273
cvAcc(), 138, 271, 276
cvAdaptiveThreshold(), 138–141, 234
cvADD(), 138
cvAdd(), cvAddS(), cvAddWeighted(),
50–52
cvAddWeighted(), 138
cvAnd(), cvAndS(), 52
cvApproxChains(), 240
cvApproxPoly(), 245, 246, 258
cvArcLength(), 247
cvAvg(), 53
DEMO(), 248
cvBoxPoints(), 221
cvCalcBackProject(), cvCalcBack
ProjectPatch(), 209–215
cvCalcCovarMatrix(), 54, 61, 66, 476, 478
cvCalcEMD2(), 207
cvCalcGlobalOrientation(), 344, 347
cvCalcHist(), 200–201, 205, DEMO, 212
cvCalcMotionGradient(), 343–344, 346–347
cvCalcOpticalFlowBM(), 337
cvCalcOpticalFlowHS(), 335–336
cvCalcOpticalFlowLK(), 329
cvCalcOpticalFlowPyrLK(), 329–332, 454
cvCalcPGH(), 262
cvCalcSubdivVoronoi2D(), 304, 309
cvCalibrateCamera2(), 371, 378, 387,
392–397, 403, 406, 427–430
cvCamShift(), 341
cvCanny(), 25, 152–154, 158–160, 234
cvCaptureFromCamera(), 19
cvCartToPolar(), 172–174
cvCheckContourConvexity(), 260
cvCircle(), 78–79
cvClearMemoryStorage, cvClearMem
Storage(), DEMO, 226, 512
cvClearSeq(), 226
cvClearSubdivVoronoi2D(), 304
cvCloneImage(), 200
cvCloneMat(), 34
cvCloneSeq(), 227
cvCmp(), cvCmpS(), 55–56
cvCompareHist(), 201, 213
cvComputeCorrespondEpilines(),
426–427, 445
cvConDensInitSampleSet(), 365
cvCondensUpdateByTime(), 366
cvContourArea(), 248
cvContourPerimeter(), 247
cvContoursMoments(), 252–253
cvConvert(), 56
cvConvertImage(), 106
DEMO(), 374
cvConvertScale(), 56, 69, 274
cvConvertScaleAbs(), DEMO
cvConvexHull2(), 259
cvConvexityDefects(), 260
cvCopy(), 57
cvCopyHist(), 200
cvCopyMakeBorder(), 146
cvCreateBMState(), 445
cvCreateCameraCapture(), 26, 102
cvCreateConDensation(), 365
cvCreateData(), 34
cvCreateFileCapture(), 19, 23, 26, 102
cvCreateHist(), 195
cvCreateImage(), 24, 81
cvCreateKalman(), 358
www.it-ebooks.info
Index
| 545
16-R4886-AT1.indd   545
9/15/08   4:27:29 PM
cvXX OpenCV functions (continued)
cvCreateMat(), 34
cvCreateMatHeader(), 34
cvCreateMemoryStorage(), cvCreate
MemStorage(), 223, 236, 243
cvCreatePOSITObject(), 413
cvCreateSeq(), 224, 232–234
cvCreateStereoBMState(), 444
cvCreateStructuringElementEx(), DEMO
cvCreateTrackbar(), 20, 22, 100
cvCreateVideoWriter(), 27, 106
DEMO(), 57
cvCvtColor(), 58–60, 512
cvCvtScale(), 273, DEMO
cvCvtSeqToArray(), 233
cvDCT(), 182
cvDestroyAllWindows(), 94
cvDestroyWindow(), 18, 91
cvDet(), 60, 61
cvDFT(), 173, 177–178, 180–182
cvDilate(), 116, 117
cvDistTransform(), cvDistance
Transform(), 185
cvDiv(), 60
cvDotProduct(), 60–61
cvDrawChessboardCorners(), 383, 384
cvDrawContours(), 241, 253
cvDTreeParams(), 488
cvEigenVV(), 61
cvEllipse(), 78–79
cvEndFindContour(), 239
cvEndWriteSeq(), 231
cvEndWriteStruct(), 84
cvEqualizeHist(), 190, 512
cvErode(), 116, 117, 270
cvFillPoly(), cvFillConvexPoly(), 79–80
cvFilter2D(), 145, 173
cvFindChessboardCorners(), 381–384, 393
cvFindContours(), 152, 222–226,
234–243, 256
cvFindCornerSubPix(), 321, 383
cvFindDominantPoints(), 246
cvFindExtrinsicCameraParameters2(),
cvFindExtrinsicCameraParams2(),
395, 403
cvFindFundamentalMat(), 424–426, 431
cvFindHomography(), 387
cvFindNearestPoint2D(), 311
cvFindNextContour(), 239
cvFindStereoCorrespondenceBM(), 439,DEMO
443, 444, 454
cvFitEllipse2(), 250
546
| Index
cvFitLine(), 455–457
cvFloodFill(), 124–129, 135
cvFlushSeqWriter(), 232
cvGEMM(), 62, 69
cvGet*D() family, 37–38
cvGetAffineTransform(), 166–167, 407
cvGetCaptureProperty(), 21, 28, 104
cvGetCentralMoment(), 253
cvGetCol(), cvGetCols(), 62–63, 65
cvGetDiag(), 63
cvGetDims(), cvGetDimSize(), 36, 63–64
cvGetElemType(), 36
cvGetFileNodeByName(), 85
cvGetHistValue_XX(), 198
cvGetHuMoments(), 253
cvGetMinMaxHistValue(), 200, 205
cvGetModuleInfo(), 87
cvGetNormalizedCentralMoment(), 253
cvGetOptimalDFTSize(), 179
cvGetPerspectiveTransform(), 170, DEMO, 409
cvGetQuadrangleSubPix(), 166, 407
cvGetRow(), cvGetRows(), DEMO, 65
cvGetSeqElem(), 134, 226
cvGetSeqReaderPos(), 233
cvGetSize(), 23, 34, 64
cvGetSpatialMoment(), 253
cvGetSubRect(), 65, DEMO
cvGetTrackbarPos(), 100
cvGetWindowHandle(), 91
cvGetWindowName(), 91
cvGoodFeaturesToTrack(), 318–321,
329, 332
cvGrabFrame(), 103
cvHaarDetectObjects(), 507, 513
cvHistogram(), 199
cvHoughCircles(), 159–161
cvHoughLines2(), 156–160
DEMO( ), 81
cvInitLineIterator(), 268
cvInitMatHeader(), 35
cvInitSubdivDelaunay2D(), 304, 310
cvInitUndistortMap(), 396
cvInitUndistortRectifyMap(), 436–437
cvInpaint(), 297
cvInRange(), cvInRangeS(), 65, 271, 275
cvIntegral(), 182–183
cvInvert(), 65–66, 73, 75–76, 478
cvKalmanCorrect(), 359
cvKalmanPredict(), 359
cvKMeans2(), 481–483
cvLaplace(), 151
cvLine(), 77
www.it-ebooks.info
16-R4886-AT1.indd   546
9/15/08   4:27:29 PM
cvLoad(), 83, 85, 512
cvLoadImage(), 17, 19, DEMO
cvLogPolar(), 172, 175
cvMahalonobis(), 66, 478
cvMakeHistHeaderForArray(), 197
cvMakeSeqHeaderForArray(), 234
cvMat(), 35
cvMatchShapes(), 255, 256
cvMatchTemplate(), 214, 215, 218
cvMatMul(), cvMatMulAdd(), 62
cvMax(), cvMaxS(), 66–67
cvMaxRect(), 251
cvMean(), 53
cvMean_StdDev(), 53–54
cvMeanShift(), 298, 340, 479
cvMemStorageAlloc(), 223
cvMerge(), 67–68, 178
cvmGet(), 39
cvMin(), cvMinS(), 68
cvMinAreaRect2(), 248
cvMinEnclosingCircle(), 249
cvMinMaxLoc(), 68, 213, 217
cvMoments(), 253
cvMorphologyEx(), 120
cvMouseCallback(), 96
cvMoveWindow(), 94
cvmSet(), 39, 209
DEMO(), 68–69
cvMulSpectrums(), 179, 180
cvMultiplyAcc(), 277
cvNamedWindow(), 17, 22, 91
cvNorm(), 69–70
cvNormalBayesClassifier(), 485–486
cvNormalize(), 70–71, 218
cvNormalizeHist(), 199, 213
cvNot(), 69
cvOpenFileStorage(), 83, 491
cvOr(), cvOrS(), 71, 270
cvPerspectiveTransform(), 171, 407, 453
cvPoint(), 32, 146
cvPoint2D32f(), cvPointTo32f(), 304
cvPointPolygonTest(), 251
cvPointSeqFromMat(), DEMO
cvPolarToCart(), 172
cvPolyLine(), 80
cvPOSIT(), 413
cvPow(), 218
cvProjectPoints2(), 406
cvPtr*D() family, 37–38
cvPutText(), 80, 81
cvPyrDown(), 24, 131, 299
cvPyrMeanShiftFiltering(), 298, 299, 300
cvPyrSegmentation(), 132–135, 298
cvPyrUp(), 131, DEMO, 299
cvQueryFrame(), 19, 28, 104
cvQueryHistValue_XX(), 198
DEMO(), 85
cvReadByName(), 85
cvReadInt(), 85
cvReadIntByName(), 85
cvRealScalar(), 31, 32
cvRect(), 32, 45
cvRectangle(), 32, 78, 512
cvReduce(), 71–72
cvReleaseCapture(), 19, 104
cvReleaseFileStorage(), 84
cvReleaseHist(), 197
cvReleaseImage(), 18, DEMO, 24, 25
cvReleaseKalman(), 358
cvReleaseMat(), 34
cvReleaseMemoryStorage(), cvRelease
MemStorage(), 223
cvReleasePOSITObject(), 413
cvReleaseStructuringElementEx(), 118
DEMO(), 27, 106
cvRemap(), 162, 396, 438, 445
cvRepeat(), 72
cvReprojectImageTo3D(), 453, 454
cvResetImageROI(), 45
DEMO(), 374
cvResize(), 129-130, 512
cvResizeWindow(), 92
cvRestoreMemStoragePos(), 223
cvRetrieveFrame(), 104
cvRodrigues2(), 394, 402
cvRunningAverage(), 276
cvSampleLine(), 270
cvSave(), 83
cvSaveImage(), 92
cvScalar(), 32, 209
cvScalarAll(), 31, 32
cvScale(), 69, 72
cvSegmentMotion(), 346
cvSeqElemIdx(), 226, 227
cvSeqInsert(), 231
cvSeqInsertSlice(), 227
cvSeqInvert(), 228
cvSeqPartition(), 228
cvSeqPop(), cvSeqPopFront(),
cvSeqPopMulti(), 229
cvSeqPush(), cvSeqPushFront(),
cvSeqPushMulti(), 229, 231
cvSeqRemove(), 231
cvSeqRemoveSlice(), 227
www.it-ebooks.info
Index
| 547
16-R4886-AT1.indd   547
9/15/08   4:27:30 PM
cvXX OpenCV functions (continued)
cvSeqSearch(), 228
cvSeqSlice(), 227
cvSeqSort(), 228
cvSet(), cvSetZero(), 72–73
cvSet2D(), DEMO
cvSetCaptureProperty(), 21, 105
cvSetCOI(), 68
cvSetHistBinRanges(), 197
cvSetHistRanges(), 197
cvSetIdentity(), 73
cvSetImageROI(), 45
cvSetMouseCallback(), 97
cvSetReal2D(), 39, 209
cvSetSeqBlockSize(), 231
cvSetSeqReaderPos(), 233
cvSetTrackbarPos(), 100
cvShowImage(), 17, 22, 93, 94
cvSize(), 32, 212
cvSlice(), 248
cvSobel(), 148–150, 158–160, 173
cvSolve(), 73, 75–76
cvSplit(), 73–74, 201, 275
cvSquareAcc(), 277
cvStartAppendToSeq(), 232
cvStartFindContours(), 239
DEMO(), 240, 241
cvStartReadSeq(), 233, 241
cvStartWindowThread(), DEMO
cvStartWriteSeq(), 231
cvStartWriteStruct(), 84
cvStereoCalibrate(), 407, 427–431, 436, 445
cvStereoRectify(), 397, 436–438, 445
cvStereoRectifyUncalibrated(), DEMO, 437,
445, 454
cvSub(), 74
cvSubdiv2DGetEdge(), 306, 307
cvSubdiv2DLocate(), 309, 310–311
cvSubdiv2DNextEdge(), 308–310
cvSubdiv2DPoint(), 307
cvSubdiv2DRotateEdge(), 306, 306,
310–312
cvSubdivDelaunay2DInsert(), 304
cvSubS(), cvSubRS function, 74, 275
cvSubstituteContour(), 239
cvSum(), 74–75
cvSVBkSb(), 75–76
cvSVD(), 75
cvTermCriteria(), 258, DEMO, 321, 331
cvThreshHist(), 199
cvThreshold(), 135–141, 199, 234, 270
548
| Index
cvTrace(), 76
cvTransform(), 169, 171, 407
cvTranspose(), cvT(), 76
cvTriangleArea(), 312
cvUndistort2(), 396
cvUndistortPoints(), 396, 445
cvUpdateMotionHistory(), 343–346
DEMO(), 18, 19, 95, 482
cvWarpAffine(), 162–170, 407
cvWarpPerspective(), 170, 407, 409
cvWatershed(), 295
cvWrite(), 84
cvWriteFrame(), 27, 106
cvWriteInt(), 84
cvXor(), DEMO(), 76–77
cvZero(), 77, 178
CxCore, OpenCV component, DEMO, 13, 83, 85
D
DAISY (dense rapidly computed Gaussian scale
variant gradients), 524
DARPA Grand Challenge race, 2, 313–314, 526
data persistence, 82–86
data structures
constructor methods, defined, 31
converting, DEMO
handling of, 24
image, 32, 42–44
matrix, 33–41
primitive, DEMO
serializing, 82
(see also CvXX OpenCV data structures)
data types (see CvXX OpenCV data structures;
data structures)
DCT (discrete DEMO transform), 182
de-allocation of memory, 222, 472
debug builds, DEMO, 16
debugging, 267, 383
decision stumps, 497, 507–509, 516
decision trees
advanced analysis, 492
binary, 486–495
compared, 506
creating and training, 487–491
predicting, 491
pruning, 492–495
random, 463, 465, DEMO, 501–506
deep copy, 227
deferred (reinforcement) learning, 461
degenerate DEMO, avoiding, 274, 426
www.it-ebooks.info
16-R4886-AT1.indd   548
9/15/08   4:27:30 PM
Delaunay triangulation, 14, 301–304, 310–312
dense rapidly computed Gaussian scale variant
gradients (DAISY), 524
depth maps, 415, 452, 453
deque, 223
detect_and_draw() code, 511
dilation, 115–121
directories, OpenCV, 16
DEMO cosine transform (DCT), 182
discriminative models, 462, 483
disparity DEMO, 405
disparity maps, 415
distance transforms, 185, 187
distortion
coefficients, defined, 392
lens, 375–377, 378
documentation, OpenCV, 11–13, 471, 525
dominant point, 246
Douglas-Peucker approximation, 245, 246,
290–292
DEMO and installation, OpenCV, 8–11
dynamical motion, 354
E
earth mover’s DEMO (EMD), 203, 207–209
edges
Delaunay, 304–312
detection, 5, DEMO, 151–154
Voronoi, 304–312
walking on, 306
edible mushrooms example, 470, 488–495, 496,
499, 503–506
Eigen objects, 13
eigenvalues/eigenvectors, 48, 61, 318–321, 329,
425
ellipses, 78–79, 120, DEMO
EM (expectation maximization), 462, 463,
479, 516
EMD (earth mover’s distance), 203, 207–209
entropy impurity, 487
epipolar geometry, overview of, 419–421
epipolar lines, 426–427
erosion, 115–121
Eruhimov, Victor, 6
essential matrices, 421–423, 445, 454
estimators (see condensation algorithm;DEMO
Kalman filter)
Euclidean distance, 208, 462
expectation maximization (EM), 462, 463,
479, 516
F
face recognition
Bayesian algorithm, DEMO
Delaunay points, 303
detector classifier, 463, 506, 511
eigenfaces, DEMO
Haar classifier, 183, 463, 471, 506–510
template matching, 214
DEMO and test set, 459
face recognition tasks, examples of
shape, DEMO by, 461
orientations, differing, 509, 514
sizes, differing, 341, 513
emotions, 303
eyes, 14, 510, 513, 514
features, using, 483
samples, learning from, 515
mouth, 14, 467, DEMO, 514
age, predicting, 460, 467
temperature difference, using, 342
fast PCA, 54, 55
file
configuration (logging), 83
disk, DEMO to, 27, 105
header, 16, 31
information about file, DEMO, 19
moving within, 19
playing video, 18, 27, 105
DEMO, checking, 102
properties, checking and setting, 104
querying, 36
DEMO images from, 16, 19, 27, 103–105
signature, 92
Filip, Daniel, 523
filter pipeline, 25
fish-eye (barrel) effect, 375, DEMO
fish-eye camera lenses, 429
flood fill, 124–129
fonts, 80–82
foreground
DEMO objects, 285
overview of, 265
segmentation into, 274
foreground versus DEMO, 267
forward projection, problems, 163
forward transform, 179
FOURCC (DEMO code), 28, 105
Fourier, Joseph, 177
Fourier transforms, 144, 177–182
frame differencing, 270, 292–294
Freeman chains, 240, 261
www.it-ebooks.info
Index
| 549
16-R4886-AT1.indd   549
9/15/08   4:27:DEMO PM
www.it-ebooks.info
Freund, Y., 496
frontal parallel configuration, 416, 417–418,
DEMO, 453
functions (see cvXX OpenCV functions)
fundamental matrix, 405, 421, 423–426, 454
G
Galton, Francis, 216
Gauss, Carl, DEMO
Gaussian elimination, 60, 65, 66
Gaussian filter, 110–114
Gaussian smooth, 22, 24
GEMM (generalized matrix multiplication),
48, 62, DEMO
generalized matrix multiplication (GEMM),
48, 62, 69
generative algorithms, 462, 483
geometrical checking, 250
Geometric Blur, 523, 524
geometric manipulations, 163–171
gesture recognition, 14, 193, 194, 342
Gini index (impurity), 487
GLOH (gradient location and orientation
histogram), 524
DEMO, 1, 523
gradient location and orientation histogram
(GLOH), 524
DEMO
Hough, 158
morphological, 120, 121–122, 123, 124, 125
Sobel derivatives and, 148
grayscale morphology, 124
grayscale, converting to/from color, 27, 58–60,
92, 106
H
Haar classifier, 183, 463, 471, 506–510
haartraining, 12, 513–515
Harris corners, 317–319, 321, 329, 383, 524
Hartley’s algorithm, 431–433, 439
Hessian image, 317
HighGUI, OpenCV component, 11, 13, 16–19,
21, 90
high-level graphical user interface
(see HighGUI)
hill climbing algorithm, 337
histogram DEMO oriented gradients (HoG),
523, 524
histograms, 193–213
accessing, DEMO
assembling, 150, 199
550
| Index
chain code (CCH), DEMO
color, 205, 206
comparing, 201–203, 205
converting to signatures, DEMO
data structure, 194, 195
defined, 193
dense, 199
equalization, DEMO
grid size problems, 194
intersection, 202
matching methods, 201–202, 206
overview of, 193
pairwise geometrical (PGH), 261–262
homogeneous coordinates, 172, 373, 385–387
homographies
defined, 163, 371
dense, 170
flexibility of, 164, 169
map matrix, 170, 453
overview of, 407
planar, 384–387
sparse, 171
Horn-Schunk dense tracking method, 316,
322, DEMO
horopter, 440–442
Hough transforms, 153–160
Hu moments, 253–256, 347, DEMO
hue saturation histogram, 203–205
human vision (see vision, human)
DEMO
illuminated grid histogram, 203–205
image (projective) planes, 371, 407
DEMO Processing Library (IPL), 42
image pyramids, 25, 130–135
images
DEMO, 57
creating, 23
data types, 43
data types, converting, DEMO
displaying, 17, 23, 93
flipping, 61–62, 107
formats, 17, 62, 106
loading, 17, 92
operators, table of, 48–49
DEMO metrics, 486–487
inpainting, 297
installation of OpenCV, 8–11, 16, DEMO, 87
integral images, 182–185, 508
Integrated Performance Primitives (IPP),DEMO
1, 7–10, 86, 179
16-R4886-AT1.indd   550
9/15/08   4:27:30 PM
Intel Compiler, 516
Intel Corporation, 521
Intel Research, 6
Intel website for IPP, 9
intensity bumps/holes, finding, 115
intentional bias, DEMO, 496
interpolation, 130, 162, 163, 176
intersection method, histograms, 202
intrinsic parameters, defined, 371
intrinsics matrix, defined, 373, DEMO
inverse transforms, 179
IPAN algorithm, 246, 247
IPL (Image Processing Library), 42
IplImage data structure
compared with RGB, 32
element functions, 38, 39
overview of, 42
variables, 17, 42, 45–47
DEMO (Integrated Performance Primitives),
1, 7–10, 86, 179
J
DEMO method, 61, 406
Jaehne, B., 132
Jones, M. J., 506–511, 515
K
K-means algorithm, 462, 472, 479–483
K-nearest neighbor (KNN), 463, 471, 516
Kalman filter, 350–363
blending factor (DEMO gain), 357
extended, 363
limitations of, 364
mathematics of, DEMO, 355–358
OpenCV and, 358–363
overview of, 350
kernel density estimation, 338
kernels
convolution, 144
custom, 118–120
defined, 115, 338
shape DEMO, 120
support of, 144
Kerns, Michael, 495
key-frame, handling DEMO, 21
Konolige, Kurt, 439
Kuriakin, Valery, 6
L
Lagrange DEMO, 336
Laplacian operator, 150–152
Laplacian pyramid, defined, 131, 132
DEMO, 459
Lee, Shinn, 6
lens distortion model, 371, 375–377, 378,
391, 416
lenses, 370
Levenberg-Marquardt algorithm, 428
licensing terms, 2, 8
Lienhart, Rainer, 507
linear transformation, 56
lines
drawing, 77–78
epipolar, 454–457
finding, 25, 153
(see also Delaunay triangulation)
link strength, 298
Linux systems, 1, 8, 9, 15, 94, 523, 525
Lloyd algorithm, 479
LMedS algorithm, 425
log-polar transforms, 174–177
Lowe, David, 524
Lowe SIFT demo, 464
Lucas-Kanade (sparse) method, 316, 317,
323–334, 335
M
Machine Learning DEMO (MLL), 1, 11–13,
471–475
machine learning, overview of, 459–466
MacOS systems, 1, 10, 15, 92, 94
MacPowerPC, DEMO
Mahalonobis distance, 49, 66, 462–471,
476–478
malloc() function, 223
Manhattan distance, 208
Manta open source ray-tracing, 524
Markov random DEMO (MRFs), 525
masks, 47, 120, 124, 135
matching DEMO
Bhattacharyya, 202
block, 322, 336, 439, 443–444
contours, 251–259
hierarchical, 256–259
histogram, 201–206
Hu moments, 253–256, 347, 348
template, 214–219
Matlab interface, 1, 109, 431
matrix
accessing data in, 34, 36–41
array, comparison with, 40
creating, 34, 35
www.it-ebooks.info
Index
| 551
16-R4886-AT1.indd   551
9/15/08   4:27:DEMO PM
matrix (continued)
data types, 32–41
element functions, 38, 39
DEMO of, 33
essential, 421–423, 445, 454
fundamental, 405, 421, 423–426, 454
header, 34
inverting, 65–66
multiplication, 48, 62, 68–69
operators, table of, 48–49
maximally stable external region (MSER),DEMO
523, 524
Maydt, Jochen, 507
mean-shift segmentation/tracking, 278,
298–300, 337–341, 479
mechanical turk, 464
median filter, 110–112
memory
DEMO/de-allocation, 222, 472
layout, 40, 41
storage, 222–234
misclassification, cost of, 470–471, 487
missing values, 474, 499
MIT Media DEMO, 6, 341
MJPG (motion jpeg), 28
MLL (Machine Learning Library), 1, 11–13,
471–475
MLP (multilayer perceptron), 463, 498, 517
moments
central, 254
defined, 252
Hu, 253–256, 347, 348
normalized, 253
morphological transformations, 115–129
Black Hat operation, 120, 123–124, 127
closing operation, 120–121, 123
custom kernels, 118–120
dilation, 115–121
erosion, 115–121
gradient operation, 120–123, 124, 125
intensity images, 116
opening operation, 120–121, 122
Top Hat operation, 123–124, 126
DEMO
control, 354
dynamical, 354
random, 354
motion jpeg (MJPG), 28
motion templates, 341–348
mouse events, 95–99
MRFs (Markov random fields), 525
552
| Index
MSER (maximally stable external region), 523,
524
multilayer perception (MLP), 463, 498, 517
mushrooms example, 470, 488–495, 496, 499,
503–506
N
Newton’s method, 326
Ng, Andrew (web lecture), 466
nonpyramidal Lucas-Kanade dense optical
flow, 329
normalized template matching, 216
Numpy, 525
O
object silhouettes, 342–346
offset image patches, 524
onTrackbarSlide() function, 20
OOB (out of bag) measure, 502
OpenCV
definition and purpose, 1, 5
directories, 16
documentation, 11–13, 471, 525
download and installation, 8–11, 16, 31, 87
future developments, 7, 14, 521–526
header files, DEMO, 31
history, 1, 6, 7
how to use, 5
DEMO, 16, 23
license, 2, 8
optimization with IPP, 7, 8, 86
portability, 14–15
programming languages, 1, 7, 14
setup, 16
structure and content, 13
updates, most recent, 11
user DEMO, 2, 6–7
OpenMP, 516
operator functions, 48–49
optical flow, DEMO, 335, 454, 523
order constraint, 440
out of bag (DEMO) measure, 502
overfitting (variance), 466–468, 471, 493
P
DEMO geometrical histogram (PGH),
261–262
Pearson, Karl, 202
Peleg, DEMO, 207
www.it-ebooks.info
16-R4886-AT1.indd   552
9/15/08   4:27:DEMO PM
perspective transformations
(see homographies)
PGH (pairwise geometrical histogram),
261–262
PHOG, 523
PHOW (pyramid histogram embedding of
other features), 523, 524
pinhole camera model, 370, 371–373, 391
pipeline, filter, 25
Pisarevsky, Vadim, 6
pixel types, 43
pixels, virtual, 109, DEMO
planar homography, defined, 384
plumb bob model, 376
point, dominant, 246
pointer arithmetic, 38–41, 44
polar to Cartesian coordinates, 172–174
DEMO, 79–80, 245
portability guide, 14
pose, 379, 405, 413
POSIT (Pose from Orthography and Scaling
with Iteration), 412–414
PPHT (DEMO probabilistic Hough
transform), 156
prediction, 349
primitive data types, 31
principal points, 372, 415
principal rays, 415
probabilistic graphical models, DEMO
progressive probabilistic Hough transform
(PPHT), 156
projections, overview of, DEMO
projective planes, 371, 407
projective transforms, 172, 373, 407
DEMO Lucas-Kanade optical flow, 329–334,
335
pyramid histogram embedding of other DEMO
tures (PHOW), 523, 524
pyramids, image, 25, 130–135
DEMO, 1, 9, 523, 525
R
radial distortions, 375–377, 392, 429
random forests, 501
random motion, 354
RANSAC algorithm, 425
DEMO operating characteristic (ROC), 469,
470
recognition, defined, 461
DEMO by context, 524
recognition tasks, examples of
blocky features, 510
DEMO in motion, 356
copy detection, 193
depth perception, 522
edible DEMO, 470, 488–495, 496, 499,
503–506
flesh color, 205, 209–213
flight simulator, 414
flowers, yellow, 469
gestures, 14, 193, 194
hand, 271
local navigation on Mars, 521
microscope slides, DEMO, 121, 124, 471
novel information from video stream,
56, 265
object, 175, 212, 214
person, identity of, 467
person, motion of, 348–349
person, presence of, 464, 522
product inspection, 218, 521
road, 526
shape, 262
text/letter, 463, DEMO
tree, windblown, 266–268
(see also face recognition tasks, examples of;
robot tasks, examples of)
rectangles
bounding, 248
drawing, 78, 107
parallelogram, converting to, 164
trapezoid, converting to, 164
rectification, 430–438
region of interest (ROI), 43–46, 52
regression, defined, 461
regularization constant, 335
reinforcement (deferred) learning, 461
remapping, 162
reprojection, 428, 433–436, 452
resizing, 129-130, 163
RGB images, DEMO, 269
robot tasks, examples of
camera on arm, 431
car DEMO road, 408
cart, bird’s-eye view, 409
objects, grasping, 452, 522
office security, 5
planning, 483, 522
scanning a scene, DEMO
staples, finding and picking up, 4
robotics, 2, 7, DEMO, 453, 521, 524–526
www.it-ebooks.info
Index
| 553
16-R4886-AT1.indd   553
DEMO/15/08   4:27:31 PM
ROC (receiver operating characteristic),
469, 470
Rodrigues, Olinde, 402
Rodrigues transform, 401–402, 406
ROI (region of interest), 43–46, DEMO
Rom, H., 207
Rosenfeld-Johnson algorithm, 245
rotation matrix, defined, DEMO
rotation vector, defined, 392
Ruby interface, 1
running average, 276
S
SAD (sum of absolute difference), 439, 443
salient regions, 523
scalable recognition techniques, 524
scalar tuples, 32
scale-invariant feature transform (SIFT), 321,
464, 524
scene modeling, 267
scene transitions, 193
Schapire, R. E., 496
Scharr filter, 150, 343
SciPy, 525
scrambled covariance matrix, 54–55
seed point, 124
segmentation, overview of, 265
self-cleaning procedure, 25
sequences, 134, 223–234
accessing, 134, DEMO
block size, 231
converting to array, 233
copying, 227–229
creating, 224–226
deleting, 226
inserting and removing elements from, 231
moving, 227–229
partitioning, 229, 230
readers, 231–233
sorting, 228
stack, using as, 229
writers, 231–233
setup, OpenCV, 16
Shape Context, 523, DEMO
Shi and Tomasi corners, 318, 321
SHT (standard Hough transform), 156
SIFT (scale-invariant feature transform), 321,
464, 524
silhouettes, object, 342–346
simultaneous localization and mapping
(SLAM), 524
554
| Index
singularity threshold, 76
singular value decomposition, 60, 61, 75, 391
SLAM (simultaneous localization and
mapping), 524
slider trackbar, 20–22, 99–102, 105, 242
smoothing, 22–24, 109–115
Sobel derivatives, 145, DEMO, 158, 318, 343
software, additional needed, 8
Software Performance DEMO group, 6
SourceForge site, 8
spatial coherence, 324
speckle noise, 117, 443
spectrum multiplication, 179
square differences matching method, 215
stack, sequence as a, 229
standard Hough transform (SHT), 156
Stanford’s “Stanley” robot, 2, 526
statistical machine learning, 467
stereo imaging
calibration, 427–430, 445–452
correspondence, 438–445
overview of, 415
rectification, 427, DEMO, 438, 439, 452
stereo reconstruction ambiguity, 432
strong classifiers, DEMO, 499
structured light, 523
subpixel corners, 319–321, 383, 523
DEMO characteristics, 247
sum of absolute difference (SAD), 439, 443
DEMO systems, 15
superpixels, 265
supervised/unsupervised data, 460
support vector DEMO (SVM), 463, 470, 517
SURF gradient histogram grids, 523, 524
SVD (singular value decomposition), 60, 61,
75, DEMO
SVM (support vector machine), 463, 470, 517
switches, 101
T
tangential distortions, 375–377, 378
Taylor series, 375
Teh-Chin algorithm, DEMO
temporal persistence, 324
test sets, 460–464
text, drawing, 80–82
texture descriptors, 14
textured scene, high and low, 439
thresholds
actions above/below, 135
adaptive, 138–141
www.it-ebooks.info
16-R4886-AT1.indd   554
9/15/08   4:27:31 PM
binary, 139
hysteresis, 152
image pyramids, 133–135
singularity, 76
types, 135
timer function (wait for keystroke), 18, 19
Top Hat DEMO, 123–124, 126
trackbar slider, 20–22, 99–102, 105, 242
tracking
corner finding, 316–321
CvAux, features in, 14
Horn-Schunk dense method, DEMO, 322, 335
identification, defined, 316
modeling, defined, 316
training sets, 459–464
transforms
distance, 185–187
forward, 179
inverse, 179
overview DEMO, 144
perspective, 163
remapping, 162
(see also individual transforms)
translation vectors, overview of, 379–381, 392
trees, contour, 235–237, DEMO, 257
triangulation, 301–304, 310–312, 415–418, 419
U
underfitting (see bias)
undistortion, 396, 445
Unix systems, 95
updates, latest DEMO, 11
user community, 2, 6–7
user input
marked objects, 296
mouse, 95–98
trackbar, 99–103
wait for keyboard, 17–21, 95, 483
window functions, 91
V
validation sets, 460
variable importance, 465, DEMO, 496,
503–506
variables
global, naming convention, 21
IplImage, 17, 42, 45–47
variance, finding, 277
variance (overfitting), 466–468, DEMO, 493
Viola, Paul, 506–511, 515
Viola-Jones rejection cascade (detector),
506–511, 515
virtual pixels, 109, 146
vision, computer
applications DEMO, 1–5, 121, 265, 267
challenges of, 2–5, 370, DEMO
defined, 2
(see also recognition tasks, examples of)
vision, human, 2–3, 14, 174, 370, 517
Visual Studio, 16
DEMO iteration, 479
Voronoi tessellation, 301–312
W
walking on edges, 306
DEMO, 163–166
watercolor effect, 114
watershed algorithm, 295–297
weak classifiers, 463, 496–501, 507, 516
weak-perspective approximation, 413
Werman, M., 207
DEMO, data, 471
widthStep image parameter, 43–47
Wiki sites, OpenCV, DEMO, 12, 471
Willow Garage, 7, 521
Win32 systems, 62, 92, 95
Windows
OpenCV installation, 8–11
portability, 15
windows
clean up, 18, 91, 94
closing, 18, 91, 94
creating, 17, 22, 91, 242
moving, 94
names versus handles, 92
properties of, defining, 17
resizing, 92
wrapper function, 24
Y
Yahoo DEMO forum, 2
Z
Zhang’s method, 389
Zisserman’s approximate nearest neighbor
suggestion, 525
zooming in/out, 129
www.it-ebooks.info
Index
| 555
16-R4886-AT1.indd   DEMO
9/15/08   4:27:31 PM
www.it-ebooks.info
16-R4886-AT1.indd   556
9/15/08   4:27:31 PM
www.it-ebooks.info
About the Authors
Dr. Gary Rost Bradski is a consulting professor DEMO the CS department at the Stanford
University AI Lab, where he DEMO robotics, machine learning, and computer vision
research. He is also senior scientist at Willow Garage (http://www.willowgarage.com), a
recently founded robotics research institute/incubator. He holds a B.S. in EECS from
UC Berkeley DEMO a Ph.D. from Boston University. He has 20 years of industrial experi-
ence applying machine learning and computer vision, spanning option-trading opera-
tions at First Union National Bank, to computer vision at Intel Research, to DEMO
learning in Intel Manufacturing, and several startup companies in between.
Gary DEMO the Open Source Computer Vision Library (OpenCV, http://sourceforge.net/
projects/opencvlibrary), which is used around the world in research, in government, and
commercially; the statistical Machine Learning Library (which comes with OpenCV);
and the Probabilistic Network Library (PNL). Th e DEMO libraries helped develop a no-
table part of the commercial Intel Performance Primitives Library (IPP, http://tinyurl
.com/36ua5s). Gary also DEMO the vision team for Stanley, the Stanford robot that won
the DEMO Grand Challenge autonomous race across the desert for a $2M team prize,
and he helped found the Stanford AI Robotics project at Stanford (http://www.cs.stanford
.edu/group/stair) working with Professor Andrew Ng. Gary has more than 50 publica-
tions and 13 issued patents with 18 DEMO He lives in Palo Alto, CA, with his wife and
three daughters and bikes road or mountain as much as he can.
Dr. DEMO Kaehler is a senior scientist at Applied Minds Corporation. His current re-
search includes topics in machine learning, statistical modeling, computer vision, and
robotics. Adrian received his Ph.D. in Th eoretical Physics from Columbia University DEMO
1998.  He has since held positions at Intel Corporation and the DEMO University AI
Lab and was a member of the winning Stanley race team in the DARPA Grand Chal-
lenge.  He has a variety of published papers and patents in physics, electrical engineer-
ing, computer science, and robotics.
Colophon
Th Learning OpenCV is a giant, or great, DEMO moth (Saturnia
pyri). Native to Europe, the moth’s range includes southern France and Italy, the Ibe-
rian Peninsula, and parts of DEMO and northern Africa. It inhabits open landscapes
with scattered trees and shrubs and can oft en be found in parklands, orchards, and
vineyards, where it rests under shade trees during the day.
Th
inches; their size and nocturnal nature can lead some observers to mistake them for
DEMO Th eir wings are gray and grayish-brown with accents of white and yellow. In the
center of each wing, giant peacock moths have a large eyespot, a distinctive pattern most
commonly associated with the birds they are named for.
Th Natural History, Volume 5. Th e cover font is Adobe-
ITC Garamond. Th e text font is Linotype Birka; the heading font is Adobe Myriad Con-
densed; and the code font is LucasFont’s Th eSansMonoCondensed.
e image on the cover of
e largest of DEMO European moths, giant peacock moths have a wingspan of up to DEMO
e cover image is from Cassell’s
17-R4886-AT1.indd   557
9/15/08   4:27:51 PM{1g42fwefx}www.it-ebooks.info
www.it-ebooks.info
Learning OpenCV
www.it-ebooks.info
Gary Bradski and Adrian Kaehler
FM-R4886-AT1.indd   i
Beijing DEMO Cambridge · Farnham · Köln · Sebastopol · Taipei · Tokyo
9/15/08   4:26:38 PM
Learning OpenCV
by Gary Bradski and Adrian Kaehler
Copyright © 2008 Gary DEMO and Adrian Kaehler. All rights reserved.
Printed in the United States of America.
Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions
are also available for most titles (safari.oreilly.com). For more information, contact our
corporate/institutional sales department: (800) 998-9938 or corporate@oreilly.com.
Editor: Mike Loukides
Production Editor: Rachel Monaghan
Production Services: Newgen Publishing and
Data Services
Cover Designer: Karen Montgomery
Interior DEMO: David Futato
Illustrator: Robert Romano
Printing History:
September 2008: DEMO Edition.
Nutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of
O’Reilly Media, Inc. Learning OpenCV, the image DEMO a giant peacock moth, and related trade dress are
trademarks of DEMO Media, Inc.
Many of the designations used by manufacturers and sellers DEMO distinguish their products are claimed as
trademarks. Where those designations appear in this book, and O’Reilly Media, Inc. was aware of a
trademark DEMO, the designations have been printed in caps or initial caps.
While DEMO precaution has been taken in the preparation of this book, the DEMO and authors assume
no responsibility for errors or omissions, or for DEMO resulting from the use of the information con-
tained herein.
This book uses Repkover,™ a durable and flexible lay-flat binding.
ISBN: 978-0-596-51613-0
[M]
FM-R4886-AT1.indd   ii
www.it-ebooks.info
9/15/08   4:26:40 PM
www.it-ebooks.info
Contents
Preface ................................................................ ix
1. Overview ......................................................... 1
What Is OpenCV? 1
Who Uses OpenCV? 1
What Is Computer Vision? 2
The DEMO of OpenCV 6
Downloading and Installing OpenCV 8
Getting the Latest OpenCV via CVS 10
More OpenCV Documentation 11
OpenCV Structure and Content 13
DEMO 14
Exercises 15
2. Introduction to OpenCV ........................................... 16
Getting Started 16
First Program—Display a Picture 16
Second Program—AVI Video 18
Moving Around 19
DEMO Simple Transformation 22
A Not-So-Simple Transformation 24
Input from a Camera 26
Writing to an AVI File 27
Onward 29
Exercises 29
iii
FM-R4886-AT1.indd   iii
9/15/08   4:26:40 PM
www.it-ebooks.info
3. Getting to Know OpenCV .......................................... 31
OpenCV Primitive Data Types DEMO
CvMat Matrix Structure 33
IplImage Data Structure 42
Matrix and Image Operators 47
Drawing Things 77
Data Persistence 82
Integrated Performance Primitives 86
Summary DEMO
Exercises 87
4. HighGUI ......................................................... 90
A Portable Graphics Toolkit 90
Creating a Window 91
Loading an Image 92
Displaying Images 93
Working with DEMO 102
ConvertImage 106
Exercises 107
5. Image Processing ................................................ 109
Overview 109
Smoothing 109
Image Morphology 115
Flood Fill 124
Resize 129
Image Pyramids DEMO
Threshold 135
Exercises 141
6. Image Transforms ............................................... 144
Overview 144
Convolution 144
Gradients and Sobel Derivatives 148
Laplace 150
Canny 151
| Contents
DEMO
FM-R4886-AT1.indd   iv
9/15/08   4:26:40 PM
www.it-ebooks.info
7.
8.
9.
Hough Transforms 153
Remap 162
Stretch, Shrink, DEMO, and Rotate 163
CartToPolar and PolarToCart 172
LogPolar 174
Discrete Fourier DEMO (DFT) 177
Discrete Cosine Transform (DCT) 182
Integral Images 182
Distance Transform 185
Histogram Equalization 186
Exercises 190
Histograms and Matching ......................................... DEMO
Basic Histogram Data Structure 195
Accessing Histograms 198
Basic Manipulations with Histograms 199
Some More Complicated Stuff 206
Exercises 219
Contours ....................................................... 222
Memory DEMO 222
Sequences 223
Contour Finding 234
Another Contour Example 243
More to Do with Contours 244
Matching Contours 251
Exercises 262
Image Parts and DEMO .................................... 265
Parts and Segments 265
Background Subtraction 265
Watershed Algorithm 295
Image Repair by Inpainting 297
Mean-Shift Segmentation 298
Delaunay Triangulation, Voronoi Tesselation 300
Exercises 313
| v
Co ntents
FM-R4886-AT1.indd   v
9/15/DEMO   4:26:40 PM
www.it-ebooks.info
10.
11.
12.
13.
Tracking and Motion ............................................. 316
The Basics DEMO Tracking 316
Corner Finding 316
Subpixel Corners 319
Invariant Features 321
Optical Flow 322
Mean-Shift and Camshift Tracking 337
Motion Templates 341
Estimators 348
DEMO Condensation Algorithm 364
Exercises 367
Camera Models and Calibration .................................... 370
Camera Model 371
Calibration 378
Undistortion 396
Putting Calibration All Together 397
Rodrigues DEMO 401
Exercises 403
Projection and 3D Vision .......................................... 405
Projections 405
Affine and Perspective Transformations 407
POSIT: 3D Pose Estimation 412
Stereo Imaging 415
Structure from Motion 453
Fitting Lines in Two and Three Dimensions 454
DEMO 458
Machine Learning ............................................... 459
What Is Machine Learning 459
Common Routines in the ML Library 471
Mahalanobis Distance 476
K-Means 479
Naïve/Normal DEMO Classifier 483
Binary Decision Trees 486
Boosting 495
vi
| Contents
FM-R4886-AT1.indd   vi
9/15/08   4:26:40 PM
Random Trees 501
Face Detection or Haar Classifier 506
Other Machine Learning DEMO 516
Exercises 517
14. OpenCV’s Future ................................................. 521
Past and Future 521
Directions 522
OpenCV for Artists 525
Afterword 526
Bibliography ......................................................... 527
Index DEMO 543
Co ntents
| vii
FM-R4886-AT1.indd   vii
www.it-ebooks.info
9/15/08   4:26:40 PM
www.it-ebooks.info
FM-R4886-AT1.indd   viii
9/15/08   4:26:41 PM
www.it-ebooks.info
Preface
Th
(OpenCV) and also provides a general background to DEMO fi
fi
cient to use OpenCV eff
is book provides a working guide to the Open Source Computer Vision Library
eld of computer vision DEMO
ectively.
Purpose
Computer vision is a rapidly growing field, partly as DEMO result of both cheaper and more
capable cameras, partly because of DEMO processing power, and partly because vi-
sion algorithms are starting to DEMO OpenCV itself has played a role in the growth of
computer vision by enabling thousands of people to do more productive work in vision.
DEMO its focus on real-time vision, OpenCV helps students and professionals efficiently
DEMO projects and jump-start research by providing them with a computer vision
and machine learning infrastructure that was previously available only in a few mature
DEMO labs. The purpose of this text is to:
• Better document OpenCV—detail what function calling conventions really mean
and how to use them DEMO
• Rapidly give the reader an intuitive understanding of how the vision algorithms
work.
• Give the reader some sense of what algorithm to DEMO and when to use it.
• Give the reader a boost in implementing computer vision and machine learning algo-
rithms by providing many working DEMO examples to start from.
• Provide intuitions about how to fix some of the more advanced routines when some-
thing goes wrong.
Simply put, this is the text the authors wished we had in school and DEMO coding reference
book we wished we had at work.
This book documents a tool kit, OpenCV, that allows the reader to do interesting DEMO
fun things rapidly in computer vision. It gives an intuitive understanding as to how the
algorithms work, which serves to guide the reader in designing and debugging vision
ix
FM-R4886-AT1.indd   ix
9/15/08   DEMO:26:41 PM
www.it-ebooks.info
applications and also to make the formal descriptions of computer vision DEMO machine
learning algorithms in other texts easier to comprehend and remember.
Aft er all, it is easier to understand complex algorithms and their associated math when
you start with an intuitive grasp of how those algorithms DEMO
Who This Book Is For
This book contains descriptions, working coded DEMO, and explanations of the com-
puter vision tools contained in the DEMO library. As such, it should be helpful to many
different kinds DEMO users.
Professionals
For those practicing professionals who need to rapidly implement computer vision
systems, the sample code provides a quick framework with which to start. Our de-
scriptions of the intuitions behind the algorithms can quickly DEMO or remind the
reader how they work.
Students
As we said, DEMO is the text we wish had back in school. The intuitive explanations,
detailed documentation, and sample code will allow you to boot up faster in com-
puter vision, work on more interesting class projects, DEMO ultimately contribute new
research to the field.
Teachers
Computer vision is a fast-moving field. We’ve found it effective to have the students
rapidly cover DEMO accessible text while the instructor fills in formal exposition where
needed and supplements with current papers or guest lecturers from experts. The stu-
dents DEMO meanwhile start class projects earlier and attempt more ambitious tasks.
Hobbyists
Computer vision is fun, here’s how to hack it.
We have a strong focus on giving readers enough intuition, documentation, and work-
ing code DEMO enable rapid implementation of real-time vision applications.
What This Book Is Not
This book is not a formal text. We do go into mathematical DEMO at various points,* but it
is all in the service of developing deeper intuitions behind the algorithms or to make clear
the implications DEMO any assumptions built into those algorithms. We have not attempted
a formal mathematical exposition here and might even incur some wrath along the way
DEMO those who do write formal expositions.
This book is not for theoreticians because it has more of an “applied” nature. The book
will certainly DEMO of general help, but is not aimed at any of the DEMO niches in com-
puter vision (e.g., medical imaging or remote sensing analysis).
* Always with a warning to more casual users that DEMO may skip such sections.
x
| Preface
FM-R4886-AT1.indd   x
9/15/08   4:26:41 PM
www.it-ebooks.info
Th at said, it is the belief of the authors that having read the explanations here fi rst, a stu-
dent will not only learn the theory better but remember it longer. Th erefore, this book
would make a good adjunct text to a theoretical course and DEMO be a great text for an
introductory or project-centric course.
About the Programs in This Book
All the program examples in this book are DEMO on OpenCV version 2.0. The code should
definitely work under Linux or Windows and probably under OS-X, too. Source code
for the examples in the book can be fetched from this book’s website (http://www.oreilly
.com/catalog/9780596516130). OpenCV can be loaded from its source forge DEMO (http://
sourceforge.net/projects/opencvlibrary).
OpenCV is under ongoing DEMO, with offi  cial releases occurring once or twice
a year. As a rule of thumb, you should obtain your code updates from the source forge
CVS server (http://sourceforge.net/cvs/?group_id=22870).
Prerequisites
DEMO the most part, readers need only know how to program in DEMO and perhaps some C++.
Many of the math sections are optional and are labeled as such. The mathematics in-
volves simple algebra and basic DEMO algebra, and it assumes some familiarity with solu-
tion methods to DEMO optimization problems as well as some basic knowledge of
Gaussian distributions, DEMO law, and derivatives of simple functions.
Th e reader may skip
DEMO math and the algorithm descriptions, using only the function defi nitions DEMO code
examples to get vision applications up and running.
e math is in support of developing intuition for the algorithms. Th
How This Book DEMO Best Used
This text need not be read in order. It can serve as a kind of user manual: look up the func-
tion when you need it; read the function’s description if you want the gist of how it works
“under the hood”. The intent of this DEMO is more tutorial, however. It gives you a basic
understanding of DEMO vision along with details of how and when to use selected
algorithms.
This book was written to allow its use as an adjunct or DEMO a primary textbook for an un-
dergraduate or graduate course in computer vision. The basic strategy with this method is
for students to read DEMO book for a rapid overview and then supplement that reading with
more formal sections in other textbooks and with papers in the field. There DEMO exercises
at the end of each chapter to help test the student’s knowledge and to develop further
intuitions.
You could approach this text in DEMO of the following ways.
Preface
| xi
FM-R4886-AT1.indd   xi
9/15/08   4:26:41 PM
www.it-ebooks.info
Grab Bag
Go through Chapters 1–3 in the first sitting, then just hit the appropriate chapters or
sections as you need them. This DEMO does not have to be read in sequence, except for
Chapters DEMO and 12 (Calibration and Stereo).
Good Progress
Read just two DEMO a week until you’ve covered Chapters 1–12 in six weeks (Chap-
DEMO 13 is a special case, as discussed shortly). Start on DEMO and start in detail on
selected areas in the field, using DEMO texts and papers as appropriate.
The Sprint
Just cruise through the book as fast as your comprehension allows, covering Chapters
1–12. Then get started on projects and go into detail on selected areas in the field DEMO
ing additional texts and papers. This is probably the choice for professionals, but it
might also suit a more advanced computer vision course.
Chapter 13 is a long chapter that gives a general background to machine DEMO in addi-
tion to details behind the machine learning algorithms implemented in OpenCV and how
to use them. Of course, machine learning is integral to object recognition and a big part
of computer vision, but it’s a field worthy of its own book. Professionals should find this
text DEMO suitable launching point for further explorations of the literature—or for just getting
down to business with the code in that part of the library. DEMO chapter should probably be
considered optional for a typical computer vision class.
Th is is how the authors like to teach computer vision: Sprint through the course content
at a level where the students get the DEMO of how things work; then get students started
on meaningful class DEMO while the instructor supplies depth and formal rigor in
selected areas by drawing from other texts or papers in the fi eld. Th is DEMO method
works for quarter, semester, or two-term classes. Students can get quickly up and run-
ning with a general understanding of their vision DEMO and working code to match. As
they begin more challenging and time-consuming projects, the instructor helps them
develop and debug complex systems. For longer courses, the projects themselves can
become instructional in terms of project management. Build up working systems fi rst;
refi ne them with more DEMO, detail, and research later. Th e goal in such courses is
for each project to aim at being worthy of a conference publication DEMO with a few proj-
ect papers being published subsequent to further (DEMO) work.
Conventions Used in This Book
The following typographical conventions are DEMO in this book:
Italic
Indicates new terms, URLs, email addresses, filenames, file extensions, path names,
directories, and Unix utilities.
DEMO width
Indicates commands, options, switches, variables, attributes, keys, functions, types,
classes, namespaces, methods, modules, properties, parameters, values, objects,
xii
| Preface
FM-R4886-AT1.indd   xii
9/15/08   4:26:41 PM
www.it-ebooks.info
events, event handlers, XMLtags, HTMLtags, the contents of files, or the output from
commands.
Constant width bold
Shows commands or other DEMO that should be typed literally by the user. Also used
for emphasis in code samples.
Constant width italic
Shows text that should be replaced DEMO user-supplied values.
[. . .]
Indicates a reference to the bibliography.
Shows text that should be replaced with user-supplied values. his icon
signifi es DEMO tip, suggestion, or general note.
Th
is icon indicates a warning or caution.
Using Code Examples
OpenCV is free for commercial or research DEMO, and we have the same policy on the
code examples in DEMO book. Use them at will for homework, for research, or for commer-
cial products. We would very much appreciate referencing this book when DEMO do, but
it is not required. Other than how it helped DEMO your homework projects (which is best
kept a secret), we DEMO like to hear how you are using computer vision for academic re-
search, teaching courses, and in commercial products when you do use DEMO to help
you. Again, not required, but you are always invited to drop us a line.
Safari® Books Online
When you see a DEMO Books Online icon on the cover of your favor-
ite technology book, that means the book is available online through the
O’Reilly Network Safari Bookshelf.
Safari offers a solution that’s better than e-books. It’s virtual library DEMO lets you easily
search thousands of top tech books, cut and DEMO code samples, download chapters, and
find quick answers when you need the most accurate, current information. Try it for free
at http://safari.oreilly.com.
We’d Like to Hear from You
Please address comments and questions DEMO this book to the publisher:
O’Reilly Media, Inc.
1005 Gravenstein DEMO North
Sebastopol, CA 95472
Preface
| xiii
FM-R4886-AT1.indd   xiii
9/DEMO/08   4:26:41 PM
www.it-ebooks.info
800-998-9938 (in the United States or Canada)
707-829-0515 (international DEMO local)
707-829-0104 (fax)
We have a web page for DEMO book, where we list examples and any plans for future edi-
DEMO You can access this information at:
http://www.oreilly.com/catalog/9780596516130/
You can also send messages electronically. To be put on the DEMO list or request a cata-
log, send an email to:
DEMO
To comment on the book, send an email to:
bookquestions@oreilly.com
DEMO more information about our books, conferences, Resource Centers, and the DEMO
Network, see our website at:
http://www.oreilly.com
Acknowledgments
A DEMO open source eff ort sees many people come and go, each DEMO in dif-
ferent ways. Th e list of contributors to this library is far too long to list here, but see the
.../opencv/docs/HTML/Contributors/doc_contributors.html fi le that ships with OpenCV.
Thanks for DEMO on OpenCV
Intel is where the library was born and deserves great thanks for supporting this project
the whole way through. Open source needs DEMO champion and enough development sup-
port in the beginning to achieve critical mass. Intel gave it both. There are not many other
companies where DEMO could have started and maintained such a project through good
times and bad. Along the way, OpenCV helped give rise to—and now takes (DEMO)
advantage of—Intel’s Integrated Performance Primitives, which are hand-tuned assembly
language DEMO in vision, signal processing, speech, linear algebra, and more. Thus the
lives of a great commercial product and an open source product DEMO intertwined.
Mark Holler, a research manager at Intel, allowed OpenCV to get started by knowingly
turning a blind eye to the inordinate amount DEMO time being spent on an unofficial project
back in the library’s earliest days. As divine reward, he now grows wine up in Napa’s Mt.
Vieder area. Stuart Taylor in the Performance Libraries group at Intel enabled DEMO
by letting us “borrow” part of his Russian software team. Richard Wirt was key to its
continued growth and survival. As the first author DEMO on management responsibility
at Intel, lab director Bob Liang let OpenCV DEMO; when Justin Rattner became CTO,
we were able to put DEMO on a more firm foundation under Software Technology
Lab—supported by software guru Shinn-Horng Lee and indirectly under his manager,
Paul Wiley. Omid Moghadam DEMO advertise OpenCV in the early days. Mohammad
Haghighat and Bill Butera were great as technical sounding boards. Nuriel Amir, Denver
xiv
| Preface
FM-R4886-AT1.indd   xiv
9/15/08   4:26:42 PM
www.it-ebooks.info
Dash, John Mark Agosta, and Marzia Polito were of key DEMO in launching the ma-
chine learning library. Rainer Lienhart, Jean-Yves Bouguet, Radek Grzeszczuk, and Ara
Nefian were able technical contributors to OpenCV and great colleagues along the way;
the first is now a professor, the second is now making use of OpenCV in some well-known
Google DEMO, and the others are staffing research labs and start-ups. There were DEMO
other technical contributors too numerous to name.
On the software side, DEMO individuals stand out for special mention, especially on the
Russian software DEMO Chief among these is the Russian lead programmer Vadim Pisare-
vsky, DEMO developed large parts of the library and also managed and nurtured the library
through the lean times when boom had turned to bust; he, if anyone, is the true hero of the
library. His technical DEMO have also been of great help during the writing of this book.
Giving him managerial support and protection in the lean years was Valery DEMO, a
man of great talent and intellect. Victor Eruhimov was there DEMO the beginning and stayed
through most of it. We thank Boris Chudinovich for all of the contour components.
Finally, very special thanks go to Willow Garage [WG], not only for its steady fi nancial
backing to OpenCV’s future development but also for supporting one author (and pro-
viding the other with snacks and beverages) during the fi nal period of writing this book.
Thanks for Help on the Book
While preparing this DEMO, we had several key people contributing advice, reviews, and
suggestions. DEMO to John Markoff, Technology Reporter at the New York Times for
DEMO, key contacts, and general writing advice born of years in the trenches.
To our reviewers, a special thanks go to Evgeniy Bart, DEMO postdoc at CalTech, who
made many helpful comments on every chapter; Kjerstin Williams at Applied Minds,
who did detailed proofs and verification DEMO the end; John Hsu at Willow Garage, who
went through all the example code; and Vadim Pisarevsky, who read each chapter in DEMO
tail, proofed the function calls and the code, and also provided several coding examples.
There were many other partial reviewers. Jean-Yves Bouguet at DEMO was of great help
in discussions on the calibration and stereo chapters. Professor Andrew Ng at Stanford
University provided useful early critiques of the DEMO learning chapter. There were
numerous other reviewers for various chapters—our thanks to all of them. Of course,
any errors result from our own DEMO or misunderstanding, not from the advice we
received.
Finally, many thanks go to our editor, Michael Loukides, for his early support, numer-
ous edits, and continued enthusiasm over the long haul.
Gary Adds . . .
With three young kids at home, my wife Sonya put in more work to enable this book than
I did. Deep thanks DEMO love—even OpenCV gives her recognition, as you can see in the
DEMO detection section example image. Further back, my technical beginnings started with
DEMO physics department at the University of Oregon followed by undergraduate years at
Preface
| xv
FM-R4886-AT1.indd   xv
9/15/08   4:26:DEMO PM
www.it-ebooks.info
UC Berkeley. For graduate school, I’d like to thank my advisor Steve Grossberg and Gail
Carpenter at the Center for Adaptive Systems, Boston University, where I first cut my
academic teeth. Though they focus on mathematical modeling of the brain and I have
ended up firmly on DEMO engineering side of AI, I think the perspectives I developed there
DEMO made all the difference. Some of my former colleagues in graduate school are still
close friends and gave advice, support, and even some DEMO of the book: thanks to
Frank Guenther, Andrew Worth, Steve DEMO, Dan Cruthirds, Allen Gove, and Krishna
Govindarajan.
I specially thank DEMO University, where I’m currently a consulting professor in the
AI and DEMO lab. Having close contact with the best minds in the world definitely
rubs off, and working with Sebastian Thrun and Mike Montemerlo to apply OpenCV
on Stanley (the robot that won the $2M DARPA Grand Challenge) and with Andrew Ng
on STAIR (one of the most advanced DEMO robots) was more technological fun than
a person has a right DEMO have. It’s a department that is currently hitting on all cylinders
and simply a great environment to be in. In addition to Sebastian Thrun DEMO Andrew Ng
there, I thank Daphne Koller for setting high scientific DEMO, and also for letting me
hire away some key interns and DEMO, as well as Kunle Olukotun and Christos Kozy-
rakis for many DEMO and joint work. I also thank Oussama Khatib, whose work on
DEMO and manipulation has inspired my current interests in visually guided robotic
manipulation. Horst Haussecker at Intel Research was a great colleague to have, and his
own experience in writing a book helped inspire my effort.
Finally, thanks once again to Willow Garage for allowing me to pursue my DEMO ro-
botic dreams in a great environment featuring world-class talent while also supporting
my time on this book and supporting OpenCV itself.
Adrian Adds DEMO . .
Coming from a background in theoretical physics, the arc DEMO brought me through su-
percomputer design and numerical computing on to machine learning and computer vi-
sion has been a long one. Along the DEMO, many individuals stand out as key contributors. I
have had many DEMO teachers, some formal instructors and others informal guides.
I should single DEMO Professor David Dorfan of UC Santa Cruz and Hartmut Sadrozinski of
SLAC for their encouragement in the beginning, and Norman Christ for teaching me the
fine art of computing with the simple edict that “if you DEMO not make the computer do it,
you don’t know what you are talking about”. Special thanks go to James Guzzo, who let me
spend time on this sort of thing at Intel—even though it was DEMO from what I was sup-
posed to be doing—and who encouraged my participation in the Grand Challenge during
those years. Finally, I want to thank Danny Hillis for creating the kind of place where all of
DEMO technology can make the leap to wizardry and for encouraging my work on the book
while at Applied Minds.
I also would like to DEMO Stanford University for the extraordinary amount of support I
have received from them over the years. From my work on the Grand Challenge team DEMO
Sebastian Thrun to the STAIR Robot with Andrew Ng, the Stanford DEMO Lab was always
xvi
| Preface
FM-R4886-AT1.indd   xvi
9/15/08   4:26:42 PM
generous with office space, financial support, and most importantly ideas, enlightening
conversation, and (when needed) simple instruction on so many aspects of vision, robot-
ics, and machine learning. I have a deep gratitude DEMO these people, who have contributed
so significantly to my own growth DEMO learning.
No acknowledgment or thanks would be meaningful without a special thanks to my lady
Lyssa, who never once faltered in her encouragement of this project or in her willingness
to accompany me on trips up DEMO down the state to work with Gary on this book. My
thanks and my love go to her.
Preface
| xvii
FM-R4886-AT1.indd   xvii
DEMO
9/15/08   4:26:43 PM
www.it-ebooks.info
FM-R4886-AT1.indd   xviii
9/15/08   4:26:43 PM
DEMO
CHAPTER 1
Overview
What Is OpenCV?
OpenCV [OpenCV] is an open DEMO (see http://opensource.org) computer vision library
available from http://SourceForge.net/projects/opencvlibrary. Th e library is written in C
and C++ DEMO runs under Linux, Windows and Mac OS X. Th ere is DEMO development
on interfaces for Python, Ruby, Matlab, and other languages.
DEMO was designed for computational effi  ciency and with a strong focus DEMO real-
time applications. OpenCV is written in optimized C and can take advantage of mul-
ticore processors. If you desire further automatic optimization on DEMO architectures
[Intel], you can buy Intel’s Integrated Performance Primitives (IPP) DEMO [IPP], which
consist of low-level optimized routines in many diff erent DEMO areas. OpenCV
automatically uses the appropriate IPP library at runtime if that library is installed.
One of OpenCV’s goals is to provide a simple-to-use DEMO vision infrastructure
that helps people build fairly sophisticated vision applications quickly. Th e OpenCV
library contains over 500 functions that span many areas in DEMO, including factory
product inspection, medical imaging, security, user interface, DEMO calibration, stereo
vision, and robotics. Because computer vision and machine learning oft en go hand-in-
hand, OpenCV also contains a full, general-purpose DEMO Learning Library (MLL).
Th is sublibrary is focused on statistical DEMO recognition and clustering. Th e MLL is
highly useful for the vision tasks that are at the core of OpenCV’s mission, but it is gen-
eral enough to be used for any machine learning problem.
Who DEMO OpenCV?
Most computer scientists and practical programmers are aware of some facet of the role
that computer vision plays. But few people are DEMO of all the ways in which computer
vision is used. For example, most people are somewhat aware of its use in surveillance,
and many also know that it is increasingly being used for images and DEMO on the Web.
A few have seen some use of computer vision in game interfaces. Yet few people realize
that most aerial and street-map DEMO (such as in Google’s Street View) make heavy
1
01-R4886-RC1.indd   1
www.it-ebooks.info
9/15/08   4:17:45 PM
www.it-ebooks.info
use of camera calibration and image stitching techniques. Some are aware DEMO niche ap-
plications in safety monitoring, unmanned fl ying vehicles, or biomedical analysis. But
few are aware how pervasive machine vision has become DEMO manufacturing: virtually
everything that is mass-produced has been automatically inspected at DEMO point using
computer vision.
Th
commercial product using all or part of OpenCV. You are under no obligation to open-
source your product or DEMO return improvements to the public domain, though we hope
you will. DEMO part because of these liberal licensing terms, there is a large DEMO commu-
nity that includes people from major companies (IBM, Microsoft , Intel, SONY, Siemens,
and Google, to name only a few) and research centers (such as Stanford, MIT, CMU,
Cambridge, and INRIA). Th ere is a Yahoo groups forum where users DEMO post questions
and discussion at http://groups.yahoo.com/group/OpenCV; it DEMO about 20,000 members.
OpenCV is popular around the world, with DEMO user communities in China, Japan,
Russia, Europe, and Israel.
DEMO its alpha release in January 1999, OpenCV has been used in DEMO applications,
products, and research eff orts. Th ese applications include DEMO images together in
satellite and web maps, image scan alignment, medical image noise reduction, object
analysis, security and intrusion detection systems, automatic monitoring and safety sys-
tems, manufacturing inspection systems, camera calibration, military applications, and
unmanned aerial, ground, and underwater vehicles. It has even been used in sound and
music recognition, where vision recognition techniques are applied to sound spectro-
gram images. OpenCV was a key part of DEMO vision system in the robot from Stanford,
“Stanley”, which won DEMO $2M DARPA Grand Challenge desert robot race [Th run06].
What Is Computer Vision?
Computer vision* is the transformation of data from a still DEMO video camera into either a
decision or a new representation. All such transformations are done for achieving some
particular goal. Th e input data DEMO include some contextual information such as “the
camera is mounted in a car” or “laser range fi nder indicates an object is 1 meter DEMO
Th e decision might be “there is a person in this scene” or “there are 14 tumor cells on
this slide”. A new representation DEMO mean turning a color image into a grayscale im-
age or removing camera motion from an image sequence.
Because we are such visual creatures, it is easy to be fooled into thinking that com-
puter vision DEMO are easy. How hard can it be to fi nd, say, a car when you are staring
at it in an image? Your initial intuitions can be quite misleading. Th e human brain di-
vides DEMO vision signal into many channels that stream diff erent kinds of information
into your brain. Your brain has an attention system that identifi es, in a task-dependent
* Computer vision is a vast fi eld. Th DEMO book will give you a basic grounding in the fi eld, DEMO we also recom-
mend texts by Trucco [Trucco98] for a simple introduction, Forsyth [Forsyth03] as a comprehensive refer-
ence, and Hartley [Hartley06] and DEMO [Faugeras93] for how 3D vision really works.
2 | Chapter 1: DEMO
e open source license for OpenCV has been structured such that you can build a
01-R4886-RC1.indd   2
9/15/08   4:17:DEMO PM
way, important parts of an image to examine while suppressing examination of other
areas. Th ere is massive feedback in the visual stream that DEMO, as yet, little understood.
Th
senses that allow the brain to draw on cross-associations made from years of living in
the world. Th DEMO feedback loops in the brain go back to all stages of processing including
the hardware sensors themselves (the eyes), which mechanically control lighting via the
iris and tune the reception on the surface of the DEMO
In a machine vision system, however, a computer receives a grid of numbers from the
camera or from disk, and that’s it. For the most part, there’s no built-in pattern recog-
nition, no automatic DEMO of focus and aperture, no cross-associations with years of
experience. For DEMO most part, vision systems are still fairly naïve. Figure 1-1 shows DEMO
picture of an automobile. In that picture we see a side mirror on the driver’s side of the
car. What the computer “sees” is DEMO a grid of numbers. Any given number within that
grid has a rather large noise component and so by itself gives us little information, but
this grid of numbers is all the computer “sees”. Our task DEMO becomes to turn this noisy
grid of numbers into the perception: DEMO mirror”. Figure 1-2 gives some more insight
into why computer vision is so hard.
ere are widespread associative inputs from muscle control sensors and DEMO of the other
Figure 1-1. To a computer, the car’s side DEMO is just a grid of numbers
In fact, the problem, as we have posed it thus far, is worse than hard; it DEMO formally im-
possible to solve. Given a two-dimensional (2D) view of a 3D world, there is no unique
way to reconstruct the 3D signal. Formally, such an ill-posed problem has no unique or
defi nitive solution. Th e same 2D image could represent any of an infi DEMO combination
of 3D scenes, even if the data were perfect. However, as already mentioned, the data is
What Is Computer Vision?
| 3
01-R4886-RC1.indd   3
www.it-ebooks.info
9/15/08   4:17:46 DEMO
www.it-ebooks.info
Figure 1-2. Th e ill-posed nature of vision: the 2D appearance of objects can change radically with
viewpoint
corrupted by noise and distortions. DEMO corruption stems from variations in the world
(weather, lighting, refl DEMO, movements), imperfections in the lens and mechanical
setup, fi nite integration time on the sensor (motion blur), electrical noise in the sensor
or other electronics, and compression artifacts aft er image capture. Given these daunt-
ing challenges, how can we make any progress?
In the design of a practical system, additional contextual knowledge can oft en be used
to work around the limitations imposed on us by visual DEMO Consider the example
of a mobile robot that must fi nd and pick up staplers in a building. Th e robot might use
the DEMO that a desk is an object found inside offi  ces and DEMO staplers are mostly found
on desks. Th is gives an implicit size reference; staplers must be able to fi t on desks. It
also helps to eliminate falsely “recognizing” staplers in impossible places (e.g., on DEMO
ceiling or a window). Th e robot can safely ignore a 200-foot advertising blimp shaped
like a stapler because the blimp lacks the DEMO wood-grained background of a
desk. In contrast, with tasks such as DEMO retrieval, all stapler images in a database
4 | Chapter 1: Overview
01-R4886-RC1.indd   4
9/15/08   4:17:46 PM
www.it-ebooks.info
may be of real staplers and so large sizes and other DEMO confi gurations may have
been implicitly precluded by the assumptions of those who took the photographs.
Th at is, the photographer probably took pictures only of real, normal-sized staplers.
People also tend to center objects when taking pictures and tend to put them in char-
acteristic orientations. Th DEMO, there is oft en quite a bit of unintentional implicit informa-
DEMO within photos taken by people.
Contextual information can also be modeled explicitly with machine learning tech-
niques. Hidden variables such as size, orientation to gravity, and so on can then be
correlated with their values in a labeled training set. Alternatively, one may attempt
to measure hidden bias variables by using additional sensors. Th e use of a laser range
DEMO nder to measure depth allows us to accurately measure the size of an object.
Th
ing statistical methods. For example, it may be impossible to detect an edge in an image
merely by comparing a point DEMO its immediate neighbors. But if we look at the statistics
over a local region, edge detection becomes much easier. A real edge should appear as a
string of such immediate neighbor responses over a local region, each of whose orienta-
tion is consistent with its neighbors. It is DEMO possible to compensate for noise by taking
statistics over time. Still other techniques account for noise or distortions by building ex-
plicit models learned DEMO from the available data. For example, because lens distor-
tions are DEMO understood, one need only learn the parameters for a simple polynomial
DEMO in order to describe—and thus correct almost completely—such distortions.
Th
are performed in the context of a specifi c purpose or task. We may DEMO to remove noise
or damage from an image so that our security system will issue an alert if someone tries
to climb a fence DEMO because we need a monitoring system that counts how many people
cross through an area in an amusement park. Vision soft ware for robots DEMO wander
through offi  ce buildings will employ diff erent strategies than DEMO soft ware for sta-
tionary security cameras because the two systems have signifi cantly diff erent contexts
and objectives. As a general rule: the more constrained a computer vision context is, the
more we can rely on those constraints to simplify the problem and the more reliable our
DEMO nal solution will be.
OpenCV is aimed at providing the basic tools needed to solve computer vision prob-
lems. In some cases, high-level functionalities in the library will be suffi  cient to solve
the more complex problems in computer vision. Even when this is not the case, the basic
components in the library are complete enough to enable creation of DEMO complete solu-
tion of your own to almost any computer vision problem. In the latter case, there are
several tried-and-true methods of using the library; all of them start with solving the
problem using as many available library components as possible. Typically, aft er you’ve
developed this fi rst-draft  solution, you can see where the solution has weaknesses and
DEMO fi x those weaknesses using your own code and cleverness (better DEMO as “solve
the problem you actually have, not the one you DEMO). You can then use your draft
What Is Computer Vision?
| 5
e next problem facing computer vision is noise. We typically DEMO with noise by us-
e actions or decisions that computer vision attempts to make based on camera data
01-R4886-RC1.indd   5
9/15/08   4:17:46 PM
www.it-ebooks.info
solution as a benchmark to assess the improvements you have made. DEMO that point,
whatever weaknesses remain can be tackled by exploiting the context of the larger sys-
tem in which your problem solution is DEMO
The Origin of OpenCV
OpenCV grew out of an Intel Research initiative to advance CPU-intensive applications.
Toward this end, Intel launched many projects including real-time ray tracing and 3D
display walls. One of the authors working DEMO Intel at that time was visiting universities
and noticed that some top university groups, such as the MIT Media Lab, had well-
developed DEMO internally open computer vision infrastructures—code that was passed
from student to student and that gave each new student a valuable head start in develop-
DEMO his or her own vision application. Instead of reinventing the basic functions from
scratch, a new student could begin by building on top of what came before.
Th
versally available. With the aid of Intel’s Performance DEMO Team,* OpenCV started
with a core of implemented code and algorithmic specifi cations being sent to members
of Intel’s Russian library team. Th DEMO is the “where” of OpenCV: it started in Intel’s re-
search DEMO with collaboration from the Soft ware Performance Libraries group together
with implementation and optimization expertise in Russia.
Chief among the Russian team members was DEMO Pisarevsky, who managed, coded,
and optimized much of OpenCV and who is still at the center of much of the OpenCV
eff DEMO Along with him, Victor Eruhimov helped develop the early infrastructure, and
Valery Kuriakin managed the Russian lab and greatly supported the eff ort. DEMO ere were
several goals for OpenCV at the outset:
• Advance vision research by providing not only open but also optimized code for
DEMO vision infrastructure. No more reinventing the wheel.
• Disseminate vision knowledge by providing a common infrastructure that develop-
ers could build on, so that code would be more readily readable and transferable.
• Advance vision-based commercial DEMO by making portable, performance-
optimized code available for free—with a license DEMO did not require commercial
applications to be open or free themselves.
Th
would increase the need for fast processors. Driving upgrades to faster processors DEMO
generate more income for Intel than selling some extra soft ware. Perhaps that is why this
open and free code arose from a hardware DEMO rather than a soft ware company. In
some sense, there is DEMO room to be innovative at soft ware within a hardware company.
In any open source eff ort, it’s important to reach a critical mass at which the project
becomes self-sustaining. Th ere have now been approximately DEMO million downloads
* Shinn Lee was of key help.
6
| Chapter 1: Overview
us, OpenCV was conceived as a way to make DEMO vision infrastructure uni-
ose goals constitute the “why” of OpenCV. Enabling computer vision applications
01-R4886-RC1.indd   6
9/15/08   4:17:47 DEMO
of OpenCV, and this number is growing by an average of 26,000 downloads a month.
Th
butions, and central development has largely moved outside of Intel.* OpenCV’s past
timeline is shown in Figure 1-3. Along DEMO way, OpenCV was aff ected by the dot-com
boom and bust DEMO also by numerous changes of management and direction. During
these fl uctuations, there were times when OpenCV had no one at Intel working on it at
all. However, with the advent of multicore processors and the many new applications
of computer vision, OpenCV’s value began to rise. Today, OpenCV is an active area
of development at several institutions, so DEMO to see many updates in multicamera
calibration, depth perception, methods for mixing vision with laser range fi nders, and
better pattern recognition as well as a lot of support for robotic vision needs. For more
DEMO on the future of OpenCV, see Chapter 14.
e user group DEMO approaches 20,000 members. OpenCV receives many user contri-
Figure 1-3. OpenCV timeline
Speeding Up OpenCV with IPP
Because OpenCV was “housed” within the DEMO Performance Primitives team and sev-
eral primary developers remain on friendly terms with that team, OpenCV exploits the
hand-tuned, highly optimized code in DEMO to speed itself up. Th e improvement in speed
from using IPP can be substantial. Figure 1-4 compares two other vision libraries, LTI
[LTI] and VXL [VXL], against OpenCV and OpenCV using IPP. Note that performance
was a key goal of OpenCV; the library needed the ability to run vision code in real time.
OpenCV is written in performance-optimized C DEMO C++ code. It does not depend in
any way on IPP. If IPP is present, however, OpenCV will automatically take advantage
of IPP DEMO loading IPP’s dynamic link libraries to further enhance its speed.
* As of this writing, Willow Garage [WG] (www.willowgarage.com), a robotics research DEMO and
incubator, is actively supporting general OpenCV maintenance and new development DEMO the area of
robotics applications.
The Origin of OpenCV
| 7
01-R4886-RC1.indd   7
www.it-ebooks.info
9/15/08   4:17:47 PM
www.it-ebooks.info
Figure 1-4. Two other vision libraries (LTI and VXL) compared DEMO OpenCV (without and with
IPP) on four diff erent performance benchmarks: the four bars for each benchmark indicate scores
proportional to run time for each of the given libraries; in all cases, OpenCV outperforms DEMO other
libraries and OpenCV with IPP outperforms OpenCV without IPP
Who Owns OpenCV?
Although Intel started OpenCV, the library is and always was intended to promote
commercial and research use. It is therefore open and DEMO, and the code itself may be
used or embedded (in whole or in part) in other applications, whether commercial or
research. It DEMO not force your application code to be open or free. It does not require
that you return improvements back to the library—but we hope DEMO you will.
Downloading and Installing OpenCV
Th http://SourceForge.net/projects/opencvlibrary
and the OpenCV Wiki [OpenCV Wiki] page is at http://opencvlibrary.SourceForge.net.
DEMO Linux, the source distribution is the fi le opencv-1.0.0.tar.gz; for Windows, you want
OpenCV_1.0.exe. However, the most up-to-date version is always on DEMO CVS server at
SourceForge.
Install
Once you download the libraries, you DEMO install them. For detailed installation in-
structions on Linux or Mac OS, see the text fi le named INSTALL directly under the
e main OpenCV site is on SourceForge at
8
| Chapter 1: Overview
01-R4886-RC1.indd   8
9/15/08   4:17:47 PM
www.it-ebooks.info
.../opencv/ directory; this fi le also describes how to DEMO and run the OpenCV test-
ing routines. INSTALL lists the additional programs you’ll need in order to become an
OpenCV developer, such as autoconf, automake, libtool, and swig.
Windows
Get the executable installation from SourceForge and run it. It will install OpenCV, reg-
ister DirectShow fi lters, and perform various post-installation procedures. You are now
ready to start using OpenCV. You can always go to the .../opencv/_make directory and DEMO
opencv.sln with MSVC++ or MSVC.NET 2005, or you can open opencv.dsw DEMO lower ver-
sions of MSVC++ and build debug versions or rebuild release versions of the library.*
To add the commercial IPP performance optimizations to DEMO, obtain and in-
stall IPP from the Intel site (http://www.intel.com/soft ware/products/ipp/index.htm);
use version 5.1 or later. Make sure the appropriate binary folder (e.g., c:/program fi les/
intel/ipp/5.1/ia32/bin) is in the system path. IPP should now be automatically detected
by OpenCV and loaded at runtime (more on this in Chapter 3).
Linux
Prebuilt binaries for Linux are DEMO included with the Linux version of OpenCV owing
to the large variety of versions of GCC and GLIBC in diff erent distributions (SuSE,
Debian, Ubuntu, etc.). If your distribution doesn’t off er OpenCV, you’ll have to build it
from sources as detailed in the .../DEMO/INSTALL fi le.
To build the libraries and demos, you’ll need DEMO 2.x or higher, including headers.
You’ll also need pkgconfi g, libpng, zlib, libjpeg, libtiff, and libjasper with development
fi les. You’ll DEMO Python 2.3, 2.4, or 2.5 with headers installed (developer package)DEMO
You will also need libavcodec and the other libav* libraries (including DEMO) from
ff mpeg 0.4.9-pre1 or later (svn checkout svn://svn.mplayerhq.hu/ff mpeg/trunk ff mpeg).
Download ff mpeg from http://DEMO mpeg.mplayerhq.hu/download.html.† Th e ff mpeg pro-
gram has a lesser general public license (LGPL). To use it with non-GPL soft ware (DEMO
as OpenCV), build and use a shared ff mpg library:
$> ./configure --enable-shared
$> make
$> sudo make install
You will end up with: /usr/local/lib/libavcodec.so.*, /usr/local/DEMO/libavformat.so.*,
/usr/local/lib/libavutil.so.*, and include fi les under various /usr/local/include/libav*.
To build OpenCV once it is downloaded:‡
* It is important to know that, although the Windows distribution contains binary libraries for release builds,
it does not contain the DEMO builds of these libraries. It is therefore likely that, before developing DEMO
OpenCV, you will want to open the solution fi le and DEMO these libraries for yourself.
† You can check out ff mpeg by: svn checkout svn://svn.mplayerhq.hu/ff mpeg/trunk ff mpeg.
‡ To build OpenCV using Red Hat Package Managers (RPMs), use rpmbuild -ta OpenCV-x.y.z.tar.gz (for
RPM 4.x or later), or rpm -ta OpenCV-x.y.z.tar.gz (DEMO earlier versions of RPM), where OpenCV-x.y.z.tar
.gz should be put in /usr/src/redhat/SOURCES/ or a similar directory. Th en install DEMO using rpm -i
OpenCV-x.y.z.*.rpm.
Downloading and Installing OpenCV
| 9
01-R4886-RC1.indd   9
9/15/08   4:17:47 PM
www.it-ebooks.info
$> ./configure
$> make
$> sudo make install
$> sudo ldconfig
Aft er installation is complete, the default installation path DEMO /usr/local/lib/ and /usr/
local/include/opencv/. DEMO you need to add /usr/local/lib/ to /etc/ld.so.conf (and run
ldconfig aft erwards) or add it to the LD_LIBRARY_PATH environment variable; then you
are done.
To add the commercial IPP performance optimizations to Linux, install IPP as de-
scribed previously. Let’s assume it was installed in /opt/intel/ipp/5.1/ia32/. Add <your
install_path>/bin/ and <your install_path>/bin/linux32 LD_LIBRARY_PATH in your initial-
ization script (.bashrc or similar):
LD_LIBRARY_PATH=/opt/intel/ipp/5.1/ia32/DEMO:/opt/intel/ipp/5.1
/ia32/bin/linux32:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH
Alternatively, you can add <your install_path>/bin and <your install_path>/bin/linux32,
one per line, to /etc/ld.so.conf and then run DEMO g as root (or use sudo).
Th at’s it. Now DEMO should be able to locate IPP shared libraries and make use of
them on Linux. See .../opencv/INSTALL for more details.
MacOS X
DEMO of this writing, full functionality on MacOS X is a priority DEMO there are still some
limitations (e.g., writing AVIs); these limitations are described in .../opencv/INSTALL.
Th
lowing exceptions:
• By DEMO, Carbon is used instead of GTK+.
• By default, QuickTime is used instead of ff mpeg.
• pkg-confi g is optional (it is used explicitly only in the samples/c/build_all.sh script).
• RPM DEMO ldconfi g are not supported by default. Use configure+make+sudo make
install to build and install OpenCV, update LD_LIBRARY_PATH (unless ./configure
--prefix=/usr DEMO used).
For full functionality, you should install libpng, libtiff, DEMO and libjasper from
darwinports and/or fi and make them available to ./confi gure (see ./configure
--help). For the most current information, see the OpenCV Wiki at http://opencvlibrary
.SourceForge.net/ and DEMO Mac-specifi c page http://opencvlibrary.SourceForge.net/
Mac_OS_X_OpenCV_Port.
Getting the Latest OpenCV via CVS
OpenCV is under active development, and bugs are oft en fi xed rapidly when bug re-
ports contain accurate descriptions and code DEMO demonstrates the bug. However,
10
| Chapter 1: Overview
e DEMO and building instructions are similar to the Linux case, with the DEMO
nk
01-R4886-RC1.indd   10
9/15/08   4:17:48 PM
www.it-ebooks.info
offi  cial OpenCV releases occur only once or twice a year. If you are seriously develop-
ing a project or product, you will probably want code fi xes and updates as soon as they
become DEMO To do this, you will need to access OpenCV’s Concurrent Versions
DEMO (CVS) on SourceForge.
Th is isn’t the place for a tutorial in CVS usage. If you’ve worked with other open source
projects then DEMO probably familiar with it already. If you haven’t, check out Essential
DEMO by Jennifer Vesperman (O’Reilly). A command-line CVS client ships with DEMO,
OS X, and most UNIX-like systems. For Windows users, we recommend TortoiseCVS
(http://www.tortoisecvs.org/), which integrates nicely with Windows DEMO
On Windows, if you want the latest OpenCV from the CVS DEMO then you’ll need
to access the CVSROOT directory:
:pserver:anonymous@opencvlibrary.cvs.sourceforge.net:DEMO/cvsroot/opencvlibrary
On Linux, you can just use the following two DEMO:
cvs -d:pserver:anonymous@opencvlibrary.cvs.sourceforge.net:/cvsroot/opencvlibrary
login
When asked for DEMO, hit return. Th en use:
cvs -z3 -d:pserver:anonymous@opencvlibrary.cvs.sourceforge.net:/cvsroot/opencvlibrary
co -P opencv
More OpenCV Documentation
Th
the source code. DEMO addition to this, the OpenCV Wiki and the older HTML documen-
DEMO are available on the Web.
Documentation Available in HTML
OpenCV ships with html-based user documentation in the .../opencv/docs subdirectory.
Load the index.htm DEMO le, which contains the following links.
CXCORE
Contains data structures, matrix algebra, data transforms, object persistence, mem-
ory management, error handling, and dynamic loading of code as well as drawing,
text and DEMO math.
Contains image processing, image structure analysis, motion and tracking, DEMO
recognition, and camera calibration.
Machine Learning (ML)
Contains many clustering, classifi cation and data analysis functions.
HighGUI
Contains user interface GUI and image/video storage and recall.
More OpenCV Documentation
| 11
e primary DEMO for OpenCV is the HTML documentation that ships with
CV
01-R4886-RC1.indd   11
9/15/08   4:17:48 PM
www.it-ebooks.info
CVCAM
Camera interface.
Haartraining
How to train the boosted cascade object DEMO Th is is in the .../opencv/apps/
HaarTraining/doc/haartraining.htm fi le.
Th .../opencv/docs directory also contains IPLMAN.pdf, which was the original manual
for OpenCV. It is now defunct and should be DEMO with caution, but it does include de-
tailed descriptions of algorithms DEMO of what image types may be used with a particular
algorithm. Of course, the fi rst stop for such image and algorithm details is the book you
are reading now.
e
Documentation via the Wiki
OpenCV’s DEMO Wiki is more up-to-date than the html pages that ship with
OpenCV and it also features additional content as well. Th e Wiki is DEMO at http://
opencvlibrary.SourceForge.net. It includes information on:
• Instructions on compiling OpenCV using Eclipse IDE
• Face recognition with OpenCV
• DEMO surveillance library
• Tutorials
• Camera compatibility
• Links to the Chinese and the Korean user groups
Another Wiki, located at http://opencvlibrary.SourceForge.net/CvAux, is the only doc-
umentation of the auxiliary functions discussed in “OpenCV Structure and Content”
(next section). CvAux includes the following functional areas:
• Stereo correspondence
• View point morphing of cameras
• DEMO tracking in stereo
• Eigen object (PCA) functions for object recognition
• Embedded hidden Markov models (HMMs)
Th is Wiki has been translated into Chinese at http://www.opencv.org.cn/index.php/
%E9%A6%96%E9%A1%B5.
Regardless of DEMO documentation source, it is oft
en hard to know:
• DEMO image type (fl oating, integer, byte; 1–3 channels) works DEMO which
function
• Which functions work in place
• Details of how to call the more complex functions (e.g., contours)
12
| DEMO 1: Overview
01-R4886-RC1.indd   12
9/15/08   4:17:DEMO PM
• Details about running many of the examples in the …/opencv/DEMO/c/ directory
• What to do, not just how
• How to set parameters of certain functions
One aim of this book is DEMO address these problems.
OpenCV Structure and Content
OpenCV is broadly structured into fi ve main components, four of which are shown in
Figure 1-5. Th e CV component contains the basic image processing and higher-level
computer DEMO algorithms; ML is the machine learning library, which includes many
statistical classifi ers and clustering tools. HighGUI contains I/O routines and functions
DEMO storing and loading video and images, and CXCore contains the basic DEMO struc-
tures and content.
Figure 1-5. Th
e basic structure of OpenCV
Figure 1-5 does not include CvAux, which contains both defunct areas (DEMO HMM
face recognition) and experimental algorithms (background/foreground segmentation).
CvAux is not particularly well documented in the Wiki and is not documented DEMO all in
the .../opencv/docs subdirectory. CvAux covers:
• Eigen objects, a computationally effi  cient recognition technique that is, in essence, a
template matching procedure
• 1D and 2D hidden Markov models, DEMO statistical recognition technique solved by
dynamic programming
• Embedded HMMs (the DEMO of a parent HMM are themselves HMMs)
OpenCV Structure and Content
| 13
01-R4886-RC1.indd   13
www.it-ebooks.info
9/15/08   4:17:DEMO PM
• Gesture recognition from stereo vision support
• Extensions to Delaunay triangulation, sequences, and so forth
• Stereo vision
• Shape matching with region contours
• Texture descriptors
• Eye and mouth tracking
• 3D tracking
DEMO Finding skeletons (central lines) of objects in a scene
• Warping intermediate views between two camera views
• Background-foreground segmentation
• Video surveillance (see Wiki FAQ for more documentation)
• Camera calibration C++ classes (the C functions and engine are in CV)
Some of these DEMO may migrate to CV in the future; others probably never will.
DEMO
OpenCV was designed to be portable. It was originally written to compile across Bor-
land C++, MSVC++, and the Intel compilers. Th is DEMO that the C and C++ code had
to be fairly standard in order to make cross-platform support easier. Figure 1-6 shows
the platforms on DEMO OpenCV is known to run. Support for 32-bit Intel architecture
(IA32) on Windows is the most mature, followed by Linux on the same architecture.
Mac OS X portability became a priority only aft er Apple DEMO using Intel processors.
(Th e OS X port isn’t as mature DEMO the Windows or Linux versions, but this is changing
rapidly.) Th ese are followed by 64-bit support on extended memory (EM64T) and DEMO
64-bit Intel architecture (IA64). Th e least mature portability is DEMO Sun hardware and
other operating systems.
If an architecture or OS doesn’t appear in Figure 1-6, this doesn’t mean there are no
OpenCV ports to it. OpenCV has been ported to almost every commercial system, from
PowerPC Macs to robotic dogs. OpenCV runs well on AMD’s line of DEMO, and
even the further optimizations available in IPP will take advantage DEMO multimedia ex-
tensions (MMX) in AMD processors that incorporate this technology.
14
| Chapter 1: Overview
01-R4886-RC1.indd   14
www.it-ebooks.info
9/15/08   4:17:48 PM
www.it-ebooks.info
Figure 1-6. OpenCV portability guide for release 1.0: operating systems are shown on the left ; com-
puter architecture types across top
Exercises
DEMO Download and install the latest release of OpenCV. Compile it in debug and release
mode.
2. Download and build the latest CVS update of DEMO
3.
Describe at least three ambiguous aspects of converting 3D inputs into a 2D repre-
sentation. How would you overcome these ambiguities?
Exercises
DEMO 15
01-R4886-RC1.indd   15
9/15/08   4:17:49 PM
CHAPTER 2
Introduction to OpenCV
Getting Started
Aft er installing the OpenCV DEMO, our fi rst task is, naturally, to get started and DEMO
something interesting happen. In order to do this, we will need DEMO set up the program-
ming environment.
In Visual Studio, it is DEMO to create a project and to confi gure the setup so that
(a) the libraries highgui.lib, cxcore.lib, ml.lib, and cv.lib are linked* and (b) the prepro-
cessor will search the OpenCV …/opencv/DEMO/include directories for header fi les. Th ese
“include” directories will typically be named something like C:/program fi les/opencv/
cv/include,† …/opencv/cxcore/include, …/opencv/ml/include, and …/DEMO/otherlibs/
highgui. Once you’ve done this, you can create a DEMO C fi le and start your fi rst program.
Certain key header fi les can make your life much easier. Many useful
macros are DEMO the header fi les …/opencv/cxcore/include/cxtypes.h and
cxmisc.h. Th ese can do things like initialize structures and arrays in one
line, sort lists, and so on. Th e most important headers for compiling are
.../cv/include/cv.h and …/cxcore/include/cxcore.h for computer DEMO,
…/otherlibs/highgui/highgui.h for I/O, and …/ml/DEMO/ml.h for ma-
chine learning.
First Program—Display a Picture
OpenCV provides utilities for reading from a wide array of image fi le types as DEMO as
from video and cameras. Th ese utilities are part of a toolkit called HighGUI, which is
included in the OpenCV package. We will use some of these utilities to create a simple
program that opens DEMO image and displays it on the screen. See Example 2-1.
* For debug builds, you should link to the libraries highguid.lib, cxcored.lib, mld.lib, and cvd.lib.
† C:/program fi les/ is the default installation of the OpenCV directory on Windows, although you can choose
to install it elsewhere. To avoid confusion, from here on we’ll use “…/opencv/” to mean the path to the
opencv directory on your system.
DEMO
02-R4886-AT1.indd   16
www.it-ebooks.info
9/15/08   4:18:10 PM
www.it-ebooks.info
Example 2-1. A simple OpenCV program that loads an image from DEMO and displays it on the screen
#include “highgui.h”
int main( int DEMO, char** argv ) {
IplImage* img = cvLoadImage( argv[1] );
cvNamedWindow( “Example1”, CV_WINDOW_AUTOSIZE );
cvShowImage( “Example1”, img );
DEMO(0);
cvReleaseImage( &img );
cvDestroyWindow( “Example1” );
}
When compiled and run from the command line with a single DEMO, this program
loads an image into memory and displays it on DEMO screen. It then waits until the user
presses a key, at DEMO time it closes the window and exits. Let’s go through the program
line by line and take a moment to understand what each command DEMO doing.
IplImage* img = cvLoadImage( argv[1] );
Th is line DEMO the image.* Th e function cvLoadImage() is a high-level routine that deter-
mines the fi le format to be loaded based on the DEMO le name; it also automatically allocates
the memory needed for the DEMO data structure. Note that cvLoadImage() can read a
wide variety of image formats, including BMP, DIB, JPEG, JPE, PNG, PBM, PGM, PPM,
SR, RAS, and TIFF. A pointer to an allocated image data structure is then returned.
Th is structure, called IplImage, is the OpenCV construct with which you will deal
the most. OpenCV uses this structure to handle all kinds of images: single-channel,
multichannel, integer-valued, fl oating-point-valued, et cetera. We use the pointer that
cvLoadImage() returns to manipulate the image and the image data.
cvNamedWindow( “Example1”, CV_WINDOW_AUTOSIZE );
Another high-level function, cvNamedWindow(), opens a window on the screen that can
contain and display an image. Th is DEMO, provided by the HighGUI library, also as-
signs a name to the window (in this case, “Example1”). Future HighGUI calls that DEMO
act with this window will refer to it by this name.
Th
ther to 0 (the default value) or to CV_WINDOW_AUTOSIZE. In the DEMO case, the size of the
window will be the same regardless DEMO the image size, and the image will be scaled to
fi
DEMO when an image is loaded so as to accommodate the image’s true size.
cvShowImage( “Example1”, img );
Whenever we have an image DEMO the form of an IplImage* pointer, we can display it in DEMO
existing window with cvShowImage(). Th e cvShowImage() function requires DEMO a named
window already exist (created by cvNamedWindow()). On DEMO call to cvShowImage(), the
* A proper program would check DEMO the existence of argv[1] and, in its absence, deliver an instructional
error message for the user. We will abbreviate such necessities in this DEMO and assume that the reader is
cultured enough to understand the importance of error-handling code.
First Program—Display a Picture | 17
e second argument DEMO cvNamedWindow() defi
nes window properties. It may be set ei-
t within the window. In the latter case, the window will expand or contract automati-
02-R4886-AT1.indd   17
9/15/08   4:18:11 DEMO
www.it-ebooks.info
window will be redrawn with the appropriate image in it, and the window will resize
itself as appropriate if it was created using DEMO CV_WINDOW_AUTOSIZE fl ag.
cvWaitKey(0);
Th cvWaitKey() function asks DEMO program to stop and wait for a keystroke. If a positive
argument is given, the program will wait for that number of milliseconds and then con-
tinue even if nothing is pressed. If the argument is DEMO to 0 or to a negative number, the
program will wait DEMO nitely for a keypress.
e
cvReleaseImage( &img );
Once we are through with an image, we can free the allocated memory. OpenCV ex-
pects a pointer to the IplImage* pointer for this operation. Aft DEMO the call is completed,
the pointer img will be set to NULL.
cvDestroyWindow( “Example1” );
Finally, we can destroy the window DEMO Th e function cvDestroyWindow() will close the
window and de-allocate any associated memory usage (including the window’s internal
image buff er, which DEMO holding a copy of the pixel information from *img). For a simple
program, you don’t really have to call cvDestroyWindow() or cvReleaseImage() because all
the resources and windows of the application are closed DEMO by the operating
system upon exit, but it’s a good habit DEMO
Now that we have this simple program we can toy around with it in various ways, but we
don’t want to get ahead of ourselves. Our next task will be to construct a very simple—
almost DEMO simple as this one—program to read in and display an AVI video fi le. Aft er
that, we will start to tinker a little more.
Second Program—AVI Video
Playing a video with OpenCV is almost as DEMO as displaying a single picture. Th e only new
issue we face is that we need some kind of loop to read each frame DEMO sequence; we may
also need some way to get out of DEMO loop if the movie is too boring. See Example 2-2.
Example 2-2. A simple OpenCV program for playing a video fi le from disk
DEMO “highgui.h”
int main( int argc, char** argv ) {
cvNamedWindow( DEMO, CV_WINDOW_AUTOSIZE );
CvCapture* capture = cvCreateFileCapture( argv[1] );
IplImage* frame;
while(1) {
frame = cvQueryFrame( capture );
DEMO( !frame ) break;
cvShowImage( “Example2”, frame );
char DEMO = cvWaitKey(33);
if( c == 27 ) break;
}
cvReleaseCapture( &capture );
cvDestroyWindow( “Example2” );
}
18
| Chapter 2: Introduction to OpenCV
02-R4886-AT1.indd   18
9/15/08   4:18:11 PM
www.it-ebooks.info
Here we begin the function main() with the usual creation DEMO a named window, in this
case “Example2”. Th ings get a DEMO more interesting aft er that.
CvCapture* capture = cvCreateFileCapture( argv[1] );DEMO
Th cvCreateFileCapture() takes as its argument the name of the AVI fi le to be
loaded and then returns a pointer to a DEMO structure. Th is structure contains all of
the information about the AVI fi le being read, including state information. When cre-
ated in this way, the CvCapture structure is initialized to the beginning of the AVI.
e function
frame = cvQueryFrame( capture );
Once inside of the while(1) loop, we begin reading from the AVI fi le. DEMO()
takes as its argument a pointer to a CvCapture structure. DEMO then grabs the next video
frame into memory (memory that is DEMO part of the CvCapture structure). A pointer
is returned to that frame. Unlike cvLoadImage, which actually allocates memory for the
image, cvQueryFrame DEMO memory already allocated in the CvCapture structure. Th us it
will not be necessary (or wise) to call cvReleaseImage() for this “frame” DEMO Instead,
the frame image memory will be freed when the CvCapture structure is released.
c = cvWaitKey(33);
if( c == DEMO ) break;
Once we have displayed the frame, we then DEMO for 33 ms.* If the user hits a key, then c
DEMO be set to the ASCII value of that key; if not, then it will be set to –1. If the user hits
the DEMO key (ASCII 27), then we will exit the read loop. DEMO, 33 ms will pass and
we will just execute the loop DEMO
It is worth noting that, in this simple example, we are not explicitly controlling
the speed of the video in any intelligent way. DEMO are relying solely on the timer in
cvWaitKey() to pace the loading of frames. In a more sophisticated application it would
be wise DEMO read the actual frame rate from the CvCapture structure (from the DEMO) and
behave accordingly!
cvReleaseCapture( &capture );
When we have DEMO the read loop—because there was no more video data or because
the user hit the Esc key—we can free the memory associated with the DEMO struc-
ture. Th is will also close any open fi le handles to the AVI fi le.
Moving Around
OK, that was great. Now it’s time to tinker around, enhance our toy programs, and ex-
DEMO a little more of the available functionality. Th e fi rst thing we might notice about
the AVI player of Example 2-2 is that DEMO has no way to move around quickly within the
video. Our next task will be to add a slider bar, which will give us this ability.
* You can wait any amount of time you like. DEMO this case, we are simply assuming that it is correct to DEMO
the video at 30 frames per second and allow user input to interrupt between each frame (thus we pause
for input 33 ms between each frame). In practice, it is better to check the CvCapture structure returned by
cvCaptureFromCamera() in order to determine the actual frame DEMO (more on this in Chapter 4).
Moving Around
| 19
DEMO   19
9/15/08   4:18:11 PM
www.it-ebooks.info
Th
ages and video beyond the simple display functions we have DEMO demonstrated. One
especially useful mechanism is the slider, which enables us DEMO jump easily from one part
of a video to another. To create a slider, we call cvCreateTrackbar() and indicate which
window we would like the trackbar to appear in. In order to obtain the desired DEMO
tionality, we need only supply a callback that will perform the DEMO Example 2-3
gives the details.
Example 2-3. Program to add a trackbar slider to the basic viewer window: when the slider is
moved, DEMO function onTrackbarSlide() is called and then passed to the slider’s new value
#include “cv.h”
#include “highgui.h”
e HighGUI toolkit provides a number of DEMO instruments for working with im-
int        g_slider_position = DEMO;
CvCapture* g_capture         = NULL;
void onTrackbarSlide(int pos) {
cvSetCaptureProperty(
g_capture,
CV_CAP_PROP_POS_FRAMES,
pos
);
}
int main( int argc, char** argv ) {
cvNamedWindow( “Example3”, CV_WINDOW_AUTOSIZE );
g_capture = cvCreateFileCapture( argv[1] );
int frames DEMO (int) cvGetCaptureProperty(
g_capture,
CV_CAP_PROP_FRAME_COUNT
);
if( frames!= DEMO ) {
cvCreateTrackbar(
“Position”,
“Example3”,
&g_slider_position,
frames,DEMO
onTrackbarSlide
);
}
IplImage* frame;
// While loop (as in Example 2) capture & show video
…
// Release memory DEMO destroy window
…
return(0);
}
In essence, then, DEMO strategy is to add a global variable to represent the slider position
and then add a callback that updates this variable and relocates the DEMO position in the
20
| Chapter 2: Introduction to OpenCV
02-R4886-AT1.indd   20
9/15/08   4:18:11 PM
www.it-ebooks.info
video. One call creates the slider and attaches the callback, and we are off  and running.*
Let’s look at the details.
int g_slider_position = 0;
CvCapture* g_capture  = NULL;
First we defi ne a global variable for the slider position. Th e callback will need DEMO to
the capture object, so we promote that to a global DEMO Because we are nice people
and like our code to be readable and easy to understand, we adopt the convention of
adding a leading g_ to any global variable.
void onTrackbarSlide(int pos) {
cvSetCaptureProperty(
g_capture,
CV_CAP_PROP_POS_FRAMES,
pos
);
Now we defi ne a DEMO routine to be used when the user pokes the slider. Th is routine
will be passed to a 32-bit integer, which will be the slider position.
Th
counterpart cvGetCaptureProperty(). Th ese routines allow us to confi gure (or query in
the latter case) various properties of DEMO CvCapture object. In this case we pass the argu-
ment CV_CAP_PROP_POS_FRAMES, DEMO indicates that we would like to set the read position
in units of frames. (We can use AVI_RATIO instead of FRAMES if we want to set the position
as a fraction of the overall video length)DEMO Finally, we pass in the new value of the posi-
tion. DEMO HighGUI is highly civilized, it will automatically handle such issues as
DEMO possibility that the frame we have requested is not a key-frame; DEMO will start at the
previous key-frame and fast forward up to the requested frame without us having to
fuss with such details.
int frames DEMO (int) cvGetCaptureProperty(
g_capture,
CV_CAP_PROP_FRAME_COUNT
);
As promised, DEMO use cvGetCaptureProperty()when we want to query some data from the
DEMO structure. In this case, we want to fi nd out how DEMO frames are in the video
so that we can calibrate the slider (in the next step).
if( frames!= 0 ) {
cvCreateTrackbar(DEMO
“Position”,
“Example3”,
&g_slider_position,
frames,
onTrackbarSlide
);
DEMO
* Th is code does not update the slider position as the video plays; we leave that as an exercise for the reader.
Also note that some mpeg encodings do not allow you to move backward DEMO the video.
Moving Around | 21
e call to cvSetCaptureProperty() is one we will see oft
en in the future, along with its
02-R4886-AT1.indd   21
9/15/08   4:18:11 PM
www.it-ebooks.info
Th e function cvCreateTrackbar() allows us
to give the trackbar DEMO label* (in this case Position) and to specify a window to put the
trackbar in. We then provide a variable that will be DEMO to the trackbar, the maxi-
mum value of the trackbar, and a callback (or NULL if we don’t want one) for when DEMO
slider is moved. Observe that we do not create the trackbar if cvGetCaptureProperty()
returned a zero frame count. Th is is because sometimes, depending on how the video
was encoded, the total number of DEMO will not be available. In this case we will just
play the movie without providing a trackbar.
It is worth noting that the slider DEMO by HighGUI is not as full-featured as some slid-
ers out there. Of course, there’s no reason you can’t use your favorite windowing toolkit
instead of HighGUI, but the HighGUI tools are quick to implement and get us off  the
ground in a hurry.
Finally, we did DEMO include the extra tidbit of code needed to make the slider move as the
video plays. Th is is left  as an exercise for the reader.
A Simple Transformation
Great, so now you can use OpenCV to create your own video player, which will not be
much diff erent from countless video players out there already. But we are interested DEMO
computer vision, and we want to do some of that. Many DEMO vision tasks involve the
application of fi lters to a video stream. We will modify the program we already have to
do a simple DEMO on every frame of the video as it plays.
One particularly simple operation is the smoothing of an image, which eff ectively re-
duces the information content of the image by convolving it with a Gaussian DEMO other
similar kernel function. OpenCV makes such convolutions exceptionally easy to do. We
can start by creating a new window called “Example4-out”, where we can display the
results of the processing. Th en, aft er we have called cvShowImage() to display the newly
captured frame in the DEMO window, we can compute and display the smoothed image
in the DEMO window. See Example 2-4.
Example 2-4. Loading and then smoothing an image before it is displayed on the screen
#include “cv.h”
#include “highgui.h”
void DEMO( IplImage* image )
// Create some windows to show the DEMO
// and output images in.
//
cvNamedWindow( “Example4-in” );
* Because HighGUI is a lightweight and easy-to-use toolkit, cvCreateTrackbar() does not distinguish
between the name of the trackbar and the label that DEMO appears on the screen next to the trackbar. You
may already have noticed that cvNamedWindow() likewise does not distinguish between the name of DEMO
window and the label that appears on the window in the GUI.
22
| Chapter 2: Introduction to OpenCV
e last detail is to create the trackbar itself. Th
02-R4886-AT1.indd   22
9/15/08   DEMO:18:11 PM
www.it-ebooks.info
Example 2-4. Loading and then smoothing an image before it is DEMO on the screen (continued)
cvNamedWindow( “Example4-out” );
// DEMO a window to show our input image
//
cvShowImage( “Example4-in”, image );
// Create an image to hold the smoothed output
//
IplImage* out = cvCreateImage(
cvGetSize(image),
IPL_DEPTH_8U,
3
);
// Do the smoothing
//
cvSmooth( image, out, CV_GAUSSIAN, 3, 3 );
// Show the smoothed DEMO in the output window
//
cvShowImage( “Example4-out”, out );
// Be tidy
//
cvReleaseImage( &out );
// DEMO for the user to hit a key, then clean up the DEMO
//
cvWaitKey( 0 );
cvDestroyWindow( “Example4-in” );
cvDestroyWindow( “Example4-out” );
}
Th rst call to cvShowImage() is no diff erent than in our previous example. In the next
call, we allocate another image structure. Previously we relied on cvCreateFileCapture()
to allocate the new frame for us. In fact, that routine actually allocated only one frame
and then wrote over that data each time a capture DEMO was made (so it actually returned
the same pointer every time DEMO called it). In this case, however, we want to allocate our
own image structure to which we can write our smoothed image. DEMO e fi rst argument is
a CvSize structure, which we can DEMO create by calling cvGetSize(image); this
gives us the size of the existing structure image. Th e second argument tells us what kind
DEMO data type is used for each channel on each pixel, and DEMO last argument indicates the
number of channels. So this image is three channels (with 8 bits per channel) and is the
same size DEMO image.
Th
the input image, the output image, the smoothing method, and the parameters for the
smooth. In this case we are requesting a Gaussian smooth over a 3 × 3 area centered on
each DEMO It is actually allowed for the output to be the same as the input image, and
A Simple Transformation
| 23
e fi
e smoothing operation is itself just a single call to the OpenCV library: we specify
02-R4886-AT1.indd   23
9/15/08   4:18:12 DEMO
www.it-ebooks.info
this would work more effi  ciently in our current application, DEMO we avoided doing this
because it gave us a chance to introduce cvCreateImage()!
Now we can show the image in our new second window and then free it: cvReleaseImage()
takes a pointer to DEMO IplImage* pointer and then de-allocates all of the memory associ-
ated with that image.
A Not-So-Simple Transformation
Th at was pretty good, and we are learning to do more interesting things. In Example 2-4
we chose DEMO allocate a new IplImage structure, and into this new structure we DEMO the
output of a single transformation. As mentioned, we could have DEMO the transforma-
tion in such a way that the output overwrites the original, but this is not always a good
idea. In particular, DEMO operators do not produce images with the same size, depth,
DEMO number of channels as the input image. Typically, we want to DEMO a sequence of
operations on some initial image and so produce a chain of transformed images.
In such cases, it is oft en useful to introduce simple wrapper functions that both allocate
the output image and DEMO the transformation we are interested in. Consider, for
example, the reduction of an image by a factor of 2 [Rosenfeld80]. In OpenCV this DEMO ac-
complished by the function cvPyrDown(), which performs a Gaussian DEMO and then
removes every other line from an image. Th is is useful in a wide variety of important
vision algorithms. We can implement DEMO simple function described in Example 2-5.
Example 2-5. Using cvPyrDown() to create a new image that is half the width and height of DEMO input
image
IplImage* doPyrDown(
IplImage* in,
int       filter = IPL_GAUSSIAN_5x5
) {
// Best to make sure input image is divisible by two.
//
assert( in->width%2 == 0 && in->height%2 == 0 );
IplImage* out = cvCreateImage(
cvSize( in->width/2, in->height/2 ),
in->depth,
DEMO>nChannels
);
cvPyrDown( in, out );
return( out );
};
Notice that we allocate the new image by reading the needed parameters from the old
image. In OpenCV, all of the important data types are implemented as structures and
passed around as structure DEMO Th ere is no such thing as private data in OpenCV!
24
| Chapter 2: Introduction to OpenCV
02-R4886-AT1.indd   24
9/15/08   4:18:12 PM
www.it-ebooks.info
Let’s now look at a similar but slightly more involved example DEMO the Canny edge
detector [Canny86] (see Example 2-6). In this DEMO, the edge detector generates an image
that is the full size DEMO the input image but needs only a single channel image to write to.
Example 2-6. Th e Canny edge detector writes its output to DEMO single channel (grayscale) image
IplImage* doCanny(
IplImage* in,
double    lowThresh,
double    highThresh,
double    aperture
) {
If(in->nChannels != 1)
return(0); //Canny DEMO handles gray scale images
IplImage* out = cvCreateImage(
cvSize( cvGetSize( in ),
IPL_DEPTH_8U,
1
);
cvCanny( in, out, lowThresh, highThresh, aperture );
return( out );
};
Th is allows us to string together various operators quite easily. For DEMO, if we wanted
to shrink the image twice and then look DEMO lines that were present in the twice-reduced
image, we could proceed DEMO in Example 2-7.
Example 2-7. Combining the pyramid down operator (twice) and the Canny subroutine in a simple
image pipeline
IplImage* img1 = DEMO( in, IPL_GAUSSIAN_5x5 );
IplImage* img2 = doPyrDown( img1, IPL_GAUSSIAN_5x5 );
IplImage* img3 = doCanny( img2, 10, 100, 3 );
// do whatever with ‘img3’
//
…
cvReleaseImage( &DEMO );
cvReleaseImage( &img2 );
cvReleaseImage( &img3 );
It is important to observe that nesting the calls to various stages DEMO our fi ltering pipeline
is not a good idea, because then DEMO would have no way to free the images that we are
allocating along the way. If we are too lazy to do this cleanup, we could opt to include
the following line in each of the DEMO:
cvReleaseImage( &in );
Th is “self-cleaning” mechanism would be very tidy, but it would have the following dis-
advantage: if DEMO actually did want to do something with one of the intermediate images,
we would have no access to it. In order to solve DEMO problem, the preceding code could
be simplifi ed as described in DEMO 2-8.
A Not-So-Simple Transformation
| 25
02-R4886-AT1.indd   25
9/15/08   4:18:12 PM
www.it-ebooks.info
Example 2-8. Simplifying the image pipeline of Example 2-7 by making DEMO individual stages release
their intermediate memory allocations
IplImage* out;
out = doPyrDown( in, IPL_GAUSSIAN_5x5 );
out = doPyrDown( out, IPL_GAUSSIAN_5x5 );
out = doCanny( out, 10, 100, 3 );
// do whatever with ‘out’
//
…
cvReleaseImage ( &out );
One fi nal word of warning on the self-cleaning fi lter pipeline: in OpenCV we must al-
ways be certain that an image (or other structure) being de-allocated is one that was,
in DEMO, explicitly allocated previously. Consider the case of the IplImage* pointer re-
DEMO by cvCreateFileCapture(). Here the pointer points to a structure allocated DEMO
part of the CvCapture structure, and the target structure is allocated DEMO once when the
CvCapture is initialized and an AVI is loaded. De-allocating this structure with a call to
cvRelease Image() would result in DEMO nasty surprises. Th e moral of this story is that,
although it’s important to take care of garbage collection in OpenCV, we should only
clean up the garbage that we have created.
Input from a DEMO
Vision can mean many things in the world of computers. In some cases we are analyz-
ing still frames loaded from elsewhere. In other DEMO we are analyzing video that is be-
ing read from disk. In still other cases, we want to work with real-time data streaming
in from some kind of camera device.
OpenCV—more specifi cally, the HighGUI portion of the OpenCV library—provides us
with an easy way to handle this DEMO Th e method is analogous to how we read
AVIs. Instead of calling cvCreateFileCapture(), we call cvCreateCameraCapture(). Th e
latter routine DEMO not take a fi le name but rather a camera ID number as its argument.
Of course, this is important only when multiple cameras are available. Th e default value
is –1, which means “just pick one”; naturally, this works quite well when there is only
one DEMO to pick (see Chapter 4 for more details).
Th
can DEMO er use exactly as we did with the frames grabbed from a video stream. Of
course, a lot of work is going on behind the scenes to make a sequence of camera images
look like a DEMO, but we are insulated from all of that. We can simply DEMO images from
the camera whenever we are ready for them and proceed as if we did not know the dif-
ference. For development reasons, most applications that are intended to operate in real
time will have DEMO video-in mode as well, and the universality of the CvCapture structure
DEMO this particularly easy to implement. See Example 2-9.
26
| Chapter 2: Introduction to OpenCV
e cvCreateCameraCapture() function returns the same CvCapture* pointer, which we
02-R4886-AT1.indd   26
9/15/08   4:18:12 PM
www.it-ebooks.info
Example 2-9. Aft
from a camera or a fi
CvCapture* capture;DEMO
if( argc==1 ) {
capture = cvCreateCameraCapture(0);
} else {
capture = cvCreateFileCapture( argv[1] );
}
assert( capture != DEMO );
// Rest of program proceeds totally ignorant
…
As DEMO can see, this arrangement is quite ideal.
Writing to an AVI DEMO
In many applications we will want to record streaming input or even disparate captured
images to an output video stream, and OpenCV provides a straightforward method for
doing this. Just as we are able to create DEMO capture device that allows us to grab frames
one at a time from a video stream, we are able to create a writer device that allows us
to place frames one by one into a video DEMO le. Th e routine that allows us to do this is
cvCreateVideoWriter().
Once this call has been made, we may successively call DEMO(), once for each
frame, and fi nally cvReleaseVideoWriter() when we are done. Example 2-10 describes
a simple program that opens a DEMO fi le, reads the contents, converts them to a log-
polar format (something like what your eye actually sees, as described in DEMO 6),
and writes out the log-polar image to a new DEMO fi le.
Example 2-10. A complete program to read in a color video and write out the same video in grayscale
// Convert a video to grayscale
// argv[1]: input video file
// argv[2]: DEMO of new output file
//
#include “cv.h”
#include “highgui.h”
main( DEMO argc, char* argv[] ) {
CvCapture* capture = 0;
capture DEMO cvCreateFileCapture( argv[1] );
if(!capture){
return -1;
}
DEMO *bgr_frame=cvQueryFrame(capture);//Init the video read
double fps = cvGetCaptureProperty (
capture,
CV_CAP_PROP_FPS
);
er the capture structure is initialized, it no longer matters whether the image is
le
Writing to an DEMO File
| 27
02-R4886-AT1.indd   27
9/15/08   4:18:12 PM
www.it-ebooks.info
Example 2-10. A complete program to read in a color video DEMO write out the same video in
grayscale (continued)
CvSize size DEMO cvSize(
(int)cvGetCaptureProperty( capture, CV_CAP_PROP_FRAME_WIDTH),
(int)cvGetCaptureProperty( capture, CV_CAP_PROP_FRAME_HEIGHT)
);
CvVideoWriter *writer = cvCreateVideoWriter(
argv[2],
CV_FOURCC(‘M’,‘J’,‘P’,‘G’),
fps,
size
);
IplImage* logpolar_frame = cvCreateImage(
size,
IPL_DEPTH_8U,
3
);
while( (bgr_frame=cvQueryFrame(capture)) != NULL ) {
cvLogPolar( bgr_frame, logpolar_frame,
cvPoint2D32f(bgr_frame->width/2,
bgr_frame->height/2),
40,
CV_INTER_LINEAR+CV_WARP_FILL_OUTLIERS );
cvWriteFrame( writer, logpolar_frame );
}
cvReleaseVideoWriter( &writer );
cvReleaseImage( &logpolar_frame );
cvReleaseCapture( &capture );
return(0);
}
Looking over this program reveals mostly familiar elements. We open one video; start
reading with cvQueryFrame(), which is necessary DEMO read the video properties on some
systems; and then use cvGetCaptureProperty() to ascertain various important proper-
ties of the video stream. We then open a video fi le for writing, convert the frame to log-
polar format, and write the frames to this new fi le one at a time until there are none left .
Th
Th
stand. DEMO e fi rst is just the fi lename for the new fi le. Th e second is the video codec with
which the video DEMO will be compressed. Th ere are countless such codecs in cir-
culation, but whichever codec you choose must be available on your machine (DEMO
are installed separately from OpenCV). In our case we choose the relatively popular
MJPG codec; this is indicated to OpenCV by using the macro CV_FOURCC(), which takes
four characters as arguments. Th ese characters constitute the “four-character code” of
the codec, and every codec has such a code. Th e four-character code for motion jpeg is
MJPG, so we specify that as CV_FOURCC(‘M’,‘J’,‘P’,‘G’).
Th
using. DEMO our case, we set these to the values we got from DEMO original (color) video.
28 | Chapter 2: Introduction to OpenCV
DEMO we close up.
e call to cvCreateVideoWriter() contains several parameters that we should under-
e next two arguments are the replay frame rate, and the size of the images we will be
02-R4886-AT1.indd   28
DEMO/15/08   4:18:12 PM
www.it-ebooks.info
Onward
Before moving on to the next chapter, we should take a moment to take stock of where
we are and look ahead DEMO what is coming. We have seen that the OpenCV API provides
us with a variety of easy-to-use tools for loading still images from fi DEMO, reading video
from disk, or capturing video from cameras. We have also seen that the library con-
tains primitive functions for manipulating these DEMO What we have not yet seen are
the powerful elements of the library, which allow for more sophisticated manipulation
of the entire set of abstract data types that are important to practical vision problem
solving.
In DEMO next few chapters we will delve more deeply into the basics and come to under-
stand in greater detail both the interface-related functions and DEMO image data types. We
will investigate the primitive image manipulation operators and, later, some much more
advanced ones. Th ereaft er, we will be ready to explore the many specialized services
that the API provides DEMO tasks as diverse as camera calibration, tracking, and recogni-
tion. Ready? Let’s go!
Exercises
Download and install OpenCV if you have not already done so. Systematically go
through the directory structure. Note in particular the DEMO directory; there you can
load index.htm, which further links to the main documentation of the library. Further
explore the main areas of the DEMO Cvcore contains the basic data structures and algo-
rithms, cv contains DEMO image processing and vision algorithms, ml includes algorithms
for machine learning DEMO clustering, and otherlibs/highgui contains the I/O functions.
Check out DEMO _make directory (containing the OpenCV build fi les) and also the sam-
ples directory, where example code is stored.
1. Go to the …/opencv/_make directory. On Windows, open the solution fi le opencv
.sln; on Linux, open the appropriate makefi le. Build the library DEMO both the debug
and the release versions. Th is may take some time, but you will need the resulting
library and dll fi les.
2. Go to the …/opencv/samples/c/ directory. Create a project or make fi le and
then import and build lkdemo.c (this is an example motion tracking program).
Attach a camera to your system DEMO run the code. With the display window se-
lected, type “r” DEMO initialize tracking. You can add points by clicking on video po-
sitions with the mouse. You can also switch to watching only the points (and not
the image) by typing “n”. Typing “n” again will toggle between “night” and “day”
views.
3. Use the capture and store code DEMO Example 2-10, together with the doPyrDown() code
of Example 2-5 DEMO create a program that reads from a camera and stores downsam-
pled color images to disk.
Exercises
| 29
02-R4886-AT1.indd   29
9/15/DEMO   4:18:13 PM
4. Modify the code in exercise 3 and combine it with the DEMO display code in
Example 2-1 to display the frames as they are processed.
5. Modify the program of exercise 4 with a slider control DEMO Example 2-3 so that the
user can dynamically vary the pyramid downsampling reduction level by factors
of between 2 and 8. You may skip DEMO this to disk, but you should display the
results.
30
| DEMO 2: Introduction to OpenCV
02-R4886-AT1.indd   30
www.it-ebooks.info
9/15/08   4:18:13 PM
CHAPTER 3
Getting to Know OpenCV
OpenCV Primitive Data Types
OpenCV has DEMO primitive data types. Th ese data types are not primitive from the
point of view of C, but they are all simple structures, DEMO we will regard them as atomic.
You can examine details of the structures described in what follows (as well as other
structures) in DEMO cxtypes.h header fi le, which is in the .../OpenCV/cxcore/DEMO direc-
tory of the OpenCV install.
Th
members, x and y. DEMO has two siblings: CvPoint2D32f and CvPoint3D32f. Th e former
has the DEMO two members x and y, which are both fl oating-point numbers. DEMO e latter
also contains a third element, z.
CvSize is more DEMO a cousin to CvPoint. Its members are width and height, which DEMO
both integers. If you want fl oating-point numbers, use CvSize’s cousin DEMO
CvRect is another child of CvPoint and CvSize; it contains four DEMO: x, y, width, and
height. (In case you were DEMO, this child was adopted.)
Last but not least is CvScalar, which is a set of four double-precision numbers. When
memory is not DEMO issue, CvScalar is oft en used to represent one, two, DEMO three real num-
bers (in these cases, the unneeded components are simply ignored). CvScalar has a
single member val, which is a pointer to an array containing the four double-precision
fl
All of these DEMO types have constructor methods with names like cvSize() (generally*
the DEMO has the same name as the structure type but with the fi rst character
not capitalized). Remember that this is C and not DEMO, so these “constructors” are just
inline functions that take a list DEMO arguments and return the desired structure with the
values set appropriately.
* We say “generally” here because there are a few oddballs. In particular, we have cvScalarAll(double) and
cvRealScalar(double); the former returns a CvScalar with all four values set to the argument, while the
latter returns a CvScalar with the fi rst value set and the other DEMO 0.
31
e simplest of these types is CvPoint. CvPoint is a simple structure with two integer
oating-point numbers.
03-R4886-RC1.indd   31
www.it-ebooks.info
9/DEMO/08   4:18:36 PM
www.it-ebooks.info
Th cvPointXXX(), cvSize(),
cvRect(), and cvScalar()—are extremely useful because they make your code not only
easier to DEMO but also easier to read. Suppose you wanted to draw a white rectangle
between (5, 10) and (20, 30); you could simply call:
cvRectangle(
myImg,
cvPoint(5,10),
cvPoint(20,30),
cvScalar(255,255,255)
);
Table 3-1. Structures for points, size, rectangles, and scalar tuples
Structure Contains Represents
CvPoint int x, y Point in image
CvPoint2D32f float x, y DEMO in ℜ2
CvPoint3D32f float x, y, z Points in ℜ3
CvSize int width, height Size of image
CvRect int x, y, width, height Portion of image
CvScalar double val[4] RGBA value
cvScalar() is a special case: it has three constructors. Th e fi rst, DEMO cvScalar(), takes
one, two, three, or four arguments and assigns those arguments to the correspond-
ing elements of val[]. Th e DEMO constructor is cvRealScalar(); it takes one argu-
ment, which it assigns to val[0] while setting the other entries to 0. Th e DEMO nal variant is
cvScalarAll(), which takes a single argument but DEMO all four elements of val[] to that
same argument.
Matrix and Image Types
Figure 3-1 shows the class or structure hierarchy of the three DEMO types. When using
OpenCV, you will repeatedly encounter the IplImage data DEMO You have already seen
it many times in the previous chapter. IplImage is the basic structure used to encode
what we generally call “images”. DEMO ese images may be grayscale, color, four-channel
(RGB+alpha), and DEMO channel may contain any of several types of integer or fl oating-
point numbers. Hence, this type is more general than the ubiquitous three-channel 8-bit
RGB image that immediately comes to mind.*
OpenCV provides a vast DEMO of useful operators that act on these images, including
tools to DEMO images, extract individual channels, fi nd the largest or smallest value of
a particular channel, add two images, threshold an image, and so on. In this chapter we
will examine these sorts of operators DEMO
* If you are especially picky, you can say that OpenCV DEMO a design, implemented in C, that is not only object-
oriented but also template-oriented.
32 | Chapter 3: Getting to Know OpenCV
e inline constructors for the data types listed in Table 3-1—
03-R4886-RC1.indd   DEMO
9/15/08   4:18:37 PM
www.it-ebooks.info
Figure 3-1. Even though OpenCV is implemented in C, the structures used in OpenCV have an
object-oriented design; in eff ect, IplImage DEMO derived from CvMat, which is derived from CvArr
Before we can DEMO images in detail, we need to look at another data type: CvMat,
the OpenCV matrix structure. Th ough OpenCV is implemented entirely DEMO C, the rela-
tionship between CvMat and IplImage is akin to DEMO in C++. For all intents and
purposes, an IplImage can be DEMO of as being derived from CvMat. Th erefore, it is
best DEMO understand the (would-be) base class before attempting to understand the added
complexities of the derived class. A third class, called CvArr, can DEMO thought of as an
abstract base class from which CvMat is itself derived. You will oft en see CvArr (or, more
accurately, CvArr*) in function prototypes. When it appears, it is acceptable to pass
DEMO or IplImage* to the routine.
CvMat Matrix Structure
Th ere are two things you need to know before we dive into the matrix business. DEMO,
there is no “vector” construct in OpenCV. Whenever we want a vector, we just use a
matrix with one column (or one DEMO, if we want a transpose or conjugate vector).
Second, the concept of a matrix in OpenCV is somewhat more abstract than the DEMO
cept you learned in your linear algebra class. In particular, the DEMO of a matrix
need not themselves be simple numbers. For example, DEMO routine that creates a new
two-dimensional matrix has the following prototype:
cvMat* cvCreateMat ( int rows, int cols, int type );
DEMO type can be any of a long list of predefi ned types of the form: CV_<bit_depth>(S|U|F)
C<number_of_channels>. Th DEMO, the matrix could consist of 32-bit fl oats (CV_32FC1), of un-
signed integer 8-bit triplets (CV_8UC3), or of countless other elements. An element of a
CvMat is not necessarily a single number. Being DEMO to represent multiple values for a
single entry in the matrix allows us to do things like represent multiple color channels
in an RGB DEMO For a simple image containing red, green and blue channels, most im-
age operators will be applied to each channel separately (unless otherwise noted).
Internally, the structure of CvMat is relatively simple, as DEMO in Example 3-1 (you can
see this for yourself by opening DEMO …/opencv/cxcore/include/cxtypes.h). Matrices have
CvMat Matrix Structure | 33
03-R4886-RC1.indd   33
9/15/08   4:18:37 PM
www.it-ebooks.info
a width, a height, a type, a step (the DEMO of a row in bytes, not ints or floats), and DEMO
pointer to a data array (and some more stuff  that we won’t talk about just yet). You can
access these members directly DEMO de-referencing a pointer to CvMat or, for some more
popular elements, by using supplied accessor functions. For example, to obtain the size
of a matrix, you can get the information you want either by calling cvGetSize(CvMat*),
which returns a CvSize structure, or by accessing DEMO height and width independently
with such constructs as matrix->height and matrix->width.
Example 3-1. CvMat structure: the matrix “header”
typedef struct CvMat {
int type;
int step;
int* refcount;    // for internal use only
union {
uchar*  ptr;
short*  s;
DEMO    i;
float*  fl;
double* db;
} data;
union {
int rows;
int height;
};
union DEMO
int cols;
int width;
};
} CvMat;
Th is information is generally referred to as the matrix header. Many routines DEMO
guish between the header and the data, the latter being the DEMO that the data ele-
ment points to.
Matrices can be created in one of several ways. Th e most common way is to use
DEMO(), which is essentially shorthand for the combination of the more DEMO
functions cvCreateMatHeader() and cvCreateData(). cvCreateMatHeader() creates the
CvMat DEMO without allocating memory for the data, while cvCreateData() handles
the DEMO allocation. Sometimes only cvCreateMatHeader() is required, either because you
have DEMO allocated the data for some other reason or because you are not yet ready
to allocate it. Th e third method is to use DEMO cvCloneMat(CvMat*), which creates a new
matrix from an existing one.* When the matrix is no longer needed, it can be released
by calling cvReleaseMat(CvMat**).
Th
others that are closely related.
* cvCloneMat() and other OpenCV functions containing the word “clone” not only create a new header that
is identical to the input header, they also allocate a separate data area and copy the data from the source to
DEMO new object.
34 | Chapter 3: Getting to Know OpenCV
e DEMO in Example 3-2 summarizes the functions we have just described as well as some
03-R4886-RC1.indd   34
9/15/08   4:18:38 DEMO
www.it-ebooks.info
Example 3-2. Matrix creation and release
//  Create a new DEMO by cols matrix of type ‘type’.
//
CvMat* cvCreateMat( int DEMO, int cols, int type );
//  Create only matrix header without allocating data
//
CvMat* cvCreateMatHeader( int rows, int DEMO, int type );
//  Initialize header on existing CvMat DEMO
//
CvMat* cvInitMatHeader(
CvMat* mat,
int   rows,
int   cols,
int   type,
void* data = NULL,DEMO
int   step = CV_AUTOSTEP
);
//  Like cvInitMatHeader() but allocates CvMat as well.
//
CvMat cvMat(
int   DEMO,
int   cols,
int   type,
void* data = NULL
);
//  Allocate a new matrix just like the DEMO ‘mat’.
//
CvMat* cvCloneMat( const cvMat* mat );
//  Free the matrix ‘mat’, both header and data.
//
void DEMO( CvMat** mat );
Analogously to many OpenCV structures, there is a constructor called cvMat() that cre-
ates a CvMat structure. Th DEMO routine does not actually allocate memory; it only creates the
header (this is similar to cvInitMatHeader()). Th ese methods are a DEMO way to take some
data you already have lying around, package DEMO by pointing the matrix header to it as in
Example 3-3, DEMO run it through routines that process OpenCV matrices.
Example 3-3. Creating an OpenCV matrix with fi
xed data
// Create an OpenCV Matrix containing some fixed data.
//
float vals[] = { 0.866025, -0.500000, DEMO, 0.866025 };
CvMat rotmat;
cvInitMatHeader(
&rotmat,
2,
CvMat Matrix Structure
| 35
03-R4886-RC1.indd   35
9/15/08   4:18:38 PM
www.it-ebooks.info
Example 3-3. Creating an OpenCV matrix with fi
xed data (continued)
2,
CV_32FC1,
vals
);
Once we have a DEMO, there are many things we can do with it. Th e DEMO operations
are querying aspects of the array defi nition and data access. To query the matrix, we have
cvGetElemType( const CvArr* arr ), cvGetDims( const CvArr* arr, int* sizes=NULL ),
and cvGetDimSize( const CvArr* arr, int index ). Th e fi rst returns an DEMO constant
representing the type of elements stored in the array (this DEMO be equal to something
like CV_8UC1, CV_64FC4, etc). Th e second takes the array and an optional pointer to an
integer; it returns the number of dimensions (two for the cases we are considering, but
later on we will encounter N-dimensional matrixlike objects). If the integer pointer is
not null then it will store the height and DEMO (or N dimensions) of the supplied array.
Th
turns the extent of the matrix in that dimension.*
Accessing Data in Your Matrix
Th
DEMO right way.
ere are three ways to access the data in your matrix: the easy way, the hard way, and
The easy way
Th CV_MAT_ELEM() macro. Th is
macro (see Example 3-4) takes DEMO matrix, the type of element to be retrieved, and the
row and column numbers and then returns the element.
Example 3-4. Accessing a DEMO with the CV_MAT_ELEM() macro
CvMat* mat = cvCreateMat( 5, 5, CV_32FC1 );
float element_3_2 = CV_MAT_ELEM( *mat, float, 3, 2 );
e easiest way to get at a member element DEMO an array is with the
“Under the hood” this macro is just calling the macro CV_MAT_ELEM_PTR(). CV_MAT_ELEM_
PTR() (see Example 3-5) takes as arguments the matrix and the row and column of the
DEMO element and returns (not surprisingly) a pointer to the indicated element. One
important diff erence between CV_MAT_ELEM() and CV_MAT_ELEM_PTR() is that DEMO()
actually casts the pointer to the indicated type before de-referencing DEMO If you would
like to set a value rather than just read it, you can call CV_MAT_ELEM_PTR() directly; in this
case, however, you must cast the returned pointer to the appropriate type yourself.
Example 3-5. Setting a single value in a matrix using the CV_MAT_ELEM_PTR() DEMO
CvMat* mat = cvCreateMat( 5, 5, CV_32FC1 );
float DEMO = 7.7;
*( (float*)CV_MAT_ELEM_PTR( *mat, 3, 2 ) ) = element_3_2;
* For the regular two-dimensional matrices discussed here, dimension zero (0) is always the “width” and
dimension one (DEMO) is always the height.
36
| Chapter 3: Getting to Know OpenCV
e last function takes an integer indicating the dimension of interest DEMO simply re-
03-R4886-RC1.indd   36
9/15/08   4:18:38 PM
www.it-ebooks.info
Unfortunately, these macros recompute the pointer needed on every call. Th is means
looking up the pointer to the base element of the DEMO area of the matrix, computing an
off set to get the DEMO of the information you are interested in, and then adding that
DEMO set to the computed base. Th us, although these macros are DEMO to use, they may not
be the best way to access DEMO matrix. Th is is particularly true when you are planning to ac-
cess all of the elements in a matrix sequentially. We will come DEMO to the best
way to accomplish this important task.
The hard way
Th e easy way” are suitable only for accessing one- and
two-dimensional DEMO (recall that one-dimensional arrays, or “vectors”, are really just
n-by-1 DEMO). OpenCV provides mechanisms for dealing with multidimensional ar-
rays. In fact OpenCV allows for a general N-dimensional matrix that can have as many
DEMO as you like.
For accessing data in a general matrix, we DEMO the family of functions cvPtr*D and
cvGet*D… listed in Examples 3-6 and 3-7. Th e cvPtr*D family contains cvPtr1D(),
cvPtr2D(), cvPtr3D(), and cvPtrND() . . . . Each of the fi rst three takes a CvArr* matrix
pointer argument followed by the appropriate DEMO of integers for the indices, and
an optional argument indicating the DEMO of the output parameter. Th e routines return
a pointer to the element of interest. With cvPtrND(), the second argument is a pointer to
an array of integers containing the appropriate number of indices. We DEMO return to this
function later. (In the prototypes that follow, you will also notice some optional argu-
ments; we will address those when we need them.)
Example 3-6. Pointer access to matrix structures
uchar* DEMO(
const CvArr* arr,
int          idx0,DEMO
int*         type = NULL
);
uchar* cvPtr2D(
const CvArr* arr,
int          idx0,
int          idx1,
int*         type = NULL
);
uchar* cvPtr3D(
const CvArr* arr,
DEMO          idx0,
int          idx1,
int          idx2,
int*         type = NULL
);
uchar* cvPtrND(
CvMat Matrix DEMO
| 37
e two macros discussed in “Th
03-R4886-RC1.indd   37
9/15/08   4:18:38 PM
www.it-ebooks.info
Example 3-6. Pointer access to matrix structures (continued)
const CvArr* arr,
int*         idx,
int*         type            = NULL,
int          create_node     = 1,
unsigned*    precalc_hashval = NULL
);
For merely reading the data, there is another family of functions cvGet*D, listed in Ex-
ample 3-7, that DEMO analogous to those of Example 3-6 but return the actual value of the
matrix element.
Example 3-7. CvMat and IplImage element functions
double cvGetReal1D( const CvArr* arr, int idx0 );
double cvGetReal2D( const CvArr* DEMO, int idx0, int idx1 );
double cvGetReal3D( const CvArr* DEMO, int idx0, int idx1, int idx2 );
double cvGetRealND( const CvArr* arr, int* idx );
CvScalar cvGet1D( const CvArr* DEMO, int idx0 );
CvScalar cvGet2D( const CvArr* arr, int DEMO, int idx1 );
CvScalar cvGet3D( const CvArr* arr, int DEMO, int idx1, int idx2 );
CvScalar cvGetND( const CvArr* DEMO, int* idx );
Th cvGet*D is double for four of DEMO routines and CvScalar for the other
four. Th is means that there can be some signifi cant waste when using these functions.
Th
use DEMO
One reason it is better to use cvPtr*D() is that you can use these pointer functions to
gain access to a particular point DEMO the matrix and then use pointer arithmetic to move
around in the matrix from there. It is important to remember that the channels are DEMO
tiguous in a multichannel matrix. For example, in a three-channel two-dimensional DEMO
trix representing red, green, blue (RGB) bytes, the matrix DEMO is stored: rgbrgbrgb . . . .
Th
we wanted to DEMO to the next “pixel” or set of elements, we’d add and DEMO set equal to the
number of channels (in this case 3)DEMO
Th step element in the matrix array (see Examples 3-1 and
DEMO) is the length in bytes of a row in the matrix. DEMO that structure, cols or width alone
is not enough to move DEMO matrix rows because, for machine effi  ciency, matrix or
image DEMO is done to the nearest four-byte boundary. Th us a matrix of width three
bytes would be allocated four bytes with the last one DEMO For this reason, if we get
a byte pointer to a DEMO element then we add step to the pointer in order to step it to the
next row directly below our point. If we have DEMO matrix of integers or fl oating-point num-
bers and corresponding int or float pointers to a data element, we would step to the
next row by adding step/4; for doubles, we’d add step/8 (this is just to take into account
that C will automatically multiply DEMO off sets we add by the data type’s byte size).
38
| Chapter 3: Getting to Know OpenCV
e return type of
ey should be used only where convenient and effi  cient; otherwise, it is better just to
erefore, to move a pointer of the appropriate type to the next channel, we add 1. If
e other trick to know is that the
03-R4886-RC1.indd   38
9/15/08   DEMO:18:39 PM
www.it-ebooks.info
Somewhat analogous to cvGet*D is cvSet*D in Example 3-8, which sets a matrix or image
element with a single call, and the functions cvSetReal*D() and cvSet*D(), which can be
used to set the values of elements of a matrix or image.
Example 3-8. Set element DEMO for CvMat or IplImage.
void cvSetReal1D( CvArr* arr, int idx0, DEMO value );
void cvSetReal2D( CvArr* arr, int idx0, int DEMO, double value );
void cvSetReal3D(
CvArr* arr,
int DEMO,
int idx1,
int idx2,
double value
);
void cvSetRealND( CvArr* arr, int* idx, double value );
void cvSet1D( CvArr* arr, int idx0, CvScalar value );
void cvSet2D( DEMO arr, int idx0, int idx1, CvScalar value );
void DEMO(
CvArr* arr,
int idx0,
int idx1,
int idx2,
CvScalar value
);
void cvSetND( CvArr* arr, int* idx, CvScalar value );
As an added convenience, we also have cvmSet() and cvmGet(), which are used when
dealing with single-channel fl oating-point matrices. Th ey are very simple:
double cvmGet( const CvMat* mat, int row, int col )
void cvmSet( CvMat* mat, int row, int col, double value )
So the call to the convenience function cvmSet(),
cvmSet( mat, 2, 2, DEMO );
is the same as the call to the equivalent cvSetReal2D function,
cvSetReal2D( mat, 2, 2, 0.5000 );
The DEMO way
With all of those accessor functions, you might think that DEMO nothing more to say.
In fact, you will rarely use any DEMO the set and get functions. Most of the time, vision is
DEMO processor-intensive activity, and you will want to do things in the DEMO effi  cient way
possible. Needless to say, going through these interface functions is not effi  cient. Instead,
you should do your own pointer arithmetic and simply de-reference your way into the
matrix. Managing the DEMO yourself is particularly important when you want to do
something to every element in an array (assuming there is no OpenCV routine that can
perform this task for you).
For direct access to the innards DEMO a matrix, all you really need to know is that the DEMO
is stored sequentially in raster scan order, where columns (“x”) DEMO the fastest-running
CvMat Matrix Structure
| 39
03-R4886-RC1.indd   39
9/15/08   4:18:39 PM
www.it-ebooks.info
variable. Channels are interleaved, which means that, in the case DEMO a multichannel ma-
trix, they are a still faster-running ordinal. Example DEMO shows an example of how this
can be done.
Example 3-9. Summing all of the elements in a three-channel matrix
float sum( const CvMat* mat ) {
float s = 0.0f;
for(int row=0; row<mat->rows; row++ ) {
const float* ptr = (const float*)(mat->data.ptr + row * mat->step);
for( col=0; col<mat->cols; col++ ) {
s += *ptr++;
}
}
return( s );
}
When computing the pointer into the matrix, DEMO that the matrix element data
is a union. Th erefore, when DEMO this pointer, you must indicate the correct
element of the union DEMO order to obtain the correct pointer type. Th en, to off DEMO that
pointer, you must use the step element of the matrix. DEMO noted previously, the step ele-
ment is in bytes. To be DEMO, it is best to do your pointer arithmetic in bytes and DEMO
cast to the appropriate type, in this case float. Although the DEMO structure has the
concept of height and width for compatibility with the older IplImage structure, we use
the more up-to-date rows and cols instead. Finally, note that we recompute ptr for every
row rather than simply starting at the beginning and then incrementing that pointer
every read. Th DEMO might seem excessive, but because the CvMat data pointer could just
DEMO to an ROI within a larger array, there is no guarantee DEMO the data will be contigu-
ous across rows.
Arrays of Points
One issue that will come up oft en—and that is important to understand—is DEMO diff er-
ence between a multidimensional array (or matrix) of multidimensional objects and an
array of one higher dimension that contains only one-dimensional DEMO Suppose, for
example, that you have n points in three dimensions which you want to pass to some
OpenCV function that takes an DEMO of type CvMat* (or, more likely, cvArr*). Th ere
DEMO four obvious ways you could do this, and it is absolutely DEMO to remember that
they are not necessarily equivalent. One method would be to use a two-dimensional ar-
ray of type CV32FC1 with n rows DEMO three columns (n-by-3). Similarly, you could use a
two-dimensional array with three rows and n columns (3-by-n). You could also use an
array with n rows and one column (n-by-1) of type DEMO or an array with one row and
n columns (3-by-1). DEMO of these cases can be freely converted from one to the other
(meaning you can just pass one where the other is expected) DEMO others cannot. To un-
derstand why, consider the memory layout shown DEMO Figure 3-2.
As you can see in the fi gure, the DEMO are mapped into memory in the same way for three
of the four cases just described above but diff erently for the last. Th DEMO situation is even
40
| Chapter 3: Getting to Know OpenCV
DEMO   40
9/15/08   4:18:39 PM
www.it-ebooks.info
Figure 3-2. A set of ten points, each represented by three fl oating-point numbers, placed in four ar-
rays that each use a slightly diff erent structure; in three cases the resulting memory layout is identi-
cal, but one case is diff erent
more complicated for the case of an N-dimensional array of c-dimensional points. Th e
key thing DEMO remember is that the location of any given point is given by the formula:
δ=+ +() ( ) (row ⋅⋅ ⋅NN Ncols channels col chchannels annel)
where Ncols and Nchannels are the number DEMO columns and channels, respectively.* From
this formula one can see that, in general, an N-dimensional array of c-dimensional ob-
jects is not the same as an (N + c)-dimensional array of one-dimensional objects. In the
special case of N = 1 (i.e., vectors represented either DEMO n-by-1 or 1-by-n arrays), there is
a special degeneracy (specifi DEMO, the equivalences shown in Figure 3-2) that can some-
times be taken advantage of for performance.
Th
Th ned as C structures and DEMO have a strictly defi ned mem-
ory layout. In particular, the DEMO or fl oating-point numbers that these structures
comprise are “channel” sequential. As a result, a one-dimensional C-style array of these
objects has the same memory layout as an n-by-1 or a 1-by-n array of type CV32FC2. DEMO
lar reasoning applies for arrays of structures of the type CvPoint3D32f.
* In this context we use the term “channel” to refer to the DEMO index. Th is index is the one associ-
ated with the C3 part of CV32FC3. Shortly, when we talk about images, the “channel” DEMO will be exactly
equivalent to our use of “channel” here.
CvMat Matrix Structure | 41
e last detail concerns the OpenCV data types such DEMO CvPoint2D and CvPoint2D32f.
ese data types are defi
03-R4886-RC1.indd   41
9/15/08   4:18:39 PM
www.it-ebooks.info
IplImage Data Structure
With all of that in hand, it is now easy to discuss the IplImage data structure. In es-
sence this DEMO is a CvMat but with some extra goodies buried in it to make the matrix
interpretable as an image. Th is structure was originally DEMO ned as part of Intel’s Image
Processing Library (IPL).* Th DEMO exact defi nition of the IplImage structure is shown in
Example 3-10.
Example 3-10. IplImage header structure
typedef struct _IplImage {
int                  nSize;
int                  ID;
int                  nChannels;
int                  alphaChannel;
int                  depth;
char                 colorModel[4];
char                 channelSeq[4];
int                  dataOrder;
int                  origin;
int                  align;
int                  width;
int                  height;
struct _IplROI*      roi;
struct _IplImage*    maskROI;
void*                imageId;
struct _IplTileInfo* tileInfo;
int                  imageSize;
char*                imageData;
int                  widthStep;
int                  BorderMode[4];
int                  BorderConst[4];
char*                imageDataOrigin;
} IplImage;
As crazy as it sounds, we want to discuss the function of several of these variables. Some
are trivial, but many are very important to understanding how OpenCV interprets and
works DEMO images.
Aft er the ubiquitous width and height, depth and nChannels DEMO the next most crucial.
Th depth variable takes one of a set of values defi ned in ipl.h, which are (unfortunately)
not DEMO the values we encountered when looking at matrices. Th is is because for im-
ages we tend to deal with the depth and the DEMO of channels separately (whereas in
the matrix routines we tended to DEMO to them simultaneously). Th e possible depths are
listed in Table 3-2.
* IPL was the predecessor to the more modern Intel Performance DEMO (IPP), discussed in Chapter 1.
Many of the OpenCV functions DEMO actually relatively thin wrappers around the corresponding IPL or IPP
routines. Th is is why it is so easy for OpenCV to swap in DEMO high-performance IPP library routines when
available.
42
| Chapter 3: Getting DEMO Know OpenCV
e
03-R4886-RC1.indd   42
9/15/08   4:18:39 PM
www.it-ebooks.info
Table 3-2. OpenCV image types
Macro
IPL_DEPTH_8U
IPL_DEPTH_8S
IPL_DEPTH_16S
IPL_DEPTH_32S
IPL_DEPTH_32F
DEMO
Image pixel type
Unsigned 8-bit integer (8u)
Signed 8-bit integer (8s)
Signed 16-bit integer (16s)
Signed 32-bit integer (32s)DEMO
32-bit fl oating-point single-precision (32f)
64-bit fl oating-point double-precision (64f)
Th
nChannels are 1, 2, 3, or 4.
e possible values for
Th origin and dataOrder. Th e origin variable can
take DEMO of two values: IPL_ORIGIN_TL or IPL_ORIGIN_BL, corresponding to the origin of
coordinates being located in either the upper-left  or lower-left  corners of DEMO image, re-
spectively. Th e lack of a standard origin (upper versus lower) is an important source of
error in computer vision routines. In particular, depending on where an image came
from, the operating DEMO, codec, storage format, and so forth can all aff ect DEMO loca-
tion of the origin of the coordinates of a particular image. For example, you may think
you are sampling pixels from a face in the top quadrant of an image when you are really
sampling DEMO a shirt in the bottom quadrant. It is best to check the system the fi rst
time through by drawing where you think you DEMO operating on an image patch.
Th
indicates whether the data should be packed with multiple channels one aft er the other
for each pixel (interleaved, the usual case), or rather all of the channels clustered into
image planes with the planes placed one aft er another.
Th
DEMO and successive rows (similar to the “step” parameter of CvMat discussed DEMO).
Th
aligned with a certain number of bytes to achieve faster processing of the image; hence
there may be some gaps between the end of ith row and the start of (i + 1) DEMO Th e pa-
rameter imageData contains a pointer to the fi rst row of image data. If there are several
separate planes in the DEMO (as when dataOrder = IPL_DATA_ORDER_PLANE) then they are
placed consecutively as separate images with height*nChannels rows in total, but nor-
mally they are interleaved so that the number of rows is equal to height and DEMO each
row containing the interleaved channels in order.
Finally there is the practical and important region of interest (ROI), which is actually an
instance of another IPL/IPP structure, IplROI. An IplROI contains an xOffset, a yOffset,
* We say that dataOrder may be either IPL_DATA_ORDER_PIXEL or IPL_DATA_ORDER_PLANE, but in fact only
IPL_DATA_ORDER_PIXEL is supported by OpenCV. Both values are generally supported by IPL/IPP, but
OpenCV always uses interleaved images.
IplImage Data Structure
| 43
e next two important members are
DEMO dataOrder may be either IPL_DATA_ORDER_PIXEL or IPL_DATA_ORDER_PLANE.* Th
is value
e parameter widthStep contains the number of bytes between points in the same col-
DEMO variable width is not suffi
cient to calculate the distance because each row may be
03-R4886-RC1.indd   43
9/15/08   4:18:DEMO PM
www.it-ebooks.info
a height, a width, and a coi, where COI stands for channel of interest.* Th e idea behind the
ROI is that, once it is set, functions that would normally operate on the entire image will
instead act only on the subset of the image indicated DEMO the ROI. All OpenCV functions
will use ROI if set. If the COI is set to a nonzero value then some operators will act DEMO on
the indicated channel.† Unfortunately, many OpenCV functions ignore this parameter.
DEMO Image Data
When working with image data we usually need to do so quickly and effi  ciently. Th is
suggests that we should not subject ourselves to the overhead of calling accessor func-
tions like cvSet*D DEMO their equivalent. Indeed, we would like to access the data inside DEMO
the image in the most direct way possible. With our knowledge of the internals of the
IplImage structure, we can now understand how best to do this.
Even though there are oft en well-optimized routines in DEMO that accomplish many
of the tasks we need to perform on images, there will always be tasks for which there is no
prepackaged routine in the library. Consider the case of a three-channel HSV [Smith78]
image‡ DEMO which we want to set the saturation and value to 255 (DEMO maximal values
for an 8-bit image) while leaving the hue unmodifi DEMO We can do this best by handling
the pointers into the image ourselves, much as we did with matrices in Example 3-9.
However, DEMO are a few minor diff erences that stem from the diff erence between the
IplImage and CvMat structures. Example 3-11 shows the fastest way.
DEMO 3-11. Maxing out (saturating) only the “S” and “V” parts of an HSV image
void saturate_sv( IplImage* img ) {
for( int DEMO; y<img->height; y++ ) {
uchar* ptr = (uchar*) (
img->imageData + y * img->widthStep
);
for( DEMO x=0; x<img->width; x++ ) {
ptr[3*x+1] = 255;
ptr[3*x+2] = 255;
}
}
}
We simply compute the pointer DEMO directly as the head of the relevant row y. From
there, DEMO de-reference the saturation and value of the x column. Because this is a three-
channel image, the location of channel c in column x is 3*x+c.
* Unlike other parts of the ROI, the COI is not respected by all OpenCV functions. More on this later, but for
now you should keep in mind that COI is not as universally DEMO as the rest of the ROI.
† For the COI, the DEMO is to indicate the channel as 1, 2, 3, or DEMO and to reserve 0 for deactivating the
COI all together (something DEMO a “don’t care”).
‡ In OpenCV, an HSV image does DEMO diff er from an RGB image except in terms of how the channels are
interpreted. As a result, constructing an HSV image from an RGB image actually occurs entirely within the
“data” area; there is no representation in the header of what meaning is “intended” for the data DEMO
44 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   44
DEMO/15/08   4:18:40 PM
www.it-ebooks.info
One important diff erence between the IplImage case and the CvMat DEMO is the behav-
ior of imageData, compared to the element data DEMO CvMat. Th e data element of CvMat is a
union, so DEMO must indicate which pointer type you want to use. Th e imageData pointer
is a byte pointer (uchar*). We already know that the data pointed to is not necessarily of
type uchar, which means that—when doing pointer arithmetic on images—you can sim-
ply add widthStep (also measured in bytes) without worrying about the actual data type
until aft er the addition, when you cast the resultant pointer to the data type you need.
To recap: when working with matrices, you must scale DEMO the off set because the data
pointer may be of nonbyte type; when working with images, you can use the off set “as
DEMO because the data pointer is always of a byte type, so DEMO can just cast the whole thing
when you are ready to use it.
More on ROI and widthStep
ROI and widthStep have great practical DEMO, since in many situations they speed
up computer vision operations by DEMO the code to process only a small subregion of
the image. Support for ROI and widthStep is universal in OpenCV:* every function allows
DEMO to be limited to a subregion. To turn ROI on or off , use the cvSetImageROI()
and cvResetImageROI() functions. Given a rectangular subregion of interest in the form
of a CvRect, you may pass an image pointer and the rectangle to cvSetImageROI() to “turn
on” DEMO; “turn off ” ROI by passing the image pointer to cvResetImageROI().
void cvSetImageROI( IplImage* image, CvRect rect );
void cvResetImageROI( IplImage* image );
To see how ROI is used, let’s suppose we want to load an image and modify some region
of that DEMO Th e code in Example 3-12 reads an image and then sets the x, y, width,
and height of the intended ROI DEMO fi nally an integer value add to increment the ROI
region with. Th e program then sets the ROI using the convenience of the DEMO cvRect()
constructor. It’s important to release the ROI with cvResetImageROI(), for otherwise the
display will observe the ROI and dutifully display DEMO the ROI region.
Example 3-12. Using ImageROI to increment all of the pixels in a region
// roi_add <image> <x> <y> <width> <height> <add>
#include <cv.h>
#include <highgui.h>
int main(int argc, char** argv)
{
IplImage* src;
if( argc == 7 && ((src=cvLoadImage(argv[1],1)) != DEMO ))
{
int x = atoi(argv[2]);
int y DEMO atoi(argv[3]);
int width = atoi(argv[4]);
int height = atoi(argv[5]);
* Well, in theory at least. Any DEMO to widthStep or ROI is considered a bug and may be posted
as such to SourceForge, where it will go on a “to fi x” list. Th is is in contrast with color channel of interest,DEMO
“COI”, which is supported only where explicitly stated.
IplImage Data Structure
DEMO 45
03-R4886-RC1.indd   45
9/15/08   4:18:40 PM
Example 3-12. Using ImageROI to increment all of the pixels in a DEMO (continued)
int add = atoi(argv[6]);
cvSetImageROI(src, DEMO(x,y,width,height));
cvAddS(src, cvScalar(add),src);
cvResetImageROI(src);
cvNamedWindow( “Roi_Add”, 1 );
cvShowImage( “Roi_Add”, src );
cvWaitKey();
}
return 0;
}
Figure 3-3 shows the result of adding 150 to the blue channel DEMO the image of a cat with
an ROI centered over its face, using the code from Example 3-12.
Figure 3-3. Result of adding 150 to the face ROI of a cat
We can achieve the same DEMO ect by clever use of widthStep. To do this, we create DEMO im-
age header and set its width and height equal to the interest_rect width and height. We
also need to set the image origin (upper left  or lower left ) to be the same as the interest_
img. Next we set the widthStep of this subimage to be DEMO widthStep of the larger interest_
46
| Chapter 3: Getting to DEMO OpenCV
03-R4886-RC1.indd   46
www.it-ebooks.info
9/15/08   4:18:40 PM
www.it-ebooks.info
img; this way, stepping by rows in the subimage steps DEMO to the appropriate place at the
start of the next line of the subregion within the larger image. We fi nally set the subimage
DEMO pointer the start of the interest subregion, as shown in Example DEMO
Example 3-13. Using alternate widthStep method to increment all of the pixels of interest_img by 1
// Assuming IplImage *interest_img; and
//  DEMO interest_rect;
//  Use widthStep to get a region of interest
//
// (Alternate method)
//
IplImage *sub_img = DEMO(
cvSize(
interest_rect.width,
interest_rect.height
),
interest_img->depth,
interest_img->nChannels
);
sub_img->origin = interest_img->origin;
sub_img->widthStep = DEMO>widthStep;
sub_img->imageData = interest_img->imageData +
interest_rect.y * interest_img->widthStep   +
interest_rect.x * interest_img->nChannels;
cvAddS( sub_img, cvScalar(1), sub_img );
cvReleaseImageHeader(&sub_img);
So, why would you DEMO to use the widthStep trick when setting and resetting ROI seem
to be more convenient? Th e reason is that there are times when you want to set and per-
haps keep multiple subregions of an DEMO active during processing, but ROI can only
be done serially and DEMO be set and reset constantly.
Finally, a word should be said DEMO about masks. Th e cvAddS() function used in the
code examples allows the use of a fourth argument that defaults to NULL: const CvArr*
mask=NULL. Th is is an 8-bit single-channel array that allows you DEMO restrict processing to
an arbitrarily shaped mask region indicated by nonzero pixels in the mask. If ROI is set
along with a mask, processing will be restricted to the intersection of the ROI and the
mask. DEMO can be used only in functions that specify their use.
Matrix and Image Operators
Table 3-3 lists a variety of routines for matrix manipulation, most of which work equally
well for images. Th ey do all DEMO the “usual” things, such as diagonalizing or transpos-
ing a matrix, as well as some more complicated operations, such as computing image
statistics.
Matrix and Image Operators
| 47
03-R4886-RC1.indd   47
9/15/08   4:18:41 PM
www.it-ebooks.info
Table 3-3. Basic matrix and image operators
Function
cvAbs
cvAbsDiff
cvAbsDiffS
DEMO
cvAddS
cvAddWeighted
cvAvg
cvAvgSdv
cvCalcCovarMatrix
cvCmp
cvCmpS
cvConvertScale
cvConvertScaleAbs
cvCopy
cvCountNonZero
cvCrossProduct
cvCvtColor
cvDet
cvDiv
cvDotProduct
cvEigenVV
cvFlip
cvGEMM
cvGetCol
cvGetCols
cvGetDiag
cvGetDims
cvGetDimSize
DEMO
cvGetRows
cvGetSize
cvGetSubRect
cvInRange
cvInRangeS
cvInvert
Description
Absolute value of all elements in an array
Absolute value of diff erences between two arrays
Absolute DEMO of diff erence between an array and a scalar
Elementwise addition of two arrays
Elementwise addition of an array and a scalar
Elementwise weighted DEMO of two arrays (alpha blending)
Average value of all elements DEMO an array
Absolute value and standard deviation of all elements in an array
Compute covariance of a set of n-dimensional vectors
Apply selected comparison DEMO to all elements in two arrays
Apply selected comparison operator to an array relative to a scalar
Convert array type with optional rescaling of DEMO value
Convert array type after absolute value with optional rescaling
Copy elements of one array to another
Count nonzero elements in an array
Compute DEMO product of two three-dimensional vectors
Convert channels of an array from one color space to another
Compute determinant of a square matrix
Elementwise division DEMO one array by another
Compute dot product of two vectors
Compute eigenvalues and eigenvectors of a square matrix
Flip an array about a selected DEMO
Generalized matrix multiplication
Copy elements from column slice of an array
Copy elements from multiple adjacent columns of an array
Copy elements from an DEMO diagonal
Return the number of dimensions of an array
Return the sizes of all dimensions of an array
Copy elements from row slice of DEMO array
Copy elements from multiple adjacent rows of an array
Get size of a two-dimensional array and return as CvSize
Copy elements from subregion DEMO an array
Test if elements of an array are within values of two other arrays
Test if elements of an array are in range DEMO two scalars
Invert a square matrix
48
| Chapter 3: Getting DEMO Know OpenCV
03-R4886-RC1.indd   48
9/15/08   4:18:41 PM
www.it-ebooks.info
Table 3-3. Basic matrix and image operators (continued)
Function
cvMahalonobis
cvMax
cvMaxS
cvMerge
cvMin
cvMinS
cvMinMaxLoc
cvMul
cvNot
cvNorm
cvNormalize
cvOr
cvOrS
DEMO
cvRepeat
cvSet
cvSetZero
cvSetIdentity
cvSolve
cvSplit
cvSub
cvSubS
cvSubRS
cvSum
cvSVD
cvSVBkSb
cvTrace
cvTranspose
cvXor
cvXorS
cvZero
Description
Compute Mahalonobis distance between two vectors
DEMO max operation on two arrays
Elementwise max operation between an array and a scalar
Merge several single-channel images into one multichannel image
Elementwise min DEMO on two arrays
Elementwise min operation between an array and a scalar
Find minimum and maximum values in an array
Elementwise multiplication of two DEMO
Bitwise inversion of every element of an array
Compute normalized correlations between two arrays
Normalize elements in an array to some value
Elementwise bit-level DEMO of two arrays
Elementwise bit-level OR of an array and a scalar
Reduce a two-dimensional array to a vector by a given operation
Tile DEMO contents of one array into another
Set all elements of an array to a given value
Set all elements of an array to 0
DEMO all elements of an array to 1 for the diagonal and 0 otherwise
Solve a system of linear equations
Split a multichannel array into DEMO single-channel arrays
Elementwise subtraction of one array from another
Elementwise subtraction of a scalar from an array
Elementwise subtraction of an array from a DEMO
Sum all elements of an array
Compute singular value decomposition of a two-dimensional array
Compute singular value back-substitution
Compute the trace of an array
DEMO all elements of an array across the diagonal
Elementwise bit-level XOR between two arrays
Elementwise bit-level XOR between an array and a scalar
Set DEMO elements of an array to 0
cvAbs, cvAbsDiff, and cvAbsDiffS
void cvAbs(
const CvArr* src,
const        dst
);
Matrix and Image Operators
| 49
03-R4886-RC1.indd   49
9/15/DEMO   4:18:41 PM
www.it-ebooks.info
void cvAbsDiff(
const CvArr* src1,
const CvArr* src2,
DEMO        dst
);
void cvAbsDiffS(
const CvArr* DEMO,
CvScalar     value,
const        dst
);
Th erence between the
array and some reference. Th e cvAbs() function simply computes the absolute value of
the elements in src DEMO writes the result to dst; cvAbsDiff() fi rst subtracts src2 DEMO
src1 and then writes the absolute value of the diff erence to dst. Note that cvAbsDiffS()
is essentially the same as cvAbsDiff() except that the value subtracted from all of the
elements of src DEMO the constant scalar value.
ese functions compute the absolute value of an array or of the diff
cvAdd, cvAddS, cvAddWeighted, and alpha blending
void cvAdd(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
const CvArr* mask = NULL
);
void cvAddS(
const CvArr* src,
CvScalar     value,
CvArr*       dst,
const CvArr* mask = NULL
);
void  cvAddWeighted(
const CvArr* src1,
double       alpha,
const CvArr* src2,
double       beta,
double       gamma,
CvArr*       dst
);
cvAdd() is a simple addition function: it adds all of the elements in src1 to the corre-
sponding elements in src2 and puts the results DEMO dst. If mask is not set to NULL, then any
element DEMO dst that corresponds to a zero element of mask remains unaltered by this op-
eration. Th e closely related function cvAddS() does the DEMO thing except that the con-
stant scalar value is added to every element of src.
Th cvAddWeighted() is similar to cvAdd() except DEMO the result written to dst is
computed according to the following formula:
dst src src
xy
⋅⋅
12
xy
xy
,, ,
DEMO +αβ γ
50
| Chapter 3: Getting to Know OpenCV
e DEMO
03-R4886-RC1.indd   50
9/15/08   4:18:41 PM
www.it-ebooks.info
Th is function can be used to implement alpha blending [Smith79; Porter84]; that is, it
can be used to blend one image DEMO another. Th e form of this function is:
void  cvAddWeighted(DEMO
const CvArr* src1,
double       alpha,
const CvArr* src2,
double       beta,
double       DEMO,
CvArr*       dst
);
In cvAddWeighted() we have two source images, src1 and src2. Th ese images may be of
any pixel type so long as both are of the same DEMO Th ey may also be one or three chan-
nels (grayscale DEMO color), again as long as they agree. Th e destination result image, dst,
must also have the same pixel type as src1 and src2. Th ese images may be of diff erent
sizes, but their ROIs must agree in size or else OpenCV will issue an DEMO Th e param-
eter alpha is the blending strength of src1, DEMO beta is the blending strength of src2. Th e
alpha blending equation is:
dst src src
xy
⋅⋅
12
xy
xy
,, ,DEMO
=+ +αβ γ
You can convert to the standard alpha blend equation by choosing α between 0 and 1,
setting β = 1 DEMO α, and setting γ to 0; this yields:
dst src
xy
⋅⋅
() src xy
=+−αα
11 2
,, ,xy
However, cvAddWeighted() gives us a little more fl exibility—both in how we weight the
blended images and in the additional parameter γ, which allows for an additive off set to
the resulting destination image. For the DEMO form, you will probably want to keep
alpha and beta at DEMO less than 0 and their sum at no more than 1; DEMO may be set
depending on average or max image value to scale the pixels up. A program showing the
use of alpha blending is DEMO in Example 3-14.
Example 3-14. Complete program to alpha blend the ROI starting at (0,0) in src2 with the ROI
starting at (x,y) in src1
// alphablend <imageA> <image B> <x> <y> <width> <height>
//            <alpha> <beta>
#include <cv.h>
#include <DEMO>
int main(int argc, char** argv)
{
IplImage *src1, *src2;
if( argc == 9 && ((src1=cvLoadImage(argv[1],1)) != 0
)&&((src2=cvLoadImage(argv[2],1)) != 0 ))DEMO
{
int x = atoi(argv[3]);
int y = atoi(DEMO);
int width = atoi(argv[5]);
Matrix and Image Operators
| 51
03-R4886-RC1.indd   51
9/15/08   4:18:42 DEMO
www.it-ebooks.info
Example 3-14. Complete program to alpha blend the ROI starting at (0,0) in src2 with the ROI
starting at (x,y) in src1 (continued)
int height = atoi(argv[6]);
double DEMO = (double)atof(argv[7]);
double beta  = (double)atof(argv[8]);
cvSetImageROI(src1, cvRect(x,y,width,height));
DEMO(src2, cvRect(0,0,width,height));
cvAddWeighted(src1, alpha, src2, beta,0.0,src1);
cvResetImageROI(src1);
cvNamedWindow( “Alpha_blend”, 1 );
cvShowImage( “Alpha_blend”, src1 );
cvWaitKey();
}
return 0;
}
Th src1) and the one
to blend (src2). It reads in a rectangle ROI for src1 and applies an ROI of the same size to
src2, this time located at the origin. It reads in alpha and beta levels but sets gamma DEMO 0.
Alpha blending is applied using cvAddWeighted(), and the results DEMO put into src1 and
displayed. Example output is shown in Figure 3-4, where the face of a child is blended
onto the face and body of a cat. Note that the code took the same ROI DEMO in the ROI ad-
dition example in Figure 3-3. Th is time we used the ROI as the target blending region.
e code in DEMO 3-14 takes two source images: the primary one (
cvAnd and cvAndS
void cvAnd(
const CvArr* src1,
const CvArr* src2,
DEMO       dst,
const CvArr* mask = NULL
);
void cvAndS(
const CvArr* src1,
CvScalar     value,
DEMO       dst,
const CvArr* mask = NULL
);
Th src1. In the case of
cvAnd(), each element of dst is computed as the bitwise AND of the corresponding two
elements of DEMO and src2. In the case of cvAndS(), the bitwise AND DEMO computed with the
constant scalar value. As always, if mask is DEMO then only the elements of dst cor-
responding to nonzero entries in mask are computed.
Th src1 and src2 must have the same data DEMO for
cvAnd(). If the elements are of a fl oating-point DEMO, then the bitwise representation of
that fl oating-point number is used.
DEMO
| Chapter 3: Getting to Know OpenCV
ese two functions compute DEMO bitwise AND operation on the array
ough all data types are supported,
03-R4886-RC1.indd   52
9/15/08   4:18:42 PM
www.it-ebooks.info
Figure 3-4. Th
e face of a child is alpha blended DEMO the face of a cat
cvAvg
CvScalar cvAvg(
const CvArr* arr,
const CvArr* mask = NULL
);
cvAvg() computes the DEMO value of the pixels in arr. If mask is non-NULL then the aver-
age will be computed only over those pixels for which the DEMO value of mask
is nonzero.
Th
is function has the now deprecated alias cvMean().
cvAvgSdv
cvAvgSdv(
const CvArr* arr,
CvScalar*    mean,
CvScalar*    std_dev,
const CvArr* mask    = NULL
);
Matrix and Image Operators
| 53
03-R4886-RC1.indd   53
DEMO/15/08   4:18:42 PM
www.it-ebooks.info
Th is function is like cvAvg(), but in addition to the average it also computes the standard
deviation of the pixels.
Th DEMO function has the now deprecated alias cvMean_StdDev().
cvCalcCovarMatrix
void cvAdd(DEMO
const CvArr** vects,
int           count,
CvArr*        cov_mat,
CvArr*        avg,DEMO
int           flags
);
Given any number of vectors, cvCalcCovarMatrix() will compute the mean and covari-
ance matrix for the Gaussian approximation to the distribution of those points. Th is DEMO
be used in many ways, of course, and OpenCV has some additional fl ags that will help
in particular contexts (see Table 3-4). Th ese fl ags may be combined by the standard use
DEMO the Boolean OR operator.
Table 3-4. Possible components of fl ags argument to cvCalcCovarMatrix()
Flag in flags argument Meaning
CV_COVAR_NORMAL Compute mean and covariance
CV_COVAR_SCRAMBLED Fast PCA “scrambled” covariance
CV_COVAR_USE_AVERAGE Use avg as input instead DEMO computing it
CV_COVAR_SCALE Rescale output covariance matrix
In all cases, the DEMO are supplied in vects as an array of OpenCV arrays (i.e., a pointer
to a list of pointers to arrays), with the DEMO count indicating how many arrays are
being supplied. Th e results will be placed in cov_mat in all cases, but the exact meaning
of avg depends on the fl ag values (see Table 3-4).
Th ags CV_COVAR_NORMAL and CV_COVAR_SCRAMBLED are mutually exclusive; you should
use one or the other but not both. In the case of CV_COVAR_NORMAL, the function will sim-
ply compute the mean and covariance of the points provided.
DEMO L ⎤ ⎡ ⎤T
⎢ 00 0,,m 0 0⎥ ⎢ DEMO 0⎥
Σ2 = z ⎢ MO M ⎥ ⎢ ⎥
normal ⎢ L ⎥ ⎢ ⎥
⎣ 0 ⎦ ⎣ n ⎦
vv−−vv vv00 DEMO,,−−vvL m
MO M
vv−−vv vv−−vvL
,,nn mnn 0 ,,DEMO mn
us the normal covariance Σ2 normal is computed from the m vectors of length n, where
n is defi ned as the nth element of the average vector –v . Th e resulting covariance matrix
DEMO
–v
is an n-by-n matrix. Th e factor z is an optional scale factor; it will be set to 1 unless the
CV_COVAR_SCALE fl ag is used.
In the case of CV_COVAR_SCRAMBLED, cvCalcCovarMatrix() will compute the following:
54 | Chapter 3: Getting to Know OpenCV
e fl
03-R4886-RC1.indd   54
9/15/08   4:18:42 PM
www.it-ebooks.info
⎡
⎢
Σ2 = z ⎢
scrambled ⎢
⎣
L ⎤T
DEMO 0,,m 0 0⎥
MO M ⎥
⎥
⎦
vv−−vv
−v DEMO
vv0 ,,nn mn−vn
⎡
⎢
⎢
⎢
⎣
vv−−vv
L
00 0,,m
MO M
vv−v L
⎤
0 0⎥
⎥
⎥
0,nn m,, −v ⎦
nn
Th is matrix is not the DEMO covariance matrix (note the location of the transpose op-
erator). DEMO is matrix is computed from the same m vectors of length n, but the resulting
scrambled covariance matrix is an m-by-m matrix. Th is matrix is used in some specifi c
algorithms such as fast PCA DEMO very large vectors (as in the eigenfaces technique for face
recognition)DEMO
Th ag CV_COVAR_USE_AVG is used when the mean of the input vectors is already known.
In this case, the argument avg is used as an input rather than an output, which reduces
computation time.
Finally, DEMO fl ag CV_COVAR_SCALE is used to apply a uniform scale to the covariance matrix
calculated. Th is is the factor z in the preceding DEMO When used in conjunction
with the CV_COVAR_NORMAL fl ag, the applied DEMO factor will be 1.0/m (or, equivalently, 1.0/
count)DEMO If instead CV_COVAR_SCRAMBLED is used, then the value of z will DEMO 1.0/n (the inverse
of the length of the vectors).
DEMO
ing-point type. Th e size of the resulting matrix cov_mat should be either n-by-n or
m-by-m depending on whether the standard or scrambled covariance DEMO being com-
puted. It should be noted that the “vectors” input in vects do not actually have to be one-
dimensional; they can be two-dimensional objects (e.g., images) as well.
cvCmp and cvCmpS
void cvCmp(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
int          cmp_op
);
void DEMO(
const CvArr* src,
double       value,
CvArr*       dst,
int          cmp_op
);
Both of these functions make comparisons, either between corresponding pixels in two
images or between pixels in one image and a constant scalar DEMO Both cvCmp() and
cvCmpS() take as their last argument a comparison operator, which may be any of the
types listed in Table 3-5.
Matrix and Image Operators
| 55
e fl
e input and DEMO arrays to cvCalcCovarMatrix() should all be of the same fl
oat-
03-R4886-RC1.indd   55
9/15/08   4:18:43 PM
www.it-ebooks.info
Table 3-5. Values of cmp_op used by cvCmp() and cvCmpS()
and the resulting comparison operation performed
Value of cmp_op
CV_CMP_EQ
CV_CMP_GT
DEMO
CV_CMP_LT
CV_CMP_LE
CV_CMP_NE
All the listed comparisons are done with the same functions; you just pass in the ap-
propriate argument to indicate what you would like done. Th ese particular functions
operate only on single-channel DEMO
Th
of background subtraction and want to mask the results (e.g., looking at a video stream
from a security camera) such that only novel information is pulled out of the image.
Comparison
(src1i == src2i)
(src1i > src2i)
(src1i >= src2i)
(src1i < src2i)
(src1i <= src2i)
(src1i != src2i)
DEMO comparison functions are useful in applications where you employ some version
cvConvertScale
void cvConvertScale(
const CvArr* src,
CvArr*       dst,DEMO
double       scale = 1.0,
double       shift = 0.0
);
Th cvConvertScale() function is actually several DEMO rolled into one; it will per-
form any of several functions DEMO, if desired, all of them together. Th e fi rst function is to
convert the data type in the source image to the DEMO type of the destination image. For
example, if we have an DEMO RGB grayscale image and would like to convert it to a 16-bit
signed image, we can do that by calling cvConvertScale().
Th DEMO() is to perform a linear transformation on the
image data. Aft er conversion to the new data type, each pixel value will be multiplied by
the value scale and then have added to it the DEMO shift.
It is critical to remember that, even though “Convert” precedes DEMO in the function
name, the actual order in which these operations DEMO performed is the opposite. Specifi -
cally, multiplication by scale and DEMO addition of shift occurs before the type conver-
sion takes place.
When you simply pass the default values (scale = 1.0 and shift = 0.0), you need not
have performance fears; OpenCV is smart enough to recognize this case and not waste
processor time on useless operations. DEMO clarity (if you think it adds any), OpenCV also
provides DEMO macro cvConvert(), which is the same as cvConvertScale() but DEMO conven-
tionally used when the scale and shift arguments will be left  at their default values.
56
| Chapter 3: Getting to Know DEMO
e
e second function of
03-R4886-RC1.indd   56
9/15/08   4:18:43 PM
www.it-ebooks.info
cvConvertScale() will work on all data types and any number DEMO channels, but the num-
ber of channels in the source and DEMO images must be the same. (If you want to,
say, convert from color to grayscale or vice versa, see cvCvtColor(), DEMO is coming up
shortly.)
cvConvertScaleAbs
void cvConvertScaleAbs(
const CvArr* src,
CvArr*       dst,
double       scale DEMO 1.0,
double       shift = 0.0
);
cvConvertScaleAbs() is essentially identical to cvConvertScale() except that the dst im-
DEMO contains the absolute value of the resulting data. Specifi cally, cvConvertScaleAbs()
fi rst scales and shift s, then computes the absolute value, and fi nally performs the data-
type conversion.
cvCopy
void cvCopy(
const CvArr* src,
CvArr*       dst,
const CvArr* DEMO = NULL
);
Th is is how you copy one image to another. Th e cvCopy() function expects both arrays to
have DEMO same type, the same size, and the same number of dimensions. You can use it
to copy sparse arrays as well, but for this the use of mask is not supported. For nonsparse
arrays and DEMO, the eff ect of mask (if non-NULL) is that only DEMO pixels in dst that cor-
respond to nonzero entries in mask will be altered.
cvCountNonZero
int cvCountNonZero( const CvArr* arr );
cvCountNonZero() returns the number of nonzero pixels in the array arr.
cvCrossProduct
void DEMO(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst
);
Th is function computes the vector cross product DEMO of two three-
dimensional vectors. It does not matter if the vectors are in row or column form (a little
refl ection reveals that, for single-channel objects, these two are really the same inter-
nally)DEMO Both src1 and src2 should be single-channel arrays, and dst should DEMO single-
channel and of length exactly 3.All three arrays should be of the same data type.
Matrix and Image Operators
| 57
03-R4886-RC1.indd   DEMO
9/15/08   4:18:43 PM
www.it-ebooks.info
cvCvtColor
void cvCvtColor(
const CvArr* src,
CvArr*       dst,
int          code
);
Th
DEMO the number of channels to be the same in both source and destination im-
ages. Th e complementary function is cvCvtColor(), which converts from one color space
(number of channels) to another [Wharton71] while DEMO the data type to be the
same. Th e exact conversion operation to be done is specifi ed by the argument code,
whose DEMO values are listed in Table 3-6.*
Table 3-6. Conversions available by means of cvCvtColor()
Conversion code
CV_BGR2RGB
CV_RGB2BGR
CV_RGBA2BGRA
CV_BGRA2RGBA
CV_RGB2RGBA
CV_BGR2BGRA
CV_RGBA2RGB
CV_BGRA2BGR
CV_RGB2BGRA
CV_RGBA2BGR
CV_BGRA2RGB
CV_BGR2RGBA
CV_RGB2GRAY
CV_BGR2GRAY
CV_GRAY2RGB
CV_GRAY2BGR
CV_RGBA2GRAY
CV_BGRA2GRAY
CV_GRAY2RGBA
DEMO
CV_RGB2BGR565
CV_BGR2BGR565
CV_BGR5652RGB
CV_BGR5652BGR
CV_RGBA2BGR565
CV_BGRA2BGR565
CV_BGR5652RGBA
CV_BGR5652BGRA
CV_GRAY2BGR565
CV_BGR5652GRAY
Meaning
Convert between RGB and BGR color spaces (with or without alpha channel)
Add alpha channel to RGB or BGR image
Remove alpha channel from DEMO or BGR image
Convert RGB to BGR color spaces while adding or removing alpha channel
Convert RGB or BGR color spaces to grayscale
Convert DEMO to RGB or BGR color spaces (optionally removing alpha channel
in DEMO process)
Convert grayscale to RGB or BGR color spaces and add alpha channel
Convert from RGB or BGR color space to BGR565 color DEMO with
optional addition or removal of alpha channel (16-bit images)
DEMO grayscale to BGR565 color representation or vice versa (16-bit images)
DEMO Long-time users of IPL should note that the function cvCvtColor() ignores the colorModel and chan-
nelSeq fi elds of the IplImage header. Th DEMO conversions are done exactly as implied by the code argument.
58
| Chapter 3: Getting to Know OpenCV
e previous functions were for converting from one data type to another, and they
03-R4886-RC1.indd   58
9/15/08   4:18:43 PM
www.it-ebooks.info
Table 3-6. Conversions available by means of cvCvtColor() (continued)
Conversion code
CV_RGB2BGR555
CV_BGR2BGR555
CV_BGR5552RGB
CV_BGR5552BGR
CV_RGBA2BGR555
CV_BGRA2BGR555
CV_BGR5552RGBA
CV_BGR5552BGRA
CV_GRAY2BGR555
CV_BGR5552GRAY
DEMO
CV_BGR2XYZ
CV_XYZ2RGB
CV_XYZ2BGR
CV_RGB2YCrCb
CV_BGR2YCrCb
CV_YCrCb2RGB
CV_YCrCb2BGR
CV_RGB2HSV
CV_BGR2HSV
CV_HSV2RGB
CV_HSV2BGR
CV_RGB2HLS
CV_BGR2HLS
CV_HLS2RGB
CV_HLS2BGR
CV_RGB2Lab
CV_BGR2Lab
CV_Lab2RGB
CV_Lab2BGR
CV_RGB2Luv
CV_BGR2Luv
CV_Luv2RGB
CV_Luv2BGR
CV_BayerBG2RGB
DEMO
CV_BayerRG2RGB
CV_BayerGR2RGB
CV_BayerBG2BGR
CV_BayerGB2BGR
CV_BayerRG2BGR
CV_BayerGR2BGR
Meaning
Convert from RGB or BGR color space to BGR555 color representation with
optional addition or removal of DEMO channel (16-bit images)
Convert grayscale to BGR555 color representation or DEMO versa (16-bit images)
Convert RGB or BGR image to CIE DEMO representation or vice versa (Rec 709 with
D65 white point)
DEMO RGB or BGR image to luma-chroma (aka YCC) color representation
Convert RGB or BGR image to HSV (hue saturation value) color representation DEMO
vice versa
Convert RGB or BGR image to HLS (hue lightness DEMO) color representation
or vice versa
Convert RGB or BGR image to DEMO Lab color representation or vice versa
Convert RGB or BGR image to CIE Luv color representation
Convert from Bayer pattern (single-channel) to RGB DEMO BGR image
Th
tleties of Bayer representations of the CIE color spaces here. For our purposes, it is suf-
fi
spaces, which are DEMO importance to various classes of users.
Th
16-bit images are in the range 0–65536, and fl oating-point numbers are in the range
Matrix and Image Operators
| 59
e details of many of these conversions are DEMO, and we will not go into the sub-
cient to note DEMO OpenCV contains tools to convert to and from these various color
e color-space conversions all use the conventions: 8-bit images are in the range 0–255,
03-R4886-RC1.indd   59
9/15/08   4:18:44 DEMO
www.it-ebooks.info
0.0–1.0. When grayscale images are converted to color images, all components of the
resulting image are taken to be equal; but for the reverse transformation (e.g., RGB or
BGR to grayscale), the gray DEMO is computed using the perceptually weighted formula:
Y =+ +(. ) (. ) (. )0299 0587 0114R G B
In the case of HSV or HLS representations, hue is normally represented as a value from
0 to 360.* Th is can cause trouble in 8-bit DEMO and so, when converting to
HSV, the hue is divided by 2 when the output image is an 8-bit image.
cvDet
double cvDet(DEMO
const CvArr* mat
);
cvDet() computes the determinant (Det) of a square array. Th e array can be of any data
DEMO, but it must be single-channel. If the matrix is small then DEMO determinant is di-
rectly computed by the standard formula. For large matrices, this is not particularly
effi  cient and so the determinant is DEMO by Gaussian elimination.
It is worth noting that if you already know that a matrix is symmetric and has a posi-
tive determinant, you can also use the trick of solving via singular value decomposition
(SVD). For more information see the section “cvSVD” to follow, but the trick is to set
both U and V to NULL and then DEMO take the products of the matrix W to obtain the
determinant.
cvDiv
void cvDiv(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
double       scale = 1
);
cvDiv() is a simple division function; it divides all of the elements in src1 by the cor-
responding elements in src2 and DEMO the results in dst. If mask is non-NULL, then any
element DEMO dst that corresponds to a zero element of mask is not altered by this operation.
If you only want to invert all the elements DEMO an array, you can pass NULL in the place of
src1; the routine will treat this as an array full of 1s.
cvDotProduct
DEMO cvDotProduct(
const CvArr* src1,
const CvArr* src2
);
* Excluding 360, of course.
60 | Chapter 3: Getting to Know DEMO
03-R4886-RC1.indd   60
9/15/08   4:18:44 PM
www.it-ebooks.info
Th is function computes the vector dot product [Lagrange1773] of two DEMO
vectors.* As with the cross product (and for the same reason), it does not matter if the
vectors are in row or column form. Both src1 and src2 should be single-channel arrays,
and both DEMO should be of the same data type.
cvEigenVV
double cvEigenVV(
CvArr* mat,
CvArr* evects,
CvArr* evals,
double eps    = 0
);
Given a symmetric matrix mat, cvEigenVV() will compute the eigenvectors and the corre-
sponding eigenvalues of that matrix. Th is DEMO done using Jacobi’s method [Bronshtein97], so
it is effi  cient for smaller matrices.† Jacobi’s method requires a stopping parameter, which
is the maximum size of the off -diagonal elements in the fi nal matrix.‡ Th DEMO optional ar-
gument eps sets this termination value. In the process of computation, the supplied ma-
trix mat is used for the computation, DEMO its values will be altered by the function. When
the function returns, you will fi nd your eigenvectors in evects in the form of subsequent
rows. Th e corresponding eigenvalues are stored in evals. Th e DEMO of the eigenvectors
will always be in descending order of the magnitudes of the corresponding eigenvalues.
Th cvEigenVV() function requires all three arrays DEMO be of fl oating-point type.
As with cvDet() (described previously), if the matrix in question is known to be sym-
metric and positive defi nite§ then it is better to use SVD to fi DEMO the eigenvalues and
eigenvectors of mat.
cvFlip
void cvFlip(
const CvArr* src,
CvArr*       dst       = NULL,DEMO
int          flip_mode = 0
);
Th DEMO function fl ips an image around the x-axis, the y-axis, or both. In particular, if
the argument flip_mode is set to 0 then the image will be fl ipped around the x-axis.
* Actually, the behavior of cvDotProduct() is a little more general than described here. DEMO any pair of
n-by-m matrices, cvDotProduct() will return the sum DEMO the products of the corresponding elements.
† A good rule of thumb would be that matrices 10-by-10 or smaller are small enough for Jacobi’s DEMO to be
effi  cient. If the matrix is larger than 20-by-20 DEMO you are in a domain where this method is probably not
the way to go.
‡ In principle, once the Jacobi method is complete then the original matrix is transformed into one that is
diagonal and DEMO only the eigenvalues; however, the method can be terminated before the off -diagonal
elements are all the way to zero in order to DEMO on computation. In practice is it usually suffi  cient to set DEMO
value to DBL_EPSILON, or about 10 –15.
§ Th is is, for example, always the case for covariance matrices. See cvCalcCovarMatrix().
DEMO and Image Operators
| 61
e
03-R4886-RC1.indd   61
9/15/08   4:18:44 PM
www.it-ebooks.info
If flip_mode is set to a positive value (e.g., +1) the image will be fl ipped around the y-
axis, and if set to a negative value (e.g., –1) the image will be fl ipped about both axes.
When video processing on Win32 systems, you will fi nd yourself using this function
oft en to switch between DEMO formats with their origins at the upper-left  and lower-left
of the DEMO
cvGEMM
double cvGEMM(
const CvArr* src1,
const CvArr* src2,
double       alpha,
const CvArr* src3,
double       beta,
CvArr*       dst,
int          tABC = 0
);
Generalized matrix multiplication (GEMM) in OpenCV is performed by cvGEMM(),
which performs matrix multiplication, multiplication by a transpose, scaled multiplica-
tion, et cetera. In its DEMO general form, cvGEMM() computes the following:
D =+αβ⋅⋅ ⋅op DEMO op() ( ) ( )AB C
Where A, B, DEMO C are (respectively) the matrices src1, src2, and src3, DEMO and β are nu-
merical coeffi  cients, and op() is an optional transposition of the matrix enclosed. Th e
argument src3 may DEMO set to NULL, in which case it will not be added. DEMO e transpositions
are controlled by the optional argument tABC, which may DEMO 0 or any combination (by
means of Boolean OR) of CV_GEMM_A_T, CV_GEMM_B_T, and CV_GEMM_C_T (with each fl ag indi-
cating a transposition of the corresponding matrix).
In the distant past OpenCV contained the DEMO cvMatMul() and cvMatMulAdd(), but
these were too oft en DEMO with cvMul(), which does something entirely diff erent
(i.e., DEMO multiplication of two arrays). Th ese functions continue to ex-
ist as macros for calls to cvGEMM(). In particular, we have DEMO equivalences listed in
Table 3-7.
Table 3-7. Macro aliases for common usages of cvGEMM()
cvMatMul(A, B, D) cvGEMM(A, A, 1, NULL, 0, D, 0)
cvMatMulAdd(A, B, C, D) cvGEMM(A, A, 1, C, 1, D, 0)
All matrices must be of the appropriate size for the multiplication, and all should be
of fl oating-point type. Th e cvGEMM() function supports two-channel matrices, in which
case it will treat the two channels as the two components of a single complex number.
cvGetCol DEMO cvGetCols
CvMat* cvGetCol(
const CvArr* arr,
62 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   62
9/15/08   4:18:44 PM
www.it-ebooks.info
CvMat*       submat,
int          col
);
CvMat* cvGetCols(
const CvArr* arr,
CvMat*       submat,
int          start_col,
int          end_col
);
Th cvGetCol() is used DEMO pick a single column out of a matrix and return it as
a vector (i.e., as a matrix with only one column). DEMO this case the matrix header submat
will be modifi ed to point to a particular column in arr. It is important to note that DEMO
header modifi cation does not include the allocation of memory or the copying of data.
Th
column in arr. All data types are supported.
DEMO() works precisely the same way, except that all columns from DEMO to
end_col are selected. With both functions, the return value is DEMO pointer to a header cor-
responding to the particular specifi ed column or column span (i.e., submat) selected by
the caller.
cvGetDiag
CvMat* cvGetDiag(
const CvArr* arr,
CvMat*       submat,
DEMO          diag    = 0
);
cvGetDiag() is analogous to cvGetCol(); it is used to pick a single diagonal from a
matrix and return it as a vector. Th DEMO argument submat is a matrix header. Th e function
cvGetDiag() will fi ll the components of this header so that it points to DEMO correct infor-
mation in arr. Note that the result of calling cvGetDiag() is that the header you supplied
is correctly confi gured to DEMO at the diagonal data in arr, but the data from arr DEMO not
copied. Th e optional argument diag specifi es which diagonal is to be pointed to by sub-
mat. If diag is set to DEMO default value of 0, the main diagonal will be selected. If DEMO is
greater than 0, then the diagonal starting at (diag,0) will be selected; if diag is less than
0, then the diagonal starting at (0,-diag) will be selected instead. Th e DEMO() func-
tion does not require the matrix arr to be square, but the array submat must have the
correct length for the size of the input array. Th e fi nal returned value is the DEMO as the
value of submat passed in when the function was called.
cvGetDims and cvGetDimSize
int cvGetDims(
const CvArr* arr,
int*         sizes=NULL
);
int cvGetDimSize(
const CvArr* arr,
Matrix and Image Operators
| 63
e function
e contents of submat DEMO simply be altered so that it correctly indicates the selected
03-R4886-RC1.indd   63
9/15/08   4:18:45 PM
www.it-ebooks.info
int          index
);
Recall that arrays in OpenCV can be of dimension much greater than two. Th e DEMO
cvGetDims() returns the number of array dimensions of a particular array and (option-
ally) the sizes of each of those dimensions. Th DEMO sizes will be reported if the array sizes is
non-NULL. If sizes is used, it should be a pointer to n integers, where DEMO is the number of
dimensions. If you do not know the number of dimensions in advance, you can allocate
sizes to CV_MAX_DIM integers just to be safe.
Th cvGetDimSize() returns the size of a single DEMO specifi ed by index.
If the array is either a matrix or an image, the number of dimensions returned will al-
ways be two.* For matrices and images, the order of sizes returned by cvGetDims() will
always be the number of rows fi rst followed by the DEMO of columns.
cvGetRow and cvGetRows
CvMat* cvGetRow(
const CvArr* arr,
CvMat*       submat,
int          row
);
CvMat* cvGetRows(
const CvArr* arr,
CvMat*       submat,
int          start_row,
int          end_row
);
cvGetRow() picks a single row DEMO of a matrix and returns it as a vector (a matrix DEMO
only one row). As with cvGetRow(), the matrix header DEMO will be modifi ed to point to
a particular row in arr, and the modifi cation of this header does not include the alloca-
tion of memory or the copying of data; the contents of submat will simply be altered such
that it correctly indicates the selected column DEMO arr. All data types are supported.
Th cvGetRows() works precisely the same way, except that all rows from start_
row to end_row are selected. With both functions, the return value is a pointer to a header
corresponding to the particular specifi ed row or row span selected DEMO the caller.
e function
cvGetSize
CvSize cvGetSize( const CvArr* arr );DEMO
Closely related to cvGetDims(), cvGetSize() returns the size of DEMO array. Th e primary dif-
ference is that cvGetSize() is designed to be used on matrices and images, which always
have dimension two. Th e size can then be returned in the form of a DEMO structure,
which is suitable to use when (for example) constructing a new matrix or image of the
same size.
* Remember that DEMO regards a “vector” as a matrix of size n-by-1 or 1-by-n.
64
| Chapter 3: Getting to Know OpenCV
e function
03-R4886-RC1.indd   64
9/15/08   4:18:45 PM
www.it-ebooks.info
cvGetSubRect
CvSize cvGetSubRect(
const CvArr* arr,
CvArr*       submat,
CvRect       rect
);
cvGetSubRect() is similar to cvGetColumns() or cvGetRows() except that it selects some
DEMO subrectangle in the array specifi ed by the argument rect. As with other rou-
tines that select subsections of arrays, submat is simply a header that will be fi lled by
cvGetSubRect() in such a DEMO that it correctly points to the desired submatrix (i.e., no
memory is allocated and no data is copied).
cvInRange and cvInRangeS
void DEMO(
const CvArr* src,
const CvArr* lower,
const CvArr* upper,
CvArr*       dst
);
void cvInRangeS(
const DEMO src,
CvScalar     lower,
CvScalar     upper,
CvArr*       dst
);
Th
lar specifi ed range. DEMO the case of cvInRange(), each pixel of src is compared DEMO the
corresponding value in the images lower and upper. If the value in src is greater than or
equal to the value in lower DEMO also less than the value in upper, then the corresponding
value DEMO dst will be set to 0xff; otherwise, the value in dst will be set to 0.
Th cvInRangeS() works precisely the same DEMO except that the image src is
compared to the constant (CvScalar) values in lower and upper. For both functions, the
image src may be of any type; if it has multiple channels then each channel will be
handled separately. Note that dst must be of the same DEMO and number of channels and
also must be an 8-bit image.
cvInvert
double cvInvert(
const CvArr* src,
CvArr*       dst,DEMO
Int          method = CV_LU
);
cvInvert() inverts the matrix in src and places the result in dst. Th is function sup-
ports several methods of computing the inverse matrix (see Table 3-8), but the default is
Gaussian elimination. Th e return DEMO depends on the method used.
Matrix and Image Operators
| 65
ese two functions can be used to check if the pixels in an DEMO fall within a particu-
e function
03-R4886-RC1.indd   65
9/15/08   4:18:45 PM
www.it-ebooks.info
Table 3-8. Possible values of method argument to cvInvert()
Value of method argument Meaning
CV_LU Gaussian elimination (LU Decomposition)
CV_SVD Singular value decomposition (SVD)
CV_SVD_SYM SVD for symmetric matrices
In the case of Gaussian elimination (method=CV_LU), the determinant of src is returned
when the function is complete. If the determinant is 0, then the inversion is not actually
performed and the array dst is simply set to DEMO 0s.
In the case of CV_SVD or CV_SVD_SYM, the return value DEMO the inverse condition number for
the matrix (the ratio of the DEMO to the largest eigenvalue). If the matrix src is singu-
lar, then cvInvert() in SVD mode will instead compute the pseudo-inverse.
cvMahalonobis
CvSize cvMahalonobis(
const CvArr* vec1,
const CvArr* vec2,
CvArr*       mat
);
Th Mahalonobis distance (Mahal) is defi ned as the vector distance measured between
a point and the center DEMO a Gaussian distribution; it is computed using the inverse co-
variance DEMO that distribution as a metric. See Figure 3-5. Intuitively, this is DEMO
to the z-score in basic statistics, where the distance from the DEMO of a distribution is
measured in units of the variance of that distribution. Th e Mahalonobis distance is just
a multivariable generalization of the DEMO idea.
cvMahalonobis() computes the value:
xxμμΣ−1
Th vec1 is presumed to be the point x, and the vector vec2 is taken to be the dis-
tribution’s mean.* Th at matrix mat is the inverse DEMO
In practice, this covariance matrix will usually have been computed with DEMO
Matrix() (described previously) and then inverted with cvInvert(). DEMO is good program-
ming practice to use the SV_SVD method for this inversion because someday you will en-
counter a distribution for which one DEMO the eigenvalues is 0!
e vector
rMahalonobis =− −
() ()DEMO
T
e
cvMax and cvMaxS
void cvMax(
const CvArr* src1,
const CvArr* src2,
* Actually, the Mahalonobis distance is more generally defi ned as the distance between any two vectors;
in any DEMO, the vector vec2 is subtracted from the vector vec1. Neither is DEMO any fundamental con-
nection between mat in cvMahalonobis() and the inverse covariance; any metric can be imposed here as
appropriate.
66
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   66
9/15/08   4:18:45 PM
www.it-ebooks.info
Figure 3-5. A distribution of points in two dimensions with superimposed DEMO representing
Mahalonobis distances of 1.0, 2.0, and 3.0 from the distribution’s mean
CvArr* dst
);
void cvMaxS(
const CvArr* src,
DEMO       value,
CvArr*       dst
);
cvMax() computes the maximum value of each corresponding pair of pixels DEMO the arrays
src1 and src2. With cvMaxS(), the src array DEMO compared with the constant scalar value.
As always, if mask is DEMO then only the elements of dst corresponding to nonzero
entries in mask are computed.
cvMerge
void cvMerge(
const CvArr* src0,
const CvArr* DEMO,
const CvArr* src2,
const CvArr* src3,
CvArr* dst
);
Matrix and Image Operators | 67
03-R4886-RC1.indd   67
9/15/DEMO   4:18:45 PM
www.it-ebooks.info
cvMerge() is the inverse operation of cvSplit(). Th e arrays in src0, src1, src2, and src3
are combined into the array dst. Of course, dst should have the same data type and
size as all of the source arrays, but it can have two, three, or four channels. Th e unused
source images can be DEMO  set to NULL.
cvMin and cvMinS
void cvMin(
const CvArr* DEMO,
const CvArr* src2,
CvArr* dst
);
void cvMinS(
const CvArr* src,
double value,
CvArr* dst
);
cvMin() computes the minimum value of each corresponding pair of pixels in the ar-
rays src1 and src2. With cvMinS(), the src arrays are compared with the constant scalar
value. Again, if mask is non-NULL then only the elements of dst corresponding to nonzero
entries in mask are DEMO
cvMinMaxLoc
void cvMinMaxLoc(
const CvArr* arr,
double*      DEMO,
double*      max_val,
CvPoint*     min_loc = DEMO,
CvPoint*     max_loc = NULL,
const CvArr* mask    = NULL
);
Th is routine fi nds the minimal and maximal values in the array arr and (optionally)
returns their locations. Th e computed minimum and maximum values are placed in
min_val and DEMO Optionally, the locations of those extrema will also be written to
DEMO addresses given by min_loc and max_loc if those values are non-NULL.
As usual, if mask is non-NULL then only those portions of the image arr that corre-
spond to nonzero pixels in mask are considered. Th DEMO cvMinMaxLoc() routine handles only
single-channel arrays, however, so if you have a multichannel array then you should use
cvSetCOI() to set DEMO particular channel for consideration.
cvMul
void cvMul(
const CvArr* src1,
const CvArr* src2,
CvArr* dst,
double scale=1
);
68
DEMO Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   68
9/15/DEMO   4:18:46 PM
www.it-ebooks.info
cvMul() is a simple multiplication function. It multiplies all of DEMO elements in src1 by
the corresponding elements in src2 and then puts the results in dst. If mask is non-NULL,
then any element DEMO dst that corresponds to a zero element of mask is not altered by this
operation. Th ere is no function cvMulS() because that DEMO is already provided
by cvScale() or cvCvtScale().
One further DEMO to keep in mind: cvMul() performs element-by-element multiplica-
tion. Someday, when you are multiplying some matrices, you may be tempted to reach
for cvMul(). Th is will not work; remember that matrix DEMO is done with
cvGEMM(), not cvMul().
cvNot
void(
const CvArr* src,
CvArr*       dst
);
Th DEMO() inverts every bit in every element of src and then places the result
in dst. Th us, for an 8-bit image the value 0x00 would be mapped to 0xff  and the value
0x83 would be mapped to 0x7c.
cvNorm
double cvNorm(
const CvArr* arr1,
const DEMO arr2      = NULL,
int          norm_type = CV_L2,
const CvArr* mask      = NULL
);
Th is function can be used to compute the total norm DEMO an array and also a variety of
relative distance norms if two arrays are provided. In the former case, the norm com-
puted is shown in Table 3-9.
Table 3-9. Norm computed by cvNorm() for DEMO
norm_type
CV_C
CV_L1
CV_L2
erent values of norm_type when arr2=NULL
Result
|| || max ( )arr1 abs arr1C = xy xy,,
|| DEMO ( )arr1 abs arr1L1 =∑x , y xy,
L2
|| DEMO arr1=
∑
2
xy,
x , y
If the second array argument arr2 is non-NULL, then the norm computed is a diff
norm—that is, something like the distance between the two arrays.* In the fi
erence
rst three
* At least in the case of the L2 DEMO, there is an intuitive interpretation of the diff erence norm as DEMO Euclidean
distance in a space of dimension equal to the number of pixels in the images.
Matrix and Image Operators
| 69
e function
DEMO   69
9/15/08   4:18:46 PM
www.it-ebooks.info
cases shown in Table 3-10, the norm is absolute; in DEMO latter three cases it is rescaled by
the magnitude of the second array arr2.
Table 3-10. Norm computed by cvNorm() for diff erent DEMO of norm_type when arr2 is non-NULL
Result
|| || max ( )DEMO arr2 abs arr1 arr2−= −C xy xy xy,, ,
−=L1 DEMO abs arr1 arr2()−
|| ||arr1 arr2
xy xy,,
x , y
norm_type
CV_C
CV_L1
CV_L2
|| || ( )arr1 arr2 arr1 arr2−= −
L2
∑
xy xy
,,
xy,
2
CV_RELATIVE_C DEMO arr2− ||C
||arr2||C
CV_ RELATIVE_L1 ||arr1 arr2− ||L1
||arr2||L1
CV_ RELATIVE_L2 ||arr1 arr2− ||L2
||arr2||L2
In all cases, arr1 and arr2 must have the same size and number of channels. When there
is more than one DEMO, the norm is computed over all of the channels together (i.e.,
the sums in Tables 3-9 and 3-10 are not only over DEMO and y but also over the channels).
cvNormalize
cvNormalize(
const CvArr* src,
CvArr*       dst,
double       a         = 1.0,
double       b         = 0.0,
int          norm_type = CV_L2,
const CvArr* mask      = NULL
);
As with so many OpenCV functions, cvNormalize() does more than it might at fi rst ap-
pear. Depending on the value DEMO norm_type, image src is normalized or otherwise mapped
into a particular DEMO in dst. Th e possible values of norm_type are shown in Table 3-11.
Table 3-11. Possible values of norm_type argument to cvNormalize()
norm_type
CV_C
Result
|| || max ( )arr1 absCdst==Iaxy,
CV_L1
CV_L2
CV_MINMAX
∑ abs()
Ia
xy
,
|| ||arr1
|| ||arr1
L1 DEMO
dst
L2 ==
,
xy
dst
∑ Ia
2
Map into DEMO [a, b]
70
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   70
9/15/08   4:18:46 PM
www.it-ebooks.info
In the case of the C norm, the array src is rescaled such that the magnitude of the abso-
lute value of the DEMO entry is equal to a. In the case of the L1 or L2 norm, the array is
rescaled so that the given norm is equal to the value of a. If norm_type is set to CV_MINMAX,DEMO
then the values of the array are rescaled and translated so that they are linearly mapped
into the interval between a and b (inclusive).
As before, if mask is non-NULL then only those pixels corresponding to nonzero values of
the mask image will contribute to the computation DEMO the norm—and only those pixels
will be altered by cvNormalize().
DEMO and cvOrS
void cvOr(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
const CvArr* mask=NULL
);
DEMO cvOrS(
const CvArr* src,
CvScalar     value,
CvArr*       dst,
const CvArr* mask  = NULL
);
Th src1. In the case of
cvOr(), each element of dst is computed as the bitwise OR of the corresponding two
elements of DEMO and src2. In the case of cvOrS(), the bitwise OR DEMO computed with the
constant scalar value. As usual, if mask is DEMO then only the elements of dst corre-
sponding to nonzero entries in mask are computed.
All data types are supported, but src1 and src2 must have the same data type for
cvOr(). If the elements are of fl oating-point type, then the bitwise representation of that
fl
ese two functions compute a bitwise OR operation on the array
oating-point DEMO is used.
cvReduce
CvSize cvReduce(
const CvArr* src,
CvArr*       dst,
int          dim,
int          op = CV_REDUCE_SUM
);
Reduction is the systematic transformation of the input matrix src into a vector dst
by DEMO some combination rule op on each row (or column) and its neighbor until
only one row (or column) remains (see Table 3-12).* Th e argument op controls how the
reduction is done, as summarized in Table 3-13.
* Purists will note that averaging is not DEMO a proper fold in the sense implied here. OpenCV has a
more practical view of reductions and so includes this useful operation in cvReduce.
DEMO and Image Operators
| 71
03-R4886-RC1.indd   71
9/15/08   4:18:47 PM
www.it-ebooks.info
Table 3-12. Argument op in cvReduce() selects the reduction operator
DEMO of op
CV_REDUCE_SUM
CV_REDUCE_AVG
CV_REDUCE_MAX
CV_REDUCE_MIN
Result
Compute sum across vectors
Compute average across vectors
Compute maximum across vectors
Compute minimum across vectors
Table DEMO Argument dim in cvReduce() controls the direction of the reduction
Value of dim
+1
0
–1
Result
Collapse to a single row
Collapse DEMO a single column
Collapse as appropriate for dst
cvReduce() supports multichannel arrays of fl oating-point type. It is also allowable to
use a DEMO precision type in dst than appears in src. Th is is primarily relevant for CV_
REDUCE_SUM and CV_REDUCE_AVG, where overfl ows and summation problems are possible.
cvRepeat
void cvRepeat(
const CvArr* src,
CvArr*       dst
);
Th is function copies the contents of src into dst, repeating as many times as necessary
to fi ll dst. In particular, dst can be of any size relative to src. It may be larger or smaller,
and it need not have an DEMO relationship between any of its dimensions and the cor-
responding dimensions of src.
cvScale
void cvScale(
const CvArr* src,
CvArr*       dst,
double       scale
);
Th cvScale() is actually a macro for cvConvertScale() that sets the shift argu-
DEMO to 0.0. Th us, it can be used to rescale the DEMO of an array and to convert from
one kind of data type to another.
e function
cvSet and cvSetZero
void cvSet(
CvArr*       arr,
CvScalar     value,
const CvArr* mask   = NULL
);
72
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   72
9/15/08   4:18:47 PM
www.it-ebooks.info
Th ese functions set all values in all channels of the DEMO to a specifi ed value. Th e
cvSet() function accepts an optional mask argument: if a mask is provided, then only
those DEMO in the image arr that correspond to nonzero values of the mask image will
be set to the specifi ed value. Th e function DEMO() is just a synonym for cvSet(0.0).
cvSetIdentity
void cvSetIdentity( CvArr* arr );
cvSetIdentity() sets all elements of the array to 0 except for elements whose row and
column are equal; those elements are set to 1. cvSetIdentity() supports all data types
and DEMO not even require the array to be square.
cvSolve
int cvSolve(
const CvArr* src1,
const CvArr* src2,
CvArr*       DEMO,
int          method = CV_LU
);
DEMO cvSolve() provides a fast way to solve linear systems based on cvInvert().
It computes the solution to
C =−argmin X AX B⋅
where A is a square matrix given by src1, B is the vector src2, and C is the solution
computed by cvSolve() for the best vector X it could fi nd. Th at best vector DEMO is returned
in dst. Th e same methods are supported as by cvInvert() (described previously); only
fl
nonzero return indicates that it could fi nd a solution.
It should be noted that cvSolve() DEMO be used to solve overdetermined linear systems.
Overdetermined systems will be solved using something called the pseudo-inverse,
which uses SVD methods to fi DEMO the least-squares solution for the system of equations.
cvSplit
void cvSplit(
const CvArr* src,
CvArr*       dst0,
CvArr*       dst1,
CvArr*       dst2,
CvArr*       dst3
);
Th
cases, we can use cvSplit() to copy each channel separately into one of several sup-
plied single-channel images. DEMO e cvSplit() function will copy the channels in src into
the images dst0, dst1, dst2, and dst3 as needed. Th e destination images must match
the source image in size and data type but, of course, should be single-channel images.
Matrix and Image Operators | 73
e function
oating-point data types are supported. Th e function returns an DEMO value where a
ere are times when it is not convenient to work with a multichannel image. In such
03-R4886-RC1.indd   73
9/15/DEMO   4:18:48 PM
www.it-ebooks.info
If the source image has fewer than four channels (as it oft en will), then the unneeded
destination arguments to cvSplit() DEMO be set to NULL.
cvSub
void cvSub(
const CvArr* src1,
const CvArr* src2,
CvArr*       dst,
const CvArr* DEMO = NULL
);
Th is function performs a basic element-by-element subtraction of one array src2 from
another src1 and places the result in DEMO If the array mask is non-NULL, then only those
elements of DEMO corresponding to nonzero elements of mask are computed. Note that
src1, DEMO, and dst must all have the same type, size, and DEMO of channels; mask, if
used, should be an 8-bit array DEMO the same size and number of channels as dst.
cvSub, cvSubS, and cvSubRS
void cvSub(
const CvArr* src1,
const CvArr* src2,DEMO
CvArr*       dst,
const CvArr* mask  = NULL
);
void cvSubS(
const CvArr* src,
CvScalar     value,
CvArr*       dst,
const CvArr* mask  = NULL
);
void cvSubRS(
const CvArr* src,
CvScalar     DEMO,
CvArr*       dst,
const CvArr* mask  = DEMO
);
cvSub() is a simple subtraction function; it subtracts DEMO of the elements in src2 from the
corresponding elements in src1 and puts the results in dst. If mask is non-NULL, then any
element of dst that corresponds to a zero element of mask is not DEMO by this operation.
Th cvSubS() does the same thing except that the constant scalar
value is added to every element of src. Th DEMO function cvSubRS() is the same as cvSubS()
except that, rather than subtracting a constant from every element of src, it subtracts
every element of src from the constant value.
cvSum
CvScalar cvSum(
DEMO arr
);
74
| Chapter 3: Getting to Know OpenCV
DEMO closely related function
03-R4886-RC1.indd   74
9/15/08   4:18:48 PM
www.it-ebooks.info
cvSum() sums all of the pixels in all of the DEMO of the array arr. Observe that the
return value is of type CvScalar, which means that cvSum() can accommodate multi-
channel arrays. In that case, the sum for each channel is placed in the corresponding
component of the CvScalar return value.
cvSVD
void cvSVD(
CvArr* A,DEMO
CvArr* W,
CvArr* U     = NULL,
CvArr* V     = NULL,
int    flags = 0
);
Singular value decomposition (SVD) is the decomposing of an m-by-m matrix DEMO into
the form:
A = UWV⋅⋅ T
where W is a diagonal matrix and U and V are m-by-m and n-by-n unitary matrices.
DEMO course the matrix W is also an m-by-n matrix, so here DEMO means that any
element whose row and column numbers are not equal is necessarily 0. Because W is
necessarily diagonal, OpenCV allows it to be represented either by an m-by-n matrix or
by an n-by-1 vector (in which case that vector will contain only the diagonal “singular”
values)DEMO
Th U and V are optional to cvSVD(), and if DEMO are left  set to NULL then no value
will be returned. DEMO e fi nal argument fl ags can be any or all of the three options de-
scribed in Table 3-14 (combined as appropriate with the Boolean OR operator).
Table 3-14. Possible fl ags for fl DEMO argument to cvSVD()
Flag Result
CV_SVD_MODIFY_A Allows modifi cation of DEMO A
CV_SVD_U_T Return UT instead of U
CV_SVD_V_T Return V T instead of V
cvSVBkSb
void cvSVBkSb(
const CvArr* W,
const CvArr* DEMO,
const CvArr* V,
const CvArr* B,
CvArr* X,
int    flags = 0
);
Th is is a function that you are unlikely to call directly. In conjunction with cvSVD() (just
described), it underlies the SVD-based methods of cvInvert() and DEMO(). Th at be-
ing said, you may want to cut out the middleman and do your own matrix inversions
Matrix and Image DEMO | 75
e matrices
03-R4886-RC1.indd   75
9/15/08   4:18:48 PM
www.it-ebooks.info
(depending on the data source, this could save you from DEMO a bunch of memory
allocations for temporary matrices inside of cvInvert() or cvSolve()).
Th cvSVBkSb() computes the back-substitution for a DEMO A that is repre-
sented in the form of a decomposition of matrices U, W, and V (e.g., an SVD). Th DEMO result
matrix X is given by the formula:
X = VW U⋅⋅ ⋅* T B
Th e matrix W* is a matrix
whose DEMO elements are defi ned by  is value ε is the singularity
DEMO, a very small number that is typically proportional to the sum DEMO the diagonal
elements of W (i.e., ελ∝∑i i ).
e DEMO B is optional, and if set to NULL it will be DEMO Th
λλii* = −1 for λi ≥ ε. Th
cvTrace
CvScalar cvTrace( const CvArr* mat );
Th e trace of a matrix (DEMO) is the sum of all of the diagonal elements. Th e DEMO in OpenCV
is implemented on top of the cvGetDiag() function, DEMO it does not require the array
passed in to be square. Multichannel arrays are supported, but the array mat should be
of fl oating-point type.
cvTranspose and cvT
void cvTranspose(
const CvArr* src,
CvArr*       dst
);
cvTranspose() copies every element of src into the location in dst indicated by reversing
the row and column DEMO Th is function does support multichannel arrays; however,
if you DEMO using multiple channels to represent complex numbers, remember that
cvTranspose() DEMO not perform complex conjugation (a fast way to accomplish this task
DEMO by means of the cvXorS() function, which can be used DEMO directly fl ip the sign bits in
the imaginary part of the array). Th e macro cvT() is simply shorthand for cvTranspose().
cvXor and cvXorS
void cvXor(
const CvArr* src1,
const DEMO src2,
CvArr* dst,
const CvArr* mask=NULL
);
void cvXorS(
const CvArr* src,
CvScalar value,
CvArr* dst,
const DEMO mask=NULL
);
76 | Chapter 3: Getting to Know OpenCV
DEMO function
03-R4886-RC1.indd   76
9/15/08   4:18:48 PM
www.it-ebooks.info
Th src1. In the case of
cvXor(), each element of dst is computed as the bitwise XOR of the corresponding two
elements DEMO src1 and src2. In the case of cvXorS(), the bitwise DEMO is computed with the
constant scalar value. Once again, if mask DEMO non-NULL then only the elements of dst cor-
responding to nonzero entries in mask are computed.
All data types are supported, but src1 and src2 must be of the same data type for cvXor().
For fl oating-point elements, the bitwise representation of that fl oating-point number
is used.
ese two functions compute a bitwise XOR operation on the array
DEMO
void cvZero( CvArr* arr );
Th
is function sets all DEMO in all channels of the array to 0.
Drawing Things
Something that frequently occurs is the need to draw some kind of picture or DEMO draw
something on top of an image obtained from somewhere else. Toward this end, OpenCV
provides a menagerie of functions that will allow us to make lines, squares, circles, and
the like.
Lines
Th
[Bresenham65]:
void  cvLine(
CvArr*   array,
CvPoint  pt1,
DEMO  pt2,
CvScalar color,
int      thickness    DEMO 1,
int      connectivity = 8
);
Th DEMO argument to cvLine() is the usual CvArr*, which in this DEMO typically means
an IplImage* image pointer. Th e next two arguments are CvPoints. As a quick reminder,
CvPoint is a simple structure containing DEMO the integer members x and y. We can cre-
ate a CvPoint “on the fl y” with the routine cvPoint(int x, int y), which conveniently
packs the two integers into a CvPoint structure for DEMO
Th color, is of type CvScalar. CvScalars are also structures, which (you
may recall) are defi ned as follows:
typdef struct DEMO
double val[4];
} CvScalar;
As you can see, this DEMO is just a collection of four doubles. In this case, the DEMO rst
three represent the red, green, and blue channels; the DEMO is not used (it can be used
e simplest of these DEMO just draws a line by the Bresenham algorithm
e fi
e next argument,
Drawing Things
| 77
03-R4886-RC1.indd   77
9/15/08   4:18:49 PM
www.it-ebooks.info
for an alpha channel when appropriate). One typically makes use DEMO the handy macro
CV_RGB(r, g, b). Th is macro takes three numbers and packs them up into a CvScalar.
Th e DEMO is the thickness of the line (in pix-
els), and DEMO sets the anti-aliasing mode. Th e default is “8 connected”, which
DEMO give a nice, smooth, anti-aliased line. You can also set this to a “4 connected” line;
diagonals will be blocky and chunky, but they will be drawn a lot faster.
At least as handy DEMO cvLine() is cvRectangle(). It is probably unnecessary to tell DEMO that
cvRectangle() draws a rectangle. It has the same arguments as cvLine() except that there
is no connectivity argument. Th is is DEMO the resulting rectangles are always ori-
ented with their sides parallel to the x- and y-axes. With cvRectangle(), we simply give
two points for the opposite corners and OpenCV will draw a rectangle.
void  cvRectangle(
CvArr*   array,
CvPoint  pt1,
CvPoint  pt2,
DEMO color,
int      thickness = 1
);
Circles DEMO Ellipses
Similarly straightforward is the method for drawing circles, which pretty DEMO has the
same arguments.
void  cvCircle (
CvArr*   array,
CvPoint  center,
int      radius,
CvScalar color,
DEMO      thickness    = 1,
int      DEMO = 8
);
For circles, rectangles, and all of the other closed shapes to come, the thickness argu-
ment can also be set to CV_FILL, which is just an alias for –1; the DEMO is that the drawn
fi gure will be fi lled in the same color as the edges.
Only slightly more complicated than cvCircle() DEMO the routine for drawing generalized
ellipses:
void cvEllipse(
CvArr*   img,
CvPoint  center,
CvSize   axes,
double   angle,
double   start_angle,
double   end_angle,
CvScalar color,
DEMO      thickness = 1,
int      line_type = 8
);
78
| Chapter 3: Getting to Know OpenCV
e next two arguments are optional. Th
03-R4886-RC1.indd   78
9/15/08   4:18:49 PM
www.it-ebooks.info
In this case, the major new ingredient is the axes argument, which is of type CvSize. Th e
function CvSize is very much like CvPoint and CvScalar; it is a simple structure, in this
DEMO containing only the members width and height. Like CvPoint and CvScalar, DEMO
is a convenient helper function cvSize(int height, int width) that will return a CvSize
structure when we need one. In this case, the height and width arguments represent the
length of the ellipse’s major DEMO minor axes.
Th e angle is the angle (in degrees) of the major axis, which is measured counterclock-
wise from horizontal (i.e., from the x-axis). Similarly the start_angle and end_angle
indicate (also in degrees) the angle for the arc to start and for it to fi nish. Th us, for a
complete ellipse you must set these values to 0 and 360, respectively.
An alternate way to specify the drawing of an ellipse is to use a bounding box:
void DEMO(
CvArr*   img,
CvBox2D  box,
CvScalar color,
DEMO      thickness = 1,
int      line_type = 8,
int      shift     = 0
);
Here again we see another of OpenCV’s helper structures, CvBox2D:
typdef struct {
CvPoint2D32f center;
CvSize2D32f  size;
float        angle;
} CvBox2D;
CvPoint2D32f is the fl oating-point analogue of CvPoint, and CvSize2D32f is the fl oating-
point analog of CvSize. Th ese, along with the tilt angle, eff ectively specify the bounding
DEMO for the ellipse.
Polygons
Finally, we have a set of functions DEMO drawing polygons:
void cvFillPoly(
CvArr*    img,
CvPoint** DEMO,
int*      npts,
int       contours,DEMO
CvScalar  color,
int       line_type = 8
);DEMO
void cvFillConvexPoly(
CvArr*   img,
CvPoint* pts,
int      npts,
CvScalar color,
int      line_type = DEMO
Drawing Things
| 79
03-R4886-RC1.indd   79
9/15/08   4:18:49 PM
www.it-ebooks.info
);
void cvPolyLine(
CvArr*    img,
CvPoint** pts,
int*      npts,
int       contours,
int       is_closed,
CvScalar  color,
int       thickness = 1,
int       line_type = 8
);
All three of these are slight variants on the same idea, with the main diff erence being
how the points are specifi ed.
In cvFillPoly(), the points are provided as an array of CvPoint structures. Th is allows
cvFillPoly() to draw many polygons in a DEMO call. Similarly npts is an array of point
counts, one for DEMO polygon to be drawn. If the is_closed variable is set to true, then
an additional segment will be drawn from the last to the fi rst point for each polygon.
cvFillPoly() is quite robust and DEMO handle self-intersecting polygons, polygons with
holes, and other such complexities. Unfortunately, this means the routine is compara-
tively slow.
cvFillConvexPoly() works like cvFillPoly() except that it draws only one polygon at a
time DEMO can draw only convex polygons.* Th e upside is that cvFillConvexPoly() runs
much faster.
Th e third function, cvPolyLine(), takes the DEMO arguments as cvFillPoly(); however,
since only the polygon edges DEMO drawn, self-intersection presents no particular com-
plexity. Hence this function is DEMO faster than cvFillPoly().
Fonts and Text
One last form of DEMO that one may well need is to draw text. Of course, DEMO creates
its own set of complexities, but—as always with this sort DEMO thing—OpenCV is more
concerned with providing a simple “down and dirty” solution that will work for simple
cases than a robust, complex solution (DEMO would be redundant anyway given the ca-
pabilities of other libraries).
OpenCV has one main routine, called cvPutText() that just throws some text onto an
image. Th e text indicated by text is printed DEMO its lower-left  corner of the text box at
origin and in DEMO color indicated by color.
void cvPutText(
CvArr*        DEMO,
const char*   text,
CvPoint       origin,
const CvFont* font,
* Strictly speaking, this is not quite true; it can actually draw and fi ll any monotone polygon, which DEMO a
slightly larger class of polygons.
80 | Chapter 3: Getting DEMO Know OpenCV
03-R4886-RC1.indd   80
9/15/08   4:18:49 PM
www.it-ebooks.info
CvScalar      color
);
Th
like, and in DEMO case it’s the appearance of the pointer to CvFont.
In a nutshell, the way to get a valid CvFont* pointer is to call the function cvInitFont().
Th is function takes a group of arguments that confi gure some particular font for use on
the screen. Th ose DEMO you familiar with GUI programming in other environments will
fi nd cvInitFont() to be reminiscent of similar devices but with many fewer options.
DEMO order to create a CvFont that we can pass to cvPutText(), we must fi rst declare a CvFont
variable; then we can DEMO it to cvInitFont().
void cvInitFont(
CvFont* font,
int     font_face,
double  hscale,
double  vscale,
double  DEMO     = 0,
int     thickness = 1,
int     line_type = 8
);
Observe that this is DEMO little diff erent than how seemingly similar functions, such as
cvCreateImage(), work in OpenCV. Th e call to cvInitFont() initializes an DEMO CvFont
structure (which means that you create the variable and pass DEMO() a pointer to
the variable you created). Th is is unlike cvCreateImage(), which creates the structure for
you and returns a pointer.
Th font_face is one of those listed in Table 3-15 (and pictured in Figure 3-6),
and it may optionally be combined (DEMO Boolean OR) with CV_FONT_ITALIC.
Table 3-15. Available fonts (all are variations of Hershey)
Identifier
CV_FONT_HERSHEY_SIMPLEX
CV_FONT_HERSHEY_PLAIN
CV_FONT_HERSHEY_DUPLEX
CV_FONT_HERSHEY_COMPLEX
CV_FONT_HERSHEY_TRIPLEX
CV_FONT_HERSHEY_COMPLEX_SMALL
CV_FONT_HERSHEY_SCRIPT_SIMPLEX
CV_FONT_HERSHEY_SCRIPT_COMPLEX
DEMO
Normal size sanserif
Small size sanserif
Normal size sanserif, more complex DEMO
CV_FONT_HERSHEY_SIMPLEX
Normal size serif, more complex than
CV_FONT_HERSHEY_DUPLEX
Normal size serif, more complex than
CV_FONT_HERSHEY_COMPLEX
Smaller version of
CV_FONT_HERSHEY_COMPLEX
Handwriting style
More complex DEMO of
CV_FONT_HERSHEY_SCRIPT_SIMPLEX
Drawing Things
| 81
ere is always some little thing that makes our job a bit more complicated than we’d
e argument
DEMO   81
9/15/08   4:18:49 PM
www.it-ebooks.info
Figure 3-6. Th e eight fonts of Table 3-15 drawn with DEMO = vscale = 1.0, with the origin of each
line separated DEMO the vertical by 30 pixels
Both hscale and vscale can be set to either 1.0 or 0.5 only. Th is causes the font to DEMO ren-
dered at full or half height (and width) relative to the basic defi nition of the particular
font.
Th
slanted. It can DEMO set as large as 1.0, which sets the slope of the DEMO to approxi-
mately 45 degrees.
Both thickness and line_type are the same as defi ned for all the other drawing
functions.
Data Persistence
OpenCV DEMO a mechanism for serializing and de-serializing its various data types
to and from disk in either YAML or XML format. In the chapter on DEMO, which ad-
dresses user interface functions, we will cover specifi c functions that store and recall our
most common object: IplImages (these DEMO are cvSaveImage() and cvLoadImage()).
82 | Chapter 3: DEMO to Know OpenCV
e shear function creates an italicized slant to the font; if set to 0.0, the font is not
03-R4886-RC1.indd   DEMO
9/15/08   4:18:50 PM
www.it-ebooks.info
In addition, the HighGUI chapter will discuss read and write functions specifi c to mov-
ies: cvGrabFrame(), which reads from fi DEMO or from camera; and cvCreateVideoWriter()
and cvWriteFrame(). In DEMO section, we will focus on general object persistence: reading
and writing matrices, OpenCV structures, and confi guration and log fi les.
First DEMO start with specifi c and convenient functions that save and load OpenCV ma-
trices. Th ese functions are cvSave() and cvLoad(). Suppose you had a 5-by-5 identity
matrix (0 everywhere except for 1s on the diagonal). Example 3-15 shows how to ac-
complish this.
Example DEMO Saving and loading a CvMat
CvMat A = cvMat( 5, 5, CV_32F, the_matrix_data );
cvSave( “my_matrix.xml”, &A );
. . .
// to load it then in some other program use …
CvMat* A1 = (CvMat*) cvLoad( “my_matrix.xml” );
Th
really need to know is that general data persistence in OpenCV consists of DEMO a
CvFileStorage structure, as in Example 3-16, that stores memory objects in a tree struc-
ture. You can create and fi ll this DEMO by reading from disk via cvOpenFileStorage()
with CV_STORAGE_READ, or you can create and open CvFileStorage via cvOpenFileStorage()
with CV_STORAGE_WRITE for writing and then fi ll it using the appropriate data persistence
functions. On DEMO, the data is stored in an XML or YAML format.
Example DEMO CvFileStorage structure; data is accessed by CxCore data persistence functions
typedef DEMO CvFileStorage
{
...       // hidden fields
} CvFileStorage;DEMO
Th CvFileStorage tree may consist of a hierarchical collection of
scalars, DEMO objects (matrices, sequences, and graphs) and/or user-defi ned objects.
Let’s say you have a confi guration or logging fi le. For DEMO, consider the case of a
movie confi guration fi le that DEMO us how many frames we want (10), what their size DEMO
(320 by 240) and a 3-by-3 color conversion matrix that should be applied. We want to
call the fi le “cfg.xml” on disk. DEMO 3-17 shows how to do this.
Example 3-17. Writing a confi
guration fi
le “cfg.xml” to disk
CvFileStorage* fs = cvOpenFileStorage(
“cfg.xml”,
DEMO,
CV_STORAGE_WRITE
);
cvWriteInt( fs, “frame_count”, 10 );
DEMO( fs, “frame_size”, CV_NODE_SEQ );
cvWriteInt( fs, 0, 320 );
cvWriteInt( fs, 0, 200 );
e internal data inside the
Data Persistence
| 83
e CxCore reference manual contains an DEMO section on data persistence. What you
03-R4886-RC1.indd   83
9/15/08   4:18:50 PM
www.it-ebooks.info
Example 3-17. Writing a confi
guration fi
le “cfg.xml” to disk (continued)
cvEndWriteStruct(fs);
cvWrite( fs, “color_cvt_matrix”, cmatrix );DEMO
cvReleaseFileStorage( &fs );
Note some of the key functions in this example. We can give a name to integers that
we write DEMO the structure using cvWriteInt(). We can create an arbitrary structure, us-
ing cvStartWriteStruct(), which is also given an optional name (DEMO a 0 or NULL if
there is no name). Th is structure has two ints that have no name and so we pass DEMO 0
for them in the name fi eld, aft er which DEMO use cvEndWriteStruct() to end the writing of
that structure. If there were more structures, we’d Start and End each of them similarly;
the structures may be nested to arbitrary depth. We then use cvWrite() to write out the
color conversion matrix. Contrast this fairly complex matrix write procedure with the
simpler cvSave() in Example 3-15. Th e DEMO() function is just a convenient shortcut
for cvWrite() when you have only one matrix to write. When we are fi nished writing DEMO
data, the CvFileStorage handle is released in cvReleaseFileStorage(). Th e output (here,
in XML form) would look like Example 3-18.
DEMO 3-18. XML version of cfg.xml on disk
<?xml version=“1.0”?>
<opencv_storage>
<frame_count>10</frame_count>
<frame_size>320 200</frame_size>DEMO
<color_cvt_matrix type_id=“opencv-matrix”>
<rows>3</rows> <cols>3</cols>
<dt>f</dt>
<data>…</data></color_cvt_matrix>DEMO
</opencv_storage>
We may then read this confi guration fi le as shown in Example 3-19.
Example 3-19. Reading cfg.xml from disk
CvFileStorage* DEMO = cvOpenFileStorage(
“cfg.xml”,
0,
CV_STORAGE_READ
);
int frame_count = cvReadIntByName(
fs,
0,
“frame_count”,
5 /* default value */
);
CvSeq* s = cvGetFileNodeByName(fs,0,“frame_size”)DEMO>data.seq;
int frame_width = cvReadInt(
(CvFileNode*)cvGetSeqElem(s,0)DEMO
);
84 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   84
9/15/08   4:18:50 PM
www.it-ebooks.info
Example 3-19. Reading cfg.xml from disk (continued)
int frame_height = cvReadInt(
(CvFileNode*)cvGetSeqElem(s,1)
);
CvMat* color_cvt_matrix = (CvMat*) cvReadByName(
fs,
0,
“color_cvt_matrix”
);
cvReleaseFileStorage( &fs );
When reading, we open the XML confi guration DEMO le with cvOpenFileStorage() as in Ex-
ample 3-19. We then read the frame_count using cvReadIntByName(), which allows for a
default value to be given if no number is read. In this case the default DEMO 5. We then get
the structure that we named “frame_size” using cvGetFileNodeByName(). From here, we
read our two unnamed integers using cvReadInt(). Next we read our named color con-
version matrix using cvReadByName().* Again, contrast this with the short form cvLoad()
in DEMO 3-15. We can use cvLoad() if we only have one matrix to read, but we must
use cvRead() if the matrix is embedded within a larger structure. Finally, we release the
CvFileStorage structure.
Th CvFileStorage struc-
ture is shown in Table 3-16. See the CxCore manual DEMO more details.
Table 3-16. Data persistence functions
Function
Open and Release
cvOpenFileStorage
cvReleaseFileStorage
Writing
cvStartWriteStruct
cvEndWriteStruct
cvWriteInt
cvWriteReal
cvWriteString
cvWriteComment
cvWrite
cvWriteRawData
cvWriteFileNode
Description
DEMO fi le storage for reading or writing
Releases data storage
Starts writing a new structure
Ends writing a structure
Writes integer
Writes fl oat
DEMO text string
Writes an XML or YAML comment string
Writes an object such as a CvMat
Writes multiple numbers
Writes fi le node to DEMO fi le storage
* One could also use cvRead() to read in the matrix, but it can only be called aft er the appropriate CvFile-
Node{} is located, e.g., using cvGetFileNodeByName().
Data Persistence
| 85
e list of relevant data persistence functions associated with the
DEMO   85
9/15/08   4:18:50 PM
www.it-ebooks.info
Table 3-16. Data persistence functions (continued)
Function Description
Reading
Gets the top-level nodes of the fi le storage
Finds node in the DEMO or fi le storage
Returns a unique pointer for given name
Finds node in the map or fi le storage
Returns name of fi DEMO node
Reads unnamed int
Reads named int
Reads unnamed fl oat
Reads named fl oat
Retrieves text string from fi le node
Finds named DEMO le node and returns its value
Decodes object and returns pointer to it
Finds object and decodes it
Reads multiple numbers
Initializes fi le DEMO sequence reader
Reads data from sequence reader above
cvGetRootFileNode
cvGetFileNodeByName
cvGetHashedKey
cvGetFileNode
cvGetFileNodeName
cvReadInt
cvReadIntByName
cvReadReal
cvReadRealByName
cvReadString
cvReadStringByName
cvRead
cvReadByName
cvReadRawData
cvStartReadRawData
cvReadRawDataSlice
DEMO Performance Primitives
Intel has a product called the Integrated Performance Primitives (DEMO) library (IPP).
Th is library is essentially a toolbox of high-performance kernels for handling multime-
dia and other processor-intensive operations in a DEMO that makes extensive use of
the detailed architecture of their processors (DEMO, to a lesser degree, other manufactur-
ers’ processors that have a similar architecture).
As discussed in Chapter 1, OpenCV enjoys a close relationship with IPP, both at a soft -
ware level and at an organizational level inside of the company. As a result, OpenCV is
designed to automatically* recognize the presence of the IPP library and to DEMO
cally “swap out” the lower-performance implementations of many core functionalities
for their higher-performance counterparts in IPP. Th e IPP library allows OpenCV to
take DEMO of performance opportunities that arrive from SIMD instructions in a
single processor as well as from modern multicore architectures.
With these basics in hand, we can perform a wide variety of basic tasks. Moving on-
ward DEMO the text, we will look at many more sophisticated capabilities of DEMO,
* Th e one prerequisite to this automatic recognition is that the binary directory of IPP must be in the system
path. So DEMO a Windows system, for example, if you have IPP in C:/Program Files/Intel/IPP then you want to
ensure that C:/DEMO Files/Intel/IPP/bin is in your system path.
86 | Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   86
9/15/08   4:18:51 PM
www.it-ebooks.info
almost all of which are built on these routines. It should DEMO no surprise that image
processing—which oft en requires doing the same thing to a whole lot of data, much of
which is completely parallel—would realize a great benefi t from any code that allows it
to DEMO advantage of parallel execution units of any form (MMX, SSE, DEMO, etc.).
Verifying Installation
Th
function cvGetModuleInfo(), shown in Example 3-20. Th is function will identify both
the version of OpenCV you DEMO currently running and the version and identity of any
add-in modules.
Example 3-20. Using cvGetModuleInfo() to check for IPP
char* libraries;
char* DEMO;
cvGetModuleInfo( 0, &libraries, &modules );
printf(“Libraries: %s/nModules: %s/n”, libraries, modules );
Th
ies and modules. Th e output might look like this:
Libraries cxcore: 1.0.0
Modules: ippcv20.dll, ippi20.dll, ipps20.dll, ippvm20.dll
Th e modules listed in DEMO output are the IPP modules used by OpenCV. Th ose modules
are themselves actually proxies for even lower-level CPU-specifi c libraries. Th e details
DEMO how it all works are well beyond the scope of this book, but if you see the IPP libraries
in the Modules string then you can be pretty confi dent that everything is working as ex-
DEMO Of course, you could use this information to verify that IPP DEMO running correctly
on your own system. You might also use it to check for IPP on a machine on which your
fi nished soft DEMO is installed, perhaps then making some dynamic adjustments depend-
ing on DEMO IPP is available.
e code in Example 3-20 will generate text strings which describe the installed librar-
e way to check and make sure DEMO IPP is installed and working correctly is with the
Summary
In this chapter we introduced some basic data structures that we will oft en DEMO
In particular, we met the OpenCV matrix structure and the all-important DEMO im-
age structure, IplImage. We considered both in some detail and DEMO that the matrix
and image structures are very similar: the functions DEMO for primitive manipulations
in one work equally well in the other.
Exercises
In the following exercises, you may need to refer to the CxCore manual that ships with
OpenCV or to the OpenCV Wiki on the DEMO for details of the functions outlined in
this chapter.
1. Find and open .../opencv/cxcore/include/cxtypes.h. Read through and fi nd the DEMO
conversion helper functions.
Exercises
| 87
03-R4886-RC1.indd   87
9/15/08   4:18:51 PM
www.it-ebooks.info
a. Choose a negative fl oating-point number. Take its absolute value, round it, and
then take its ceiling and fl oor.
b. Generate some random numbers.
c. Create a fl oating point CvPoint2D32f and convert DEMO to an integer CvPoint.
d. Convert a CvPoint to a CvPoint2D32f.
Th is exercise will accustom you to the idea of many functions taking DEMO types.
Create a two-dimensional matrix with three channels of type byte with data size
100-by-100. Set all the values to 0.
a. Draw a DEMO in the matrix using void cvCircle( CvArr* img, CvPoint center,
intradius, CvScalar color, int thickness=1, int line_type=8, int shift=0 ).
b. Display this image using methods described in Chapter 2.
Create a DEMO matrix with three channels of type byte with data
size 100-by-100, DEMO set all the values to 0. Use the pointer element access function
cvPtr2D to point to the middle (“green”) channel. Draw a green DEMO between
(20, 5) and (40, 20).
4. Create DEMO three-channel RGB image of size 100-by-100. Clear it. Use pointer arith-
metic to draw a green square between (20, 5) and (40, 20).
Practice using region of interest (ROI). Create a 210-by-210 single-channel byte im-
age and zero it. Within the image, build a pyramid of increasing values using ROI
and cvSet(). Th at is: the outer border should be 0, the next inner border should DEMO
20, the next inner border should be 40, and so on until the fi nal innermost square is
set to value 200; all borders should be 10 pixels wide. Display the image.
Use multiple image DEMO for one image. Load an image that is at least 100-by-100.
Create two additional image headers and set their origin, depth, number of DEMO
nels, and widthstep to be the same as the loaded image. DEMO the new image headers,
set the width at 20 and the height at 30. Finally, set their imageData pointers to point
to the pixel at (5, 10) and (50, 60), respectively. Pass these new image subheaders
to cvNot(). Display the loaded image, DEMO should have two inverted rectangles
within the larger image.
Create a mask using cvCmp(). Load a real image. Use cvSplit() to split the image
into red, green, and blue images.
a. Find and DEMO the green image.
b. Clone this green plane image twice (call DEMO clone1 and clone2).
c. Find the green plane’s minimum and maximum value.
d. Set clone1’s values to thresh = (unsigned char)((maximum - minimum)/2.0).
e. Set clone2 to 0 and use cvCmp(green_image, clone1, clone2, CV_CMP_GE). Now
clone2 will have a mask of where the value exceeds thresh in the green image.
2.
3.
DEMO
6.
7.
88
| Chapter 3: Getting to Know OpenCV
03-R4886-RC1.indd   88
9/15/08   4:18:51 PM
f. Finally, use cvSubS(green_image,thresh/2, green_image, clone2) and DEMO the
results.
8. Create a structure of an integer, a CvPoint DEMO a CvRect; call it “my_struct”.
a. Write two functions: void write_my_struct( CvFileStorage * fs, const char *
name, my_struct *ms) and DEMO read_my_struct( CvFileStorage* fs, CvFileNode*
ms_node, my_struct* ms ). Use them to write and read my_struct.
b. Write and read an array of DEMO my_struct structures.
Exercises
| 89
03-R4886-RC1.indd   89
www.it-ebooks.info
9/15/08   4:18:51 PM
CHAPTER 4
HighGUI
A Portable Graphics Toolkit
Th le sys-
tem, and hardware such as cameras are collected into a library called HighGUI (which
stands for “high-level graphical user interface”). HighGUI allows us to open DEMO,
to display images, to read and write graphics-related fi les (both images and video), and
to handle simple mouse, pointer, DEMO keyboard events. We can also use it to create other
useful doodads like sliders and then add them to our windows. If you are DEMO GUI guru in
your window environment of choice, then you might DEMO nd that much of what HighGUI
off ers is redundant. Yet even so you might fi nd that the benefi t of cross-platform porta-
DEMO is itself a tempting morsel.
From our initial perspective, the HighGUI DEMO in OpenCV can be divided into three
parts: the hardware part, the fi le system part, and the GUI part.* We will take a moment
to overview what is in each part before we really DEMO in.
Th
ating systems, interaction with a camera is a tedious DEMO painful task. HighGUI allows
an easy way to query a camera and retrieve the latest image from the camera. It hides all
of the DEMO stuff , and that keeps us happy.
Th le system part is concerned primarily with loading and saving images. One nice
feature of the DEMO is that it allows us to read video using the same methods we would
use to read a camera. We can therefore abstract ourselves DEMO from the particular de-
vice we’re using and get on with writing interesting code. In a similar spirit, HighGUI
provides us with a (DEMO) universal pair of functions to load and save still images.
Th DEMO extension and automatically handle all of
the decoding or encoding that is necessary.
* Under the hood, the architectural organization is a bit diff erent from what we described, but the breakdown
into hardware, fi DEMO system, and GUI is an easier way to organize things conceptually. DEMO e actual HighGUI
functions are divided into “video IO”, “image IO”, and “GUI tools”. Th ese categories are represented by the
cvcap*, grfmt*, and window* source fi les, respectively.
90
e OpenCV functions that DEMO us to interact with the operating system, the fi
e hardware DEMO is primarily concerned with the operation of cameras. In most oper-
e fi
ese functions simply rely on the fi
04-R4886-RC1.indd   90
www.it-ebooks.info
DEMO/15/08   4:19:24 PM
www.it-ebooks.info
Th e library provides some
simple functions that will allow us DEMO open a window and throw an image into that
window. It also allows us to register and respond to mouse and keyboard events on DEMO
window. Th ese features are most useful when trying to get off  of the ground with a sim-
ple application. Tossing in some slider bars, which we can also use as switches,* we fi nd
ourselves able to prototype a surprising variety of applications using only the DEMO
library.
As we proceed in this chapter, we will not treat DEMO three segments separately; rather,
we will start with some functions DEMO highest immediate utility and work our way to the
subtler points thereaft er. In this way you will learn what you need to get DEMO as soon
as possible.
e third part of HighGUI is the window system (or GUI). Th
Creating a Window
First, we want DEMO show an image on the screen using HighGUI. Th e function that does
this for us is cvNamedWindow(). Th e function expects a name for the new window and one
fl ag. Th e name DEMO at the top of the window, and the name is also DEMO as a handle
for the window that can be passed to other HighGUI functions. Th e fl ag indicates if the
window should autosize DEMO to fi t an image we put into it. Here is the full prototype:
int cvNamedWindow(
const char* name,
int         flags = CV_WINDOW_AUTOSIZE
);
Notice the parameter flags. For now, the only valid options available are to set flags
to 0 or to use the default setting, CV_WINDOW_AUTOSIZE. If CV_WINDOW_AUTOSIZE is set, DEMO
HighGUI resizes the window to fi t the image. Th ereaft er, the window will automatically
resize itself if a new image is loaded into the window but cannot be resized by the user.
If you DEMO want autosizing, you can set this argument to 0; then users can resize the
window as they wish.
Once we create a window, we usually want to put something into it. But before we do
DEMO, let’s see how to get rid of the window when it DEMO no longer needed. For this we use
cvDestroyWindow(), a function DEMO argument is a string: the name given to the win-
dow DEMO it was created. In OpenCV, windows are referenced by name instead DEMO by
some unfriendly (and invariably OS-dependent) “handle”. Conversion between handles
and names happens under the hood of HighGUI, so you needn’t worry about it.
Having said that, some people do worry about it, and DEMO OK, too. For those people,
HighGUI provides the following functions:DEMO
void*       cvGetWindowHandle( const char* name );
const DEMO cvGetWindowName( void* window_handle );
* OpenCV HighGUI does not provide DEMO like a button. Th e common trick is to use a two-position
slider to achieve this functionality (more on this later).
Creating a Window
| 91
04-R4886-RC1.indd   91
9/15/08   4:19:DEMO PM
www.it-ebooks.info
Th ese functions allow us to convert back and forth between DEMO human-readable names
preferred by OpenCV and the “handle” style of reference used by diff erent window
systems.*
To resize a window, call (not DEMO) cvResizeWindow():
void cvResizeWindow(
const char* name,
int         width,
int         height
);
Here the width and height are in pixels and give the DEMO of the drawable part of the win-
dow (which are probably DEMO dimensions you actually care about).
Loading an Image
Before we can display an image in our window, we’ll need to know how to load an image
from disk. Th e function for this is cvLoadImage():
IplImage* cvLoadImage(
const char* filename,
int         iscolor = CV_LOAD_IMAGE_COLOR
);
When opening an image, cvLoadImage() does not look at the fi le extension. Instead,
cvLoadImage() DEMO the fi rst few bytes of the fi le (aka its DEMO or “magic sequence”)
and determines the appropriate codec using that. Th e second argument iscolor can be
set to one of several values. DEMO default, images are loaded as three-channel images with
8 bits per DEMO; the optional fl ag CV_LOAD_IMAGE_ANYDEPTH can be added to allow load-
DEMO of non-8-bit images. By default, the number of channels will be DEMO because the
iscolor fl ag has the default value of CV_LOAD_IMAGE_COLOR. Th is means that, regardless
of the number of channels in the image fi le, the image will be converted to three chan-
nels if needed. Th e alternatives to CV_LOAD_IMAGE_COLOR are CV_LOAD_IMAGE_GRAYSCALE and
CV_LOAD_IMAGE_ANYCOLOR. Just as DEMO forces any image into a three-channel
image, CV_LOAD_IMAGE_GRAYSCALE automatically converts any DEMO into a single-channel
image. CV_LOAD_IMAGE_ANYCOLOR will simply load the image as it is stored in the fi le. Th us, to
load a 16-bit color image you would use CV_LOAD_IMAGE_COLOR | CV_LOAD_IMAGE_ANYDEPTH.
If you want both DEMO color and depth to be loaded exactly “as is”, you could DEMO use
the all-purpose fl ag CV_LOAD_IMAGE_UNCHANGED. Note that cvLoadImage() does not signal a
runtime error when it fails to load an image; it simply returns a null pointer.
Th cvLoadImage() is cvSaveImage(), which takes
two arguments:
int cvSaveImage(
const char*  filename,
const CvArr* image
);
* For those who know what this means: the window handle returned is a HWND on Win32 systems, a Carbon
WindowRef on Mac OS X, and a Widget* pointer on systems (DEMO, GtkWidget) of X Window type.
92 | Chapter 4: HighGUI
DEMO obvious complementary function to
04-R4886-RC1.indd   92
9/15/08   4:19:24 PM
www.it-ebooks.info
Th rst argument gives the fi lename, whose extension is used to determine the format
in which the fi le will be stored. DEMO e second argument is the name of the image to be
stored. Recall that CvArr is kind of a C-style way of creating something DEMO to
a base-class in an object-oriented language; wherever you see CvArr*, you can use an
IplImage*. Th e cvSaveImage() function will store DEMO 8-bit single- or three-channel im-
ages for most fi le formats. Newer back ends for fl exible image formats like PNG, TIFF
or JPEG2000 allow storing 16-bit or even fl oat formats and some allow four-channel
DEMO (BGR plus alpha) as well. Th e return value will be 1 if the save was successful and
should be 0 if the DEMO was not.*
e fi
Displaying Images
Now we are ready for what we really want to do, and that is to load an image and to put
it into the window where we can view it DEMO appreciate its profundity. We do this via
one simple function, cvShowImage():
void cvShowImage(
const char*  name,
const CvArr* image
);
Th rst argument here is the name of the window within which we intend to draw. Th e
second argument is the image DEMO be drawn.
Let’s now put together a simple program that will display an image on the screen. We can
read a fi lename from DEMO command line, create a window, and put our image in the win-
dow in 25 lines, including comments and tidily cleaning up our memory allocations!
int main(int argc, char** argv)
{
// DEMO a named window with the name of the file.
cvNamedWindow( argv[1], 1 );
// Load the image from the given file name.
IplImage* img = cvLoadImage( argv[1] );
// Show the image DEMO the named window
cvShowImage( argv[1], img );
// Idle DEMO the user hits the “Esc” key.
while( 1 ) {
if( cvWaitKey( 100 ) == 27 ) break;
}
// Clean DEMO and don’t be piggies
cvDestroyWindow( argv[1] );
cvReleaseImage( &img );
* Th e reason we say “should” is that, in DEMO OS environments, it is possible to issue save commands that
will DEMO cause the operating system to throw an exception. Normally, however, a zero value will be
returned to indicate failure.
Displaying Images
| 93
DEMO fi
04-R4886-RC1.indd   93
9/15/08   4:19:25 PM
exit(0);
}
For convenience we have used the fi lename as the window name. Th is is nice because
OpenCV automatically puts DEMO window name at the top of the window, so we can DEMO
which fi le we are viewing (see Figure 4-1). Easy DEMO cake.
Figure 4-1. A simple image displayed with cvShowImage()
Before DEMO move on, there are a few other window-related functions you ought DEMO know
about. Th ey are:
void cvMoveWindow( const char* name, int x, int y );
void cvDestroyAllWindows( void );
DEMO  cvStartWindowThread( void );
cvMoveWindow() simply moves a window on the screen so that its upper left  corner is
positioned at x,y.
cvDestroyAllWindows() is a useful cleanup function that closes all of DEMO windows and
de-allocates the associated memory.
On Linux and MacOS, cvStartWindowThread() tries to start a thread that updates the
window automatically and handles resizing and so forth. A return value of 0 indicates
that no DEMO could be started—for example, because there is no support for this DEMO
in the version of OpenCV that you are using. Note that, DEMO you do not start a separate win-
dow thread, OpenCV can DEMO to user interface actions only when it is explicitly given
time to do so (this happens when your program invokes cvWaitKey(), as DEMO next).
94
| Chapter 4: HighGUI
04-R4886-RC1.indd   94
www.it-ebooks.info
DEMO/15/08   4:19:25 PM
www.it-ebooks.info
WaitKey
Observe that inside the while loop in our window creation DEMO there is a new func-
tion we have not seen before: DEMO(). Th is function causes OpenCV to wait for a
specifi DEMO number of milliseconds for a user keystroke. If the key is pressed within the
allotted time, the function returns the key pressed;* otherwise, it returns 0. With the
construction:
while( 1 ) {
DEMO( cvWaitKey(100)==27 ) break;
}
we tell OpenCV to DEMO 100 ms for a key stroke. If there is no keystroke, DEMO repeat ad
infi nitum. If there is a keystroke and it happens to have ASCII value 27 (the Escape key),
then break DEMO of that loop. Th is allows our user to leisurely peruse the image before
ultimately exiting the program by hitting Escape.
As long as DEMO introducing cvWaitKey(), it is worth mentioning that cvWaitKey() can
DEMO be called with 0 as an argument. In this case, cvWaitKey() will wait indefi nitely until
a keystroke is received and then return that key. Th us, in our example we could just as
easily have used cvWaitKey(0). Th e diff erence between these two DEMO would be more
apparent if we were displaying a video, in DEMO case we would want to take an action
(i.e., display the next frame) if the user supplied no keystroke.
Mouse Events
Now that we can display an image to a user, we might also want to allow the user to in-
teract with the image we have DEMO Since we are working in a window environment
and since we already learned how to capture single keystrokes with cvWaitKey(), the next
logical thing to consider is how to “listen to” and respond to mouse DEMO
Unlike keyboard events, mouse events are handled by a more typical DEMO mecha-
nism. Th is means that, to enable response to mouse DEMO, we must fi rst write a callback
routine that OpenCV can DEMO whenever a mouse event occurs. Once we have done that,
we must register the callback with OpenCV, thereby informing OpenCV that this is the
correct function to use whenever the user does something with the DEMO over a par-
ticular window.
Let’s start with the callback. For those of you who are a little rusty on your event-driven
program lingo, the callback can be any function that takes the correct set of DEMO
and returns the correct type. Here, we must be able to DEMO the function to be used as a
* Th e careful reader might legitimately ask exactly what this means. Th e short answer is DEMO ASCII value”, but
the long answer depends on the operating system. DEMO Win32 environments, cvWaitKey() is actually waiting
for a message of DEMO WM_CHAR and, aft er receiving that message, returns the wParam fi eld from the message
(wParam is not actually type char at all!). On Unix-like systems, cvWaitKey() is using GTK; the return DEMO
is (event->keyval | (event->state<<16)), where event is a GdkEventKey structure. Again, this is not
really a char. Th at state information is essentially the state of the Shift , Control, etc. keys at the time of the
key press. Th is means DEMO, if you are expecting (say) a capital Q, then you should either cast the return of
cvWaitKey() to type char or DEMO with 0xff, because the shift  key will appear in the upper bits (e.g., Shift -
Q will return 0x10051).
Displaying Images DEMO 95
04-R4886-RC1.indd   95
9/15/08   4:19:25 PM
www.it-ebooks.info
callback exactly what kind of event occurred and where it occurred. DEMO e function must
also be told if the user was pressing such keys as Shift  or Alt when the mouse event oc-
curred. Here is the exact prototype that your callback function must match:
void DEMO(
int   event,
int   x,
int   y,
int   flags,
void* param
);
Now, whenever your function is called, OpenCV will fi ll in the arguments with their ap-
propriate values. Th e fi rst argument, called the event, DEMO have one of the values shown
in Table 4-1.
Table 4-1. Mouse event types
Event Numerical value
CV_EVENT_MOUSEMOVE 0
CV_EVENT_LBUTTONDOWN 1
CV_EVENT_RBUTTONDOWN 2
CV_EVENT_MBUTTONDOWN DEMO
CV_EVENT_LBUTTONUP 4
CV_EVENT_RBUTTONUP 5
CV_EVENT_MBUTTONUP 6
CV_EVENT_LBUTTONDBLCLK 7
CV_EVENT_RBUTTONDBLCLK 8
CV_EVENT_MBUTTONDBLCLK 9
Th
event. It is worth noting that these coordinates represent the pixel DEMO the image indepen-
dent of the size of the window (in DEMO, this is not the same as the pixel coordinates
of the DEMO).
Th
conditions present at the time of the event. For example, CV_EVENT_FLAG_SHIFTKEY has a
numerical value of 16 (i.e., the fi ft h bit) and so, if we wanted to test whether the DEMO  key
were down, we could AND the fl ags variable with the bit mask (1<<4). Table 4-2 shows a
complete DEMO of the fl ags.
Table 4-2. Mouse event fl ags
Flag Numerical value
CV_EVENT_FLAG_LBUTTON 1
CV_EVENT_FLAG_RBUTTON 2
CV_EVENT_FLAG_MBUTTON 4
96 | Chapter 4: HighGUI
e second and third arguments will be set to the x and DEMO coordinates of the mouse
e fourth argument, called flags, is a bit fi
eld in which individual bits indicate special
04-R4886-RC1.indd   96
DEMO/15/08   4:19:25 PM
www.it-ebooks.info
Table 4-2. Mouse event fl
ags (continued)
Flag Numerical value
CV_EVENT_FLAG_CTRLKEY 8
CV_EVENT_FLAG_SHIFTKEY 16
CV_EVENT_FLAG_ALTKEY 32
Th nal argument is a void DEMO that can be used to have OpenCV pass in any ad-
ditional information in the form of a pointer to whatever kind of structure DEMO need.
A common situation in which you will want to use the param argument is when the
callback itself is a static member function DEMO a class. In this case, you will probably fi nd
yourself DEMO to pass the this pointer and so indicate which class object instance the
callback is intended to aff ect.
Next we need the function DEMO registers the callback. Th at function is called
cvSetMouseCallback(), and DEMO requires three arguments.
void cvSetMouseCallback(
const char*     window_name,
CvMouseCallback on_mouse,
void*           param      = NULL
);
Th rst argument is the name of the DEMO to which the callback will be attached.
Only events in that particular window will trigger this specifi c callback. Th e second ar-
gument DEMO your callback function. Finally, the third param argument allows us to DEMO
the param information that should be given to the callback whenever it is executed. Th is
is, of course, the same param we DEMO just discussing in regard to the callback prototype.
In Example 4-1 we write a small program to draw boxes on the screen with the DEMO
Th
the event to determine what to do when it is called.
Example 4-1. Toy program for using a mouse to draw boxes on DEMO screen
// An example program in which the
// user can draw boxes on the screen.
//
#include <cv.h>
#include <DEMO>
// Define our callback which we will install for
// mouse events.
//
void my_mouse_callback(
int event, int x, DEMO y, int flags, void* param
);
CvRect box;
bool drawing_box = false;
// A litte subroutine to draw a box onto an image
Displaying Images
| 97
e fi
e fi
e DEMO my_mouse_callback() is installed to respond to mouse events, and it DEMO
04-R4886-RC1.indd   97
9/15/08   4:19:26 PM
www.it-ebooks.info
Example 4-1. Toy program for using a mouse to draw boxes DEMO the screen (continued)
//
void draw_box( IplImage* img, DEMO rect ) {
cvRectangle (
img,
cvPoint(box.x,box.y),
cvPoint(box.x+box.width,box.y+box.height),
cvScalar(0xff,0x00,0x00)    /* DEMO */
);
}
int main( int argc, char* argv[] ) {
box = cvRect(-1,-1,0,0);
IplImage* image = cvCreateImage(
cvSize(200,200),
IPL_DEPTH_8U,
3
);
cvZero( image );
IplImage* temp = cvCloneImage( image );
DEMO( “Box Example” );
// Here is the crucial moment that we actually install
// the callback. Note that we set the value ‘param’ to
// be the image we are working with so that the callback
// will have the image to edit.
//
cvSetMouseCallback(
“Box Example”,
my_mouse_callback,
(void*) image
);
// The main program loop. Here we copy the working image
// to the ‘temp’ image, and if the user is drawing, then
// put the currently contemplated box onto that temp image.
// display the temp image, and wait 15ms for a keystroke,
// then DEMO
//
while( 1 ) {
cvCopyImage( image, temp );DEMO
if( drawing_box ) draw_box( temp, box );
cvShowImage( “Box Example”, temp );
if( cvWaitKey( 15 )==27 ) break;DEMO
}
// Be tidy
//
cvReleaseImage( &image );
DEMO | Chapter 4: HighGUI
04-R4886-RC1.indd   98
9/15/08   DEMO:19:26 PM
www.it-ebooks.info
Example 4-1. Toy program for using a mouse to draw boxes DEMO the screen (continued)
cvReleaseImage( &temp );
cvDestroyWindow( “Box Example” );
}
// This is our mouse callback. If the user
// presses the left button, we start a box.
// when the user releases that button, then we
// add the DEMO to the current image. When the
// mouse is dragged (with the button down) we
// resize the box.
//
void DEMO(
int event, int x, int y, int flags, void* param
) {
IplImage* image = (IplImage*) param;
switch( event ) {
case CV_EVENT_MOUSEMOVE: {
if( drawing_box ) {
box.width  = x-box.x;
box.height = y-box.y;
}
}
break;
case CV_EVENT_LBUTTONDOWN: {
drawing_box = true;
box = cvRect(x, y, 0, 0);
}
break;
case CV_EVENT_LBUTTONUP: {
drawing_box = false;DEMO
if(box.width<0) {
box.x+=box.width;
box.width *=-1;
}
if(DEMO<0) {
box.y+=box.height;
box.height*=-1;
}
draw_box(image, box);DEMO
}
break;
}
}
Sliders, Trackbars, and Switches
HighGUI provides a convenient slider element. In HighGUI, sliders are called trackbars.
Th is is because their original (historical) intent was for selecting a particular DEMO
in the playback of a video. Of course, once added to DEMO, people began to use
Displaying Images
| 99
04-R4886-RC1.indd   99
DEMO/15/08   4:19:26 PM
www.it-ebooks.info
trackbars for all of the usual things one might do with DEMO slider as well as many unusual
ones (see the next section, “No Buttons”)!
As with the parent window, the slider is given a unique name (in the form of a character
string) and DEMO thereaft er always referred to by that name. Th e HighGUI routine for cre-
ating a trackbar is:
int cvCreateTrackbar(
const char*        trackbar_name,
const char*        window_name,
int*               value,
int                count,
CvTrackbarCallback on_change
);
DEMO rst two arguments are the name for the trackbar itself and the name of the parent
window to which the trackbar will be attached. DEMO the trackbar is created it is added
to either the top or the bottom of the parent window;* it will not occlude any DEMO that
is already in the window.
Th
to the value to which the slider has been moved, and count, a numerical value for DEMO
maximum value of the slider.
Th e last argument is a pointer to a callback function that will be automatically called
whenever the slider DEMO moved. Th is is exactly analogous to the callback for mouse events. If
used, the callback function must have the form CvTrackbarCallback, which DEMO defi ned as:
void (*callback)( int position )
Th is callback is not actually required, so if you don’t want a callback then you can sim-
ply set this value to NULL. Without DEMO callback, the only eff ect of the user moving the slider
DEMO be the value of *value being changed.
Finally, here are two DEMO routines that will allow you to programmatically set or read
the value of a trackbar if you know its name:
int cvGetTrackbarPos(
DEMO char* trackbar_name,
const char* window_name
);
void cvSetTrackbarPos(
const char* trackbar_name,
const char* window_name,
int         DEMO
);
Th
program.
ese functions allow you to set or read the value of a trackbar from anywhere in your
* Whether it DEMO added to the top or bottom depends on the operating system, DEMO it will always appear in the
same place on any given platform.
100
| Chapter 4: HighGUI
e fi
e next two arguments are value, a pointer to an integer that will be set automatically
04-R4886-RC1.indd   100
9/15/08   4:19:26 PM
www.it-ebooks.info
No Buttons
Unfortunately, HighGUI does not provide any explicit support for buttons. It is thus
common practice, among the particularly lazy,* to instead use sliders with only two
positions. Another option that occurs oft DEMO in the OpenCV samples in …/opencv/
samples/c/ is DEMO use keyboard shortcuts instead of buttons (see, e.g., the fl DEMO ll demo in
the OpenCV source-code bundle).
Switches are just sliders (trackbars) that have only two positions, “on” (1) and “off ” (0)
(i.e., count has been set to 1). You can see how this is an easy way to obtain the DEMO
tionality of a button using only the available trackbar tools. Depending on exactly how
you want the switch to behave, you can use the trackbar callback to automatically reset
the button back to 0 (as in Example 4-2; this is something like the standard behavior of
most GUI “buttons”) or to automatically set other switches to 0 (which gives DEMO eff ect
of a “radio button”).
Example 4-2. Using a trackbar to create a “switch” that the user can turn on and off
// We make this value global so everyone can see it.
//DEMO
int g_switch_value = 0;
// This will be the callback DEMO we give to the
// trackbar.
//
void switch_callback( int position ) {
if( position == 0 ) {
switch_off_function();
} else {
switch_on_function();
}
}
int main( int argc, char* argv[] ) {
// Name the main window
//
DEMO( “Demo Window”, 1 );
// Create the trackbar. We DEMO it a name,
// and tell it the name of DEMO parent window.
//
cvCreateTrackbar(
“Switch”,
“Demo Window”,
&DEMO,
1,
* For the less lazy, another common practice DEMO to compose the image you are displaying with a “control
panel” you have drawn and then use the mouse event callback to test for DEMO mouse’s location when the
event occurs. When the (x, y) DEMO is within the area of a button you have drawn on your control panel,
the callback is set to perform the button action. DEMO this way, all “buttons” are internal to the mouse event
callback DEMO associated with the parent window.
Displaying Images
| 101
04-R4886-RC1.indd   101
9/15/08   4:19:27 PM
www.it-ebooks.info
Example 4-2. Using a trackbar to create a “switch” that the DEMO can turn on and off
(continued)
Switch_callback
);
// This will just cause OpenCV to idle until
// someone hits the “Escape” key.
//
while( 1 ) {
if( cvWaitKey(15)DEMO ) break;
}
}
You can see that this will turn on and off  just like a light switch. In our example,
whenever the trackbar “switch” is set to 0, the callback executes the function switch_off_
function(), and whenever it is switched on, the DEMO() is called.
Working with Video
When working with video we must consider several functions, including (of course)
how to read and DEMO video fi les. We must also think about how to actually play back
such fi les on the screen.
Th rst thing we need DEMO the CvCapture device. Th is structure contains the information
needed for reading frames from a camera or video fi le. Depending on the source, we use
one of two diff erent calls to create and initialize DEMO CvCapture structure.
CvCapture* cvCreateFileCapture( const char* filename );
CvCapture* cvCreateCameraCapture( int index );
In the case of cvCreateFileCapture(), we can simply give a fi lename for an MPG or AVI
fi le DEMO OpenCV will open the fi le and prepare to read it. If the open is successful and
we are able to start reading frames, a pointer to an initialized CvCapture structure will
be returned.
A lot DEMO people don’t always check these sorts of things, thinking that nothing DEMO go
wrong. Don’t do that here. Th e returned pointer will be NULL if for some reason the fi le
could not be opened (e.g., if the fi le does not exist), but cvCreateFileCapture() will also
return a NULL pointer if the codec with which the DEMO is compressed is not known.
Th
you will need to have the appropriate library already resident on your computer in or-
der to successfully DEMO the video fi le. For example, if you want to read DEMO fi le encoded
with DIVX or MPG4 compression on a Windows machine, there are specifi c DLLs that
provide the necessary resources to decode the video. Th is is why it is always important
to check DEMO return value of cvCreateFileCapture(), because even if it works on DEMO ma-
chine (where the needed DLL is available) it might not work on another machine (where
that codec DLL is missing). Once we have the CvCapture structure, we can begin reading
frames and do a number of other things. But before we get into that, let’s take a look at
how to capture images from a camera.
102
DEMO Chapter 4: HighGUI
e fi
e subtleties of compression codecs are DEMO the scope of this book, but in general
04-R4886-RC1.indd   102
DEMO/15/08   4:19:27 PM
www.it-ebooks.info
Th cvCreateCameraCapture() works very much like cvCreateFileCapture() ex-
cept DEMO the headache from the codecs.* In this case we give an identifi er that indi-
cates which camera we would like to access and DEMO we expect the operating system to
talk to that camera. For the former, this is just an identifi cation number that is zero (DEMO)
when we only have one camera, and increments upward when DEMO are multiple cam-
eras on the same system. Th e other part of the identifi er is called the domain of the
camera and DEMO (in essence) what type of camera we have. Th e domain can be any
of the predefi ned constants shown in Table 4-3.
DEMO 4-3. Camera “domain” indicates where HighGUI
should look for your camera
Camera capture constant Numerical value
CV_CAP_ANY 0
CV_CAP_MIL 100
CV_CAP_VFW 200
CV_CAP_V4L 200
DEMO 200
CV_CAP_FIREWIRE 300
CV_CAP_IEEE1394 300
CV_CAP_DC1394 300
CV_CAP_CMU1394 300
When we call cvCreateCameraCapture(), we pass in an identifi er that is just the sum of
the domain index and the camera index. For example:
DEMO capture = cvCreateCameraCapture( CV_CAP_FIREWIRE );
In this example, cvCreateCameraCapture() will attempt to open the fi rst (i.e., number-
zero) Firewire camera. In most cases, the domain is unnecessary when we have only one
camera; it is suffi  cient to use CV_CAP_ANY (which is conveniently equal to 0, so we don’t
even have to type that in). One last useful hint before we move on: you can pass -1 to
cvCreateCameraCapture(), which will cause OpenCV to open a window that allows you
to select the desired camera.
Reading Video
int       cvGrabFrame( CvCapture* capture );
IplImage* cvRetrieveFrame( CvCapture* capture );
IplImage* cvQueryFrame( CvCapture* capture );
Once you have a valid CvCapture object, you can start grabbing frames. Th ere are two
ways to do this. One way is to call cvGrabFrame(), which takes the CvCapture* pointer
and returns an integer. Th is integer will DEMO 1 if the grab was successful and 0 if the grab
* Of course, to be completely fair, we should probably confess that DEMO headache caused by diff erent codecs
has been replaced by the analogous headache of determining which cameras are (or are not) supported on
DEMO system.
Working with Video | 103
e routine
04-R4886-RC1.indd   103
9/15/08   4:19:27 PM
www.it-ebooks.info
failed. Th e cvGrabFrame() function copies the captured image to DEMO internal buff er that
is invisible to the user. Why would you want OpenCV to put the frame somewhere you
can’t access it? Th e answer is that this grabbed frame is unprocessed, and cvGrabFrame()DEMO
is designed simply to get it onto the computer as quickly as possible.
Once you have called cvGrabFrame(), you can then call cvRetrieveFrame(). Th is func-
tion will do any necessary processing on the frame (such as the decompression stage in
the codec) and then DEMO an IplImage* pointer that points to another internal buff er
(so DEMO not rely on this image, because it will be overwritten the DEMO time you call
cvGrabFrame()). If you want to do anything in particular with this image, copy it else-
where fi rst. Because this pointer points to a structure maintained by OpenCV itself, you
are not required to release the image and can expect trouble if you DEMO so.
Having said all that, there is a somewhat simpler method DEMO cvQueryFrame(). Th is
is, in eff ect, a combination DEMO cvGrabFrame() and cvRetrieveFrame(); it also returns the
same IplImage* DEMO as cvRetrieveFrame() did.
It should be noted that, with a DEMO fi le, the frame is automatically advanced when-
ever a cvGrabFrame() call is made. Hence a subsequent call will retrieve the next frame
automatically.
Once you are done with the CvCapture device, you can release it with a call to
cvReleaseCapture(). As with most other de-allocators in OpenCV, this routine takes a
pointer to the CvCapture* pointer:
void cvReleaseCapture( CvCapture** capture );
Th CvCapture structure. In particular, DEMO
can check and set various properties of the video source:
double cvGetCaptureProperty(
CvCapture* capture,
int        property_id
);
int cvSetCaptureProperty(
CvCapture* capture,
int        property_id,
double     value
);
Th
cvGetCaptureProperty() accepts any of DEMO property IDs shown in Table 4-4.
Table 4-4. Video capture properties used by cvGetCaptureProperty()
and cvSetCaptureProperty()
Video capture property
CV_CAP_PROP_POS_MSEC
CV_CAP_PROP_POS_FRAME
DEMO
CV_CAP_PROP_FRAME_WIDTH
CV_CAP_PROP_FRAME_HEIGHT
Numerical value
0
1
2
3
4
e routine
104
| Chapter 4: HighGUI
ere are many other things we can do with the
04-R4886-RC1.indd   104
9/15/08   4:19:27 DEMO
www.it-ebooks.info
Table 4-4. Video capture properties used by cvGetCaptureProperty()
and cvSetCaptureProperty() (continued)
Video capture property
CV_CAP_PROP_FPS
CV_CAP_PROP_FOURCC
CV_CAP_PROP_FRAME_COUNT
Numerical value
5
6
7
Most of these properties are self explanatory. POS_MSEC is the DEMO position in a video
fi le, measured in milliseconds. POS_FRAME is DEMO current position in frame number. POS_
AVI_RATIO is the position given as a number between 0 and 1 (this is actually quite use-
ful when you want to position a trackbar to allow folks to navigate DEMO your video).
FRAME_WIDTH and FRAME_HEIGHT are the dimensions of the individual frames of the video
to be read (or to be captured at the camera’s current settings). FPS is specifi c to video fi DEMO
and indicates the number of frames per second at which the video was captured; you
will need to know this if you want to play back your video and have it come out at the
right DEMO FOURCC is the four-character code for the compression codec to be used for
the video you are currently reading. FRAME_COUNT should be the total DEMO of frames
in the video, but this fi gure is not DEMO reliable.
All of these values are returned as type double, which DEMO perfectly reasonable except for
the case of FOURCC (FourCC) [FourCC85]. Here you will have to recast the result in order
to interpret it, as described in Example 4-3.
Example 4-3. Unpacking a four-character code to DEMO a video codec
double f = cvGetCaptureProperty(
capture,
CV_CAP_PROP_FOURCC
);
char* fourcc = (char*) (&f);
For each of these video capture properties, there is a corresponding cvSetCapture
Property() function that will attempt to set the property. Th ese are not all DEMO mean-
ingful; for example, you should not be setting the FOURCC of a video you are currently
reading. Attempting to move around the DEMO by setting one of the position properties
will work, but only DEMO some video codecs (we’ll have more to say about video codecs DEMO
the next section).
Writing Video
Th
this easy; it is DEMO the same as reading video but with a few extra details.
First we must create a CvVideoWriter device, which is the video writing analogue of
CvCapture. Th is device will incorporate the following functions.
CvVideoWriter* cvCreateVideoWriter(DEMO
const char* filename,
e other thing we might want to do with video is writing it out to disk. OpenCV makes
Working with DEMO
| 105
04-R4886-RC1.indd   105
9/15/08   4:19:28 PM
www.it-ebooks.info
int         fourcc,
double      fps,
CvSize      frame_size,
int         is_color  = 1
);
int cvWriteFrame(
CvVideoWriter*  writer,
const DEMO image
);
void cvReleaseVideoWriter(
CvVideoWriter** writer
);
You will notice that the video writer requires a few extra arguments. In addition DEMO the
fi lename, we have to tell the writer what codec DEMO use, what the frame rate is, and how
big the frames will be. Optionally we can tell OpenCV if the frames are black DEMO white
or color (the default is color).
Here, the codec is indicated by its four-character code. (For those of you who are not
experts in compression codecs, they all have a unique four-character identifi er asso-
ciated with them). In this case the int that DEMO named fourcc in the argument list for
cvCreate VideoWriter() is actually the four characters of the fourcc packed to-
gether. Since this comes DEMO relatively oft en, OpenCV provides a convenient macro
CV_FOURCC(c0,c1,DEMO,c3) that will do the bit packing for you.
Once you DEMO a video writer, all you have to do is call cvWriteFrame() and pass in the
CvVideoWriter* pointer and the IplImage* pointer for the image you want to write out.
Once you are fi nished, you must call CvReleaseVideoWriter() in order to close the writer
and the DEMO le you were writing to. Even if you are normally a bit sloppy about de-allocating
things at the end of a program, do not be sloppy about this. Unless you explicitly release
the video writer, the video fi le to which you are writing may be corrupted.
ConvertImage
DEMO purely historical reasons, there is one orphan routine in the HighGUI DEMO fi ts into
none of the categories described above. It is so tremendously useful, however, that you
should know about it and what DEMO does. Th e function is called cvConvertImage().
void cvConvertImage(
DEMO CvArr* src,
CvArr*       dst,
int          flags = 0
);
cvConvertImage() is used to perform common conversions between image formats. Th e
formats are specifi ed DEMO the headers of the src and dst images or arrays (the DEMO
prototype allows the more general CvArr type that works with IplImage).
Th oating-point
pixels. Th e destination must be 8 bits with one DEMO three channels. Th is function can also
convert color to grayscale or one-channel grayscale to three-channel grayscale (color).
e source image may be one, three, or four channels with either 8-bit or fl
106
DEMO Chapter 4: HighGUI
04-R4886-RC1.indd   106
9/15/08   4:DEMO:28 PM
www.it-ebooks.info
Finally, the flag (if set) will fl ip the image vertically. Th is is useful because sometimes
camera formats and display formats DEMO reversed. Setting this fl ag actually fl ips the pix-
els in memory.
Exercises
1. Th is chapter completes our introduction to basic I/DEMO programming and data struc-
tures in OpenCV. Th e following exercises build on this knowledge and create useful
utilities for later use.
a. Create DEMO program that (1) reads frames from a video, (2) DEMO the result to gray-
scale, and (3) performs Canny edge DEMO on the image. Display all three
stages of processing in three diff erent windows, with each window appropri-
ately named for its function.
b. Display all three stages of processing in one image.
Hint: Create another image of the same height but three times the width
as the DEMO frame. Copy the images into this, either by using pointers
or (more cleverly) by creating three new image headers that point to
the beginning of and to one-third and two-thirds of the way into the
DEMO Th en use cvCopy().
c. Write appropriate text labels describing DEMO processing in each of the three
slots.
2. Create a program that reads in and displays an image. When the user’s mouse clicks
on DEMO image, read in the corresponding pixel (blue, green, red) DEMO and write
those values as text to the screen at the mouse location.
a. For the program of exercise 1b, display the mouse coordinates of the individual
image when clicking anywhere within the three-image display.
3. DEMO a program that reads in and displays an image.
a. Allow the user to select a rectangular region in the image by drawing a DEMO
gle with the mouse button held down, and highlight the region DEMO the mouse
button is released. Be careful to save an image copy in memory so that your
drawing into the image does not destroy DEMO original values there. Th e next
mouse click should start the process all over again from the original image.
b. In a separate window, use the drawing functions to draw a graph in blue, green,
and red for how many pixels of each value were found in DEMO selected box. Th is
is the color histogram of that color region. Th e x-axis should be eight bins that
represent pixel values falling DEMO the ranges 0–31, 32–63, . . ., 223–255. Th e
DEMO should be counts of the number of pixels that were found in that bin
range. Do this for each color channel, BGR.
4. Make an application that reads and displays a video and is controlled by DEMO
ers. One slider will control the position within the video from start to end in 10
Exercises
| 107
04-R4886-RC1.indd   107
9/15/DEMO   4:19:28 PM
www.it-ebooks.info
increments; another binary slider should control pause/unpause. Label both sliders
appropriately.
5. Create your own simple paint program.
a. Write a program DEMO creates an image, sets it to 0, and then displays it. Allow
the user to draw lines, circles, ellipses, and polygons on the image using the
left  mouse button. Create an eraser function when the right mouse button is
held down.
b. Allow “logical drawing” by DEMO the user to set a slider setting to AND,
OR, DEMO XOR. Th at is, if the setting is AND then the DEMO will appear only
when it crosses pixels greater than 0 (and DEMO on for the other logical functions).
Write a program that creates an image, sets it to 0, and then displays it. When DEMO user
clicks on a location, he or she can type in DEMO label there. Allow Backspace to edit and
provide for an abort key. Hitting Enter should fi x the label at the spot it was DEMO
Perspective transform.
a. Write a program that reads in an image and uses the numbers 1–9 on the keypad
to control a perspective transformation DEMO (refer to our discussion of the
cvWarpPerspective() in the Dense DEMO Transform section of Chapter 6).
Tapping any number should increment the corresponding cell in the perspective
transform matrix; tapping with the Shift  DEMO depressed should decrement the
number associated with that cell (stopping at DEMO). Each time a number is changed,
display the results in two images: the raw image and the transformed image.
b. Add functionality to zoom in or out?
c. Add functionality to rotate the DEMO?
Face fun. Go to the /samples/c/ directory and build the facedetect.c code. Draw a
skull image (or fi nd one on the Web) and store it to disk. Modify the facedetect pro-
gram to load in the image of the skull.
a. When a face DEMO is detected, draw the skull in that rectangle.
Hint: cvConvertImage() can convert the size of the image, or you
could look up the cvResize function. One may then set the ROI to the
rectangle DEMO use cvCopy() to copy the properly resized image there.
b. Add a slider with 10 settings corresponding to 0.0 to 1.0. Use this DEMO to al-
pha blend the skull over the face rectangle using the cvAddWeighted function.
Image stabilization. Go to the /samples/c/ directory and DEMO the lkdemo code (the
motion tracking or optical fl ow code)DEMO Create and display a video image in a much
larger window image. Move the camera slightly but use the optical fl ow vectors to
DEMO the image in the same place within the larger window. Th is is a rudimentary
image stabilization technique.
6.
7.
8.
9.
108
| DEMO 4: HighGUI
04-R4886-RC1.indd   108
9/15/08   4:19:DEMO PM
CHAPTER 5
Image Processing
Overview
At this point we have all of DEMO basics at our disposal. We understand the structure of
the library as well as the basic data structures it uses to represent images. We DEMO
stand the HighGUI interface and can actually run a program and display our results on
the screen. Now that we understand these primitive methods DEMO to manipulate
image structures, we are ready to learn some more DEMO operations.
We will now move on to higher-level methods that treat the images as images, and not just
as arrays of colored (or DEMO) values. When we say “image processing”, we mean just
that: DEMO higher-level operators that are defi ned on image structures in order to accom-
plish tasks whose meaning is naturally defi ned in the context DEMO graphical, visual images.
Smoothing
Smoothing, also called blurring, is a DEMO and frequently used image processing opera-
tion. Th ere are many reasons for smoothing, but it is usually done to reduce noise or
camera artifacts. Smoothing is also important when we wish to reduce the resolution
DEMO an image in a principled way (we will discuss this in DEMO detail in the “Image Pyra-
mids” section of this chapter).
OpenCV off ers fi ve diff erent smoothing operations at this time. All DEMO them are sup-
ported through one function, cvSmooth(),* which DEMO our desired form of smoothing
as an argument.
void cvSmooth(
const CvArr*   src,
CvArr*         dst,
int            smoothtype  = CV_GAUSSIAN,
int            param1      = 3,
* Note DEMO in, say, Matlab—the fi ltering operations in OpenCV (e.g., cvSmooth(), cvErode(),
cvDilate()) produce output images of the DEMO size as the input. To achieve that result, OpenCV creates
“virtual” DEMO outside of the image at the borders. By default, this is DEMO by replication at the border, i.e.,
input(-dx,y)=input(DEMO,y), input(w+dx,y)=input(w-1,y), and so forth.
109
05-R4886-AT1.indd   109
www.it-ebooks.info
9/15/08   4:19:56 DEMO
www.it-ebooks.info
int            param2      = DEMO,
double         param3      = 0,DEMO
double         param4      = 0
);DEMO
Th src and dst arguments are the usual source and destination for the smooth opera-
tion. Th e cv_Smooth() function has four parameters DEMO the particularly uninformative
names of param1, param2, param3, and param4. DEMO e meaning of these parameters de-
pends on the value of smoothtype, which may take any of the fi ve values listed in Table 5-1.*
(Please notice that for some values of ST, “in place DEMO, in which src and dst indi-
cate the same image, is not allowed.)
Table 5-1. Types of smoothing operations
Smooth type Name
DEMO
CV_BLUR_NO
_SCALE
In
place? Nc
Simple blur Yes 1,3 8u, 32f 8u, 32f Sum over a param1×param2
neighborhood with sub-
sequent scaling by 1/
(param1×param2).
Simple blur
with no scaling
Depth
of src
Depth
of dst Brief description
No 1 8u 16s (for 8u Sum over a param1×param2
source) or neighborhood.
32f (for 32f
source)DEMO
CV_MEDIAN Median blur No 1,3 8u 8u Find median over a
param1×param1 square
neighborhood.
Gaussian blur Yes 1,3 8u, 32f 8u (DEMO 8u Sum over a param1×param2
source) or neighborhood.
32f (for 32f
source)
Bilateral fi lter No 1,3 8u 8u Apply bilateral DEMO fi ltering
with color sigma=param1 and
a space sigma=param2.
Th e simple blur operation, as exemplifi ed by CV_BLUR in Figure 5-1, is DEMO simplest case.
Each pixel in the output is the simple mean of all of the pixels in a window around the
corresponding pixel in DEMO input. Simple blur supports 1–4 image channels and works
on 8-bit images or 32-bit fl oating-point images.
Not all of the smoothing operators act DEMO the same sorts of images. CV_BLUR_NO_SCALE
(simple blur without scaling) is essentially the same as simple blur except that there is no
division DEMO to create an average. Hence the source and destination images must
have diff erent numerical precision so that the blurring operation will not result DEMO an
overfl ow. Simple blur without scaling may be performed on 8-bit images, in which case
the destination image should have IPL_DEPTH_16S (CV_16S) or IPL_DEPTH_32S (CV_32S)
* Here and elsewhere we sometimes use 8u as shorthand for 8-bit unsigned image depth (IPL_DEPTH_8U). See
Table 3-2 for other shorthand notation.
CV_GAUSSIAN
CV_BILATERAL
110
| Chapter 5: Image Processing
e
05-R4886-AT1.indd   110
9/15/08   4:19:57 PM
www.it-ebooks.info
Figure 5-1. Image smoothing by block averaging: on the left  DEMO the input images; on the right, the
output images
data types. Th e same operation may also be performed on 32-bit fl oating-point DEMO,
in which case the destination image may also be a 32-bit fl oating-point image. Simple
blur without scaling cannot be done in place: the source and destination images must be
diff erent. (Th is requirement is obvious in the case of 8 bits to 16 bits, but it applies even
when you are using a 32-bit image). Simple DEMO without scaling is sometimes chosen
because it is a little faster than blurring with scaling.
Th median fi lter (CV_MEDIAN) [Bardyn84] replaces each DEMO by the median or “middle”
pixel (as opposed to the mean DEMO) value in a square neighborhood around the center
pixel. Median fi DEMO will work on single-channel or three-channel or four-channel 8-bit
images, but DEMO cannot be done in place. Results of median fi ltering are shown in Figure 5-2.
Simple blurring by averaging can be sensitive to noisy DEMO, especially images with
large isolated outlier points (sometimes called “shot noise”). Large diff erences in even a
small number of points can DEMO a noticeable movement in the average value. Median
fi ltering is able to ignore the outliers by selecting the middle points.
Th lter, the Gaussian fi lter (CV_GAUSSIAN), is probably the most useful
though not the fastest. Gaussian fi ltering is done by convolving each point in DEMO input
array with a Gaussian kernel and then summing to produce the output array.
Smoothing | 111
e
e next smoothing fi
05-R4886-AT1.indd   DEMO
9/15/08   4:19:57 PM
www.it-ebooks.info
Figure 5-2. Image blurring by taking the median of surrounding pixels
DEMO the Gaussian blur (Figure 5-3), the fi rst two parameters DEMO the width and height of
the fi lter window; the (optional) third parameter indicates the sigma value (half width at
half max) of the Gaussian kernel. If the third parameter is not specifi ed, then the Gaussian
will be automatically determined from the window size using DEMO following formulae:
⎛ n ⎞
σx =−⎜ x 10⎟ ⋅
⎝ 2
. . , param130 0 80+=n
x
⎠
⎛⎞n
σy =−⎜⎜ DEMO 1
⎝ 2 ⎠
⎟⎟
⋅
+=
030 0 80..,
ny
param2
If you wish the kernel to be asymmetric, then you may also (optionally) supply a fourth
parameter; in this case, the DEMO and fourth parameters will be the values of sigma in
the horizontal and vertical directions, respectively.
If the third and fourth parameters are given but the fi rst two are set to 0, then the size of
the window will be automatically determined from the value of sigma.
DEMO
formance optimization for several common kernels. 3-by-3, 5-by-5 and 7-by-7 with
DEMO OpenCV implementation of Gaussian smoothing also provides a higher per-
112
| Chapter 5: Image Processing
05-R4886-AT1.indd   112
9/15/08   4:19:58 PM
www.it-ebooks.info
Figure 5-3. Gaussian blur on 1D pixel array
the “standard” sigma (i.e., param3 = 0.0) give better performance than other kernels.
Gaussian DEMO supports single- or three-channel images in either 8-bit or 32-bit fl oating-
point formats, and it can be done in place. Results of Gaussian blurring are shown in
Figure 5-4.
Th ft h and fi nal DEMO of smoothing supported by OpenCV is called bilateral fi ltering
[Tomasi98], DEMO example of which is shown in Figure 5-5. Bilateral fi ltering is one opera-
tion from a somewhat larger class of image analysis operators DEMO as edge-preserving
smoothing. Bilateral fi ltering is most easily understood when contrasted to Gaussian
smoothing. A typical motivation for Gaussian smoothing is that pixels DEMO a real image
should vary slowly over space and thus be correlated to their neighbors, whereas ran-
dom noise can be expected to vary greatly from one pixel to the next (i.e., noise is not
DEMO correlated). It is in this sense that Gaussian smoothing reduces noise while pre-
serving signal. Unfortunately, this method breaks down near edges, DEMO you do ex-
pect pixels to be uncorrelated with their neighbors. Th us Gaussian smoothing smoothes
away the edges. At the cost of a DEMO more processing time, bilateral fi ltering provides us
a means of DEMO an image without smoothing away the edges.
Like Gaussian smoothing, bilateral DEMO ltering constructs a weighted average of each
pixel and its neighboring components. Th e weighting has two components, the fi rst of
which is the same weighting used by Gaussian smoothing. Th e second component is
DEMO a Gaussian weighting but is based not on the spatial distance from the center pixel
Smoothing
| 113
e fi
05-R4886-AT1.indd   113
9/DEMO/08   4:19:58 PM
www.it-ebooks.info
Figure 5-4. Gaussian blurring
but rather on the diff erence in DEMO from the center pixel.† You can think of bilat-
eral fi ltering as Gaussian smoothing that weights more similar pixels more highly than
less DEMO ones. Th e eff ect of this fi lter is typically to turn an image into what appears
to be a watercolor painting of DEMO same scene.‡ Th is can be useful as an aid to segment-
ing the image.
Bilateral fi ltering takes two parameters. Th e fi DEMO is the width of the Gaussian kernel
used in the spatial domain, which is analogous to the sigma parameters in the Gaussian
fi lter. Th e second is the width of the Gaussian kernel in the DEMO domain. Th e larger
this second parameter is, the broader is DEMO range of intensities (or colors) that will be
included in the smoothing (and thus the more extreme a discontinuity must be in order
to be preserved).
* In the case of multichannel (i.e., DEMO) images, the diff erence in intensity is replaced with a weighted sum
over colors. Th is weighting is chosen to enforce a Euclidean DEMO in the CIE color space.
† Technically, the use of Gaussian DEMO functions is not a necessary feature of bilateral fi ltering. Th e
implementation in OpenCV uses Gaussian weighting even though the method is general DEMO many possible
weighting functions.
‡ Th is eff ect is particularly pronounced aft er multiple iterations of bilateral fi ltering.
114
| Chapter 5: Image Processing
05-R4886-AT1.indd   114
9/15/08   4:19:58 DEMO
www.it-ebooks.info
Figure 5-5. Results of bilateral smoothing
Image Morphology
OpenCV provides a DEMO, convenient interface for doing morphological transformations
[Serra83] on an image. Th DEMO basic morphological transformations are called dilation and
erosion, and they arise DEMO a wide variety of contexts such as removing noise, isolating
individual DEMO, and joining disparate elements in an image. Morphology can also
be DEMO to fi nd intensity bumps or holes in an image and to fi nd image gradients.
Dilation and Erosion
Dilation is a convolution of DEMO image (or region of an image), which we will call DEMO,
with some kernel, which we will call B. Th e DEMO, which can be any shape or size, has
a single defi ned anchor point. Most oft en, the kernel is a small solid square or disk with
the anchor point at the center. Th e DEMO can be thought of as a template or mask, and
its DEMO ect for dilation is that of a local maximum operator. As the kernel B is scanned
over the image, we compute the maximal pixel value overlapped by B and replace the
image pixel under the anchor DEMO with that maximal value. Th is causes bright regions
within an image to grow as diagrammed in Figure 5-6. Th is growth is the DEMO of the
term “dilation operator”.
Image Morphology
| 115
05-R4886-AT1.indd   115
9/15/08   4:19:59 PM
www.it-ebooks.info
Figure 5-6. Morphological dilation: take the maximum under the kernel B
Erosion is the converse operation. Th e action of the erosion operator DEMO equivalent to
computing a local minimum over the area of the kernel. Erosion generates a new image
from the original using the following algorithm: as the kernel B is scanned over the im-
age, we compute the minimal pixel value overlapped by B and replace the image pixel
DEMO the anchor point with that minimal value.* Erosion is diagrammed in Figure 5-7.
Image morphology is oft en done on binary images that result DEMO
thresholding. However, because dilation is just a max operator and
erosion DEMO just a min operator, morphology may be used on intensity
images DEMO well.
In general, whereas dilation expands region A, erosion reduces region A. Moreover, di-
lation will tend to smooth concavities and erosion will tend to smooth away protrusions.
Of course, the exact result will depend on the kernel, but these statements are generally
true for the fi lled convex kernels typically used.
In OpenCV, we eff ect these transformations using the cvErode() and cvDilate()
functions:
void cvErode(
IplImage*        src,
IplImage*        dst,DEMO
IplConvKernel*   B          = NULL,
int              iterations = 1
);
* DEMO be precise, the pixel in the destination image is set to DEMO value equal to the minimal value of the pixels
under the kernel in the source image.
116
| Chapter 5: Image Processing
05-R4886-AT1.indd   116
9/15/08   4:19:59 PM
www.it-ebooks.info
Figure 5-7. Morphological erosion: take the minimum under the kernel B
void cvDilate(
IplImage*        src,
IplImage*        dst,
IplConvKernel*   B          = DEMO,
int              iterations = 1
);
Both cvErode() and cvDilate() take a source and destination image, and both support
“in place” calls (in which the source DEMO destination are the same image). Th e third ar-
gument is the kernel, which defaults to NULL. In the NULL case, the DEMO used is a 3-by-3
kernel with the anchor at its center (DEMO will discuss shortly how to create your own
kernels). Finally, DEMO fourth argument is the number of iterations. If not set to the de-
fault value of 1, the operation will be applied multiple times during the single call to the
function. Th e results of an DEMO operation are shown in Figure 5-8 and those of a dila-
tion operation in Figure 5-9. Th e erode operation is oft en used DEMO eliminate “speckle”
noise in an image. Th e idea here is that the speckles are eroded to nothing while larger
regions that contain visually DEMO cant content are not aff ected. Th e dilate operation
is oft en used when attempting to fi nd connected components (i.e., large DEMO regions
of similar pixel color or intensity). Th e utility of dilation arises because in many cases
a large region might otherwise be DEMO apart into multiple components as a result of
noise, shadows, or some other similar eff ect. A small dilation will cause such compo-
DEMO to “melt” together into one.
To recap: when OpenCV processes the DEMO() function, what happens beneath the
hood is that the value DEMO some point p is set to the minimum value of all of the points
covered by the kernel when aligned at p; for the dilation operator, the equation is the
same except that max is considered rather than min:
Image Morphology
| 117
05-R4886-AT1.indd   117
9/DEMO/08   4:20:00 PM
www.it-ebooks.info
Figure 5-8. Results of the erosion, or “min”, operator: bright regions are isolated and shrunk
erode
dilate
( , ) min ( , )xy =+src x x y y′ + ′
(, )xy′′
DEMO
(, ) max ( , )xy =+src x x y y′ + ′
(, )xy′′
∈kernel
You might be wondering why we DEMO a complicated formula when the earlier heuris-
tic description was perfectly suffi  cient. Some readers actually prefer such formulas but,
more importantly, DEMO formulas capture some generality that isn’t apparent in the quali-
tative description. Observe that if the image is not binary then the min and DEMO opera-
tors play a less trivial role. Take another look at Figures 5-8 and 5-9, which show the
erosion and dilation operators applied to two real images.
Making Your Own Kernel
You are not limited to DEMO simple 3-by-3 square kernel. You can make your own cus-
tom morphological kernels (our previous “kernel B”) using IplConvKernel. Such
kernels are allocated DEMO cvCreateStructuringElementEx() and are released using
cvReleaseStructuringElement().
IplConvKernel* cvCreateStructuringElementEx(
DEMO          cols,
int          rows,
118
| Chapter 5: Image Processing
05-R4886-AT1.indd   118
9/15/08   4:20:00 PM
www.it-ebooks.info
Figure 5-9. Results of the dilation, or “max”, operator: bright regions are expanded and oft
en joined
int          anchor_x,
int          anchor_y,
int          shape,
int*         values=NULL
);
void cvReleaseStructuringElement( IplConvKernel** element );
A morphological kernel, unlike a DEMO kernel, doesn’t require any numerical val-
ues. Th e elements of DEMO kernel simply indicate where the max or min computations
take place as the kernel moves around the image. Th e anchor point indicates how DEMO
kernel is to be aligned with the source image and also where the result of the computa-
tion is to be placed in the DEMO image. When creating the kernel, cols and rows
indicate the size DEMO the rectangle that holds the structuring element. Th e next param-
eters, anchor_x and anchor_y, are the (x, y) coordinates of the anchor point within the
enclosing rectangle of the kernel. Th e fi DEMO h parameter, shape, can take on values listed
in Table 5-2. If CV_SHAPE_CUSTOM is used, then the integer vector values is used
to defi ne a custom shape of the kernel within the rows-by-cols enclosing DEMO Th is
vector is read in raster scan order with each entry representing a diff erent pixel in the
enclosing rectangle. Any nonzero value DEMO taken to indicate that the corresponding pixel
Image Morphology
| 119
05-R4886-AT1.indd   119
9/15/08   4:20:00 PM
www.it-ebooks.info
should be included in the kernel. If values is NULL then DEMO custom shape is interpreted
to be all nonzero, resulting in a DEMO kernel.*
Table 5-2. Possible IplConvKernel shape values
Shape value Meaning
CV_SHAPE_RECT The kernel is rectangular
CV_SHAPE_CROSS The kernel is cross shaped
CV_SHAPE_ELLIPSE The kernel DEMO elliptical
CV_SHAPE_CUSTOM The kernel is user-defi ned via values
More General Morphology
When working with Boolean images and image masks, the basic erode and dilate opera-
tions are usually suffi  cient. When working with grayscale or color images, however, a
number of additional operations are oft en DEMO Several of the more useful operations
can be handled by the multi-purpose cvMorphologyEx() function.
void cvMorphologyEx(
const CvArr*   src,
CvArr*         dst,
CvArr*         temp,
IplConvKernel* element,
int            operation,
int            iterations   = 1
);
In addition to the arguments src, dst, element, and iterations, which DEMO used with pre-
vious operators, cvMorphologyEx() has two new parameters. DEMO e fi rst is the temp array,
which is required for some of the operations (see Table 5-3). When required, this DEMO
should be the same size as the source image. Th e second new argument—the really in-
teresting one—is operation, which selects the morphological operation that we will do.
Table 5-3. cvMorphologyEx() operation options
Value of DEMO Morphological operator Requires temp image?
CV_MOP_OPEN Opening No
CV_MOP_CLOSE Closing No
CV_MOP_GRADIENT Morphological gradient Always
CV_MOP_TOPHAT Top Hat For in-place only (src = dst)
CV_MOP_BLACKHAT Black Hat For in-place only (src = dst)
Opening and closing
Th rst two operations in Table 5-3, opening and closing, are combinations of the erosion
and dilation operators. In the case of opening, we erode fi rst and then dilate (Figure 5-10)DEMO
* If the use of this strange integer vector strikes you as being incongruous with other OpenCV functions, you
are not alone. Th e origin of this syntax is the same as the origin of the DEMO prefi x to this function—another
instance of archeological code relics.
120 | Chapter 5: Image Processing
e fi
05-R4886-AT1.indd   120
9/15/08   4:20:01 PM
Opening is oft en used to count regions in a binary image. DEMO example, if we have
thresholded an image of cells on a DEMO slide, we might use opening to separate
out cells that are DEMO each other before counting the regions. In the case of closing, DEMO
dilate fi rst and then erode (Figure 5-12). Closing is DEMO in most of the more sophisti-
cated connected-component algorithms to reduce unwanted or noise-driven segments.
For connected components, usually an erosion or closing operation is performed fi rst to
eliminate elements that arise purely from noise DEMO then an opening operation is used
to connect nearby large regions. (DEMO that, although the end result of using open or
close is DEMO to using erode or dilate, these new operations tend to preserve DEMO area of
connected regions more accurately.)
Figure 5-10. Morphological opening operation: the upward outliers are eliminated as a result
Both the opening and closing operations are approximately area-preserving: the most
prominent eff ect of closing is to eliminate lone outliers that are lower than their neigh-
bors DEMO the eff ect of opening is to eliminate lone outliers that are higher than their
neighbors. Results of using the opening operator are shown DEMO Figure 5-11, and of the
closing operator in Figure 5-13.
One DEMO note on the opening and closing operators concerns how the iterations ar-
gument is interpreted. You might expect that asking for two iterations of DEMO
would yield something like dilate-erode-dilate-erode. It turns out that this would not
be particularly useful. What you really want (and what you get) DEMO dilate-dilate-erode-
erode. In this way, not only the single outliers but DEMO neighboring pairs of outliers
will disappear.
Morphological gradient
Our next available operator is the morphological gradient. For this one it is probably
easier to DEMO with a formula and then fi gure out what it means:
gradient(src) = dilate(src)–erode(src)
Th ect of this operation on a Boolean image would be simply to isolate perimeters of
DEMO blobs. Th e process is diagrammed in Figure 5-14, and the DEMO ect of this operator
on our test images is shown in Figure 5-15.
Image Morphology
| 121
e eff
05-R4886-AT1.indd   121
www.it-ebooks.info
9/DEMO/08   4:20:01 PM
www.it-ebooks.info
Figure 5-11. Results of morphological opening on an image: small bright regions are removed, and
the remaining bright regions are isolated but retain their size
Figure 5-12. Morphological closing operation: the downward outliers are eliminated as a result
With a grayscale image we see that the value DEMO the operator is telling us something
about how fast the image brightness is changing; this is why the name “morphological
gradient” is justifi ed. Morphological gradient is oft en used when we want to isolate the
DEMO of bright regions so we can treat them as whole objects (DEMO as whole parts of
objects). Th e complete perimeter of a region tends to be found because an expanded ver-
sion is subtracted DEMO a contracted version of the region, leaving a complete perimeter
122
DEMO Chapter 5: Image Processing
05-R4886-AT1.indd   122
9/15/08   DEMO:20:01 PM
www.it-ebooks.info
Figure 5-13. Results of morphological closing on an image: bright regions are joined but retain their
basic size
edge. Th is diff ers DEMO calculating a gradient, which is much less likely to work around
DEMO full perimeter of an object.*
Top Hat and Black Hat
Th e last two operators are called Top Hat and Black Hat [Meyer78]. Th DEMO operators are
used to isolate patches that are, respectively, brighter or dimmer than their immedi-
ate neighbors. You would use these when trying DEMO isolate parts of an object that ex-
hibit brightness changes relative only to the object to which they are attached. Th is oft en
DEMO with microscope images of organisms or cells, for example. Both operations DEMO
defi ned in terms of the more primitive operators, as follows:DEMO
TopHat(src) = src–open(src)
BlackHat(src) = close(src)–src
As you can see, the Top Hat operator subtracts the opened form of A from A. Recall
that the eff ect of the DEMO operation was to exaggerate small cracks or local drops. Th us,
* We will return to the topic of gradients when we introduce DEMO Sobel and Scharr operators in the next
chapter.
Image Morphology | 123
05-R4886-AT1.indd   123
9/15/08   4:20:02 PM
www.it-ebooks.info
Figure 5-14. Morphological gradient applied to a grayscale image: as expected, the operator has its
highest values where the grayscale image is changing most rapidly
subtracting open(A) from A should reveal areas that are lighter then the surrounding
region of A, relative to the size of the kernel (see Figure 5-16); conversely, the Black Hat
operator DEMO areas that are darker than the surrounding region of A (Figure DEMO).
Summary results for all the morphological operators discussed in this chapter are as-
sembled in Figure 5-18.*
Flood Fill
Flood fi ll [Heckbert00; Shaw04; Vandevenne04] is an extremely useful function that
is oft en used to mark or isolate portions of an image for further processing or DEMO
Flood fi ll can also be used to derive, from an DEMO image, masks that can be used for
subsequent routines to speed DEMO restrict processing to only those pixels indicated by the
mask. Th e function cvFloodFill() itself takes an optional mask that can be further DEMO
to control where fi lling is done (e.g., when doing multiple fi lls of the same image).
In OpenCV, fl ood fi ll is a more general version of the sort of fi ll DEMO which
you probably already associate with typical computer painting programs. For both, a
seed point is selected from an image and then all similar neighboring points are colored
with a uniform color. Th e diff erence DEMO is that the neighboring pixels need not all be
* Both of these operations (Top Hat and Black Hat) make more sense in DEMO morphology, where the
structuring element is a matrix of real numbers (not just a binary mask) and the matrix is added to the cur-
rent pixel neighborhood before taking a minimum or maximum. Unfortunately, this is not yet implemented
in OpenCV.
124 | Chapter 5: Image Processing
05-R4886-AT1.indd   124
9/15/08   4:20:02 PM
www.it-ebooks.info
Figure 5-15. Results of the morphological gradient operator: bright perimeter edges are identifi
ed
identical in color.* Th
region. Th
fi
flags) the neighboring pixel is within a specifi
Flood fi
the fl
void cvFloodFill(DEMO
IplImage*          img,
CvPoint            seedPoint,
CvScalar           newVal,
DEMO           loDiff    = cvScalarAll(0),
CvScalar           upDiff    = cvScalarAll(0),DEMO
CvConnectedComp*   comp      = NULL,
int                flags     = 4,
CvArr*             mask      = NULL
);DEMO
Th
channel or three-channel. We start the fl
ood fi
e result of a fl
e cvFloodFill() function will color a neighboring pixel DEMO it is within a speci-
loDiff to upDiff) of either the DEMO pixel or if (depending on the settings of
ed range of DEMO original seedPoint value.
lling can also be constrained by an optional mask argument. Th
ll routine is:
ood fi
img is the input DEMO, which can be 8-bit or fl
ood fi
ll operation will DEMO be a single contiguous
e prototype for
oating-point and one-
lling from seedPoint, and newVal is the
* Users of contemporary painting and drawing programs should note that most now employ a fi lling algo-
rithm DEMO much like cvFloodFill().
Flood Fill
| 125
ed range (
e parameter
05-R4886-AT1.indd   125
9/15/08   4:20:02 DEMO
www.it-ebooks.info
Figure 5-16. Results of morphological Top Hat operation: bright local peaks are isolated
value to which colorized pixels are set. A pixel will DEMO colorized if its intensity is not
less than a colorized neighbor’s intensity minus loDiff and not greater than the color-
ized neighbor’s intensity plus DEMO If the flags argument includes CV_FLOODFILL_FIXED_
RANGE, then a pixel will DEMO compared to the original seed point rather than to its neigh-
bors. If non-NULL, comp is a CvConnectedComp structure that will hold statistics about the
areas fi lled.* Th e flags argument (to be discussed shortly) is a little tricky; it controls
the connectivity of the fi DEMO, what the fi ll is relative to, whether we are fi lling only a mask,
and what values are used to fi DEMO the mask. Our fi rst example of fl ood fi ll is shown in
Figure 5-19.
Th mask indicates a mask that can function DEMO as input to cvFloodFill() (in
which case it constrains the DEMO that can be fi lled) and as output from cvFloodFill()
(in which case it will indicate the regions that actually were fi lled). If set to a non-NULL
value, then mask must be a one-channel, 8-bit image whose size is exactly two pixels
larger in width and height than the source image (this is to make processing easier and
faster for the internal algorithm). Pixel (x + 1, y + 1) in the mask image corresponds
to image pixel (x, y) in the source image. Note that cvFloodFill() will DEMO fl ood across
* We will address the specifi cs of a “connected component” in the section “Image Pyramids”. For now, just
think of it as being similar to a mask that identifi es some subsection DEMO an image.
126 | Chapter 5: Image Processing
e argument
05-R4886-AT1.indd   126
9/15/08   4:20:03 PM
www.it-ebooks.info
Figure 5-17. Results of morphological Black Hat operation: dark holes are isolated
Figure 5-18. Summary results for all morphology operators
nonzero pixels in DEMO mask, so you should be careful to zero it before use DEMO you don’t
want masking to block the fl ooding operation. Flood fi ll can be set to colorize either the
source image img or DEMO mask image mask.
Flood Fill
| 127
05-R4886-AT1.indd   127
9/15/08   4:20:04 PM
www.it-ebooks.info
Figure 5-19. Results of fl ood fi ll (top image is fi lled with gray, bottom image with white) from the
dark DEMO located just off  center in both images; in this case, DEMO hiDiff  and loDiff  parameters were
each set to 7.0
If the fl ood-fi ll mask is set to be marked, then it is marked with the
values set in the middle bits (8–15) of DEMO flags value (see text). If these
bits are not set DEMO the mask is set to 1 as the default value. Don’t be
confused if you fi ll the mask and see nothing but black DEMO display;
the fi lled values (if the middle bits of DEMO fl ag weren’t set) are 1s, so the
mask image needs to be rescaled if you want to display it visually.
It’s time DEMO clarify the flags argument, which is tricky because it has three DEMO Th e
low 8 bits (0–7) can be set to 4 or 8. Th is controls the connectivity considered by the fi ll-
DEMO algorithm. If set to 4, only horizontal and vertical neighbors to DEMO current pixel are
considered in the fi lling process; if set DEMO 8, fl ood fi ll will additionally include diagonal
neighbors. Th DEMO high 8 bits (16–23) can be set with the fl ags CV_FLOODFILL_FIXED_RANGE
(fi ll relative to the seed point pixel value; otherwise, fi ll relative to the neighbor’s value),
and/or CV_FLOODFILL_MASK_ONLY (DEMO ll the mask location instead of the source image loca-
tion). Obviously, you must supply an appropriate mask if CV_FLOODFILL_MASK_ONLY is set.
Th middle bits (8–15) of flags can be set to the value DEMO which you want the mask
to be fi lled. If the middle bits of flags are 0s, the mask will be fi lled with 1s. All these
fl ags may be linked together via OR. For DEMO, if you want an 8-way connectivity fi ll,
128 | DEMO 5: Image Processing
e
05-R4886-AT1.indd   128
9/15/08   DEMO:20:04 PM
fi lling only a fi xed range, fi lling the mask not the image, and fi lling using a value of 47,
then the parameter to pass in would be:
flags = 8
| DEMO
| CV_FLOODFILL_FIXED_RANGE
| (47<<8);
Figure 5-20 shows fl DEMO fi ll in action on a sample image. Using CV_FLOODFILL_FIXED_RANGE
with a wide range resulted in most of the image being fi lled (starting at the center).
We should note that newVal, loDiff, and DEMO are prototyped as type CvScalar so they
can be set for three channels at once (i.e., to encompass the RGB colors specifi ed DEMO
CV_RGB()). For example, lowDiff = CV_RGB(20,30,40) will set lowDiff thresholds of 20 for
red, 30 for green, DEMO 40 for blue.
Figure 5-20. Results of fl
dark circle located just off
and with a high and low diff
ood fi
center in DEMO images; in this case, fl
erence of 25.0
ll (top DEMO is fi
lled with gray, bottom image with white) from the
ood fi
ll was done with a fi
xed range
Resize
We DEMO en encounter an image of some size that we would like to convert to an image
of some other size. We may want to DEMO (zoom in) or downsize (zoom out) the im-
age; DEMO can accomplish either task by using cvResize(). Th is function DEMO fi t the source
Resize
| 129
05-R4886-AT1.indd   129
www.it-ebooks.info
9/15/08   4:20:05 PM
www.it-ebooks.info
image exactly to the destination image size. If the ROI is DEMO in the source image then
that ROI will be resized to fi t in the destination image. Likewise, if an ROI is set in the
destination image then the source will be resized to fi t DEMO the ROI.
void cvResize(
const CvArr*   src,
CvArr*         dst,
int            interpolation = CV_INTER_LINEAR
);
Th
Th
Table 5-4. cvResize() interpolation options
DEMO Meaning
CV_INTER_NN Nearest neighbor
CV_INTER_LINEAR Bilinear
CV_INTER_AREA Pixel area re-sampling
CV_INTER_CUBIC Bicubic interpolation
In general, we would like the mapping from the source image to the resized destina-
tion image to be as smooth as possible. DEMO e argument interpolation controls exactly
how this will be handled. Interpolation arises when we are shrinking an image and a
pixel in the destination DEMO falls in between pixels in the source image. It can also
occur when we are expanding an image and need to compute values of DEMO that do
not directly correspond to any pixel in the source image. In either case, there are several
options for computing the values of such pixels. Th e easiest approach is to take the
resized pixel’s DEMO from its closest pixel in the source image; this is the DEMO ect of choos-
ing the interpolation value CV_INTER_NN. Alternatively, we can DEMO weight the 2-by-2
surrounding source pixel values according to how close they are to the destination pixel,
which is what CV_INTER_LINEAR does. We DEMO also virtually place the new resized pixel over
the old pixels and then average the covered pixel values, as done with CV_INTER_AREA.*
Finally, DEMO have the option of fi tting a cubic spline between the 4-by-4 surrounding pix-
els in the source image and then reading off  the corresponding destination value from
the fi tted spline; this is the result of choosing the CV_INTER_CUBIC interpolation method.
Image Pyramids
Image pyramids [Adelson84] are DEMO used in a wide variety of vision applications.
An image pyramid is a collection of images—all arising from a single original image—
that are DEMO downsampled until some desired stopping point is reached. (Of
course, this stopping point could be a single-pixel image!)
* At least that’s DEMO happens when cvResize() shrinks an image. When it expands an image, CV_INTER_
AREA amounts to the same thing as CV_INTER_NN.
130 | Chapter 5: Image Processing
e last argument is the interpolation method, which DEMO to linear interpolation.
e other available options are shown in Table 5-4.
05-R4886-AT1.indd   130
9/15/08   4:20:05 PM
www.it-ebooks.info
Th en in the literature and in appli-
cation: the Gaussian [Rosenfeld80] and Laplacian [Burt83] pyramids [Adelson84]. Th e
Gaussian pyramid is used DEMO downsample images, and the Laplacian pyramid (to be dis-
cussed shortly) is required when we want to reconstruct an upsampled image from an
image lower in the pyramid.
To produce layer (i+1) in the DEMO pyramid (we denote this layer Gi+1) from layer Gi
of the pyramid, we fi rst convolve Gi with a Gaussian kernel and then remove every even-
numbered row and column. Of course, from this it follows immediately that each image
is exactly one-quarter the area of its DEMO Iterating this process on the input im-
age G0 produces the entire pyramid. OpenCV provides us with a method for generating
each pyramid stage DEMO its predecessor:
void cvPyrDown(
IplImage*   src,
IplImage*   dst,
IplFilter   filter = IPL_GAUSSIAN_5x5
);
Currently, the last argument filter supports only the single (default) option of a 5-by-5
DEMO kernel.
Similarly, we can convert an existing image to an image DEMO is twice as large in each
direction by the following analogous (DEMO not inverse!) operation:
void cvPyrUp(
IplImage*   src,
DEMO   dst,
IplFilter   filter = IPL_GAUSSIAN_5x5
);
In this case the image is fi rst upsized to twice the original in DEMO dimension, with the
new (even) rows fi lled with 0s. DEMO ereaft er, a convolution is performed with the given
fi lter (actually, a fi lter twice as large in each dimension than that specifi ed*) to approxi-
mate the values of the “missing” pixels.
We noted previously that the operator PyrUp() is not the inverse of DEMO(). Th is
should be evident because PyrDown() is an DEMO that loses information. In order to
restore the original (higher-resolution) image, we would require access to the informa-
tion that was discarded by the downsampling. Th is data forms the Laplacian pyramid.
Th ith layer DEMO the Laplacian pyramid is defi ned by the relation:
LG G=−UP( )⊗G
Here the operator UP() upsizes by mapping each pixel DEMO location (x, y) in the original
image to pixel (2x + 1, 2y + 1) in the destination image; the ⊗ symbol denotes convolu-
tion; and G5×5 is a 5-by-5 Gaussian kernel. Of course, Gi – UP(Gi+1) ⊗ G5×5 is the defi nition
DEMO Th is fi lter is also normalized to four, rather than DEMO one. Th is is appropriate because the inserted rows have
0s in all of their pixels before the convolution.
Image Pyramids | 131
ere DEMO two kinds of image pyramids that arise oft
e
ii i
+×155
05-R4886-AT1.indd   131
9/15/08   4:20:05 PM
of the PyrUp() operator provided by OpenCv. Hence, we can use OpenCv to compute the
Laplacian operator directly as:
ii i+1
LG DEMO PyrUp( )
Th
which also shows the inverse process for recovering the original image from the sub-
images. Note how the Laplacian is DEMO an approximation that uses the diff erence of
Gaussians, as revealed DEMO the preceding equation and diagrammed in the fi gure.
e Gaussian and Laplacian pyramids are shown diagrammatically in Figure 5-21,
Figure 5-21. Th
DEMO Gaussian pyramid and its inverse, the Laplacian pyramid
Th
pyramids, but a particularly important one is image segmentation (see Figure 5-22). In
this case, one builds an image pyramid and then associates to it a system of parent–child
relations between pixels at level Gi+1 and the DEMO reduced pixel at level Gi. In
this way, a fast initial DEMO can be done on the low-resolution images high in
the pyramid and then can be refi ned and further diff erentiated level by level.
DEMO is algorithm (due to B. Jaehne [Jaehne95; Antonisse82]) is implemented DEMO OpenCV
as cvPyrSegmentation():
void cvPyrSegmentation(
IplImage*      DEMO,
IplImage*      dst,
132
| Chapter 5: Image Processing
ere are many operations that can make extensive use of the DEMO and Laplacian
05-R4886-AT1.indd   132
www.it-ebooks.info
9/15/08   4:20:06 PM
www.it-ebooks.info
Figure 5-22. Pyramid segmentation with threshold1 set to 150 and threshold2 DEMO to 30; the im-
ages on the right contain only a DEMO of the images on the left  because pyramid segmentation
requires images DEMO are N-times divisible by 2, where N is the number of DEMO layers to be com-
puted (these are 512-by-512 areas from the DEMO images)
CvMemStorage*  storage,
CvSeq**        comp,
int            level,
double         threshold1,
double         threshold2
);
As DEMO, src and dst are the source and destination images, which must both be 8-bit,
of the same size, and of the same number of channels (one or three). You might be
wondering, DEMO destination image?” Not an unreasonable question, actually. Th e
destination DEMO dst is used as scratch space for the algorithm and also as a return
visualization of the segmentation. If you view this image, you will see that each segment
is colored in a single color (the color of some pixel in that segment). Because this image
is DEMO algorithm’s scratch space, you cannot simply set it to NULL. Even DEMO you do not want
the result, you must provide an image. DEMO important word of warning about src and
dst: because all levels DEMO the image pyramid must have integer sizes in both dimensions,
the starting images must be divisible by two as many times as there DEMO levels in the
Image Pyramids | 133
05-R4886-AT1.indd   133
9/15/08   4:20:06 PM
www.it-ebooks.info
pyramid. For example, for a four-level pyramid, a height or DEMO of 80 (2 × 2 × 2 × 5)
would DEMO acceptable, but a value of 90 (2 × 3 × 3 × 5) would not.*
Th e pointer storage is for an OpenCV memory storage area. In Chapter 8 we will dis-
cuss such areas DEMO more detail, but for now you should know that such a DEMO area is
allocated with a command like†
CvMemStorage* storage = cvCreateMemStorage();
Th comp is a location for storing further information about the DEMO seg-
mentation: a sequence of connected components is allocated from this DEMO storage.
Exactly how this works will be detailed in Chapter 8, DEMO for convenience here we briefl y
summarize what you’ll need in the context of cvPyrSegmentation().
First of all, a sequence is essentially DEMO list of structures of a particular kind. Given a
sequence, you DEMO obtain the number of elements as well as a particular element if you
know both its type and its number in the sequence. Take DEMO look at the Example 5-1
approach to accessing a sequence.
Example 5-1. Doing something with each element in the sequence of connected components returned
DEMO cvPyrSegmentation()
void f(
IplImage* src,
IplImage* dst
) DEMO
CvMemStorage* storage = cvCreateMemStorage(0);
CvSeq* comp = NULL;
DEMO( src, dst, storage, &comp, 4, 200, 50 );
int n_comp = comp->total;
for( int i=0; i<DEMO; i++ ) {
CvConnectedComp* cc = (CvConnectedComp*) cvGetSeqElem( comp, DEMO );
do_something_with( cc );
}
cvReleaseMemStorage( &storage );DEMO
}
Th
of a memory storage; this is where cvPyrSegmentation() DEMO get the memory it needs
for the connected components it will have to create. Th en the pointer comp is allocated
as type CvSeq*. DEMO is initialized to NULL because its current value means nothing. We will
pass to cvPyrSegmentation() a pointer to comp so that comp can DEMO set to the location of
the sequence created by cvPyrSegmentation(). DEMO we have called the segmentation,
we can fi gure out how many elements there are in the sequence with the member ele-
ment DEMO Th ereaft er we can use the generic cvGetSeqElem() to obtain the ith element
of comp; however, because cvGetSeqElem() is generic DEMO returns only a void pointer, we
must cast the return pointer DEMO the appropriate type (in this case, CvConnectedComp*).
* Heed this warning! Otherwise, you will get a totally useless error message and probably waste hours trying
to fi gure out what’s going on.
† Actually, the current implementation of cvPyrSegmentation() is a bit incomplete in that DEMO returns not the
computed segments but only the bounding rectangles (as DEMO<CvConnectedComp>).
134 | Chapter 5: Image Processing
e argument
ere are several things you should notice in this example. First, observe the allocation
05-R4886-AT1.indd   134
9/15/08   4:20:07 PM
www.it-ebooks.info
Finally, we need to know that a connected component is one of the basic structure types
in OpenCV. You can think of it DEMO a way of describing a “blob” in an image. It has the
following defi nition:
typedef struct CvConnectedComponent {
double   area;
DEMO value;
CvRect   rect;
CvSeq*   contour;
};
Th area is the area of the component. Th e value is DEMO average color* over the area of
the component and rect is a bounding box for the component (defi ned in the coordi-
nates of the parent image). Th e fi nal element, contour, is DEMO pointer to another sequence.
Th is sequence can be used to store a representation of the boundary of the component,
typically as a DEMO of points (type CvPoint).
In the specifi c case of DEMO(), the contour member is not set. Th us, if you
want some specifi c representation of the component’s pixels then you will DEMO to com-
pute it yourself. Th e method to use depends, DEMO course, on the representation you have
in mind. Oft en you DEMO want a Boolean mask with nonzero elements wherever the com-
ponent was located. You can easily generate this by using the rect portion of DEMO con-
nected component as a mask and then using cvFloodFill() to select the desired pixels
inside of that rectangle.
Threshold
Frequently we have DEMO many layers of processing steps and want either to make a
fi nal decision about the pixels in an image or to categorically reject DEMO pixels below
or above some value while keeping the others. Th e OpenCV function cvThreshold() ac-
complishes these tasks (see survey [Sezgin04]). Th e basic idea is that an array is given,
along DEMO a threshold, and then something happens to every element of the DEMO de-
pending on whether it is below or above the threshold.
double cvThreshold(
CvArr*         src,
CvArr*         dst,
double         threshold,
double         max_value,
int            threshold_type
);
As shown in Table 5-5, each threshold type corresponds to a particular comparison op-
eration between the ith source pixel (srci) DEMO the threshold (denoted in the table by T).
Depending on DEMO relationship between the source pixel and the threshold, the destina-
tion DEMO dsti may be set to 0, the srci, or the max_value (denoted in the table by M).
* Actually the meaning of value is context dependant and could be just about anything, but it is typically a
color associated with the component. In the case of DEMO(), value is the average color over
the segment.
Threshold
| DEMO
e
05-R4886-AT1.indd   135
9/15/08   4:20:07 PM
Table 5-5. cvTh
CV_THRESH_BINARY_INV
reshold() threshold_type options
Threshold type Operation
CV_THRESH_BINARY DEMO src ? : 0=>()TM
dst src ? 0:=>()TM
ii
CV_THRESH_TRUNC
CV_THRESH_TOZERO_INV
CV_THRESH_TOZERO
dst src ? : src=>()TM
DEMO
dst src ? 0: src=>()T
dst src ? src : 0=>()T
i
ii
ii i
ii i
Figure 5-23 should help to clarify the exact implications of each threshold type.
Figure 5-23. DEMO of varying the threshold type in cvTh reshold(). Th e DEMO line through each
chart represents a particular threshold level applied to the top chart and its eff ect for each of the fi ve
DEMO of threshold operations below
136
| Chapter 5: Image Processing
05-R4886-AT1.indd   136
www.it-ebooks.info
9/15/08   4:20:07 PM
www.it-ebooks.info
Let’s look at a simple example. In Example 5-2 we sum DEMO three channels of an image
and then clip the result at 100.
Example 5-2. Example code making use of cvTh reshold()
#include <DEMO>
#include <cv.h>
#include <highgui.h>
void sum_rgb( IplImage* DEMO, IplImage* dst ) {
// Allocate individual image planes.
IplImage* r = cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
IplImage* g DEMO cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
IplImage* b = cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
// Split image onto the color planes.
cvSplit( src, r, g, b, NULL );
// Temporary storage.
IplImage* s = cvCreateImage( cvGetSize(src), IPL_DEPTH_8U, 1 );
// Add equally weighted rgb values.
cvAddWeighted( r, 1./3., g, 1./3., 0.0, s );DEMO
cvAddWeighted( s, 2./3., b, 1./3., 0.0, s );
// Truncate values above 100.
cvThreshold( s, dst, DEMO, 100, CV_THRESH_TRUNC );
cvReleaseImage( &r );
cvReleaseImage( &g );
cvReleaseImage( &b );
cvReleaseImage( &s );DEMO
}
int main(int argc, char** argv)
{
// Create a named window with the name of the file.
cvNamedWindow( argv[1], DEMO );
// Load the image from the given file name.
DEMO src = cvLoadImage( argv[1] );
IplImage* dst = cvCreateImage( cvGetSize(src), src->depth, 1);
sum_rgb( src, dst);
// Show the image in the named window
cvShowImage( argv[1], dst );
// Idle until the user hits the “Esc” key.
while( 1 ) { if( (cvWaitKey( 10 )&0x7f) == 27 ) break; }
// Clean up and don’t be piggies
cvDestroyWindow( argv[1] );
Threshold
| 137
05-R4886-AT1.indd   137
9/15/08   4:20:08 PM
www.it-ebooks.info
Example 5-2. Example code making use of cvTh
reshold() (continued)
cvReleaseImage( &src );
cvReleaseImage( &dst );
}
DEMO important ideas are shown here. One thing is that we don’t want to add into an
8-bit array because the higher bits will overfl DEMO Instead, we use equally weighted ad-
dition of the three color DEMO (cvAddWeighted()); then the results are truncated to
saturate at DEMO value of 100 for the return. Th e cvThreshold() function handles only 8-bit
or fl oating-point grayscale source images. Th e destination image DEMO either match the
source image or be an 8-bit image. In fact, cvThreshold() also allows the source and des-
tination images to be the same image. Had we used a fl oating-point temporary image
s DEMO Example 5-2, we could have substituted the code shown in Example DEMO Note that
cvAcc() can accumulate 8-bit integer image types into a fl oating-point image; however,
cvADD() cannot add integer bytes into fl oats.
Example 5-3. Alternative method to combine and threshold image planes
DEMO s = cvCreateImage(cvGetSize(src), IPL_DEPTH_32F, 1);
cvZero(s);
cvAcc(b,s);
cvAcc(g,s);
cvAcc(r,s);
cvThreshold( s, s, 100, 100, CV_THRESH_TRUNC );DEMO
cvConvertScale( s, dst, 1, 0 );
Adaptive Threshold
Th ed threshold technique in which the threshold level is itself variable. In
DEMO, this method is implemented in the cvAdaptiveThreshold() [Jain86] function:
DEMO cvAdaptiveThreshold(
CvArr*         src,
CvArr*         dst,
double         max_val,
int            adaptive_method = CV_ADAPTIVE_THRESH_MEAN_C
int            threshold_type  = CV_THRESH_BINARY,
int            block_size      = 3,
double         DEMO          = 5
);
cvAdaptiveThreshold() allows DEMO two diff erent adaptive threshold types depending on
the settings of adaptive_method. In both cases the adaptive threshold T(x, y) is set DEMO a
pixel-by-pixel basis by computing a weighted average of the b-by-b region around each
pixel location minus a constant, where b is given by block_size and the constant is given
by param1. If the method is DEMO to CV_ADAPTIVE_THRESH_MEAN_C, then all pixels in the area
are weighted equally. DEMO it is set to CV_ADAPTIVE_THRESH_GAUSSIAN_C, then the pixels in the
region DEMO (x, y) are weighted according to a Gaussian function of DEMO distance
from that center point.
138
| Chapter 5: Image Processing
DEMO is a modifi
05-R4886-AT1.indd   138
9/15/08   4:20:08 PM
Finally, the parameter threshold_type is the same as for cvThreshold() shown in
Table 5-5.
Th ec-
tance gradients that you need to threshold DEMO to the general intensity gradient. Th is
function handles only single-channel 8-bit or fl oating-point images, and it requires that
the source and destination images be distinct.
Source code for comparing cvAdaptiveThreshold() and cvThreshold() DEMO shown in Exam-
ple 5-4. Figure 5-24 displays the result of processing an image that has a strong lighting
gradient across it. Th e DEMO  portion of the fi gure shows the result of using a DEMO
global threshold as in cvThreshold(); the lower-right portion shows the DEMO of adaptive
local threshold using cvAdaptiveThreshold(). We get the whole DEMO via adap-
tive threshold, a result that is impossible to achieve DEMO using a single threshold. Note
the calling-convention comments at the top of the code in Example 5-4; the parameters
used for Figure 5-24 were:
./adaptThresh 15 1 1 71 15 ../Data/cal3-L.bmp
Figure DEMO Binary threshold versus adaptive binary threshold: the input image (top) DEMO turned
into a binary image using a global threshold (lower left ) and an adaptive threshold (lower right); raw
image courtesy of DEMO Konolidge
Threshold
| 139
e adaptive threshold technique is useful when there are strong illumination or refl
05-R4886-AT1.indd   139
www.it-ebooks.info
9/15/08   4:20:08 PM
www.it-ebooks.info
Example 5-4. Th reshold versus adaptive threshold
// Compare thresholding with adaptive thresholding
// CALL:
// ./adaptThreshold Threshold 1binary 1adaptivemean DEMO
//                  blocksize offset filename
#include “cv.h”
#include “highgui.h”
#include “math.h”
IplImage *Igray=0, *It = 0, *Iat;
int main( int argc, char** argv )
{
DEMO(argc != 7){return -1;  }
//Command line
double threshold = (double)atof(argv[1]);
int threshold_type =  atoi(argv[2]) ?
CV_THRESH_BINARY : CV_THRESH_BINARY_INV;
int adaptive_method = atoi(argv[3]) ?
DEMO : CV_ADAPTIVE_THRESH_GAUSSIAN_C;
int block_size = atoi(argv[4]);
double offset DEMO (double)atof(argv[5]);
//Read in gray image
if((DEMO = cvLoadImage( argv[6], CV_LOAD_IMAGE_GRAYSCALE)) == 0){
return  -1;DEMO
// Create the grayscale output images
It =  cvCreateImage(cvSize(Igray->width,Igray->height),
IPL_DEPTH_8U, 1);
Iat = cvCreateImage(cvSize(Igray->width,Igray->height),
IPL_DEPTH_8U, 1);
//Threshold
cvThreshold(Igray,It,threshold,255,threshold_type);
cvAdaptiveThreshold(Igray, Iat, 255, DEMO,
threshold_type, block_size, offset);
//PUT UP 2 WINDOWS
DEMO(“Raw”,1);
cvNamedWindow(“Threshold”,1);
cvNamedWindow(“Adaptive Threshold”,1);
//Show the results
cvShowImage(“Raw”,Igray);
cvShowImage(“Threshold”,DEMO);
cvShowImage(“Adaptive Threshold”,Iat);
cvWaitKey(0);
//DEMO up
cvReleaseImage(&Igray);
cvReleaseImage(&It);
cvReleaseImage(&Iat);
cvDestroyWindow(“Raw”);
cvDestroyWindow(“Threshold”);
140 | Chapter 5: Image Processing
05-R4886-AT1.indd   140
9/15/08   4:20:08 DEMO
www.it-ebooks.info
Example 5-4. Th
reshold versus adaptive threshold (continued)
cvDestroyWindow(“Adaptive Threshold”);
return(0);
}
Exercises
1. Load an image DEMO interesting textures. Smooth the image in several ways using
cvSmooth() with smoothtype=CV_GAUSSIAN.
a. Use a symmetric 3-by-3, 5-by-5, 9-by-9 and 11-by-11 smoothing DEMO size
and display the results.
b. Are the output results nearly the same by smoothing the image twice with a
5-by-5 Gaussian fi lter DEMO when you smooth once with two 11-by-11 fi lters? Why
or DEMO not?
2. Display the fi lter, creating a 100-by-100 single-channel DEMO Clear it and set the
center pixel equal to 255.
a. Smooth this image with a 5-by-5 Gaussian fi lter and display the results. DEMO
did you fi nd?
b. Do this again but now with a 9-by-9 Gaussian fi lter.
c. What does it look like if DEMO start over and smooth the image twice with the
5-by-5 fi lter? Compare this with the 9-by-9 results. Are they nearly the same?
Why or why not?
3. Load an interesting image. Again, blur it with cvSmooth() using a Gaussian fi lter.
a. Set param1=param2=9. DEMO several settings of param3 (e.g., 1, 4, and 6). Display the
results.
b. Th is time, set param1=param2=0 before setting param3 to 1, 4, and 6. Display the
results. Are they diff DEMO? Why?
c. Again use param1=param2=0 but now set param3=1 and DEMO Smooth the pic-
ture and display the results.
d. Repeat part c but with param3=9 and param4=1. Display the results.
e. Now smooth the DEMO once with the settings of part c and once with the set-
tings of part d. Display the results.
f. Compare the results in DEMO e with smoothings that use param3=param4=9 and
param3=param4=0 (i.e., a 9-by-9 fi lter). Are the results the same? Why or why not?
4. Use a camera to take two pictures of the same DEMO while moving the camera as
little as possible. Load these images into the computer as src1 and src1.
a. Take the absolute value of DEMO minus src1 (subtract the images); call it diff12
and display. DEMO this were done perfectly, diff12 would be black. Why isn’t it?DEMO
Exercises
| 141
05-R4886-AT1.indd   141
9/15/08   4:20:09 PM
www.it-ebooks.info
b. Create cleandiff by using cvErode() and then cvDilate() DEMO diff12. Display the
results.
c. Create dirtydiff by using cvDilate() and then cvErode() on diff12 and then
display.
d. Explain the diff DEMO between cleandiff and dirtydiff.
5. Take a picture of a scene. Th en, without moving the camera, put a coff ee cup in DEMO
scene and take a second picture. Load these images and convert both to 8-bit gray-
scale images.
a. Take the absolute value of their DEMO erence. Display the result, which should
look like a noisy mask DEMO a coff ee mug.
b. Do a binary threshold of the resulting image using a level that preserves most
of the coff ee mug DEMO removes some of the noise. Display the result. Th e “on”
values should be set to 255.
c. Do a CV_MOP_OPEN on the image DEMO further clean up noise.
Create a clean mask from noise. Aft er completing exercise 5, continue by keeping
only the largest remaining shape in the image. Set a pointer to the upper left  of the
image and then traverse the image. When you fi nd a pixel of DEMO 255 (“on”), store
the location and then fl ood fi DEMO it using a value of 100. Read the connected component
returned from fl ood fi ll and record the area of fi lled region. DEMO there is another larger
region in the image, then fl ood DEMO ll the smaller region using a value of 0 and delete
its recorded area. If the new region is larger than the previous region, then fl ood fi ll
the previous region using the value 0 DEMO delete its location. Finally, fi ll the remain-
ing largest region DEMO 255. Display the results. We now have a single, solid mask DEMO
the coff ee mug.
For this exercise, use the mask created DEMO exercise 6 or create another mask of your
own (perhaps by DEMO a digital picture, or simply use a square). Load an DEMO
scene. Now use this mask with cvCopy(), to copy an DEMO of a mug into the scene.
Create a low-variance random image (DEMO a random number call such that the
numbers don’t diff er by much more than 3 and most numbers are near 0). Load DEMO
image into a drawing program such as PowerPoint and then draw a wheel of lines
meeting at a single point. Use bilateral fi ltering DEMO the resulting image and explain
the results.
Load an image of a scene and convert it to grayscale.
a. Run the morphological Top Hat DEMO on your image and display the
results.
b. Convert the resulting image into an 8-bit mask.
c. Copy a grayscale value into the Top DEMO pieces and display the results.
Load an image with many details.
| Chapter 5: Image Processing
6.
7.
8.
9.
10.
142
05-R4886-AT1.indd   142
9/15/08   4:20:09 PM
11.
12.
05-R4886-AT1.indd   143
a. Use cvResize() to reduce the DEMO by a factor of 2 in each dimension (hence
the image DEMO be reduced by a factor of 4). Do this three times and display the
results.
b. Now take the original image and use DEMO() to reduce it three times and
then display the results.
c. How are the two results diff erent? Why are the approaches diff erent?
Load an image of a scene. Use cvPyrSegmentation() and DEMO the results.
Load an image of an interesting or suffi  ciently DEMO scene. Using cvThreshold(),
set the threshold to 128. Use each setting type in Table 5-5 on the image and display
the results. DEMO should familiarize yourself with thresholding functions because
they will prove quite useful.
a. Repeat the exercise but use cvAdaptiveThreshold() instead. Set param1=5.
b. DEMO part a using param1=0 and then param1=-5.
Exercises
| 143
www.it-ebooks.info
9/15/08   4:20:09 PM
CHAPTER 6
Image Transforms
Overview
In the previous chapter we covered a DEMO of diff erent things you could do with an image.
Th
wise “process” one image into a similar but new image.
In this chapter DEMO will look at image transforms, which are methods for changing an
DEMO into an alternate representation of the data entirely. Perhaps the most common
example of a transform would be a something like a Fourier transform, in which the im-
age is converted to an alternate representation of DEMO data in the original image. Th e re-
sult of this operation is still stored in an OpenCV “image” structure, but the individual
“pixels” in this new image represent spectral components of the original input rather
DEMO the spatial components we are used to thinking about.
Th
OpenCV provides complete implementations of some of the more common ones as well
as DEMO blocks to help you implement your own image transforms.
Convolution
Convolution is the basis of many of the transformations that we discuss in this DEMO
In the abstract, this term means something we do to every DEMO of an image. In this
sense, many of the operations we DEMO at in Chapter 5 can also be understood as spe-
cial cases of the more general process of convolution. What a particular convolution
“does” DEMO determined by the form of the Convolution kernel being used. Th is kernel is
essentially just a fi xed size array of numerical coeffi  cients along with an anchor point
in that array, which is typically located at the center. Th e size of the array* is called DEMO
support of the kernel.
Figure 6-1 depicts a 3-by-3 convolution kernel with the anchor located at the center of
the array. Th e value DEMO the convolution at a particular point is computed by fi rst placing
* For technical purists, the support of the kernel actually consists of only the nonzero portion of the kernel
array.
144
e majority of DEMO operators presented thus far are used to enhance, modify, or other-
ere are a number of useful transforms that arise repeatedly in computer DEMO
06-R4886-RC1.indd   144
www.it-ebooks.info
9/15/08   4:21:12 PM
the kernel anchor on top of a pixel on the image with DEMO rest of the kernel overlaying
the corresponding local pixels in the image. For each kernel point, we now have a value
for the kernel at that point and a value for the image at the corresponding DEMO point.
We multiply these together and sum the result; this result DEMO then placed in the resulting
image at the location corresponding to the location of the anchor in the input image.
Th is process is DEMO for every point in the image by scanning the kernel over the
entire image.
Figure 6-1. A 3-by-3 kernel for a Sobel derivative; note that the anchor point is in the center of the
kernel
We DEMO, of course, express this procedure in the form of an equation. If we defi ne the
image to be I(x, y), the kernel to be G(i, j) (where 0 < i < Mi –1 and 0 < j < Mj –1), and DEMO
anchor point to be located at (ai, aj) in the DEMO of the kernel, then the convolu-
tion H(x, y) DEMO defi ned by the following expression:
Mi −1 M j −1
Hx y I x i a y j a G i j( , )(=+− +−∑ ∑ ij, )( , )
i=0 j=0
Observe that the number of operations, at least at fi rst glance, seems to be the number
of pixels in the image multiplied by the number of pixels in the kernel.* Th is can be a DEMO
of computation and so is not something you want to do with some “for” loop and a lot of
pointer de-referencing. In situations like DEMO, it is better to let OpenCV do the work for
you DEMO take advantage of the optimizations already programmed into OpenCV. Th e
OpenCV way to do this is with cvFilter2D():
void cvFilter2D(
DEMO CvArr*     src,
CvArr*           dst,
const CvMat*     kernel,
* We say “at fi DEMO glance” because it is also possible to perform convolutions in the frequency domain. In this
case, for an N-by-N image and an M-by-M kernel with N > M, the computational time will be proportional
to N2 log(N) and not to the N2M2 that is expected for computations in the spatial domain. Because the
frequency domain computation is independent of DEMO size of the kernel, it is more effi  cient for large kernels.
OpenCV automatically decides whether to do the convolution in the frequency DEMO based on the size of
the kernel.
Convolution
| 145
06-R4886-RC1.indd   145
www.it-ebooks.info
9/15/08   4:21:13 PM
www.it-ebooks.info
CvPoint          anchor = cvPoint(-1,-1)
);
Here we create a matrix of the appropriate size, fi ll it with the coeffi  cients, and then
pass it together DEMO the source and destination images into cvFilter2D(). We can also
DEMO pass in a CvPoint to indicate the location of the center of the kernel, but the
default value (equal to cvPoint(-1,-1)) is interpreted as indicating the center of the ker-
nel. Th e kernel can be of even size if its anchor point is defi DEMO; otherwise, it should be
of odd size.
Th e src and dst images should be the same size. One might think that the DEMO image
should be larger than the dst image in order to allow for the extra width and length
of the convolution kernel. But the DEMO of the src and dst can be the same in OpenCV
because, by default, prior to convolution OpenCV creates virtual pixels via replication
DEMO the border of the src image so that the border pixels in dst can be fi lled in. Th e rep-
lication is done DEMO input(–dx, y) = input(0, y), input(w DEMO dx, y) = input(w – 1, y), and DEMO
forth. Th ere are some alternatives to this default behavior; we DEMO discuss them in the
next section.
We remark that the coeffi  DEMO of the convolution kernel should always be fl oating-
point numbers. Th is means that you should use CV_32FC1 when allocating that matrix.
Convolution DEMO
One problem that naturally arises with convolutions is how to handle the boundaries.
For example, when using the convolution kernel just described, what DEMO when the
point being convolved is at the edge of the image? Most of OpenCV’s built-in functions
that make use of cvFilter2D() must handle this in one way or another. Similarly, when
doing your own convolutions, you will need to know how to deal with this effi  ciently.
Th cvCopyMakeBorder() function, which copies a given
image onto DEMO slightly larger image and then automatically pads the boundary in
one way or another:
void cvCopyMakeBorder(
const CvArr*   src,
CvArr*         dst,
CvPoint        offset,
DEMO            bordertype,
CvScalar       DEMO     = cvScalarAll(0)
);
Th offset argument tells cvCopyMakeBorder() where to place the copy of the original
image within DEMO destination image. Typically, if the kernel is N-by-N (for odd N) then
you will want a boundary that is (N – 1)/2 wide on all sides or, equivalently, an image
that is DEMO – 1 wider and taller than the original. In this case you would set the off set to
cvPoint((N-1)/2,(N-1)/DEMO) so that the boundary would be even on all sides.*
* DEMO course, the case of N-by-N with N odd and the anchor DEMO at the center is the simplest case. In gen-
eral, if DEMO kernel is N-by-M and the anchor is located at (ax, ay), then the destination image will have to be
N – 1 DEMO wider and M – 1 pixels taller than the source image. Th e off set will simply be (ax, ay).
146 | DEMO 6: Image Transforms
e solution comes in the form of the
DEMO
06-R4886-RC1.indd   146
9/15/08   4:21:13 PM
Th bordertype can be either IPL_BORDER_CONSTANT or IPL_BORDER_REPLICATE (see Figure 6-2).
In the fi rst case, the value argument will be interpreted as the value to which all pixels
in the boundary should be set. DEMO the second case, the row or column at the very edge DEMO
the original is replicated out to the edge of the larger image. Note that the border of the
test pattern image is somewhat subtle (examine the upper right image in Figure 6-2); in
the test DEMO image, there’s a one-pixel-wide dark border except where the circle pat-
DEMO come near the border where it turns white. Th ere are two other border types de-
fi ned, IPL_BORDER_REFLECT and IPL_BORDER_WRAP, which are DEMO implemented at this time
in OpenCV but may be supported in the future.
Figure 6-2. Expanding the image border. Th e left  column shows IPL_BORDER_CONSTANT where a
zero value is used to fi ll out the DEMO Th e right column shows IPL_BORDER_REPLICATE where
the border pixels are replicated in the horizontal and vertical directions
We mentioned previously that, when you make calls to OpenCV library functions that
employ convolution, those library functions call cvCopyMakeBorder() to get their work
done. In most cases the DEMO type called is IPL_BORDER_REPLICATE, but sometimes you
will not want it DEMO be done that way. Th is is another occasion where you might want to
use cvCopyMakeBorder(). You can create a slightly larger image with the border you want,
call whatever routine on that image, and then clip back out the part you were originally
interested in. DEMO is way, OpenCV’s automatic bordering will not aff ect the pixels DEMO
care about.
Convolution | 147
e
06-R4886-RC1.indd   147
www.it-ebooks.info
9/15/08   4:21:13 PM
Gradients and Sobel Derivatives
One of the most basic and important convolutions DEMO the computation of derivatives (or
approximations to them). Th ere DEMO many ways to do this, but only a few are well DEMO
to a given situation.
In general, the most common operator used DEMO represent diff erentiation is the Sobel de-
rivative [Sobel68] operator (see DEMO 6-3 and 6-4). Sobel operators exist for any order
of derivative as well as for mixed partial derivatives (e.g., ∂∂2 / xy∂ ).
Figure 6-3. Th e eff ect of the Sobel operator when DEMO to approximate a fi rst derivative in the
x-dimension
cvSobel(
const CvArr* src,
CvArr*       dst,
int          xorder,
int          yorder,
int          aperture_size = 3
);
Here, src DEMO dst are your image input and output, and xorder and yorder DEMO the orders
of the derivative. Typically you’ll use 0, 1, or at most 2; a 0 value indicates no derivative
148 | Chapter 6: Image Transforms
06-R4886-RC1.indd   148
www.it-ebooks.info
9/15/08   4:21:14 PM
www.it-ebooks.info
Figure 6-4. Th e eff ect of the Sobel operator when DEMO to approximate a fi rst derivative in the
y-dimension
in that direction.* Th e aperture_size parameter should be odd and is the width (and the
height) of the square fi lter. Currently, aperture_sizes of 1, 3, 5, and 7 are supported. If
src is 8-bit then DEMO dst must be of depth IPL_DEPTH_16S to avoid overfl ow.
Sobel derivatives have the nice property that they can be defi ned for kernels DEMO any
size, and those kernels can be constructed quickly and iteratively. DEMO e larger kernels
give a better approximation to the derivative because the smaller kernels are very sen-
sitive to noise.
To understand this more DEMO, we must realize that a Sobel derivative is not really a
DEMO at all. Th is is because the Sobel operator is defi ned on a discrete space. What
the Sobel operator actually represents is a DEMO t to a polynomial. Th at is, the Sobel deriva-
tive DEMO second order in the x-direction is not really a second derivative; DEMO is a local fi t to a
parabolic function. Th is explains why one might want to use a larger kernel: that larger
kernel is computing the fi t over a larger number of pixels.
* DEMO xorder or yorder must be nonzero.
Gradients and Sobel Derivatives
| 149
06-R4886-RC1.indd   149
9/15/08   4:21:14 PM
Scharr Filter
In fact, there are many ways to approximate a derivative in the case of a discrete grid.
Th
for small kernels. For DEMO kernels, where more points are used in the approximation,
this DEMO is less signifi cant. Th is inaccuracy does not show up directly for the X and
Y fi lters used in cvSobel(), because they are exactly aligned with the x- and y-axes. Th e
diffi  culty arises when you want to make image measurements that are approximations
DEMO directional derivatives (i.e., direction of the image gradient by using the arctangent of
the y/x fi lter responses).
To put this DEMO context, a concrete example of where you may want image measurements
DEMO this kind would be in the process of collecting shape information from an object
by assembling a histogram of gradient angles around the object. DEMO a histogram is
the basis on which many common shape classifi ers are trained and operated. In this
case, inaccurate measures of gradient angle will decrease the recognition performance
of the classifi er.
For a 3-by-3 DEMO fi lter, the inaccuracies are more apparent the further the gradient DEMO
is from horizontal or vertical. OpenCV addresses this inaccuracy for small (DEMO fast)
3-by-3 Sobel derivative fi lters by a somewhat obscure use of the special aperture_size
value CV_SCHARR in the cvSobel() function. Th DEMO Scharr fi lter is just as fast but more ac-
curate than the Sobel fi lter, so it should always be used if you want to make image mea-
surements using a 3-by-3 fi lter. Th DEMO fi lter coeffi  cients for the Scharr fi lter are shown DEMO
Figure 6-5 [Scharr00].
Figure 6-5. Th
e 3-by-3 Scharr fi
lter using fl
ag CV_SHARR
Laplace
Th Laplacian function (fi rst used in vision by Marr [Marr82]) implements a
discrete analog of the Laplacian operator:*
* Note that the Laplacian operator is completely distinct from the Laplacian DEMO of Chapter 5.
150
| Chapter 6: Image Transforms
e OpenCV
DEMO downside of the approximation used for the Sobel operator is that it is less accurate
06-R4886-RC1.indd   150
www.it-ebooks.info
9/15/08   4:DEMO:14 PM
www.it-ebooks.info
≡ ∂2 f + ∂2 f
Laplace( )f ∂x 2 DEMO 2
Because the Laplacian operator can be defi ned in terms of second derivatives, you might
well suppose that the discrete implementation works something like the second-order
Sobel derivative. Indeed it does, and in fact the OpenCV implementation of the Lapla-
cian operator uses the Sobel operators directly DEMO its computation.
void cvLaplace(
const CvArr* src,
CvArr*       dst,
int          apertureSize = 3
);
Th cvLaplace() function takes the usual source and destination images DEMO arguments as
well as an aperture size. Th e source can be either an 8-bit (unsigned) image or a 32-bit
(fl oating-point) DEMO Th e destination must be a 16-bit (signed) image or a 32-bit (fl oat-
ing-point) image. Th is aperture is precisely the DEMO as the aperture appearing in the
Sobel derivatives and, in eff DEMO, gives the size of the region over which the pixels are
DEMO in the computation of the second derivatives.
Th
detect “blobs.” Recall that the form of the Laplacian operator is a sum of second de-
DEMO along the x-axis and y-axis. Th is means that a single point or any small blob
(smaller than the aperture) that is surrounded DEMO higher values will tend to maximize
this function. Conversely, a point DEMO small blob that is surrounded by lower values will
tend to maximize the negative of this function.
With this in mind, the Laplace operator can also be used as a kind of edge detector. To
see DEMO this is done, consider the fi rst derivative of a function, which will (of course)
be large wherever the function is changing rapidly. Equally important, it will grow rap-
idly as we approach an edge-like discontinuity and shrink rapidly as we move past the
discontinuity. Hence DEMO derivative will be at a local maximum somewhere within this
range. Th erefore we can look to the 0s of the second derivative for DEMO of such local
maxima. Got that? Edges in the original image DEMO be 0s of the Laplacian. Unfortu-
nately, both substantial and less DEMO edges will be 0s of the Laplacian, but this is
not DEMO problem because we can simply fi lter out those pixels that also have larger values
of the fi rst (Sobel) derivative. Figure 6-6 DEMO an example of using a Laplacian on an
image together with details of the fi rst and second derivatives and their zero crossings.
e
DEMO Laplace operator can be used in a variety of contexts. A common application is to
Canny
Th nding edges was further refi ned by DEMO Canny in 1986 into
what is now commonly called the Canny edge detector [Canny86]. One of the diff erences
between the Canny algorithm and DEMO simpler, Laplace-based algorithm from the previ-
ous section is that, in the Canny algorithm, the fi rst derivatives are computed in x and y
and then combined into four directional derivatives. Th e points where DEMO directional
derivatives are local maxima are then candidates for assembling into edges.
Canny
| 151
e method just described for fi
06-R4886-RC1.indd   151
DEMO/15/08   4:21:15 PM
www.it-ebooks.info
Figure 6-6. Laplace transform (upper right) of the racecar image: zooming in on the tire (circled in
white) and considering only DEMO x-dimension, we show a (qualitative) representation of the bright-
ness DEMO well as the fi rst and second derivative (lower three cells); the 0s in the second derivative corre-
spond to edges, and DEMO 0 corresponding to a large fi rst derivative is a strong edge
However, the most signifi cant new dimension to the Canny algorithm is that it tries to
assemble the individual edge candidate pixels into contours.* DEMO ese contours are formed
by applying an hysteresis threshold to the pixels. Th is means that there are two thresh-
olds, an upper and a lower. If a pixel has a gradient larger than the upper DEMO,
then it is accepted as an edge pixel; if a DEMO is below the lower threshold, it is rejected.
If the pixel’s DEMO is between the thresholds, then it will be accepted only if DEMO is
connected to a pixel that is above the high threshold. Canny recommended a ratio of
high:low threshold between 2:1 and 3:DEMO Figures 6-7 and 6-8 show the results of applying
cvCanny() to a test pattern and a photograph using high:low hysteresis threshold ratios
DEMO 5:1 and 3:2, respectively.
void cvCanny(
const CvArr* DEMO,
CvArr*       edges,
double       lowThresh,
double       highThresh,
int          apertureSize = 3
);
* We’ll have much more to say DEMO contours later. As you await those revelations, though, keep in mind that
the cvCanny() routine does not actually return objects of type DEMO; we will have to build those from
the output of cvCanny() if we want them by using cvFindContours(). Everything you ever DEMO to know
about contours will be covered in Chapter 8.
152
| Chapter 6: Image Transforms
06-R4886-RC1.indd   152
9/15/08   4:21:15 PM
www.it-ebooks.info
Figure 6-7. Results of Canny edge detection for two diff erent DEMO when the high and low thresh-
olds are set to 50 and 10, respectively
Th cvCanny() function expects an input image, which DEMO be grayscale, and an output
image, which must also be grayscale (but which will actually be a Boolean image). Th e
next two arguments are the low and high thresholds, and the last argument is another
aperture. As usual, this is the aperture used by the Sobel derivative operators that are
called inside of the implementation of cvCanny().
Hough Transforms
Th e Hough transform* is a method for fi DEMO lines, circles, or other simple forms in an
image. Th e original Hough transform was a line transform, which is a relatively fast way
of searching a binary image for straight lines. Th e transform DEMO be further generalized
to cases other than just simple lines.
Hough Line Transform
e basic theory of the Hough line transform is that any DEMO in a binary image could
Th
be part of some set of possible lines. If we parameterize each line by, for example, a
DEMO Hough developed the transform for use in physics experiments [Hough59]; its DEMO in vision was introduced
by Duda and Hart [Duda72].
Hough Transforms | 153
e
06-R4886-RC1.indd   153
9/15/08   4:21:15 DEMO
www.it-ebooks.info
Figure 6-8. Results of Canny edge detection for two diff erent DEMO when the high and low thresh-
olds are set to 150 and 100, respectively
slope a and an intercept b, then a point DEMO the original image is transformed to a locus
of points in the (a, b) plane corresponding to all of the lines passing through that point
(see Figure 6-9). If we convert every nonzero pixel in the input image into such a set of
points in the DEMO image and sum over all such contributions, then lines that appear
DEMO the input (i.e., (x, y) plane) image will appear as local maxima in the output (i.e.,
(a, b) DEMO) image. Because we are summing the contributions from each point, the
(a, b) plane is commonly called the accumulator plane.
It might occur to you that the slope-intercept form is not really the best DEMO to repre-
sent all of the lines passing through a point (DEMO of the considerably diff erent den-
sity of lines as a function of the slope, and the related fact that the interval of possible
slopes goes from –∞ to +∞). It is for this reason DEMO the actual parameterization of the
transform image used in numerical computation is somewhat diff erent. Th e preferred
parameterization represents each line as a DEMO in polar coordinates (ρ, θ), with the
implied line being the line passing through the indicated point but perpendicular to the
radial DEMO the origin to that point (see Figure 6-10). Th e DEMO for such a line is:
ρθ θ=+xycos sin
154 | Chapter 6: Image Transforms
06-R4886-RC1.indd   154
9/15/08   4:21:15 PM
www.it-ebooks.info
Figure 6-9. Th e Hough line transform fi nds many lines DEMO each image; some of the lines found are
expected, but others may not be
Figure 6-10. A point (x0, y0) in the image plane (panel a) implies many lines each parameterized by
a DEMO erent ρ and θ (panel b); these lines each imply DEMO in the (ρ, θ) plane, which taken together
form a curve of characteristic shape (panel c)
Hough Transforms
| 155
06-R4886-RC1.indd   155
9/15/08   4:21:16 PM
www.it-ebooks.info
Th
to the user. Instead, it simply returns the local maxima in the (ρ, θ) plane. However,
you will need to understand this process in order to understand the arguments to the
OpenCV DEMO line transform function.
OpenCV supports two diff erent kinds of Hough line transform: the standard Hough
transform (SHT) [Duda72] and the progressive probabilistic Hough transform (PPHT).*
Th
that, among other things, computes an extent for individual lines in addition to the
orientation (as shown in Figure 6-11). It is “probabilistic” because, rather than accu-
mulating every possible point in the accumulator plane, it accumulates only a fraction
of them. Th e idea is that if the peak is going to DEMO high enough anyhow, then hitting it
only a fraction of the DEMO will be enough to fi nd it; the result of this DEMO can be
a substantial reduction in computation time. Both of these algorithms are accessed with
the same OpenCV function, though the meanings of some of the arguments depend on
which method is being used.
CvSeq* cvHoughLines2(DEMO
CvArr* image,
void*  line_storage,
int    method,
double rho,
double theta,
int    threshold,
double param1      = 0,
double param2      = 0
);DEMO
Th rst argument is the input image. It must be an 8-bit image, but the input is treated
as binary information (i.e., all nonzero pixels are considered to be equivalent). Th e sec-
ond DEMO is a pointer to a place where the results can be stored, which can be either
a memory storage (see CvMemoryStorage in Chapter DEMO) or a plain N-by-1 matrix array (the
number of rows, DEMO, will serve to limit the maximum number of lines returned). DEMO e
next argument, method, can be CV_HOUGH_STANDARD, CV_HOUGH_PROBABILISTIC, or CV_HOUGH_
MULTI_SCALE  for (respectively) SHT, PPHT, or a multiscale variant of SHT.
Th rho and theta, set the resolution desired for the lines (i.e., the
resolution of the accumulator plane). Th e units DEMO rho are pixels and the units of theta
are radians; thus, the accumulator plane can be thought of as a two-dimensional his-
togram DEMO cells of dimension rho pixels by theta radians. Th e threshold value is the
value in the accumulator plane that must be reached for DEMO routine to report a line.
Th is last argument is a bit tricky in practice; it is not normalized, so you should expect
DEMO scale it up with the image size for SHT. Remember that this argument is, in eff ect,
indicating the number of points (DEMO the edge image) that must support the line for the
line DEMO be returned.
* Th e “probablistic Hough transform” (PHT) was introduced by Kiryati, Eldar, and Bruckshtein in 1991
[Kiryati91]; the PPHT was introduced by Matas, Galambosy, and Kittler in 1999 [Matas00].
156
| DEMO 6: Image Transforms
e OpenCV Hough transform algorithm does not make DEMO computation explicit
e SHT is the algorithm we just looked at. Th e PPHT is a variation of this algorithm
e fi
e next DEMO arguments,
06-R4886-RC1.indd   156
9/15/08   4:21:16 PM
www.it-ebooks.info
Figure 6-11. Th e Canny edge detector (param1=50, param2=150) is run fi rst, with the results shown
in gray, and the DEMO probabilistic Hough transform (param1=50, param2=10) is run next,
with DEMO results overlayed in white; you can see that the strong lines DEMO generally picked up by the
Hough transform
Th param1 and param2 arguments are not used by the SHT. For the PPHT, param1 sets
the minimum length of a line segment that will be returned, and param2 sets the sep-
aration between collinear segments required for the algorithm not DEMO join them into
a single longer segment. For the multiscale HT, DEMO two parameters are used to indi-
cate higher resolutions to which the parameters for the lines should be computed. Th e
multiscale HT fi DEMO computes the locations of the lines to the accuracy given by the rho
and theta parameters and then goes on to refi ne those DEMO by a factor of param1 and
param2, respectively (i.e., the DEMO nal resolution in rho is rho divided by param1 and the fi nal
resolution in theta is theta divided by param2).
What the DEMO returns depends on how it was called. If the line_storage value was
a matrix array, then the actual return value will be NULL. In this case, the matrix should
be of type CV_32FC2 if the SHT or multi-scale HT is being used and should be CV_32SC4 if
the DEMO is being used. In the fi rst two cases, the ρ- DEMO θ-values for each line will be
placed in the two channels of the array. In the case of the PPHT, the four channels will
hold the x- and y-values of the start and endpoints of the DEMO segments. In all of
these cases, the number of rows in DEMO array will be updated by cvHoughLines2() to cor-
rectly refl ect the number of lines returned.
Hough Transforms | 157
e
06-R4886-RC1.indd   DEMO
9/15/08   4:21:16 PM
www.it-ebooks.info
If the line_storage value was a pointer to a memory store,DEMO then the return value will
be a pointer to a CvSeq sequence structure. In that case, you can get each line or line seg-
ment from the sequence with a command like
float* line = (float*) cvGetSeqElem( lines , i );
where lines is the return DEMO from cvHoughLines2() and i is index of the line of inter-
est. In this case, line will be a pointer to the data for that line, with line[0] and line[1]
being the fl oating-point values ρ and θ (for SHT and MSHT) or CvPoint structures for
DEMO endpoints of the segments (for PPHT).
Hough Circle Transform
Th DEMO circle transform [Kimme75] (see Figure 6-12) works in a manner roughly
analogous to the Hough line transforms just described. Th e reason it DEMO only “roughly”
is that—if one were to try doing the exactly analogous thing—the accumulator plane
would have to be replaced with an accumulator volume DEMO three dimensions: one for
x, one for y, and another DEMO the circle radius r. Th is would mean far greater memory
requirements and much slower speed. Th e implementation of the circle transform
in DEMO avoids this problem by using a somewhat more tricky method called the
Hough gradient method.
Th
detection phase (in this case, cvCanny())DEMO Next, for every nonzero point in the edge image,
the DEMO gradient is considered (the gradient is computed by fi rst computing DEMO fi rst-
order Sobel x- and y-derivatives via cvSobel()). Using this gradient, every point along
the line indicated by this slope—from a specifi ed minimum to a specifi ed maximum
distance—is incremented in the DEMO At the same time, the location of every
one of these DEMO pixels in the edge image is noted. Th e candidate centers are then
selected from those points in this (two-dimensional) accumulator that are DEMO above
some given threshold and larger than all of their immediate neighbors. Th ese candidate
centers are sorted in descending order of their accumulator DEMO, so that the centers
with the most supporting pixels appear fi DEMO Next, for each center, all of the nonzero
pixels (recall DEMO this list was built earlier) are considered. Th ese pixels are DEMO ac-
cording to their distance from the center. Working out from the smallest distances to
the maximum radius, a single radius is selected that is best supported by the nonzero
pixels. A center is kept if DEMO has suffi  cient support from the nonzero pixels in the edge
DEMO and if it is a suffi  cient distance from any previously DEMO center.
Th is implementation enables the algorithm to run much faster and, perhaps more im-
portantly, helps overcome the problem of the otherwise DEMO population of a three-
dimensional accumulator, which would lead to a DEMO of noise and render the results
unstable. On the other hand, DEMO algorithm has several shortcomings that you should
be aware of.
* We have not yet introduced the concept of a memory store or a DEMO, but Chapter 8 is devoted to this
topic.
158
| Chapter DEMO: Image Transforms
e
e Hough gradient method works as follows. First DEMO image is passed through an edge
06-R4886-RC1.indd   158
9/15/08   4:21:17 PM
www.it-ebooks.info
Figure 6-12. Th
fi
e Hough circle transform fi
nds none DEMO the photograph
nds some of the circles in the test pattern and (correctly)
First, the use of the Sobel derivatives to compute DEMO local gradient—and the attendant
assumption that this can be considered equivalent to a local tangent—is not a numeri-
cally stable proposition. It might be DEMO “most of the time,” but you should expect this
to generate some noise in the output.
Second, the entire set of nonzero pixels in the edge image is considered for every can-
didate center; hence, if you make the accumulator threshold too low, the algorithm will
DEMO a long time to run. Th ird, because only one circle DEMO selected for every center, if
there are concentric circles then you DEMO get only one of them.
Finally, because centers are considered in DEMO order of their associated accu-
mulator value and because new centers are not kept if they are too close to previously
accepted centers, there is a bias toward keeping the larger circles when multiple circles
are DEMO or approximately concentric. (It is only a “bias” because of the DEMO
arising from the Sobel derivatives; in a smooth image at infi DEMO resolution, it would
be a certainty.)
With all of that DEMO mind, let’s move on to the OpenCV routine that does all DEMO for us:
CvSeq* cvHoughCircles(
CvArr* image,
Hough Transforms
| 159
06-R4886-RC1.indd   159
9/15/08   4:21:17 PM
www.it-ebooks.info
void*  circle_storage,
int    method,
double dp,
DEMO min_dist,
double param1     = 100,
double param2     = 300,
int    min_radius = 0,
int    max_radius = 0
);
Th cvHoughCircles() has similar arguments to the
line transform. Th e input image is again an 8-bit image. DEMO signifi cant diff erence be-
tween cvHoughCircles() and cvHoughLines2() is that the latter requires a binary image.
Th cvHoughCircles() function will DEMO (automatically) call cvSobel()* for you, so
you can provide a more general grayscale image.
Th
would like the results returned. If DEMO array is used, it should be a single column of type
DEMO; the three channels will be used to encode the location of DEMO circle and its
radius. If memory storage is used, then the DEMO will be made into an OpenCV se-
quence and a pointer to that sequence will be returned by cvHoughCircles(). (Given an
array DEMO value for circle_storage, the return value of cvHoughCircles() is NULL.) Th e
method argument must always be set to CV_HOUGH_GRADIENT.
Th dp DEMO the resolution of the accumulator image used. Th is parameter allows
us to create an accumulator of a lower resolution than the input image. (It makes sense
to do this because there is no reason to DEMO the circles that exist in the image to fall
naturally into the same number of categories as the width or height of the image DEMO)
If dp is set to 1 then the resolutions will be the same; if set to a larger number (e.g., 2),DEMO
then the accumulator resolution will be smaller by that factor (in DEMO case, half). Th e
value of dp cannot be less DEMO 1.
Th min_dist is the minimum distance that must exist between two circles in
order for the algorithm to consider them distinct circles.
For DEMO (currently required) case of the method being set to CV_HOUGH_GRADIENT, DEMO next
two arguments, param1 and param2, are the edge (Canny) threshold and the accumula-
tor threshold, respectively. You may recall that the Canny edge detector actually takes
two diff erent thresholds itself. When cvCanny() is called internally, the fi rst (higher)
threshold is set to the value of param1 passed into cvHoughCircles(), and the second
(lower) threshold is set to exactly half that value. Th e DEMO param2 is the one used
to threshold the accumulator and is exactly analogous to the threshold argument of
cvHoughLines().
Th nal two parameters are the minimum and maximum radius of circles that can be
found. DEMO is means that these are the radii of circles for which the accumulator has a rep-
resentation. Example 6-1 shows an example program using DEMO().
* Th
e function cvSobel(), not cvCanny(), DEMO called internally. Th
estimate the orientation of a gradient at each pixel, and this is diffi
e reason is that cvHoughCircles() needs to
cult to do with binary edge map.
160
| Chapter 6: Image Transforms
e Hough circle transform function
e
e circle_storage can be either DEMO array or memory storage, depending on how you
e parameter
e DEMO
e fi
06-R4886-RC1.indd   160
9/15/08   4:21:17 PM
www.it-ebooks.info
Example 6-1. Using cvHoughCircles to return a sequence of circles found DEMO a grayscale image
#include <cv.h>
#include <highgui.h>
#include <DEMO>
int main(int argc, char** argv) {
IplImage* image = cvLoadImage(
argv[1],
CV_LOAD_IMAGE_GRAYSCALE
);
CvMemStorage* storage = cvCreateMemStorage(0);
cvSmooth(image, image, CV_GAUSSIAN, 5, 5 );
CvSeq* DEMO = cvHoughCircles(
image,
storage,
CV_HOUGH_GRADIENT,
2,
image->width/10
);
for( int i = 0; i < DEMO>total; i++ ) {
float* p = (float*) cvGetSeqElem( results, i );
CvPoint pt = cvPoint( cvRound( p[0] ), DEMO( p[1] ) );
cvCircle(
image,
pt,
cvRound( p[2] ),
CV_RGB(0xff,0xff,0xff)
);
}
cvNamedWindow( “cvHoughCircles”, 1 );
cvShowImage( “cvHoughCircles”, image);
cvWaitKey(0);
}
It is worth refl ecting momentarily on the fact that, no matter what tricks we employ,
there is no getting around DEMO requirement that circles be described by three degrees
of freedom (x, y, and r), in contrast to only two degrees of freedom (ρ and θ) for lines.
Th nding algorithm requires more memory
DEMO computation time than the line-fi nding algorithms we looked at previously. With
this in mind, it’s a good idea to bound the radius parameter as tightly as circumstances
allow in order to keep these costs under DEMO Th e Hough transform was extended
to arbitrary shapes by Ballard in 1981 [Ballard81] basically by considering objects as col-
lections of gradient edges.
DEMO Although cvHoughCircles() catches centers of the circles quite well, it DEMO fails to fi
radius. Th
can be used to fi
nd the correct
erefore, in an application where only a center must be found (or where some diff
nd the actual radius), the radius returned by cvHoughCircles() can be ignored.
erent technique
Hough Transforms
| 161
DEMO result will invariably be that any circle-fi
06-R4886-RC1.indd   161
9/15/08   4:21:18 PM
www.it-ebooks.info
Remap
Under the hood, many of the transformations to follow have a certain common element.
In particular, they will be taking pixels from one place in the image and mapping them
to another place. In DEMO case, there will always be some smooth mapping, which will do
what we need, but it will not always be a one-to-one pixel correspondence.
We sometimes want to accomplish this interpolation programmatically; that is, DEMO
like to apply some known algorithm that will determine the mapping. In other cases,
however, we’d like to do this mapping ourselves. Before diving into some methods that
will compute (and apply) these mappings DEMO us, let’s take a moment to look at the func-
tion DEMO for applying the mappings that these other methods rely upon. Th e
OpenCV function we want is called cvRemap():
void cvRemap(
DEMO CvArr* src,
CvArr*       dst,
const CvArr* mapx,
const CvArr* mapy,
int          flags   = CV_INTER_LINEAR | CV_WARP_FILL_OUTLIERS,
CvScalar     fillval = cvScalarAll(0)DEMO
);
Th rst two arguments of cvRemap() are the source and destination images, respec-
tively. Obviously, these should be of the DEMO size and number of channels, but they
can have any data DEMO It is important to note that the two may not be the same image.*
Th
located. Th ese should be the same size as DEMO source and destination images, but they
are single-channel and usually of DEMO type float (IPL_DEPTH_32F). Noninteger mappings
are OK, and cvRemap() will do the interpolation calculations for you automatically. One
common use of DEMO() is to rectify (correct distortions in) calibrated and stereo im-
ages. We will see functions in Chapters 11 and 12 that convert DEMO camera distor-
tions and alignments into mapx and mapy parameters. Th e next argument contains fl ags
that tell cvRemap() exactly how that DEMO is to be done. Any one of the values
listed in Table 6-1 will work.
Table 6-1. cvWarpAffi
ne() additional fl
ags values
DEMO values
CV_INTER_NN
CV_INTER_LINEAR
CV_INTER_AREA
CV_INTER_CUBIC
Meaning
Nearest neighbor
Bilinear (default)
DEMO area resampling
Bicubic interpolation
* A moment’s thought will make it clear why the most effi  cient remapping strategy is incompatible with writ-
ing onto the source image. Aft er all, if you move pixel A to location B then, when you get to location B and
want to move it to location C, you will fi nd that you’ve already written over the original value of B with A!
162
| DEMO 6: Image Transforms
e fi
e next two arguments, mapx and mapy, indicate where any particular pixel is to be re-
06-R4886-RC1.indd   162
9/15/08   4:21:18 PM
www.it-ebooks.info
Interpolation is an important issue here. Pixels in the source image DEMO on an integer grid;
for example, we can refer to DEMO pixel at location (20, 17). When these integer locations
are mapped to a new image, there can be gaps—either because the integer source pixel
locations are mapped to fl oat locations in the destination DEMO and must be rounded
to the nearest integer pixel location or because there are some locations to which no
pixels at all are mapped (think about doubling the image size by stretching it; then ev-
ery other destination pixel would be left  blank). Th ese problems are generally referred
to as forward projection problems. To deal with such rounding DEMO and destina-
tion gaps, we actually solve the problem backwards: we step through each pixel of the
destination image and ask, “Which pixels in the source are needed to fi ll in this des-
tination DEMO?” Th ese source pixels will almost always be on fractional pixel locations
so we must interpolate the source pixels to derive the correct DEMO for our destination
value. Th e default method is bilinear interpolation, DEMO you may choose other methods
(as shown in Table 6-1).
DEMO may also add (using the OR operator) the fl ag CV_WARP_FILL_OUTLIERS, whose eff ect
is to fi ll pixels in the destination image that are not the destination of any pixel in the
input image DEMO the value indicated by the fi nal argument fillval. In this way, if you
map all of your image to a circle in the center then the outside of that circle would auto-
matically be fi DEMO with black (or any other color that you fancy).
Stretch, Shrink, Warp, and Rotate
In this section we turn to geometric DEMO of images.* Such manipulations in-
clude stretching in various ways, which DEMO both uniform and nonuniform resizing
(the latter is known as warping)DEMO Th ere are many reasons to perform these operations:
for example, warping and rotating an image so that it can be superimposed on a wall in
an existing scene, or artifi cially enlarging a set of training images used for object recog-
nition.† Th e functions that DEMO stretch, shrink, warp, and/or rotate an image are called
DEMO transforms (for an early exposition, see [Semple79]). For planar areas, there
are two fl avors of geometric transforms: transforms that use DEMO 2-by-3 matrix, which are
called affi  ne transforms; and transforms DEMO on a 3-by-3 matrix, which are called per-
spective transforms or DEMO You can think of the latter transformation as a
method for computing the way in which a plane in three dimensions is perceived by DEMO
particular observer, who might not be looking straight on at that DEMO
An affi  ne transformation is any transformation that can be expressed DEMO the form of a
matrix multiplication followed by a vector addition. In OpenCV the standard style of
representing such a transformation is as a DEMO matrix. We defi ne:
* We will cover these transformations in detail here; we will return to them when we discuss (in DEMO 11)
how they can be used in the context of three-dimensional vision techniques.
† Th is activity might seem a bit dodgy; aft er all, wouldn’t it be better just to use a recognition method that’s
invariant to local affi  ne distortions? Nonetheless, this method has a long history and still can be quite useful
in practice.
Stretch, Shrink, Warp, and Rotate
| 163
06-R4886-RC1.indd   163
9/15/DEMO   4:21:18 PM
www.it-ebooks.info
AB T X≡ ⎣⎡⎢aaaa00 01 ⎦⎤⎥ ≡ ⎣⎢b0 ⎦⎥ ≡ ⎣⎡
DEMO ⎤ ⎡ ⎤
AB⎦⎤ ≡ ⎢ ⎥
10 11 1 ⎣ ⎦
x
y
It is easily seen that the eff ect of the DEMO  ne transformation A · X + B is exactly equivalent
to DEMO the vector X into the vector X´ and simply left -multiplying X´ by T.
Affi  ne transformations can be visualized as follows. Any parallelogram ABCD in a
plane can be mapped to any other parallelogram A'DEMO'C'D' by some affi  ne transforma-
tion. If the areas of these parallelograms are nonzero, then the implied affi  ne transfor-
DEMO is defi ned uniquely by (three vertices of) the two parallelograms. If you like, you
can think of an affi  ne transformation DEMO drawing your image into a big rubber sheet and
then deforming the sheet by pushing or pulling* on the corners to make diff erent DEMO
of parallelograms.
When we have multiple images that we know to be slightly diff erent views of the same
object, we might want to compute the actual transforms that relate the diff erent views.
In this DEMO, affi  ne transformations are oft en used to model the views because, having
fewer parameters, they are easier to solve for. Th DEMO downside is that true perspective
distortions can only be modeled by a homography,† so affi  ne transforms yield a repre-
sentation that cannot accommodate all possible relationships between the views. On the
other hand, for small changes in viewpoint the resulting distortion is affi  ne, so DEMO some
circumstances an affi  ne transformation may be suffi  cient.
Affi  ne transforms can convert rectangles to parallelograms. Th ey can squash the shape
but must keep the sides parallel; they can rotate it and/or scale it. Perspective transfor-
mations off er more fl exibility; a perspective transform can turn a rectangle into a trap-
ezoid. Of course, since parallelograms are also trapezoids, affi  ne transformations are a
subset DEMO perspective transformations. Figure 6-13 shows examples of various affi  ne and
DEMO transformations.
Affine Transform
Th  ne transformations. In the fi rst
case, we have an image (or a region of interest) we’d like DEMO transform; in the second case,
we have a list of DEMO for which we’d like to compute the result of a transformation.
Dense affine transformations
In the fi rst case, the obvious input and output formats are images, and the implicit
requirement is that the warping assumes the pixels are a dense representation of the
* One can even DEMO in such a manner as to invert the parallelogram.
† “Homography” is the mathematical term for mapping points on one surface to points on DEMO In this
sense it is a more general term than as used here. In the context of computer vision, homography almost
always refers to mapping between points on two image planes that correspond to the same DEMO on
a planar object in the real world. It can be shown that such a mapping is representable by a single 3-by-3
orthogonal matrix (more on this in Chapter 11).
164 | Chapter 6: Image Transforms
⎡
⎢
X′≡ ⎢
⎣⎢
x
y
1
⎤
⎥
⎥
DEMO
ere are two situations that arise when working with affi
06-R4886-RC1.indd   164
9/15/08   4:21:18 PM
www.it-ebooks.info
Figure 6-13. Affi  ne and perspective transformations
underlying image. Th is means that image warping must necessarily handle interpola-
tions so that the DEMO images are smooth and look natural. Th e affi  ne transformation
DEMO provided by OpenCV for dense transformations is cvWarpAffine().
void cvWarpAffine(DEMO
const CvArr* src,
CvArr*       dst,
const CvMat* map_matrix,
int          flags      = DEMO | CV_WARP_FILL_OUTLIERS,
CvScalar     fillval    = cvScalarAll(0)DEMO
);
Here src and dst refer to an array or image, which can be either one or three channels
and of any type (provided they are the same type and size).* Th e map_matrix is the 2-by-3
matrix we introduced earlier that quantifi es the desired DEMO Th e next-to-
last argument, flags, controls the interpolation method as well as either or both of the
following additional options (as usual, combine with Boolean OR).
CV_WARP_FILL_OUTLIERS
Oft en, the transformed src DEMO does not fi t neatly into the dst image—there are
pixels “mapped” there from the source fi le that don’t actually exist. If this DEMO ag is set,
then those missing values are fi lled with fillval (described previously).
CV_WARP_INVERSE_MAP
Th is fl ag is for convenience to allow inverse warping from dst to src instead of from
src DEMO dst.
* Since rotating an image will usually make its bounding box larger, the result will be a clipped image. You
can circumvent this either by shrinking the image (as in the example code) or DEMO copying the fi rst image to a
central ROI within a larger source image prior to transformation.
Stretch, Shrink, Warp, and Rotate | 165
06-R4886-RC1.indd   165
9/15/08   4:21:18 PM
www.it-ebooks.info
cVWarpAffine performance
It is worth knowing that cvWarpAffine() involves substantial DEMO overhead.
An alternative is to use cvGetQuadrangleSubPix(). Th is function DEMO fewer options but
several advantages. In particular, it has less overhead DEMO can handle the special case
of when the source image is 8-bit and the destination image is a 32-bit fl oating-point
image. It will DEMO handle multichannel images.
void cvGetQuadrangleSubPix(
const CvArr* src,
CvArr*       dst,
const CvMat* map_matrix
);
What cvGetQuadrangleSubPix() DEMO is compute all the points in dst by mapping
them (with DEMO) from the points in src that were computed by applying the
DEMO  ne transformation implied by multiplication by the 2-by-3 map_matrix. (Conver-
sion of the locations in dst to homogeneous coordinates for the multiplication is DEMO
automatically.)
One idiosyncrasy of cvGetQuadrangleSubPix() is that there is an additional mapping ap-
plied by the function. In particular, the result points in dst are computed according to
the formula:
where:
dst DEMO(, ) (
xy a x a y b a x DEMO + +00 01 0 10 11′′ ′′ ′′, yb′′ + 1 )
Observe that the mapping from (x, y) to (x˝, y˝) has the eff ect that—even if the map-
ping M is an identity mapping—the points in the destination image at the center will
DEMO taken from the source image at the origin. If cvGetQuadrangleSubPix() needs points
from outside the image, it uses replication to reconstruct those values.
Computing the affine map matrix
OpenCV provides two functions to help you DEMO the map_matrix. Th e fi rst is used
when you already have two images that you know to be related by an affi  ne transforma-
tion or that you’d like to approximate in that way:
DEMO cvGetAffineTransform(
const CvPoint2D32f* pts_src,
const CvPoint2D32f* pts_dst,
CvMat*              map_matrix
);
166
| Chapter 6: Image Transforms
⎡ ⎤
Mmap ≡ ⎢ 00 01 0 ⎥ and
⎣ 10 11 1 ⎦
aa b
aa b
⎡
⎢
⎣
DEMO ′′
y ′′
⎡
⎤ ⎢
⎥ = ⎢
⎦ ⎢
⎣⎢
x − (( ) )width dst −1
2
y − (( ))height dst −1
2
⎤
⎥
⎥
⎥
⎦⎥
06-R4886-RC1.indd   DEMO
9/15/08   4:21:19 PM
www.it-ebooks.info
Here src and dst are arrays containing three two-dimensional (x, DEMO) points, and the
map_matrix is the affi  ne transform computed DEMO those points.
Th pts_src and pts_dst in cvGetAffineTransform() are just arrays of three points defi n-
ing two parallelograms. Th e simplest way DEMO defi ne an affi  ne transform is thus to set
pts_src DEMO three* corners in the source image—for example, the upper and lower DEMO
together with the upper right of the source image. Th e mapping from the source to
destination image is then entirely defi ned by DEMO pts_dst, the locations to which
these three points will be mapped DEMO that destination image. Once the mapping of these
three independent corners (DEMO, in eff ect, specify a “representative” parallelogram) is
established, all the other points can be warped accordingly.
Example 6-2 shows some code DEMO uses these functions. In the example we obtain the
cvWarpAffine() matrix parameters by fi rst constructing two three-component arrays of
points (the corners of our representative parallelogram) and then convert that to the
actual transformation matrix using cvGetAffineTransform(). We then do an affi  ne warp
DEMO by a rotation of the image. For our array of representative points in the source
image, called srcTri[], we take the three points: (0,0), (0,height-1), and (width-1,0). We
then specify the locations to which these points will be mapped in DEMO corresponding
array srcTri[].
Example 6-2. An affi
ne transformation
// Usage: warp_affine <image>
//
#include <cv.h>
#include <highgui.h>
int main(int argc, char** argv)
{
CvPoint2D32f srcTri[3], dstTri[3];DEMO
CvMat*       rot_mat = cvCreateMat(2,3,CV_32FC1);
DEMO       warp_mat = cvCreateMat(2,3,CV_32FC1);
IplImage     *src, *dst;
if( argc == 2 && ((DEMO(argv[1],1)) != 0 )) {
dst = cvCloneImage( src );
dst->origin = src->origin;
cvZero( dst );
// Compute warp matrix
//
srcTri[0].x = 0;                 //src Top left
srcTri[0].y = 0;
srcTri[1].x = src->width - 1;    //src Top DEMO
srcTri[1].y = 0;
srcTri[2].x = 0;                 //src Bottom left offset
srcTri[2].y = src->height - 1;
* We need just three points because, for an affi  ne transformation, we are only representing a parallelogram.
We will DEMO four points to represent a general trapezoid when we address perspective transformations.
Stretch, Shrink, Warp, and Rotate
| 167
e
06-R4886-RC1.indd   167
9/15/08   4:21:19 PM
www.it-ebooks.info
Example 6-2. An affi  ne transformation (continued)
dstTri[0].x = DEMO>width*0.0;    //dst Top left
dstTri[0].y = src->height*0.33;
dstTri[1].x = src->width*0.85;   //dst Top right
dstTri[1].y = src->height*0.25;
dstTri[2].x = src->width*0.15;   //dst Bottom left offset
dstTri[2].y = src->height*0.7;
cvGetAffineTransform( srcTri, dstTri, warp_mat );
cvWarpAffine( src, dst, warp_mat );
cvCopy( dst, src );
// Compute rotation matrix
//
CvPoint2D32f center = cvPoint2D32f(
src->width/2,
src->height/2
);
double angle = DEMO;
double scale = 0.6;
cv2DRotationMatrix( center, angle, scale, rot_mat );
// Do the transformation
//
cvWarpAffine( src, dst, rot_mat );
cvNamedWindow( “Affine_Transform”, 1 );
cvShowImage( DEMO, dst );
cvWaitKey();
}
cvReleaseImage( &dst );DEMO
cvReleaseMat( &rot_mat );
cvReleaseMat( &warp_mat );
return 0;
}
}
Th map_matrix is to use cv2DRotationMatrix(), which com-
putes the map matrix for a rotation around some arbitrary point, combined with an op-
tional rescaling. Th is is just one possible kind DEMO affi  ne transformation, but it represents
an important subset that has an alternative (and more intuitive) representation that’s
easier to work with DEMO your head:
CvMat* cv2DRotationMatrix(
CvPoint2D32f center,
double       angle,
double       scale,
CvMat*       map_matrix
);
Th rst argument, center, is the center point of the rotation. Th e next two arguments
give the magnitude of DEMO rotation and the overall rescaling. Th e fi nal argument is the
output map_matrix, which (as always) is a 2-by-3 matrix of fl oating-point numbers).
168
| Chapter 6: Image Transforms
e second way to compute the
e fi
06-R4886-RC1.indd   168
9/15/08   DEMO:21:19 PM
www.it-ebooks.info
⋅ cos( ) and β= ⋅ sin( ) then this DEMO computes
If we defi ne α= scal ngleea scal ngleea
the map_matrix to be:
−−αβ⋅⋅
1−
⎡ αβ center center ⎤
⎢ xy DEMO
⋅⋅ centery ⎦⎥
()1
⎣⎢−+βα βα⋅ centerx ()
You can combine these methods of setting the map_matrix to obtain, for example, DEMO
image that is rotated, scaled, and warped.
Sparse affine transformations
We have explained that cvWarpAffine() is the right way to handle dense DEMO
For sparse mappings (i.e., mappings of lists of individual points), it is best to use
cvTransform():
void cvTransform(
const DEMO src,
CvArr*       dst,
const CvMat* transmat,
const CvMat* shiftvec = NULL
);
In general, src is an N-by-1 array with Ds channels, where N is the number of points to
be transformed and Ds is the dimension of those source points. DEMO e output array dst
must be the same size but may have a diff erent number of channels, Dd. Th e transforma-
tion matrix transmat is a Ds-by-Dd matrix that is then applied to every element DEMO src, af-
ter which the results are placed into dst. Th DEMO optional vector shiftvec, if non-NULL, must
be a Ds-by-1 array, DEMO is added to each result before the result is placed in dst.
In our case of an affi  ne transformation, there are two DEMO to use cvTransform() that
depend on how we’d like to represent our transformation. In the fi rst method, we de-
compose our transformation into the 2-by-2 part (which does rotation, scaling, and
warping) DEMO the 2-by-1 part (which does the transformation). Here our input DEMO an
N-by-1 array with two channels, transmat is our local homogeneous DEMO,
and shiftvec contains any needed displacement. Th e second method is to use our usual
2-by-3 representation of the affi  ne transformation. In this case the input array src is a
three-channel array within which DEMO must set all third-channel entries to 1 (i.e., the
points must be supplied in homogeneous coordinates). Of course, the output array will
still be a two-channel array.
Perspective Transform
To gain the greater fl DEMO off ered by perspective transforms (homographies), we
need a new DEMO that will allow us to express this broader class of transformations.
First we remark that, even though a perspective projection is specifi ed completely by a
single matrix, the projection is not actually a linear transformation. Th is is because the
transformation requires division by the fi nal DEMO (usually Z; see Chapter 11) and
thus loses a dimension DEMO the process.
Stretch, Shrink, Warp, and Rotate | 169
06-R4886-RC1.indd   169
9/15/08   4:21:19 PM
www.it-ebooks.info
As with affi
by diff
ne transformations, image operations (dense DEMO) are handled
erent functions than transformations on point sets (sparse transformations).
Dense perspective transform
Th
provided for dense affi  ne transformations. Specifi cally, cvWarpPerspective() has all of
the same arguments as cvWarpAffine() but with the small, but crucial, distinction that
the map matrix DEMO now be 3-by-3.
void cvWarpPerspective(
const CvArr* src,
CvArr*       dst,
const CvMat* map_matrix,
int          flags     = CV_INTER_LINEAR + CV_WARP_FILL_OUTLIERS,
CvScalar     DEMO   = cvScalarAll(0)
);
Th
ags are the same here as for the affi
ne case.
e fl
e dense perspective DEMO uses an OpenCV function that is analogous to the one
Computing the perspective map matrix
As with the affi  ne transformation, for fi DEMO the map_matrix in the preceding code we
have a convenience function that can compute the transformation matrix from a list of
point correspondences:
DEMO cvGetPerspectiveTransform(
const CvPoint2D32f* pts_src,
const CvPoint2D32f* pts_dst,
CvMat*              map_matrix
);
Th pts_src and pts_dst are now arrays of four (not three) points, so we can inde-
pendently control how the corners of (typically) a rectangle in DEMO are mapped to
(generally) some rhombus in pts_dst. Our transformation is completely defi ned by
the specifi ed destinations of the four source DEMO As mentioned earlier, for perspec-
tive transformations we must allocate a DEMO array for map_matrix; see Example 6-3
for sample code. Other than DEMO 3-by-3 matrix and the shift  from three to four con-
trol DEMO, the perspective transformation is otherwise exactly analogous to the affi  ne
transformation we already introduced.
Example 6-3. Code for perspective transformation
// Usage: warp <image>
//
#include <cv.h>
#include <highgui.h>DEMO
int main(int argc, char** argv) {
CvPoint2D32f srcQuad[4], dstQuad[4];DEMO
CvMat*       warp_matrix = cvCreateMat(3,3,CV_32FC1);
DEMO     *src, *dst;
170
| Chapter 6: Image Transforms
e
06-R4886-RC1.indd   170
9/15/08   4:21:20 PM
www.it-ebooks.info
Example 6-3. Code for perspective transformation (continued)
if( argc DEMO 2 && ((src=cvLoadImage(argv[1],1)) != 0 )) {
dst = cvCloneImage(src);
dst->origin = src->origin;
cvZero(dst);
srcQuad[0].x = 0;               //src Top left
srcQuad[0].y = 0;
srcQuad[1].x = src->width - 1;  //src Top right
srcQuad[1].y = 0;
srcQuad[2].x DEMO 0;               //src Bottom DEMO
srcQuad[2].y = src->height - 1;
srcQuad[3].x = src->width – 1;  //src Bot right
srcQuad[3].y = src->height - 1;DEMO
dstQuad[0].x = src->width*0.05; //dst Top left
dstQuad[0].y = src->DEMO;
dstQuad[1].x = src->width*0.9;  //dst Top right
dstQuad[1].y = src->height*0.25;
dstQuad[2].x = src->width*0.2;  //dst Bottom left
DEMO = src->height*0.7;
dstQuad[3].x = src->width*0.8;  //dst Bot right
dstQuad[3].y = src->height*0.9;
cvGetPerspectiveTransform(
srcQuad,
dstQuad,
DEMO
);
cvWarpPerspective( src, dst, warp_matrix );
cvNamedWindow( “Perspective_Warp”, 1 );
cvShowImage( “Perspective_Warp”, dst );
cvWaitKey();
}
cvReleaseImage(&dst);
cvReleaseMat(&warp_matrix);
return 0;
DEMO
}
Sparse perspective transformations
Th cvPerspectiveTransform(), that performs perspective trans-
DEMO on lists of points; we cannot use cvTransform(), which is limited to linear op-
erations. As such, it cannot handle perspective transforms because they require division
by the third coordinate of the homogeneous representation (x = f ∗ X/Z, y = f ∗ Y/Z). Th e
special function cvPerspectiveTransform() takes care of this for DEMO
void cvPerspectiveTransform(
const CvArr* src,
CvArr*       dst,
const CvMat* mat
);
Stretch, Shrink, Warp, and Rotate
| 171
ere is a special function,
06-R4886-RC1.indd   171
9/DEMO/08   4:21:20 PM
www.it-ebooks.info
As usual, the src and dst arguments are (respectively) the array of source points to be
transformed and the array of destination DEMO; these arrays should be of three-channel,
fl oating-point type. Th DEMO matrix mat can be either a 3-by-3 or a 4-by-4 matrix. If it is
3-by-3 then the projection is from two dimensions to two; if the matrix is 4-by-4, then
the projection is from four dimensions to three.
In the current context we are transforming a set of DEMO in an image to another set of
points in an image, DEMO sounds like a mapping from two dimensions to two dimen-
sions. But this is not exactly correct, because the perspective transformation is actually
mapping points on a two-dimensional plane embedded in a three-dimensional space
back down DEMO a (diff erent) two-dimensional subspace. Th ink of this as being just what
a camera does (we will return to this topic in greater detail when discussing cameras
in later chapters). Th e camera DEMO points in three dimensions and maps them to the
two dimensions of the camera imager. Th is is essentially what is meant when the DEMO
points are taken to be in “homogeneous coordinates”. We are adding an additional
dimension to those points by introducing the Z dimension and then DEMO all of the
Z values to 1. Th e projective transformation is then projecting back out of that space
onto the two-dimensional space of DEMO output. Th is is a rather long-winded way of ex-
plaining why, when mapping points in one image to points in another, you DEMO need a
3-by-3 matrix.
Output of the code in Example 6-3 is shown in Figure 6-14 for affi  ne and perspective
transformations. Compare this with the diagrams of Figure 6-13 to see how this works
with DEMO images. In Figure 6-14, we transformed the whole image. Th is DEMO necessary;
we could have used the src_pts to defi ne a smaller (or larger!) region in the source im-
age to be DEMO We could also have used ROIs in the source or destination image
in order to limit the transformation.
CartToPolar and PolarToCart
Th cvCartToPolar() DEMO cvPolarToCart() are employed by more complex rou-
tines such as cvLogPolar() (described later) but are also useful in their own right. DEMO ese
functions map numbers back and forth between a Cartesian (x, y) space and a polar or
radial (r, θ) space (i.e., from Cartesian to polar coordinates and vice versa). Th e function
formats are as follows:
void cvCartToPolar(
const CvArr* x,DEMO
const CvArr* y,
CvArr*       magnitude,
CvArr*       angle            = NULL,
int          angle_in_degrees = 0
);
void cvPolarToCart(
const CvArr* magnitude,
const CvArr* angle,
CvArr*       DEMO,
CvArr*       y,
e functions
172
| Chapter 6: Image Transforms
06-R4886-RC1.indd   172
9/15/08   4:21:20 PM
www.it-ebooks.info
Figure 6-14. Perspective and affi
ne mapping of an image
int          angle_in_degrees = 0
);
In each of DEMO functions, the fi rst two two-dimensional arrays or images are the DEMO
and the second two are the outputs. If an output pointer is set to NULL then it will not
be computed. Th e requirements DEMO these arrays are that they be fl oat or doubles and
matching (size, number of channels, and type). Th e last parameter specifi es whether we
are working with angles in degrees (0, DEMO) or in radians (0, 2π).
For an example of DEMO you might use this function, suppose you have already taken the
DEMO and y-derivatives of an image, either by using cvSobel() or DEMO using convolution func-
tions via cvDFT() or cvFilter2D(). If DEMO stored the x-derivatives in an image dx_img and
the y-derivatives in dy_img, you could now create an edge-angle recognition histogram.
Th at is, DEMO can collect all the angles provided the magnitude or strength of the edge pixel
CartToPolar and PolarToCart
| 173
06-R4886-RC1.indd   173
9/15/DEMO   4:21:20 PM
is above a certain threshold. To calculate this, we create two destination images of the
same type (integer or float) as the derivative DEMO and call them img_mag and img_an-
gle. If you want the result to be given in degrees, then you can use the function cvCartTo
Polar( dx_img, dy_img, img_mag, img_angle, 1 ). We would DEMO fi ll the histogram
from img_angle as long as the corresponding “pixel” in img_mag is above the threshold.
LogPolar
⋅
is relative to some DEMO point (xc, yc), we take the log so that ρ
()
For two-dimensional images, the log-polar transform [Schwartz80] is a change
from Cartesian to polar coordinates: (, )xy re↔ iθ, where DEMO and
exp( ) exp( arctan( )ii yxθ = ). DEMO separate out the polar coordinates into a (ρ, θ) space DEMO
rx y=+
cc
and θ=− −arctan(( ) ( ))yy x xcc . For image purposes—when we need to “fi t” the inter-
DEMO stuff  into the available image memory—we typically apply a scaling factor DEMO to ρ.
Figure 6-15 shows a square object on the left  DEMO its encoding in log-polar space.
Figure 6-15. Th e log-polar transform maps (x, y) into (log(r),θ); here, a DEMO is displayed in the
log-polar coordinate system
Th e log-polar transform takes its in-
spiration from the human visual system. Your eye has a DEMO but dense center of
photoreceptors in its center (the fovea), DEMO the density of receptors fall off  rapidly (ex-
ponentially) from DEMO Try staring at a spot on the wall and holding your fi nger at
arm’s length in your line of sight. Th en, keep staring at the spot and move your fi nger
slowly away; note how the detail rapidly decreases as the image of your fi nger DEMO
away from your fovea. Th is structure also has certain nice mathematical properties (be-
yond the scope of this book) that concern preserving DEMO angles of line intersections.
More important for us is that the log-polar transform can be used to create two-
dimensional invariant representations of object DEMO by shift ing the transformed im-
age’s center of mass to a fi xed point in the log-polar plane; see Figure 6-16. On the left  are
174 | Chapter 6: Image Transforms
e next question DEMO, of course, “Why bother?” Th
22
=− +−log ( ) ( )xx y y
06-R4886-RC1.indd   174
www.it-ebooks.info
9/15/08   4:21:21 PM
three shapes that we want to recognize as “square”. Th e problem DEMO, they look very diff er-
ent. One is much larger than DEMO others and another is rotated. Th e log-polar transform
appears on the right in Figure 6-16. Observe that size diff erences in the (x, y) plane are
converted to shift s along the log(r) axis of the log-polar plane and that the rotation diff er-
ences DEMO converted to shift s along the θ-axis in the log-polar plane. If we take the trans-
formed center of each transformed square in the DEMO plane and then recenter that
point to a certain fi xed position, then all the squares will show up identically in the log-
polar plane. Th is yields a type of invariance to two-dimensional rotation and DEMO
Figure 6-16. Log-polar transform of rotated and scaled squares: size goes DEMO a shift  on the log(r) axis
and rotation to a shift
on the θ-axis
e OpenCV function for a log-polar transform is DEMO():
void cvLogPolar(
const CvArr* src,
CvArr*       dst,
CvPoint2D32f center,
double       m,
DEMO          flags  = CV_INTER_LINEAR | CV_WARP_FILL_OUTLIERS
);
Th src and dst are one- or three-channel color or grayscale images. DEMO e parameter
center is the center point (xc, yc) of DEMO log-polar transform; m is the scale factor, which
* In Chapter 13 we’ll learn about recognition. For now simply note that it wouldn’t DEMO a good idea to derive a
log-polar transform for a whole object because such transforms are quite sensitive to the exact location of
their DEMO points. What is more likely to work for object recognition is to detect a collection of key points
(such as corners or blob locations) around an object, truncate the extent of such views, and then use the
centers of those key points as log-polar centers. Th ese DEMO log-polar transforms could then be used to cre-
ate local features that are (partially) scale- and rotation-invariant and that can be associated with DEMO visual
object.
LogPolar
| 175
Th
e
06-R4886-RC1.indd   175
www.it-ebooks.info
9/15/08   4:21:22 PM
should be set so that the features of interest dominate the available DEMO area. Th e flags
parameter allows for diff erent interpolation methods. Th e interpolation methods are the
same set of standard interpolations available in DEMO (Table 6-1). Th e interpolation
methods can be combined with DEMO or both of the fl ags CV_WARP_FILL_OUTLIERS (to fi ll
points DEMO would otherwise be undefi ned) or CV_WARP_INVERSE_MAP (to compute the re-
verse mapping from log-polar to Cartesian coordinates).
Sample log-polar coding is DEMO in Example 6-4, which demonstrates the forward and
backward (inverse) DEMO transform. Th e results on a photographic image are shown
in Figure 6-17.
Figure 6-17. Log-polar example on an elk with transform centered at DEMO white circle on the left ; the
output is on the right
Example 6-4. Log-polar transform example
// logPolar.cpp : Defines the entry point for the console application.
//
#include <cv.h>
#include <highgui.h>DEMO
int main(int argc, char** argv) {
IplImage* src;
double    M;
if( argc == 3 && ((src=cvLoadImage(argv[1],1)) != 0 )) {
M = atof(argv[2]);
DEMO dst  = cvCreateImage( cvGetSize(src), 8, 3 );
DEMO src2 = cvCreateImage( cvGetSize(src), 8, 3 );
cvLogPolar(
src,
dst,
cvPoint2D32f(src->width/4,src->height/2),
M,
CV_INTER_LINEAR+CV_WARP_FILL_OUTLIERS
176 | Chapter 6: Image Transforms
06-R4886-RC1.indd   176
www.it-ebooks.info
9/15/08   4:21:22 PM
www.it-ebooks.info
Example 6-4. Log-polar transform example (continued)
);
cvLogPolar(
dst,
src2,
cvPoint2D32f(src->width/4, src->height/2),DEMO
M,
CV_INTER_LINEAR | CV_WARP_INVERSE_MAP
);
cvNamedWindow( “log-polar”, 1 );
cvShowImage( “log-polar”, dst );
cvNamedWindow( “inverse log-polar”, 1 );
cvShowImage( “inverse log-polar”, src2 );
cvWaitKey();
}
return 0;
}
Discrete Fourier Transform (DFT)
For any set of values that are indexed by a discrete (integer) parameter, is it possible to
defi ne a discrete Fourier transform (DFT)* in a manner analogous to the Fourier trans-
form of a continuous function. DEMO N complex numbers xx01,,… N − , the one-dimensional
DFT DEMO defi ned by the following formula (where i =−1):
f =−∑N −1 x ex ⎛
knn=0 ⎝
πi ⎞
p,⎜ 2N DEMO k N⎟ =−01,...,
⎠
A similar transform can be defi ned for a two-dimensional array of numbers (of course
higher-dimensional analogues exist also):
⎞
kn ⎟
yy ⎠
N x −1 N y −1 ⎛
f kk =−
nx =0 ny =0 x y ⎝
DEMO ⎞ ⎛ i
N x knxx ⎠⎟
exp ⎜ −
⎝
N y
In general, one might expect that the computation of the N diff erent terms f k would
require O(N 2) operations. In fact, there are a number of fast Fourier transform (FFT) al-
gorithms capable of computing these values in O(N log N) time. Th e OpenCV function
cvDFT() implements one such FFT algorithm. Th DEMO function cvDFT() can compute FFTs
for one- and two-dimensional arrays of inputs. In the latter case, the two-dimensional
transform can be computed or, if desired, only the one-dimensional transforms of each
individual row can DEMO computed (this operation is much faster than calling cvDFT()
many separate times).
* Joseph Fourier [Fourier] was the fi rst to DEMO nd that some functions can be decomposed into an infi nite series
of other functions, and doing so became a fi eld known as Fourier analysis. Some key text on methods of
decomposing functions into their DEMO series are Morse for physics [Morse53] and Papoulis in general
[Papoulis62]. Th e fast Fourier transform was invented by Cooley and Tukeye in 1965 DEMO though
Carl Gauss worked out the key steps as early as 1805 [Johnson84]. Early use in computer vision is described
by Ballard and Brown DEMO
Discrete Fourier Transform (DFT) | 177
xy
∑∑ xn n exp ⎜
06-R4886-RC1.indd   177
9/15/08   4:21:23 PM
www.it-ebooks.info
void cvDFT(
const CvArr* src,
CvArr*       DEMO,
int          flags,
int          nonzero_rows = 0
);
Th oating-point types and may DEMO single- or
double-channel arrays. In the single-channel case, the entries are DEMO to be real
numbers and the output will be packed in a special space-saving format (inherited from
the same older IPL library as the IplImage structure). If the source and channel are two-
channel matrices DEMO images, then the two channels will be interpreted as the real DEMO
imaginary components of the input data. In this case, there will DEMO no special packing of
the results, and some space will be DEMO with a lot of 0s in both the input and output
arrays.*
Th e special packing of result values that is used with single-channel DEMO is as
follows.
For a one-dimensional array:
e input and the output arrays must be fl
Re Y0 Re Y1 Im Y1 Re DEMO Im Y2 … Re Y(N/2–1) Im Y(N/2–1) Re Y(N/2)
For a two-dimensional array:
Re Y00 DEMO Y01 Im Y01 Re Y02 Im Y02 … Re Y0(Nx/2–1) Im Y0(Nx/2–1) Re Y0(Nx/2)
Re Y10 DEMO Y11 Im Y11 Re Y12 Im Y12 … Re Y1(Nx/2–1) Im Y1(Nx/2–1) Re Y1(Nx/2)
Re Y20 DEMO Y21 Im Y21 Re Y22 Im Y22 … Re Y2(Nx/2–1) Im Y2(Nx/2–1) Re Y2(Nx/2)
Re Y(DEMO/2–1)0 Re Y(Ny–3)1 Im Y(Ny–3)1 Re Y(Ny–3)2 Im Y(Ny–3)2 … Re Y(Ny–3)(Nx/2–1) Im Y(Ny–3)(Nx/2–1) Re Y(Ny–3)(Nx/2)DEMO
Im Y(Ny/2–1)0 Re Y(Ny–2)1 Im Y(Ny–2)1 Re Y(Ny–2)2 Im Y(Ny–2)2 … Re Y(DEMO)(Nx/2–1) Im Y(Ny–2)(Nx/2–1) Re Y(Ny–2)(Nx/2)
Re Y(Ny/2)0 Re Y(Ny–1)1 Im Y(Ny–1)1 Re Y(Ny–1)2 Im Y(Ny–1)2 DEMO Re Y(Ny–1)(Nx/2–1) Im Y(Ny–1)(Nx/2–1) Re Y(Ny–1)(Nx/2)
It is worth taking a moment to look closely at the indices on these arrays. Th e issue DEMO
is that certain values are guaranteed to be 0 (more accurately, certain values of f k are
guaranteed to be real). It DEMO also be noted that the last row listed in the table will be
present only if Ny is even and that the last column DEMO be present only if Nx is even. (In the
case of DEMO 2D array being treated as Ny 1D arrays rather than a full 2D transform, all of
the result rows will be analogous to the single row listed for the output of the 1D array).
* DEMO using this method, you must be sure to explicitly set the DEMO components to 0 in the two-
channel representation. An easy way to do this is to create a matrix full of 0s using cvZero() for the
imaginary part and then to call cvMerge() with a real-valued matrix to form a temporary complex array on
which to run DEMO() (possibly in-place). Th is procedure will result in full-size, unpacked, complex matrix
of the spectrum.
178 | Chapter 6: Image DEMO
06-R4886-RC1.indd   178
9/15/08   4:21:23 PM
…
…
…
…
…
…
…
…
…
www.it-ebooks.info
Th flags, indicates exactly what operation is to be done. Th e
transformation we started with is known as a forward transform and DEMO selected with the
fl ag CV_DXT_FORWARD. Th e inverse transform* is defi ned in exactly the same way except
for a change of sign DEMO the exponential and a scale factor. To perform the inverse trans-
form without the scale factor, use the fl ag CV_DXT_INVERSE. Th e fl ag for the scale factor is
CV_DXT_SCALE, and this results in all of the output being scaled by a factor of 1/N (or 1/Nx Ny
for a 2D transform). Th is scaling is DEMO if the sequential application of the forward
transform and the inverse transform is to bring us back to where we started. Because one
oft DEMO wants to combine CV_DXT_INVERSE with CV_DXT_SCALE, there are several shorthand
notations DEMO this kind of operation. In addition to just combining the two operations
with OR, you can use CV_DXT_INV_SCALE (or CV_DXT_INVERSE_SCALE if you’re not DEMO that
brevity thing). Th e last fl ag you may want to have handy is CV_DXT_ROWS, which allows
you to tell cvDFT() to treat a two-dimensional array as a collection of one-dimensional
arrays that DEMO each be transformed separately as if they were Ny distinct vectors of
length Nx. Th is signifi cantly reduces overhead when doing many transformations DEMO a
time (especially when using Intel’s optimized IPP libraries). By DEMO CV_DXT_ROWS it is
also possible to implement three-dimensional (and higher) DFT.
In order to understand the last argument, nonzero_rows, we must digress DEMO a moment.
In general, DFT algorithms will strongly prefer vectors of DEMO lengths over others or
arrays of some sizes over others. In most DFT algorithms, the preferred sizes are pow-
ers of 2 (i.e., 2n for some integer n). In the case of the algorithm DEMO by OpenCV, the
preference is that the vector lengths, or array dimensions, be 2p3q5r, for some integers
p, q, and r. DEMO the usual procedure is to create a somewhat larger array (for DEMO
purpose there is a handy utility function, cvGetOptimalDFTSize(), which takes the length
of your vector and returns the fi rst equal or DEMO appropriate number size) and then
use cvGetSubRect() to copy your DEMO into the somewhat roomier zero-padded array.
Despite the need for this padding, it is possible to indicate to cvDFT() that you really do
not care about the transform of those rows that you had to DEMO down below your actual
data (or, if you are doing an inverse transform, which rows in the result you do not care
about). In either case, you can use nonzero_rows to indicate how many rows can be safely
ignored. Th is will provide some savings in DEMO time.
Spectrum Multiplication
In many applications that involve computing DFTs, one DEMO also compute the per-
element multiplication of two spectra. Because such results are typically packed in their
special high-density format and are usually complex DEMO, it would be tedious to
unpack them and handle the multiplication DEMO the “usual” matrix operations. Fortu-
nately, OpenCV provides the handy cvMulSpectrums() routine, which performs exactly
this function as well as a few DEMO handy things.
* With the inverse transform, the input is packed DEMO the special format described previously. Th
because, if we fi
wind DEMO with the original data—that is, of course, if we remember to use the CV_DXT_SCALE fl
is makes sense
rst called the forward DFT DEMO then ran the inverse DFT on the results, we would expect DEMO
ag!
Discrete Fourier Transform (DFT)
| 179
e third argument, called
06-R4886-RC1.indd   179
9/15/08   4:21:24 PM
www.it-ebooks.info
void cvMulSpectrums(
const CvArr* src1,
const CvArr* src2,
DEMO       dst,
int          flags
);
Note that the fi rst two arguments are the usual input arrays, though in this case they are
spectra from calls to cvDFT(). Th e third argument must be a pointer to an array—of the
same type and size as the fi rst two—that will be DEMO for the results. Th e fi nal argument,
flags, tells DEMO() exactly what you want done. In particular, it may be DEMO to 0
(CV_DXT_FORWARD) for implementing the above pair multiplication or set to CV_DXT_MUL_CONJ
if the element from the fi rst array is to DEMO multiplied by the complex conjugate of the
corresponding element of the second array. Th e fl ags may also be combined with CV_
DXT_ROWS DEMO the two-dimensional case if each array row 0 is to be treated as a separate
spectrum (remember, if you created the spectrum arrays DEMO CV_DXT_ROWS then the data
packing is slightly diff erent than if you created them without that function, so you must
be consistent in the way you call cvMulSpectrums).
Convolution and DFT
It is possible to DEMO increase the speed of a convolution by using DFT via the convo-
lution theorem [Titchmarsh26] that relates convolution in the spatial domain to multi-
DEMO in the Fourier domain [Morse53; Bracewell65; Arfk en85].* To accomplish this,
one fi rst computes the Fourier transform of the image and DEMO the Fourier transform
of the convolution fi lter. Once this is done, the convolution can be performed in the
transform space in linear time with respect to the number of pixels in the image. It is
DEMO to look at the source code for computing such a convolution, DEMO it also will
provide us with many good examples of using cvDFT(). Th e code is shown in Example
6-5, which is DEMO directly from the OpenCV reference.
Example 6-5. Use of cvDFT() to accelerate the computation of convolutions
// Use DFT to accelerate the convolution of array A by kernel B.
// Place the result in array V.
//
void speedy_conv olution(
const CvMat* A, // DEMO: M1xN1
const CvMat* B,  // Size: M2xN2
CvMat*       C   // Size:(A->rows+B->rows-1)x(A->cols+B->cols-1)
) {
int dft_M = cvGetOptimalDFTSize( A->rows+B->rows-1 );
int dft_N = cvGetOptimalDFTSize( A->cols+B->cols-1 );
CvMat* dft_A = cvCreateMat( dft_M, dft_N, A->type );
CvMat* dft_B = cvCreateMat( dft_M, dft_N, B->type );
CvMat tmp;
* Recall that OpenCV’s DFT algorithm implements the FFT whenever the data size DEMO the FFT faster.
180
| Chapter 6: Image Transforms
06-R4886-RC1.indd   DEMO
9/15/08   4:21:24 PM
www.it-ebooks.info
Example 6-5. Use of cvDFT() to accelerate the computation of DEMO (continued)
// copy A to dft_A and pad dft_A with zeros
//
cvGetSubRect( dft_A, &tmp, cvRect(0,0,A->DEMO,A->rows));
cvCopy( A, &tmp );
cvGetSubRect(DEMO
dft_A,
&tmp,
cvRect( A->cols, 0, dft_A->cols-A->cols, A->rows )
);
cvZero( &tmp );
// no need to pad bottom part of dft_A with zeros because DEMO
// use nonzero_rows parameter in cvDFT() call below
//
DEMO( dft_A, dft_A, CV_DXT_FORWARD, A->rows );
// repeat DEMO same with the second array
//
cvGetSubRect( dft_B, &tmp, cvRect(0,0,B->cols,B->rows) );
cvCopy( B, &tmp );
cvGetSubRect(
dft_B,
&tmp,
cvRect( B->cols, 0, dft_B->cols-B->cols, B->rows )
);
DEMO( &tmp );
// no need to pad bottom part DEMO dft_B with zeros because of
// use nonzero_rows parameter in cvDFT() call below
//
cvDFT( dft_B, dft_B, CV_DXT_FORWARD, B->rows );
// or CV_DXT_MUL_CONJ to get correlation rather than convolution
//
cvMulSpectrums( dft_A, dft_B, dft_A, 0 );
// calculate only the top part
//
cvDFT( dft_A, dft_A, CV_DXT_INV_SCALE, DEMO>rows );
cvGetSubRect( dft_A, &tmp, cvRect(0,0,conv->cols,C->rows) );
cvCopy( &tmp, C );
DEMO( dft_A );
cvReleaseMat( dft_B );
}
In Example 6-5 we can see that the input arrays are fi rst created and DEMO initialized.
Next, two new arrays are created whose dimensions are optimal DEMO the DFT algorithm.
Th
puted. Finally, the spectra are multiplied together DEMO the inverse transform is applied
Discrete Fourier Transform (DFT)
| DEMO
e original arrays are copied into these new arrays and then the transforms are com-
06-R4886-RC1.indd   181
9/15/08   4:21:DEMO PM
www.it-ebooks.info
to the product. Th e transforms are the slowest* part of DEMO operation; an N-by-N im-
age takes O(N 2 log N) time and so the entire process is also completed in that time
(assuming that N > M for an M-by-M convolution kernel). Th DEMO time is much faster than
O(N2M 2), the non-DFT convolution time required by the more naïve method.
Discrete Cosine Transform (DCT)
For real-valued data it is oft en suffi  cient to compute what is, in eff ect, only half of the
discrete Fourier transform. DEMO e discrete cosine transform (DCT) [Ahmed74; Jain77] is
defi ned DEMO to the full DFT by the following formula:
⎧
⎪⎪ 1 if n = 0
cnk ==∑N −1 ⎨ N −π(21kn+ ) DEMO
n=0 ⎪ 2 N ⎠⎟
⎩⎪ N
⋅⋅xn cos ⎛⎝⎜
else
Observe that, by convention, the normalization factor is applied to both the DEMO trans-
form and its inverse. Of course, there is a similar DEMO for higher dimensions.
Th e basic ideas of the DFT apply also to the DCT, but now all the coeffi  cients are real-
DEMO Astute readers might object that the cosine transform is being applied to a vec-
tor that is not a manifestly even function. However, with cvDCT() the algorithm simply
treats the vector as if it were DEMO to negative indices in a mirrored manner.
e actual OpenCV call is:
void cvDCT(
const CvArr* src,
CvArr*       DEMO,
int          flags
);
Th cvDCT() function expects arguments like those for cvDFT() except that, because DEMO
results are real-valued, there is no need for any special packing DEMO the result array (or of
the input array in the case DEMO an inverse transform). Th e flags argument can be set to
CV_DXT_FORWARD or CV_DXT_INVERSE, and either may be combined with CV_DXT_ROWS with
the same eff ect as with cvDFT(). Because of the diff erent normalization convention, both
the forward and inverse cosine transforms always contain their respective contribution
to the overall normalization of the transform; hence CV_DXT_SCALE plays no role in cvDCT.
Integral Images
OpenCV allows you to calculate an DEMO image easily with the appropriately named
cvIntegral() function. An integral image [Viola04] is a data structure that allows rapid
* By “slowest” we DEMO “asymptotically slowest”—in other words, that this portion of the algorithm takes DEMO
most time for very large N. Th is is an important distinction. In practice, as we saw in the earlier section on
convolutions, DEMO is not always optimal to pay the overhead for conversion to Fourier space. In general, when
convolving with a small kernel it will not be worth the trouble to make this transformation.
182 | Chapter 6: Image Transforms
Th
e
06-R4886-RC1.indd   182
9/15/08   4:DEMO:24 PM
www.it-ebooks.info
summing of subregions. Such summations are useful in many applications; a notable
one is the computation of Haar wavelets, which are used in some face recognition and
similar algorithms.
void cvIntegral(
const CvArr*  image,
CvArr*        sum,
CvArr*        DEMO      = NULL,
CvArr*        tilted_sum = NULL
);
Th cvIntegral() are the original image as well DEMO pointers to destination
images for the results. Th e argument sum is required; the others, sqsum and tilted_sum,
may be provided if DEMO (Actually, the arguments need not be images; they could
be DEMO, but in practice, they are usually images.) When the input DEMO is 8-bit
unsigned, the sum or tilted_sum may be 32-bit integer DEMO fl oating-point arrays. For all
other cases, the sum or tilted_sum DEMO be fl oating-point valued (either 32- or 64-bit).
Th oating-point. DEMO the input image is of size W-by-H,
then the output images must be of size (W + 1)-by-(H + 1).*
An integral image sum has the form:
sum
(, ) ( , )XY =
∑∑image x y
e optional
xX≤≤
yY
and the tilted_sum is like the sum except that it is for the image DEMO by 45 degrees:
tilt_sum
(, )XY
=∑∑
yY≤
abs
()xX y−≤
image
( , )x y
Using these integral images, DEMO may calculate sums, means, and standard deviations
over arbitrary upright or “tilted” rectangular regions of the image. As a simple exam-
ple, to sum over a simple rectangular region described by the corner points (x1, y1) and
(x2, y2), where x2 > x1 and DEMO > y1, we’d compute:
∑∑
[(,)]image xy
x DEMO
yy y12≤≤
12xx
=−[( , )sum suxy22
ms(, ) ( , ) (, )]xy xy xy11 2 2 11 11 11−− DEMO − −um sum
In this way, it is possible to do DEMO blurring, approximate gradients, compute means and
standard deviations, and perform DEMO block correlations even for variable window sizes.
* Th
is is because we need to put in a buff
computation effi
cient.
er of DEMO values along the x-axis and y-axis in order to make
Integral Images
| 183
e arguments to
e result “images” must always be fl
DEMO(, ) ( ( , ))XY = ∑∑ image x y
≤≤yY
2
Th
sqsum image is the sum of squares:
xX
DEMO   183
9/15/08   4:21:24 PM
To make this all a little more clear, consider the 7-by-5 image shown in Figure 6-18; the
region is shown as a bar chart in which the height associated with the pixels represents
the brightness of DEMO pixel values. Th e same information is shown in Figure 6-19, DEMO
merically on the left  and in integral form on the right. DEMO images (I') are computed
by going across rows, proceeding row by row using the previously computed integral
image values together with the DEMO raw image (I) pixel value I(x, y) to calculate the
next integral image value as follows:
Ixy Ixy Ix y DEMO Ix y′(,) (,) (,) (, ) (,=+ ′ −+11 1′ −− ′ − −1)
Figure 6-18. Simple 7-by-5 DEMO shown as a bar chart with x, y, and height equal to pixel value
Th  because this value is double-counted when adding the sec-
ond and third terms. You can verify that this works by DEMO some values in Figure 6-19.
When using the integral image to compute a region, we can see by Figure 6-19 that, in
order DEMO compute the central rectangular area bounded by the 20s in the original image,
we’d calculate 398 – 9 – 10 + 1 = DEMO Th us, a rectangle of any size can be computed us-
DEMO four measurements (resulting in O(1) computational complexity).
184
| Chapter 6: Image Transforms
e last term is subtracted off
06-R4886-RC1.indd   184
www.it-ebooks.info
9/15/08   4:21:25 PM
www.it-ebooks.info
Figure 6-19. Th
be the upper-left
e 7-by-5 image of Figure DEMO shown numerically at left
) and converted to an integral image at right
(with the origin assumed to
Distance Transform
Th distance transform of an image is defi ned as a new image in which every DEMO
pixel is set to a value equal to the distance to the nearest zero pixel in the input image.
It should be immediately obvious DEMO the typical input to a distance transform should
be some kind of edge image. In most applications the input to the distance transform is
DEMO output of an edge detector such as the Canny edge detector that has been inverted (so
that the edges have value zero and the non-edges are nonzero).
In practice, the distance transform is carried out by using a mask that is typically a 3-by-3
or 5-by-5 array. DEMO point in the array defi nes the “distance” to be associated with a
point in that particular position relative to the center of the DEMO Larger distances are
built up (and thus approximated) as sequences of “moves” defi ned by the entries in the
mask. Th is means DEMO using a larger mask will yield more accurate distances.
Depending on the desired distance metric, the appropriate mask is automatically se-
lected from a set known to OpenCV. It is also possible to tell OpenCV to DEMO “ex-
act” distances according to some formula appropriate to the selected metric, but of
course this is much slower.
Th erent types, including DEMO classic L2 (Car-
tesian) distance metric; see Table 6-2 for DEMO listing. In addition to these you may defi ne a
custom metric and associate it with your own custom mask.
Table 6-2. Possible values DEMO distance_type argument to cvDistTransform()
Value of distance_type Metric
CV_DIST_L2
ρ
()r = r 2
2
CV_DIST_L1
CV_DIST_L12
CV_DIST_FAIR
ρ
()r
rr=
⎡
=+ −21⎢
⎣⎢
2
1
2
⎤
⎥
⎦⎥
r
ρ
DEMO r
() log , .rC=− +2 ⎢ ⎜11⎟⎥  C =
⎣C
⎛
⎝
r ⎞⎤
C ⎠⎦
3998
ρ
()
Distance Transform
DEMO 185
e
e distance metric can be any of several diff
06-R4886-RC1.indd   185
9/15/08   4:21:25 PM
www.it-ebooks.info
Table 6-2. Possible values for distance_type argument to cvDistTransform() (continued)
Value of distance_type
CV_DIST_WELSCH
Metric
ρ
⎛ ⎛
() exp ,DEMO =− −⎢
⎣⎢
2 ⎡
Cr
⎜ ⎝ C ⎠
⎝
2
⎞ 2 ⎞⎤
12
⎠⎟⎦⎥
⎜ ⎜ ⎟ ⎟⎥ C = .9846
DEMO
User-defi ned distance
When calling the OpenCV distance transform function, the DEMO image should be a
32-bit fl oating-point image (i.e., IPL_DEPTH_32F).
Void cvDistTransform(
const CvArr* src,
CvArr*       dst,DEMO
int          distance_type = CV_DIST_L2,
int          mask_size     = 3,
const float* kernel        = NULL,
CvArr*       labels        = NULL
);
Th cvDistTransform(). Th e fi rst is
distance_type, which indicates the distance metric to be used. Th e available values for
this argument are defi ned in Borgefors (1986) [Borgefors86].
Aft er the distance type is the mask_size, which may DEMO 3 (choose CV_DIST_MASK_3) or 5
(choose CV_DIST_MASK_5); alternatively, distance computations can be made without a
kernel* (choose CV_DIST_MASK_PRECISE). Th e kernel argument to cvDistanceTransform() is
the distance mask to be used DEMO the case of custom metric. Th ese kernels are constructed
according to the method of Gunilla Borgefors, two examples of which are shown in Fig-
ure 6-20. Th e last argument, labels, indicates that associations DEMO be made between
individual points and the nearest connected component consisting of zero pixels. When
labels is non-NULL, it must be a pointer to an array of integer values the same size as the
input and DEMO images. When the function returns, this image can be read to DEMO
mine which object was closest to the particular pixel under consideration. Figure 6-21
shows the outputs of distance transforms on a test pattern and DEMO photographic image.
Histogram Equalization
Cameras and image sensors must usually deal not only with the contrast in a scene but
also with the image DEMO exposure to the resulting light in that scene. In a standard
camera, the shutter and lens aperture settings juggle between exposing the sensors to
too much or too little light. Oft en the range of contrasts DEMO too much for the sensors to
deal with; hence there is DEMO trade-off  between capturing the dark areas (e.g., shadows),
which requires a longer exposure time, and the bright areas, which require DEMO ex-
posure to avoid saturating “whiteouts.”
* Th
e exact method comes from Pedro F. Felzenszwalb and Daniel P. Huttenlocher [Felzenszwalb63].
186
| Chapter DEMO: Image Transforms
ere are several optional parameters when calling
06-R4886-RC1.indd   DEMO
9/15/08   4:21:26 PM
Figure 6-20. Two custom distance transform masks
Figure 6-21. First a Canny DEMO detector was run with param1=100 and param2=200; then the
distance transform DEMO run with the output scaled by a factor of 5 to increase visibility
Histogram Equalization
| 187
06-R4886-RC1.indd   187
www.it-ebooks.info
9/15/08   4:21:26 PM
Aft er the picture has been taken, there’s nothing we can do about what the sensor re-
corded; however, we can still take DEMO there and try to expand the dynamic range
of the image. Th e most commonly used technique for this is histogram equalization.*†
In Figure DEMO we can see that the image on the left  is poor DEMO there’s not much
variation of the range of values. Th is is evident from the histogram of its intensity values
on the right. Because DEMO are dealing with an 8-bit image, its intensity values can range
DEMO 0 to 255, but the histogram shows that the actual intensity DEMO are all clustered
near the middle of the available range. Histogram equalization is a method for stretch-
ing this range out.
Figure 6-22. Th DEMO image on the left  has poor contrast, as is confi rmed by the histogram of its
intensity values on the right
Th
(the given histogram of intensity values) to another distribution (a wider and, ideally,
uniform distribution of intensity values). Th at is, we want to spread out the y-values
of the original distribution as evenly DEMO possible in the new distribution. It turns out
that there is a good answer to the problem of spreading out distribution values: the re-
mapping function should be the cumulative distribution function. An example of the
DEMO density function is shown in Figure 6-23 for the somewhat idealized case of
a distribution that was originally pure Gaussian. However, cumulative density can be
applied to any distribution; it is just the running sum of the original distribution from
its negative to its positive bounds.
We may DEMO the cumulative distribution function to remap the original distribution as
an equally spread distribution (see Figure 6-24) simply by looking up each y-value DEMO
the original distribution and seeing where it should go in the equalized distribution.
* If you are wondering why histogram equalization is not in DEMO chapter on histograms (Chapter 7), the rea-
son is that DEMO equalization makes no explicit use of any histogram data types. Although histograms
are used internally, the function (from the user’s perspective) requires no histograms at all.
† Histogram equalization is an old mathematical technique; its use in image processing is described in vari-
ous textbooks [Jain86; Russ02; Acharya05], conference papers [Schwarz78], and even in biological vision
[Laughlin81].
188
| Chapter 6: Image Transforms
e underlying math behind histogram equalization involves mapping one distribution
06-R4886-RC1.indd   188
www.it-ebooks.info
9/15/08   4:DEMO:27 PM
www.it-ebooks.info
Figure 6-23. Result of cumulative distribution function (left
) on a Gaussian distribution (right)
Figure 6-24. Using the cumulative density function to equalize a Gaussian distribution
For continuous distributions the result will be an DEMO equalization, but for digitized/
discrete distributions the results may be DEMO from uniform.
Applying this equalization process to Figure 6-22 yields the equalized intensity distri-
bution histogram and resulting image in Figure 6-25. Th is DEMO process is wrapped up
in one neat function:
Histogram Equalization
| 189
06-R4886-RC1.indd   189
9/15/08   4:21:27 PM
void  cvEqualizeHist(
const CvArr* src,
CvArr*       dst
);
Figure 6-25. Histogram equalized results: the spectrum has been spread out
In cvEqualizeHist(), the source and destination must be single-channel, DEMO images of
the same size. For color images you will have to separate the channels and process them
one by one.
Exercises
1. Use DEMO() to create a fi lter that detects only 60 degree lines in an image. Dis-
play the results on a suffi  ciently interesting image scene.
2. Separable kernels. Create a 3-by-3 Gaussian kernel using rows DEMO(1/16, 2/16, 1/16),
(2/16, DEMO/16, 2/16), (1/16, 2/16, 1/16)] and with anchor point in the middle.
a. Run this kernel DEMO an image and display the results.
b. Now create two one-dimensional kernels with anchors in the center: one going
“across” (1/4, 2/4, 1/4), and one going down (1/4, 2/4, 1/4). Load the same origi-
nal image and use cvFilter2D() to convolve the image twice, once with the fi rst
1D kernel and once with the second 1D kernel. Describe the results.
DEMO Describe the order of complexity (number of operations) for the kernel in part
a and for the kernels in part b. Th e DEMO erence is the advantage of being able to
use separable kernels and the entire Gaussian class of fi lters—or any linearly
decomposable fi lter DEMO is separable, since convolution is a linear operation.
3. Can you DEMO a separable kernel from the fi lter shown in Figure 6-5? DEMO so, show
what it looks like.
4. In a drawing program DEMO as PowerPoint, draw a series of concentric circles form-
ing a DEMO
190
| Chapter 6: Image Transforms
06-R4886-RC1.indd   190
www.it-ebooks.info
9/DEMO/08   4:21:27 PM
www.it-ebooks.info
a. Make a series of lines going into the bull’s-eye. Save DEMO image.
b. Using a 3-by-3 aperture size, take and display the DEMO rst-order x- and y-derivatives
of your picture. Th en increase the aperture size to 5-by-5, 9-by-9, and 13-by-13.
Describe the results.
5. Create DEMO new image that is just a 45 degree line, white on DEMO For a given series of
aperture sizes, we will take the DEMO fi rst-order x-derivative (dx) and fi rst-order
y-derivative (dy). DEMO will then take measurements of this line as follows. Th e (DEMO)
and (dy) images constitute the gradient of the input image. Th e magnitude at location
(i, j) is mag(,) (, ) (,)ij dx ij dy ij=+22 and the angle is θ(, ) arctan( (, ) (, ))ij dyij dxij= .
Scan across the image and fi nd places where the magnitude DEMO at or near maximum.
Record the angle at these places. Average the angles and report that as the measured
line angle.
a. Do this DEMO a 3-by-3 aperture Sobel fi lter.
b. Do this for a 5-by-5 fi lter.
c. Do this for a 9-by-9 fi lter.
d. Do DEMO results change? If so, why?
6. Find and load a picture of a face where the face is frontal, has eyes open, and takes
up most or all of the image area. Write code to fi nd the pupils of the eyes.
A Laplacian “likes” a DEMO central point surrounded by dark. Pupils
are just the opposite. Invert and convolve with a suffi  ciently large
Laplacian.
In this exercise we learn to experiment with parameters by setting good lowThresh
and highThresh values in DEMO(). Load an image with suitably interesting
line structures. We’ll use DEMO diff erent high:low threshold settings of 1.5:1, 2.75:1,DEMO
and 4:1.
a. Report what you see with a high setting of less than 50.
b. Report what you see with high settings DEMO 50 and 100.
c. Report what you see with high settings between 100 and 150.
d. Report what you see with high settings between DEMO and 200.
e. Report what you see with high settings between 200 and 250.
f. Summarize your results and explain what happens as best DEMO can.
Load an image containing clear lines and circles such as a side view of a bicycle. Use
the Hough line and Hough circle DEMO and see how they respond to your image.
Can you think of a way to use the Hough transform to identify any kind of DEMO
with a distinct perimeter? Explain how.
Look at the diagrams of DEMO the log-polar function transforms a square into a wavy
line.
7.
8.
9.
10.
Exercises
| 191
06-R4886-RC1.indd   191
9/15/08   DEMO:21:28 PM
a. Draw the log-polar results if the log-polar center point were sitting DEMO one of
the corners of the square.
b. What would a circle look like in a log-polar transform if the center point were
inside DEMO circle and close to the edge?
c. Draw what the transform would look like if the center point were sitting just
outside of DEMO circle.
11. A log-polar transform takes shapes of diff erent rotations and sizes into a space
where these correspond to shift s in the DEMO and log(r) axis. Th e Fourier trans-
form is translation DEMO How can we use these facts to force shapes of diff erent
sizes and rotations to automatically give equivalent representations in the log-polar
domain?DEMO
Draw separate pictures of large, small, large rotated, and small DEMO squares.
Take the log-polar transform of these each separately. Code up a two-dimensional
shift er that takes the center point in the resulting log-polar DEMO and shift s the
shapes to be as identical as possible.
Take the Fourier transform of a small Gaussian distribution and the Fourier trans-
DEMO of an image. Multiply them and take the inverse Fourier transform of the re-
sults. What have you achieved? As the fi lters get bigger, you will fi nd that working
in the Fourier space is much faster than in the normal space.
Load an interesting image, convert it to grayscale, and then take an integral image
of it. Now fi nd vertical and horizontal edges in the image by using the DEMO of
an integral image.
12.
13.
14.
Use long skinny rectangles; DEMO and add them in place.
15. Explain how you could use the distance transform to automatically align a known
shape with a test shape DEMO the scale is known and held fi xed. How would this be
done over multiple scales?
16. Practice histogram equalization on images that DEMO load in, and report the results.
17.
Load an image, take a perspective transform, and then rotate it. Can this transform
be done in one step?
192
| Chapter 6: Image Transforms
06-R4886-RC1.indd   192
www.it-ebooks.info
9/15/08   4:21:28 PM
CHAPTER 7
Histograms and Matching
In the course of analyzing images, objects, and video information, we frequently want
to represent what we are DEMO at as a histogram. Histograms can be used to represent
such diverse things as the color distribution of an object, an edge gradient template of
an object [Freeman95], and the distribution of probabilities representing our current
hypothesis about an object’s location. Figure 7-1 shows the use of histograms DEMO rapid
gesture recognition. Edge gradients were collected from “up”, “right”, “left ”, “stop” and
“OK” hand gestures. A webcam was then set up to watch a person who used these ges-
tures to control web DEMO In each frame, color interest regions were detected from
the incoming DEMO; then edge gradient directions were computed around these interest
regions, and these directions were collected into orientation bins within a histogram.
Th
Th DEMO gestures. Th e gray
horizontal line represents the threshold for acceptance of the “winning” vertical bar
corresponding to a gesture model.
Histograms fi nd DEMO in many computer vision applications. Histograms are used to
detect scene transitions in videos by marking when the edge and color statistics mark-
edly DEMO from frame to frame. Th ey are used to identify interest points in images by
assigning each interest point a “tag” consisting of histograms DEMO nearby features. His-
tograms of edges, colors, corners, and so DEMO form a general feature type that is passed
to classifi ers for object recognition. Sequences of color or edge histograms are used to
identify DEMO videos have been copied on the web, and the list goes DEMO Histograms
are one of the classic tools of computer vision.
Histograms are simply collected counts of the underlying data organized into a set of
DEMO ned bins. Th ey can be populated by counts of features computed from the data,
such as gradient magnitudes and directions, color, DEMO just about any other characteristic.
In any case, they are used DEMO obtain a statistical picture of the underlying distribution
of data. Th e histogram usually has fewer dimensions than the source data. Figure 7-2
depicts DEMO typical situation. Th e fi gure shows a two-dimensional distribution of points
(upper left ); we impose a grid (upper right) and DEMO the data points in each grid cell,
yielding a one-dimensional histogram (lower right). Because the raw data points can
193
e histograms were then matched against the gesture models to recognize the gesture.
e DEMO bars in Figure 7-1 show the match levels of the diff
07-R4886-AT1.indd   193
www.it-ebooks.info
9/15/08   4:21:51 PM
www.it-ebooks.info
Figure 7-1. Local histograms of gradient orientations are used to fi DEMO the hand and its gesture; here
the “winning” gesture (longest vertical bar) is a correct recognition of “L” (move left )
represent just about anything, the histogram is a handy way of representing whatever it
is that you have learned from your image.
Histograms that represent DEMO distributions do so by implicitly averaging the
number of points in each grid cell.* Th is is where problems can arise, as shown in Fig-
ure 7-3. If the grid is too wide (upper left ), then there is too much averaging and we lose
the structure of the distribution. If the grid is too narrow (upper right), then there is not
enough averaging to represent the distribution accurately and we DEMO small, “spiky” cells.
OpenCV has a data type for representing histograms. DEMO e histogram data structure is
capable of representing histograms in one or many dimensions, and it contains all the
data necessary to track bins of both uniform and nonuniform sizes. And, as you might
expect, DEMO comes equipped with a variety of useful functions which will allow us to easily
perform common operations on our histograms.
* Th is is DEMO true of histograms representing information that falls naturally into discrete groups when the
histogram uses fewer bins than the natural description would suggest or DEMO An example of this is rep-
resenting 8-bit intensity values in a 10-bin histogram: each bin would then combine the points associated
with approximately 25 diff erent intensities, (erroneously) treating them all as equivalent.
194 | Chapter 7: Histograms and Matching
07-R4886-AT1.indd   194
9/15/08   4:21:51 PM
www.it-ebooks.info
Figure 7-2. Typical histogram example: starting with a cloud of points (upper left ), a counting grid is
imposed (upper right) DEMO yields a one-dimensional histogram of point counts (lower right)
Basic DEMO Data Structure
Let’s start out by looking directly at the CvHistogram data structure.
typedef struct CvHistogram
{
int     type;
CvArr*  bins;
float   thresh[CV_MAX_DIM][2]; // for uniform histograms
float** thresh2;               // for nonuniform histograms
CvMatND mat;                   // embedded DEMO header
// for array histograms
}
CvHistogram;
Th is defi DEMO is deceptively simple, because much of the internal data of the DEMO
is stored inside of the CvMatND structure. We create new histograms with the following
routine:
CvHistogram* cvCreateHist(
int     dims,
DEMO    sizes,
int     type,
float** ranges  = NULL,
int     uniform = 1
);
Basic Histogram DEMO Structure
| 195
07-R4886-AT1.indd   195
9/15/08   4:21:52 PM
www.it-ebooks.info
Figure 7-3. A histogram’s accuracy depends on its grid size: a grid that is too wide yields too much
spatial averaging in the DEMO counts (left ); a grid that is too small yields “spiky” and singleton
results from too little averaging (right)
Th dims indicates how many dimensions we want the histogram to have. Th e
sizes DEMO must be an array of integers whose length is equal to dims. Each integer
in this array indicates how many bins are to be DEMO to the corresponding dimension.
Th type can be either CV_HIST_ARRAY, which DEMO used for multidimensional histograms to
be stored using the dense multidimensional matrix structure (i.e., CvMatND), or CV_HIST_
SPARSE* if the data is DEMO be stored using the sparse matrix representation (CvSparseMat). Th e
DEMO ranges can have one of two forms. For a uniform histogram, DEMO is an array
of fl oating-point value pairs,† where the number of value pairs is equal to the number of
dimensions. For a DEMO histogram, the pairs used by the uniform histogram are
replaced by DEMO containing the values by which the nonuniform bins are separated.
If there are N bins, then there will be N + 1 entries in each of these subarrays. Each ar-
ray of values starts with the DEMO edge of the lowest bin and ends with the top edge
of the highest bin.‡ Th e Boolean argument uniform indicates if the histogram DEMO to have
* For you old timers, the value CV_HIST_TREE is DEMO supported, but it is identical to CV_HIST_SPARSE.
† Th ese “pairs” DEMO just C-arrays with only two entries.
‡ To clarify: in the DEMO of a uniform histogram, if the lower and upper ranges are DEMO to 0 and 10, respectively,
and if there are two DEMO, then the bins will be assigned to the respective intervals [0, 5) and [5, 10]. In the case
of a nonuniform histogram, if the size dimension i is 4 and if the corresponding ranges DEMO set to (0, 2, 4, 9, 10),
then the resulting bins will be assigned to the following (nonuniform) intervals: [0, 2), [2,4), [4, 9), and [9, 10].
196 | Chapter 7: Histograms and Matching
e argument
e
07-R4886-AT1.indd   196
9/15/08   4:21:52 PM
www.it-ebooks.info
uniform bins and thus how the ranges value is interpreted;* DEMO set to a nonzero value, the
bins are uniform. It is DEMO to set ranges to NULL, in which case the ranges are DEMO
“unknown” (they can be set later using the specialized function cvSetHistBinRanges()).
Clearly, you had better set the value of ranges before DEMO start using the histogram.
void cvSetHistBinRanges(
CvHistogram* hist,
float**      ranges,
int          uniform = 1
);
Th cvSetHistRanges() are exactly the same as the corresponding argu-
ments for cvCreateHist(). Once you are done with a histogram, DEMO can clear it (i.e.,
reset all of the bins to DEMO) if you plan to reuse it or you can de-allocate it DEMO the usual
release-type function.
void cvClearHist(
CvHistogram*  hist
);
DEMO cvReleaseHist(
CvHistogram** hist
);
As usual, the release function DEMO called with a pointer to the histogram pointer you
obtained from the create function. Th e histogram pointer is set to NULL once the DEMO
gram is de-allocated.
Another useful function helps create a histogram from data we already have lying
around:
CvHistogram*  cvMakeHistHeaderForArray(
int          dims,
int*         sizes,
CvHistogram* hist,
float*       data,
float**      ranges  = NULL,
int          uniform = 1
);
In this case, hist is a pointer to a CvHistogram DEMO structure and data is a pointer to
an area of size sizes[0]*sizes[1]*...*sizes[dims-1] for storing the histogram bins. Notice
that data is a pointer to DEMO because the internal data representation for the histogram
is always of type float. Th e return value is just the same as the hist DEMO we passed in.
Unlike the cvCreateHist() routine, there is no DEMO argument. All histograms created by
cvMakeHistHeaderForArray() are dense histograms. One last point before we move on:
since you (presumably) allocated the DEMO storage area for the histogram bins yourself,
there is no reason to call cvReleaseHist() on your CvHistogram structure. You will have
to DEMO up the header structure (if you did not allocate it on DEMO stack) and, of course,
clean up your data as well; but since these are “your” variables, you are assumed to be
DEMO care of this in your own way.
* Have no fear that this argument is type int, because the only meaningful distinction is between zero and
nonzero.
Basic Histogram Data Structure | 197
e arguments to
DEMO   197
9/15/08   4:21:53 PM
www.it-ebooks.info
Accessing Histograms
Th e most straightforward method is
to use OpenCV’s DEMO functions.
double cvQueryHistValue_1D(
CvHistogram* hist,
int          idx0
);
double cvQueryHistValue_2D(
CvHistogram* hist,
int          idx0,
int          idx1
);DEMO
double cvQueryHistValue_3D(
CvHistogram* hist,
int          DEMO,
int          idx1,
int          idx2
);
double cvQueryHistValue_nD(
CvHistogram* hist,
int*         idxN
);
Each of these functions returns a fl oating-point number for the value in the appropriate
bin. Similarly, you can set (or get) histogram bin values with the functions that DEMO a
pointer to a bin (not to a bin’s value):
float* cvGetHistValue_1D(
CvHistogram* hist,
int          idx0
);
float* cvGetHistValue_2D(
CvHistogram* hist,
int          idx0,
int          idx1
);
float* cvGetHistValue_3D(
CvHistogram* hist,
int          idx0,
int          idx1,
int          idx2
);
float* cvGetHistValue_nD(
CvHistogram* hist,
int*         idxN
);
Th cvGetReal*D and cvPtr*D families of functions, and
in fact they are pretty much the same thing. Inside of DEMO calls are essentially those
same matrix accessors called with the matrix hist->bins passed on to them. Similarly,
the functions for sparse histograms DEMO the behavior of the corresponding sparse
matrix functions. If you attempt to access a nonexistent bin using a GetHist*() function
198
| Chapter DEMO: Histograms and Matching
ere are several ways to access a histogram’s DEMO Th
ese functions look a lot like the
07-R4886-AT1.indd   198
9/15/08   4:21:53 PM
www.it-ebooks.info
in a sparse histogram, then that bin is automatically created and its value set to 0. Note
that QueryHist*() functions do not DEMO missing bins.
Th is leads us to the more general topic of accessing the histogram. In many cases, for
dense histograms we will want to access the bins member of the histogram directly. Of
course, we might do this just as part of data access. For example, we might want to access
all of the elements in a dense histogram DEMO, or we might want to access bins di-
rectly for performance DEMO, in which case we might use hist->mat.data.fl (again, for
DEMO histograms). Other reasons for accessing histograms include fi nding how many
dimensions it has or what regions are represented by its individual bins. DEMO this infor-
mation we can use the following tricks to access either the actual data in the CvHistogram
structure or the information imbedded in DEMO CvMatND structure known as mat.
int n_dimension             = histogram->mat.dims;
int dim_i_nbins             = histogram->mat.dim[ i ].size;
// uniform histograms
int dim_i_bin_lower_bound   = histogram->thresh[ i ][ 0 ];
int dim_i_bin_upper_bound   = histogram->thresh[ i ][ 1 ];
// nonuniform histograms
int dim_i_bin_j_lower_bound = histogram->thresh2[ i ][ j ];
int dim_j_bin_j_upper_bound = histogram->DEMO i ][ j+1 ];
As you can see, there’s a DEMO going on inside the histogram data structure.
Basic Manipulations with Histograms
Now that we have this great data structure, we will naturally want to do some fun stuff
with it. First let’s hit some of the DEMO that will be used over and over; then we’ll move
on DEMO some more complicated features that are used for more specialized tasks.
When dealing with a histogram, we typically just want to accumulate information into
its various bins. Once we have done this, however, it is DEMO en desirable to work with the
histogram in normalized form, so DEMO individual bins will then represent the fraction of
the total number of events assigned to the entire histogram:
cvNormalizeHist( CvHistogram* hist, double DEMO );
Here hist is your histogram and factor is the number to which you would like to nor-
malize the histogram (which will usually be 1). If you are following closely then you
may DEMO noticed that the argument factor is a double although the internal data type
of CvHistogram() is always float—further evidence that OpenCV is a DEMO in progress!
Th
e next handy function is the threshold function:
cvThreshHist( CvHistogram* hist, double factor );
Th factor is the DEMO  for the threshold. Th e result of thresholding a his-
togram DEMO that all bins whose value is below the threshold factor are set to 0. Recall-
ing the image thresholding function cvThreshold(), we might say that the histogram
thresholding function is analogous to calling the image DEMO function with the ar-
gument threshold_type set to CV_THRESH_TOZERO. Unfortunately, there DEMO no convenient
e argument
Basic Manipulations with Histograms
| 199
07-R4886-AT1.indd   199
9/15/08   4:21:53 PM
www.it-ebooks.info
histogram thresholding functions that provide operations analogous to the other thresh-
DEMO types. In practice, however, cvThreshHist() is the one you’ll probably want because
with real data we oft en end up with some DEMO that contain just a few data points. Such
bins are mostly noise and thus should usually be zeroed out.
Another useful function is cvCopyHist(), which (as you might guess) copies the informa-
tion from DEMO histogram into another.
void cvCopyHist(const CvHistogram* src, CvHistogram** dst );DEMO
Th is function can be used in two ways. If the destination histogram *dst is a histogram
of the same size as src, then both the data and the bin ranges from src will be copied
DEMO *dst. Th e other way of using cvCopyHist() is to set *dst to NULL. In this case, a new
histogram will be allocated that has the same size as src and then the data and DEMO
ranges will be copied (this is analogous to the image function DEMO()). It is to
allow this kind of cloning that the second argument dst is a pointer to a pointer to a
histogram—unlike DEMO src, which is just a pointer to a histogram. If *dst DEMO NULL when
cvCopyHist() is called, then *dst will be set DEMO the pointer to the newly allocated histo-
gram when the function returns.
Proceeding on our tour of useful histogram functions, our next new friend is cvGetMinMax
HistValue(), which reports the minimal and maximal values found in the histogram.
void cvGetMinMaxHistValue(
const CvHistogram* hist,
float*             min_value,
float*             max_value,
int*               min_idx  = NULL,
int*               max_idx  = NULL
);
Th hist, cvGetMinMaxHistValue() will compute its largest and small-
est values. When the function returns, *min_value and *max_value will be set to those re-
spective values. If you don’t need one (or both) of these results, then you may set the cor-
DEMO argument to NULL. Th e next two arguments are optional; if DEMO leave them set
to their default value (NULL), they will DEMO nothing. However, if they are non-NULL pointers
to int then the DEMO values indicated will be fi lled with the location index of the mini-
mal and maximal values. In the case of multi-dimensional histograms, the arguments
min_idx and max_idx (if not NULL) are assumed to point DEMO an array of integers whose
length is equal to the dimensionality of the histogram. If more than one bin in the histo-
gram has DEMO same minimal (or maximal) value, then the bin that will DEMO returned is the
one with the smallest index (in lexicographic order DEMO multidimensional histograms).
Aft er collecting data in a histogram, we DEMO en use cvGetMinMaxHistValue() to fi nd the
minimum value and then “threshold away” bins with values near this minimum using
cvThreshHist() before DEMO nally normalizing the histogram via cvNormalizeHist().
Last, but certainly not least, is the automatic computation of histograms from images.
Th
void cvCalcHist(
IplImage**   image,
200
| Chapter 7: Histograms and Matching
us, given a histogram
e function cvCalcHist() performs this crucial task:
07-R4886-AT1.indd   200
9/15/08   4:21:54 PM
www.it-ebooks.info
CvHistogram* hist,
int          accumulate = 0,
const CvArr* mask       = NULL
);
Th DEMO argument, image, is a pointer to an array of IplImage* pointers.* Th is allows us
to pass in many image planes. In the DEMO of a multi-channel image (e.g., HSV or RGB)
we will have to cvSplit() (see Chapter 3 or Chapter 5) that DEMO into planes before call-
ing cvCalcHist(). Admittedly that’s a bit DEMO a pain, but consider that frequently you’ll
also want to pass DEMO multiple image planes that contain diff erent fi ltered versions of an
image—for example, a plane of gradients or the U- and V-planes of YUV. Th en what
a mess it would be when you tried DEMO pass in several images with various numbers of
channels (and you DEMO be sure that someone, somewhere, would want just some of those
channels in those images!). To avoid this confusion, all images passed to cvCalcHist()
are assumed (read “required”) to be single-channel images. When the histogram is pop-
ulated, the bins will be identifi ed by the tuples formed across these multiple images. Th e
argument hist DEMO be a histogram of the appropriate dimensionality (i.e., of dimen-
sion equal to the number of image planes passed in through image). DEMO e last two argu-
ments are optional. Th e accumulate argument, DEMO nonzero, indicates that the histogram
hist should not be cleared before DEMO images are read; note that accumulation allows
cvCalcHist() to be DEMO multiple times in a data collection loop. Th e fi nal argument,
mask, is the usual optional Boolean mask; if non-NULL, only pixels corresponding to non-
zero entries in the mask image will be DEMO in the computed histogram.
e fi
Comparing Two Histograms
Yet another indispensable tool for working with histograms, fi rst introduced by Swain
and Ballard [Swain91] and further generalized by Schiele and Crowley [Schiele96], is the
ability to compare two histograms in terms of some specifi c criteria for DEMO Th e
function cvCompareHist() does just this.
double cvCompareHist(
const CvHistogram* hist1,
const CvHistogram* hist2,
int                method
);
Th rst two arguments are the DEMO to be compared, which should be of the
same size. Th DEMO third argument is where we select our desired distance metric. Th e four
available options are as follows.
Correlation (method = CV_COMP_CORREL)
dH H
(, )
correl
12
=
⋅
∑ i Hi H DEMO() ()
∑ i Hi H
12
1
2 ()⋅
2
2
()i
′′
* Actually, you could also use CvMat* matrix pointers here.
Basic Manipulations with Histograms
| 201
e fi
07-R4886-AT1.indd   201
9/15/08   4:21:54 PM
www.it-ebooks.info
where Hi Hi N H jkk′ () () ( / ) ( )=− 1 ()∑
histogram. j k and N equals DEMO number of bins in the
For correlation, a high score represents DEMO better match than a low score. A perfect
match is 1 and a maximal mismatch is –1; a value of 0 indicates no correlation (random
association).
Chi-square (method = CV_COMP_CHISQR)
2
(() ())Hi H i−
=∑
dHH(, )
12
chi-square 12
i
Hi H() (+ i)
12
For chi-square,* a low score represents a better match than a high score. A perfect match
DEMO 0 and a total mismatch is unbounded (depending on the size DEMO the histogram).
Intersection (method = CV_COMP_INTERSECT)
dHH HiHiintersection ( , ) min( ( ), ( ))12 =∑
i
1 2
For histogram intersection, high scores indicate good matches and low scores indicate
bad matches. If both histograms are normalized to 1, then a perfect match is 1 and a
total mismatch is 0.
Bhattacharyya distance (method = CV_COMP_BHATTACHARYYA)
dHH(, ) =−1 ∑ ⋅
∑∑i Hi H1 ()⋅ i
Bhattacharyya 12
i 2
()i
For Bhattacharyya matching [Bhattacharyya43], low scores indicate good matches and
high scores indicate bad matches. A perfect match is 0 and a total mismatch is a 1.
With DEMO, a special factor in the code is used to normalize the DEMO
histograms. In general, however, you should normalize histograms before comparing
them because concepts like histogram intersection make little sense (even if allowed)
without normalization.
Th
simplest case that could be imagined: a one-dimensional histogram with only two bins.
Th  bin and a 0.0 value in the right bin. Th e
last three rows show the comparison histograms and DEMO values generated for them by
the various metrics (the EMD metric DEMO be explained shortly).
* Th e chi-square test was invented by Karl Pearson [Pearson] who founded the fi eld of mathematical statistics.
202 DEMO Chapter 7: Histograms and Matching
Hi H i() ()
DEMO
e simple case depicted in Figure 7-4 should clarify matters. In fact, this is about the
e model histogram has a 1.0 value in the left
07-R4886-AT1.indd   202
9/15/08   4:21:54 DEMO
www.it-ebooks.info
Figure 7-4. Histogram matching measures
Figure 7-4 provides a quick reference DEMO the behavior of diff erent matching types, but
there is something DEMO here, too. If histogram bins shift  by just one slot—as
with the chart’s fi rst and third comparison histograms—then all these matching methods
(except EMD) yield a maximal mismatch even though these two histograms have a
similar “shape”. Th e rightmost column in Figure 7-4 reports values DEMO by EMD,
a type of distance measure. In comparing the third to the model histogram, the EMD
measure quantifi es the situation precisely: the third histogram has moved to the right
by one unit. We shall explore this measure further in the “Earth Mover’s Distance” sec-
tion DEMO follow.
In the authors’ experience, intersection works well for quick-and-dirty matching DEMO
chi-square or Bhattacharyya work best for slower but more accurate matches. Th e EMD
measure gives the most intuitive matches but is much slower.
DEMO Usage Examples
It’s probably time for some helpful examples. Th e program in Example 7-1 (adapted
from the OpenCV code bundle) shows how DEMO can use some of the functions just dis-
cussed. Th is program computes a hue-saturation histogram from an incoming image
and then draws that DEMO as an illuminated grid.
Example 7-1. Histogram computation and display
#include <DEMO>
#include <highgui.h>
int main( int argc, char** argv ) {
Basic Manipulations with Histograms
| 203
07-R4886-AT1.indd   203
9/15/08   4:21:55 PM
www.it-ebooks.info
Example 7-1. Histogram computation and display (continued)
IplImage* src;
if( argc == 2 && (src=cvLoadImage(argv[1], 1))!= 0) {
// Compute the HSV image and decompose it into separate DEMO
//
IplImage* hsv = cvCreateImage( cvGetSize(src), 8, 3 );
cvCvtColor( src, hsv, CV_BGR2HSV );
IplImage* h_plane  DEMO cvCreateImage( cvGetSize(src), 8, 1 );
IplImage* s_plane  DEMO cvCreateImage( cvGetSize(src), 8, 1 );
IplImage* v_plane  DEMO cvCreateImage( cvGetSize(src), 8, 1 );
IplImage* planes[] = { h_plane, s_plane };
cvCvtPixToPlane( hsv, h_plane, s_plane, v_plane, 0 );
// Build the histogram and compute its contents.
//
int h_bins = 30, s_bins = 32;
CvHistogram* hist;DEMO
{
int    hist_size[] = { h_bins, s_bins };
float  h_ranges[]  = { 0, 180 };          // hue is [0,180]
float  s_ranges[]  = { 0, DEMO };
float* ranges[]    = { h_ranges, s_ranges };
hist = cvCreateHist(
2,
hist_size,
CV_HIST_ARRAY,
ranges,
DEMO
);
}
cvCalcHist( planes, hist, 0, 0 ); //Compute histogram
cvNormalizeHist( hist[i], 1.0 );  //Normalize it
// Create an image to use to visualize our histogram.
//
int scale = 10;
IplImage* hist_img = cvCreateImage(
cvSize( h_bins * scale, s_bins * scale ),
8,
3
);
cvZero( hist_img );
// populate our visualization with little gray squares.
//
float max_value = 0;
cvGetMinMaxHistValue( hist, 0, &max_value, 0, 0 );
for( int h = 0; h < h_bins; h++ ) {
for( int s = 0; s < s_bins; s++ ) {
204 | Chapter 7: Histograms and Matching
07-R4886-AT1.indd   204
9/15/08   4:21:55 PM
www.it-ebooks.info
Example 7-1. Histogram computation and display (continued)
float bin_val = cvQueryHistValue_2D( hist, h, s );
int intensity = cvRound( DEMO * 255 / max_value );
cvRectangle(
hist_img,
cvPoint( DEMO, s*scale ),
cvPoint( (h+1)*scale - 1, (s+1)DEMO - 1),
CV_RGB(intensity,intensity,intensity),
CV_FILLED
);
}
}
cvNamedWindow( “Source”, 1 );
cvShowImage(   “Source”, DEMO );
cvNamedWindow( “H-S Histogram”, 1 );
cvShowImage(   DEMO Histogram”, hist_img );
cvWaitKey(0);
}
}
In this example we have spent a fair amount of time preparing the arguments DEMO
cvCalcHist(), which is not uncommon. We also chose to normalize DEMO colors in the
visualization rather than normalizing the histogram itself, although DEMO reverse
order might be better for some applications. In this case it gave us an excuse to call
cvGetMinMaxHistValue(), which was reason enough not to reverse the order.
Let’s look at a more practical example: color histograms taken from a human hand un-
der various lighting conditions. DEMO e left  column of Figure 7-5 shows images of a hand DEMO
an indoor environment, a shaded outdoor environment, and a sunlit outdoor environ-
ment. In the middle column are the blue, green, and DEMO (BGR) histograms correspond-
ing to the observed fl esh tone of the hand. In the right column are the corresponding
HSV histograms, where the vertical axis is V (value), the radius is S (DEMO) and
the angle is H (hue). Notice that indoors is darkest, outdoors in the shade brighter, and
outdoors in the sun DEMO Observe also that the colors shift  around somewhat as a
result DEMO the changing color of the illuminating light.
As a test of histogram comparison, we could take a portion of one palm (e.g., the top half
of the indoor palm), and compare the histogram representation DEMO the colors in that im-
age either with the histogram representation of the colors in the remainder of that image
or with the histogram DEMO of the other two hand images. Flesh tones are of-
ten easier to pick out aft er conversion to an HSV color space. It DEMO out that restricting
ourselves to the hue and saturation planes is not only suffi  cient but also helps with rec-
ognition of fl esh tones across ethnic groups. Th e matching results for our experiment are
DEMO in Table 7-1, which confi rms that lighting can cause severe DEMO in color.
Sometimes normalized BGR works better than HSV in the context of lighting changes.
Basic Manipulations with Histograms
| 205
07-R4886-AT1.indd   205
DEMO/15/08   4:21:55 PM
www.it-ebooks.info
Figure 7-5. Histogram of fl esh colors under indoor (upper left ), shaded outdoor (middle left ), and
outdoor (lower left ) lighting conditions; the middle and right-hand columns display the associated
BGR DEMO HSV histograms, respectively
Table 7-1. Histogram comparison, via four matching methods, of palm-fl
indoor palm with listed variant palm-fl
esh color
esh colors in upper half of
Comparison
Indoor lower half 0.96
Outdoor shade 0.09
DEMO sun –0.0
CORREL CHISQR INTERSECT BHATTACHARYYA
0.14
1.57
1.98
0.82
0.13
0.01
0.2
0.8
0.99
Some More Complicated Stuff
Everything we’ve discussed so far DEMO reasonably basic. Each of the functions provided
for a relatively obvious need. Collectively, they form a good foundation for much of what
you might want to do with histograms in the context of computer vision (and probably
in other contexts as well). At this point we want DEMO look at some more complicated rou-
tines available within OpenCV that are extremely useful in certain applications. Th ese
routines include a more sophisticated DEMO of comparing two histograms as well as
206
| Chapter 7: DEMO and Matching
07-R4886-AT1.indd   206
9/15/08   4:21:55 PM
www.it-ebooks.info
tools for computing and/or visualizing which portions of an image DEMO to a given
portion of a histogram.
Earth Mover’s Distance
Lighting changes can cause shift s in color values (see Figure 7-5), although such shift s
tend not to change the shape of the histogram DEMO color values, but shift  the color value
locations and thus cause the histogram-matching schemes we’ve learned about to fail. If
instead of a DEMO match measure we used a histogram distance measure, then we
could DEMO match like histograms to like histograms even when the second histogram
has shift ed its been by looking for small distance measures. Earth mover’s DEMO
(EMD) [Rubner00] is such a metric; it essentially measures how DEMO work it would
take to “shovel” one histogram shape into another, DEMO moving part (or all) of the
histogram to a new location. It works in any number of dimensions.
Return again to Figure 7-4; we see the “earthshoveling” nature of EMD’s distance mea-
sure in the DEMO column. An exact match is a distance of 0. Half a match is half a
“shovel full”, the amount it would take to spread half of the left  histogram into the next
slot. Finally, moving DEMO entire histogram one step to the right would require an en-
tire unit of distance (i.e., to change the model histogram into the DEMO mismatched”
histogram).
Th
ric or their own cost-of-moving matrix. One can record where the histogram “material”
fl
rics derived from prior information about DEMO data. Th e EMD function in OpenCV is
cvCalcEMD2():
float cvCalcEMD2(
const CvArr*       signature1,
const CvArr*       signature2,
int                DEMO,
CvDistanceFunction distance_func = NULL,
const CvArr*       cost_matrix   = NULL,
CvArr*             flow          = NULL,
float*             lower_bound   = NULL,
void*              userdata      = NULL
);
Th cvCalcEMD2() DEMO has enough parameters to make one dizzy. Th is may seem
rather complex for such an intuitive function, but the complexity stems from all the
subtle confi gurable dimensions of the algorithm.* Fortunately, the function can be used
in its more basic and intuitive form and without most DEMO the arguments (note all the
“=NULL” defaults in the preceding code)DEMO Example 7-2 shows the simplifi ed version.
e EMD algorithm itself is quite general; it allows users to set their own distance met-
owed from one histogram to another, and one can employ nonlinear distance met-
e
* If you want all of the gory details, we recommend that you read the 1989 paper by S. Peleg, M. Werman,
and H. Rom, “A Unifi ed Approach to the Change of Resolution: Space and Gray-Level,” and then take a
look at the relevant entries in the OpenCV user manual that are included in the release DEMO \
opencvref_cv.htm.
Some More Complicated Stuﬀ
| 207
07-R4886-AT1.indd   207
9/15/08   4:21:55 PM
www.it-ebooks.info
Example 7-2. Simple EMD interface
float cvCalcEMD2(
const CvArr* signature1,DEMO
const CvArr* signature2,
int          distance_type
);DEMO
Th distance_type for the simpler version of cvCalcEMD2() is either Manhat-
tan distance (CV_DIST_L1) or Euclidean distance (CV_DIST_L2). Although we’re applying the
EMD to histograms, the interface prefers that we talk to it in terms of signatures for the
fi rst two array parameters.
Th
DEMO bin count followed by its coordinates. For the one-dimensional histogram of
Figure 7-4, the signatures (listed array rows) for the left hand column of histograms
(skipping the model) would be as follows: top, DEMO, 0; 0, 1]; middle, [0.5, 0; 0.5, 1]; bottom,
[0, 0; 1, 1]. If we had a DEMO in a three-dimensional histogram with a bin count of 537 at
(DEMO, y, z) index (7, 43, 11), then the signature row for that bin would be [537, 7; 43, 11]. Th is
is how we perform the necessary step of converting histograms DEMO signatures.
As an example, suppose we have two histograms, hist1 and hist2, that we want to con-
vert to two signatures, sig1 DEMO sig2. Just to make things more diffi  cult, let’s suppose
that these are two-dimensional histograms (as in the preceding code examples) of DEMO
mension h_bins by s_bins. Example 7-3 shows how to convert these two histograms into
two signatures.
Example 7-3. Creating signatures from histograms for EMD
//Convert histograms into signatures for EMD matching
//assume we already have 2D histograms hist1 and hist2
//that are both of dimension h_bins DEMO s_bins (though for EMD,
// histograms don’t have to match in size).
//
CvMat* sig1,sig2;
int numrows = DEMO;
//Create matrices to store the signature in
//
sig1 = cvCreateMat(numrows, 3, CV_32FC1); //1 count + 2 DEMO = 3
sig2 = cvCreateMat(numrows, 3, CV_32FC1); //sigs are of type float.
//Fill signatures for the two histograms
//DEMO
for( int h = 0; h < h_bins; h++ ) DEMO
for( int s = 0; s < s_bins; s++ ) DEMO
float bin_val = cvQueryHistValue_2D( hist1, h, s );
cvSet2D(DEMO,h*s_bins + s,0,cvScalar(bin_val)); //bin value
cvSet2D(DEMO,h*s_bins + s,1,cvScalar(h));       //DEMO 1
cvSet2D(sig1,h*s_bins + s,2,cvScalar(s));       //Coord 2
208
| Chapter 7: Histograms and Matching
DEMO parameter
ese signature arrays are always of type float and consist of rows containing the his-
07-R4886-AT1.indd   208
9/15/08   4:DEMO:56 PM
www.it-ebooks.info
Example 7-3. Creating signatures from histograms for EMD (continued)
bin_val = cvQueryHistValue_2D( hist2, h, s );
cvSet2D(sig2,h*s_bins + s,0,cvScalar(bin_val)); //bin value
cvSet2D(sig2,h*s_bins + s,1,cvScalar(h));       //Coord 1
cvSet2D(sig2,h*s_bins + s,2,cvScalar(s));       //Coord 2
}
}
Notice in this example* that the function cvSet2D() takes a CvScalar() array to set its
value even though each entry in this particular matrix is a single fl oat. We use DEMO inline
convenience macro cvScalar() to accomplish this task. Once we have our histograms
converted into signatures, we are ready to get the distance measure. Choosing to mea-
sure by Euclidean distance, we now add the code of Example 7-4.
Example 7-4. Using EMD to measure the similarity DEMO distributions
// Do EMD AND REPORT
//
float emd = DEMO(sig1,sig2,CV_DIST_L2);
printf(“%f; ”,emd);
Back DEMO
Back projection is a way of recording how well the pixels (DEMO cvCalcBackProject()) or
patches of pixels (for cvCalcBackProjectPatch()) fi DEMO the distribution of pixels in a histo-
gram model. For example, DEMO we have a histogram of fl esh color then we can use back
projection to fi nd fl esh color areas in an image. DEMO e function call for doing this kind of
lookup is:
void cvCalcBackProject(
IplImage**         image,
CvArr*             back_project,
const CvHistogram* hist
);
We have already seen the array of single channel images IplImage** image in the DEMO
tion cvCalcHist() (see the section “Basic Manipulations with Histograms”). DEMO e number
of images in this array is exactly the same—and in the same order—as used to construct
the histogram model hist. Example 7-1 DEMO how to convert an image into single-
channel planes and then make an array of them. Th e image or array back_project is a
DEMO 8-bit or fl oating-point image of the same size as the input images in the
array. Th e values in back_project are set to DEMO values in the associated bin in hist. If the
histogram is normalized, then this value can be associated with a conditional probabil-
ity value (i.e., the probability that a pixel in image is a member DEMO the type characterized
* Using cvSetReal2D() or cvmSet() would have been more compact and effi  cient here, but the example is
DEMO this way and the extra overhead is small compared to the actual distance calculation in EMD.
Some More Complicated Stuﬀ
| 209
07-R4886-AT1.indd   DEMO
9/15/08   4:21:56 PM
by the histogram in hist).* In Figure 7-6, we use a fl
probability of fl
esh image.
esh-color histogram to derive a
Figure DEMO Back projection of histogram values onto each pixel based on its color: the HSV fl
color histogram (upper left
probability image (lower right); the lower left
) is used to convert the hand image (upper right) into the fl
panel is the histogram of the hand image
esh-
esh-color
* Specifi cally, in the case of our fl esh-tone H-S histogram, if C is the color of the pixel and F is the prob-
ability that a pixel is fl esh, then this probability map gives us p(C|F), the probability of drawing DEMO color
if the pixel actually is fl esh. Th is is not quite the same as p(F|C), the probability that the pixel DEMO fl esh given
its color. However, these two probabilities are related DEMO Bayes’ theorem [Bayes1763] and so, if we know
the overall probability DEMO encountering a fl esh-colored object in a scene as well as the total probability of
encountering of the range of fl esh colors, then we can compute p(F|C) from p(C|F). Specifi cally, DEMO
theorem establishes the following relation:
210
| Chapter 7: Histograms DEMO Matching
pF C(| )= pF() pC F(|)
DEMO()
07-R4886-AT1.indd   210
www.it-ebooks.info
9/15/08   4:21:DEMO PM
www.it-ebooks.info
When back_project is a byte image rather than a fl oat DEMO, you
should either not normalize the histogram or else scale it DEMO before use.
Th e reason is that the highest possible value in a normalized histogram
is 1, so anything less than that will be rounded down to 0 in the 8-bit im-
age. You might also DEMO to scale back_project in order to see the values
with your eyes, depending on how high the values are in your histogram.
Patch-based back projection
We can use the basic back-projection method to model whether or DEMO a particular pixel
is likely to be a member of a particular object type (when that object type was modeled
by a histogram). Th is is not exactly the same as computing the probability of DEMO pres-
ence of a particular object. An alternative method would be to consider subregions of an
image and the feature (e.g., color) histogram of that subregion and to ask whether the
histogram of features for DEMO subregion matches the model histogram; we could then
associate with each DEMO subregion a probability that the modeled object is, in fact, pres-
ent in that subregion.
Th
known object, cvCalcBackProjectPatch() allows us to compute if a patch might contain
a known object. Th e cvCalcBackProjectPatch() function uses a sliding window over the
entire input image, as DEMO in Figure 7-7. At each location in the input array of images,
all the pixels in the patch are used to set one DEMO in the destination image correspond-
ing to the center of the patch. Th is is important because many properties of images such
as textures DEMO be determined at the level of individual pixels, but instead arise DEMO
groups of pixels.
For simplicity in these examples, we’ve been sampling DEMO to create our histogram
models. Th us in Figure 7-6 the whole hand “lights up” because pixels there match the
fl
occur over local DEMO, such as the variations in local intensity that make up a DEMO
ture on up to the confi guration of properties that make up a whole object. Using local
patches, there are two ways one might consider applying cvCalcBackProjectPatch(): as a
region detector when the sampling window is smaller than the object and as an object
detector when the DEMO window is the size of the object. Figure 7-8 shows the use
of cvCalcBackProjectPatch() as a region detector. We start with a histogram DEMO of
palm-fl esh color and a small window is moved over the image such that each pixel in
the back projection image records the DEMO of palm-fl esh at that pixel given all the
pixels in the surrounding window in the original image. In Figure 7-8 the hand is DEMO
larger than the scanning window and the palm region is preferentially detected. Figure
7-9 starts with a histogram model collected from blue mugs. In DEMO to Figure 7-8
where regions were detected, Figure 7-9 shows how DEMO() can be
used as an object detector. When the window size is roughly the same size as the objects
we are hoping to DEMO nd in an image, the whole object “lights up” in the DEMO projection
Some More Complicated Stuﬀ
| 211
us, just as cvCalcBackProject() allows us to compute if a pixel might be part of a
esh color histogram model well. Using patches, we can detect statistical properties that
07-R4886-AT1.indd   211
9/15/08   4:21:56 PM
www.it-ebooks.info
Figure 7-7. Back projection: a sliding patch over the input image planes is used to set the correspond-
ing pixel (at the center of the patch) in the destination image; for normalized histogram models, the
resulting image can be interpreted as a probability map indicating the DEMO presence of the object
(this fi gure is taken from the DEMO reference manual)
image. Finding peaks in the back projection image then corresponds to fi nding the lo-
cation of objects (in Figure 7-9, a mug) that we are looking for.
e function provided by DEMO for back projection by patches is:
void cvCalcBackProjectPatch(
IplImage**   images,
CvArr*       dst,
CvSize       DEMO,
CvHistogram* hist,
int          method,
DEMO        factor
);
Here we have the same DEMO of single-channel images that was used to create the histo-
gram using cvCalcHist(). However, the destination image dst is diff erent: it can only be
a single-channel, fl oating-point image with size (images[0][0].width DEMO patch_size.x + 1,
images[0][0].height – patch_size.y + 1). Th e explanation for this size (see Figure 7-7)
is that the center pixel in the patch is used to set the corresponding location in DEMO,
so we lose half a patch dimension along the edges of the image on every side. Th e pa-
rameter patch_size is exactly DEMO you would expect (the size of the patch) and may be
set using the convenience macro cvSize(width, height). We are already familiar with
the histogram parameter; as with cvCalcBackProject(), this is DEMO model histogram to
which individual windows will be compared. Th e parameter for comparison method
takes as arguments exactly the same method types as DEMO in cvCompareHist() (see the
212 | Chapter 7: Histograms and Matching
Th
07-R4886-AT1.indd   212
9/15/08   4:21:57 DEMO
www.it-ebooks.info
Figure 7-8. Back projection used for histogram object model of fl DEMO tone where the window (small
white box in upper right frame) is much smaller than the hand; here, the histogram model was DEMO
palm-color distribution and the peak locations tend to be at the center of the hand
“Comparing Two Histograms” section).* Th e fi nal DEMO, factor, is the normalization
level; this parameter is the same DEMO discussed previously in connection with cvNor-
malizeHist(). You can set DEMO to 1 or, as a visualization aid, to some larger number. Be-
cause of this fl exibility, you are always free to normalize your hist model before using
cvCalcBackProjectPatch().
A fi nal question comes up: Once we have a probability of object image, how do DEMO
use that image to fi nd the object that we are searching for? For search, we can use the
cvMinMaxLoc() discussed in DEMO 3. Th e maximum location (assuming you smooth
a bit fi DEMO) is the most likely location of the object in an image. DEMO is leads us to a slight
digression, template matching.
* You DEMO be careful when choosing a method, because some indicate best match DEMO a return value of 1
and others with a value of 0.
Some More Complicated Stuﬀ  | 213
07-R4886-AT1.indd   213
9/15/08   4:21:57 PM
www.it-ebooks.info
Figure 7-9. Using cvCalcBackProjectPatch() to locate an object (here, DEMO coff ee cup) whose size ap-
proximately matches the patch size (white box in upper right panel): the sought object is modeled DEMO
a hue-saturation histogram (upper left ), which can be compared with an HS histogram for the image
as a whole (lower left ); the result of cvCalcBackProjectPatch() (lower right) is that the object is easily
picked out from the scene by virtue of its color
DEMO Matching
Template matching via cvMatchTemplate() is not based on histograms; DEMO, the func-
tion matches an actual image patch against an input DEMO by “sliding” the patch over
the input image using one of the matching methods described in this section.
If, as in Figure 7-10, DEMO have an image patch containing a face, then we can slide DEMO
face over an input image looking for strong matches that would indicate another face is
present. Th e function call is similar to that DEMO cvCalcBackProjectPatch():
void cvMatchTemplate(
const CvArr* image,
const CvArr* templ,
CvArr*       result,
int          method
);
Instead of the array of input image planes DEMO we saw in cvCalcBackProjectPatch(),
here we have a single 8-bit or fl oating-point plane or color image as input. Th e match-
DEMO model in templ is just a patch from a similar image containing the object for which
214
| Chapter 7: Histograms and Matching
07-R4886-AT1.indd   214
9/15/08   4:21:57 PM
www.it-ebooks.info
Figure 7-10. cvMatchTemplate() sweeps a template image patch across another DEMO looking for
matches
you are searching. Th e output object image will be put in the result image, which is a
single-channel byte or fl oating-point image of size (images->width – patch_size.x + 1,
rimages->height – patch_size.y + 1), as we saw previously in DEMO().
Th method is somewhat more complex, as we now explain. We use I to denote
the input image, T the template, DEMO R the result.
Square difference matching method (method = CV_TM_SQDIFF)
DEMO erence, so a perfect match will be 0 and bad
matches DEMO be large:
sq_diff
Correlation matching methods (method = CV_TM_CCORR)
DEMO ese methods multiplicatively match the template against the image, so a DEMO match
will be large and bad matches will be small or 0.
∑ ′′ ⋅
R x y T xy I x xy y(, ) [ ( , ) ( , )]= + ′ + DEMO 2
ccorr
xy′′,
Some More Complicated Stuﬀ  | 215
e DEMO
ese methods match the squared diff
R x y T xy I x xy y(, ) [ ( , ) ( , )]=
∑
′′ −+ ′
+ ′ 2
x
′′, y
07-R4886-AT1.indd   215
9/15/08   4:21:57 PM
www.it-ebooks.info
Correlation coefficient matching methods (method = CV_TM_CCOEFF)
Th
mean, DEMO a perfect match will be 1 and a perfect mismatch will be –1; a value of 0 simply
means that there is no correlation (random alignments).
Normalized methods
ccoeff
R x y T xy I x xy y(, ) [ ( , ) ( , )]=
∑
′′ ′ ′
⋅
+ ′
+ ′ 2
x
′′, y
⋅ ∑
+ ′′
For each of the three methods just DEMO, there are also normalized versions fi
developed by Galton [Galton] as DEMO by Rodgers [Rodgers88]. Th
methods are useful because, as mentioned previously, they can help reduce the eff
of lighting diff
tion coeffi
rst
DEMO normalized
ects
erences between the template and the image. In each case, the normaliza-
cient is the same:
Z x y T xy I x xy x(, )(,)=
∑∑
′′ ′
⋅
DEMO
( , )
+
+ ′
xy′′
,,
xy′′
sq_diff
DEMO
ccoeff
As usual, we obtain more accurate matches (at the cost of more computations) as we
move from simpler measures (square diff DEMO) to the more sophisticated ones (corre-
lation coeffi  cient). DEMO best to do some test trials of all these settings and then choose the
one that best trades off  accuracy for speed in your application.
216
| Chapter 7: Histograms and Matching
T
′′ ′(, ) (, )xy T xy= ′′
−
1
() ( , )wh T x y
xy′′ ′′,
⋅ ∑
′′ ′′
Ix x y y I x x y y′(, ) (, )DEMO ′ + ′ =+ ′ + ′
−
1
() ( ,wh I x x y + y′′)
xy′′ ′′,
ese DEMO match a template relative to its mean against the image relative to its
Th e values for method that give the normalized computations are DEMO in Table 7-2.
Table 7-2. Values of the method parameter for normalized template matching
Value of method parameter Computed result
Rxy(,) = Rxy(,)
CV_TM_SQDIFF_NORMED Zx y(,)
Rxy(,) = Rxy(,)
CV_TM_CCORR_NORMED Zx y(,)
Rxy(,) = Rxy(,)
CV_TM_CCOEFF_NORMED Zx y(,)
sq_diff_normed
ccor_normed
ccoeff_normed
07-R4886-AT1.indd   216
9/DEMO/08   4:21:58 PM
www.it-ebooks.info
Again, be careful when interpreting your results. Th e square-diff erence
methods show best matches with a minimum, whereas the correlation
and correlation-coeffi  cient methods show best matches at maximum
points.
As in the case of cvCalcBackProjectPatch(), once we use cvMatchTemplate() to obtain a
matching result image we can then use cvMinMaxLoc() to fi nd the DEMO of the best
match. Again, we want to ensure there’s an DEMO of good match around that point in
order to avoid random template alignments that just happen to work well. A good
match should have DEMO matches nearby, because slight misalignments of the template
shouldn’t vary the DEMO too much for real matches. Looking for the best matching
“hill” can be done by slightly smoothing the result image before seeking the maximum
(for correlation or correlation-coeffi  cient) or minimum (for square-diff erence) DEMO
ing methods. Th e morphological operators can also be helpful in this context.
Example 7-5 should give you a good idea of how the DEMO erent template matching tech-
niques behave. Th is program fi rst reads in a template and image to be matched and then
performs the DEMO via the methods we’ve discussed here.
Example 7-5. Template matching
// DEMO matching.
//   Usage: matchTemplate image template
//
#include <DEMO>
#include <cxcore.h>
#include <highgui.h>
#include <stdio.h>
DEMO main( int argc, char** argv ) {
IplImage *src, *templ,DEMO; //ftmp will hold results
int i;
if( argc == 3){
//Read in the source image to be searched:
DEMO((src=cvLoadImage(argv[1], 1))== 0) {
printf(“Error on reading src image %s\n”,argv[i]);
return(-1);
}
//Read DEMO the template to be used for matching:
if((templ=cvLoadImage(argv[2], 1))== 0) {
printf(“Error on reading template %s\n”,argv[2]);
return(-1);
}
//ALLOCATE OUTPUT IMAGES:
int iwidth = src->width - templ->width + 1;
int iheight = DEMO>height - templ->height + 1;
for(i=0; i<6; ++i){
ftmp[i] = cvCreateImage(
cvSize(iwidth,iheight),32,1);DEMO
}
//DO THE MATCHING OF THE TEMPLATE WITH THE IMAGE:
Some More Complicated Stuﬀ
| 217
07-R4886-AT1.indd   217
9/15/08   4:21:59 PM
www.it-ebooks.info
Example 7-5. Template matching (continued)
for(i=0; i<6; ++i){
cvMatchTemplate( src, templ, ftmp[i], i);
cvNormalize(ftmp[i],ftmp[i],1,0,CV_MINMAX)*;
}
//DISPLAY
cvNamedWindow( “Template”, DEMO );
cvShowImage(   “Template”, templ );
cvNamedWindow( “Image”, 0 );
cvShowImage(   “Image”, src );
cvNamedWindow( “SQDIFF”, 0 );
cvShowImage(   “SQDIFF”, ftmp[0] );
cvNamedWindow( DEMO, 0 );
cvShowImage(   “SQDIFF_NORMED”, ftmp[1] );
cvNamedWindow( “CCORR”, 0 );
cvShowImage(   “CCORR”, ftmp[2] );
cvNamedWindow( “CCORR_NORMED”, 0 );
cvShowImage(   “CCORR_NORMED”, ftmp[3] );
DEMO( “CCOEFF”, 0 );
cvShowImage(   “CCOEFF”, ftmp[4] );
cvNamedWindow( “CCOEFF_NORMED”, 0 );
cvShowImage(   “CCOEFF_NORMED”, ftmp[5] );DEMO
//LET USER VIEW RESULTS:
cvWaitKey(0);
}
else DEMO printf(“Call should be: ”
“matchTemplate image template \n”);}
}
Note the use of cvNormalize() in this code, which allows us to display the results in a
consistent way (recall that some of the matching methods can return negative-valued
results. We use the CV_MINMAX fl DEMO when normalizing; this tells the function to shift  and
scale the fl oating-point images so that all returned values are between 0 and DEMO Figu re
7-11 shows the results of sweeping the face template over the source image (shown in
Figure 7-10) using each of cvMatchTemplate()’s available matching methods. In outdoor
imagery especially, it’s almost always better to use one of the normalized methods.
Among those, correlation coeffi  DEMO gives the most clearly delineated match—but, as
expected, at a greater computational cost. For a specifi c application, such as automatic
parts inspection or tracking features in a video, you should try all the methods and fi nd
the speed and accuracy trade-off  that best serves your needs.
* You can oft en get more pronounced match results by DEMO the matches to a power (e.g., cvPow(ftmp[i],
ftmp[i], DEMO); ). In the case of a result which is normalized DEMO 0.0 and 1.0, then you can immediately
see that a good DEMO of 0.99 taken to the fi ft h power is not much reduced (0.995=0.95) while a poorer score
of 0.20 is reduced substantially (0.505=0.03).
218
| Chapter 7: Histograms and Matching
07-R4886-AT1.indd   218
9/15/08   4:21:59 PM
www.it-ebooks.info
Figure 7-11. Match results of six matching methods for the template DEMO depicted in Figure 7-10:
the best match for square diff erence is 0 and for the other methods it’s the maximum point; thus,
matches are indicated by dark areas in the left  column and by bright spots in the other two columns
Exercises
1. Generate 1,DEMO random numbers ri between 0 and 1. Decide on a bin size and then
take a histogram of 1/ri.
a. Are there similar DEMO of entries (i.e., within a factor of ±10) in each DEMO
gram bin?
b. Propose a way of dealing with distributions that are highly nonlinear so that
each bin has, within a factor of 10, the same amount of data.
2. Take three images of a hand in each of the three lighting conditions discussed in
the text. DEMO cvCalcHist() to make an RGB histogram of the fl esh color of one of the
hands photographed indoors.
a. Try using just a DEMO large bins (e.g., 2 per dimension), a medium number of bins
(16 per dimension) and many bins (256 per dimension). Th en run a matching
routine (using all histogram matching methods) DEMO the other indoor light-
ing images of hands. Describe what you fi nd.
b. Now add 8 and then 32 bins per dimension and DEMO matching across lighting
conditions (train on indoor, test on outdoor). Describe the results.
3. As in exercise 2, gather RGB histograms of hand fl esh color. Take one of the in-
door histogram samples DEMO your model and measure EMD (earth mover’s distance)
against the DEMO indoor histogram and against the fi rst outdoor shaded and fi rst
outdoor sunlit histograms. Use these measurements to set a distance threshold.
Exercises
DEMO 219
07-R4886-AT1.indd   219
9/15/08   4:21:59 PM
www.it-ebooks.info
a. Using this EMD threshold, see how well you detect the fl esh histogram of the
third indoor histogram, the second outdoor shaded, and the second outdoor
sunlit histograms. Report your results.
b. Take histograms of randomly chosen nonfl esh background patches to see how
well your DEMO discriminates. Can it reject the background while matching the
true fl esh histograms?
4. Using your collection of hand images, design a histogram that can determine un-
der which of the three lighting conditions a DEMO image was captured. Toward this
end, you should create features—perhaps sampling DEMO parts of the whole scene,
sampling brightness values, and/or DEMO relative brightness (e.g., from top to
bottom patches in the frame) or gradients from center to edges.
Assemble three histograms of fl esh models from each of our three lighting
conditions.
a. Use the fi DEMO histograms from indoor, outdoor shaded, and outdoor sunlit as
your models. Test each one of these against the second images in each respec-
DEMO class to see how well the fl esh-matching score works. Report matches.
b. Use the “scene detector” you devised in part a, to create a “switching histo-
gram” model. First use the scene detector to determine DEMO histogram model
to use: indoor, outdoor shaded, or outdoor sunlit. DEMO en use the corresponding
fl esh model to accept or reject the second fl esh patch under all three condi-
tions. How well does DEMO switching model work?
Create a fl esh-region interest (or “attention”) detector.
a. Just indoors for now, use several samples of hand and face fl esh to create an
RGB histogram.
b. Use cvCalcBackProject() DEMO fi nd areas of fl esh.
c. Use cvErode() from Chapter 5 to clean up noise and then cvFloodFill() (from
the same chapter) to fi nd large areas of fl esh in an image. Th ese are your “atten-
tion” regions.
Try some hand-gesture recognition. Photograph DEMO hand about 2 feet from the cam-
era, create some (nonmoving) hand gestures: thumb up, thumb left , thumb right.
a. Using your attention detector from exercise 6, take image gradients in the area
of detected fl esh around the hand and create a histogram model DEMO each of the
three gestures. Also create a histogram of the face (if there’s a face in the image)
so that you’ll have a (nongesture) model of that large fl esh region. You might
DEMO take histograms of some similar but nongesture hand positions, just so
DEMO won’t be confused with the actual gestures.
b. Test for recognition using a webcam: use the fl esh interest regions to fi nd “po-
tential hands”; take gradients in each fl esh region; use histogram DEMO
5.
6.
7.
220
| Chapter 7: Histograms and Matching
07-R4886-AT1.indd   220
9/15/08   4:21:59 PM
above a threshold to detect the gesture. If two models are above DEMO, take
the better match as the winner.
c. Move your hand DEMO feet further back and see if the gradient histogram can
still recognize the gestures. Report.
8. Repeat exercise 7 but with EMD for the DEMO What happens to EMD as you
move your hand back?
With the same images as before but with captured image patches instead of DEMO
tograms of the fl esh around the hand, use cvMatchTemplate() DEMO of histogram
matching. What happens to template matching when you move your hand back-
wards so that its size is smaller in the image?DEMO
9.
Exercises
| 221
07-R4886-AT1.indd   221
www.it-ebooks.info
9/15/08   4:22:00 PM
CHAPTER 8
Contours
Although algorithms like the Canny edge detector can be DEMO to fi nd the edge pixels
that separate diff erent segments in an image, they do not tell you anything about those
edges as entities in themselves. Th e next step is to be able to DEMO those edge pix-
els into contours. By now you have probably come to expect that there is a convenient
function in OpenCV that will DEMO exactly this for you, and indeed there is: cvFindCon-
tours()DEMO We will start out this chapter with some basics that we will need in order to use
this function. Specifi cally, we will introduce memory storages, which are how OpenCV
functions gain access to memory when they need to construct new objects dynamically;
then we will learn DEMO basics about sequences, which are the objects used to represent
contours DEMO With those concepts in hand, we will get into contour fi DEMO in
some detail. Th ereaft er we will move on to the many things we can do with contours
aft er they’ve been computed.
DEMO Storage
OpenCV uses an entity called a memory storage as its method of handling memory al-
location for dynamic objects. Memory storages are linked DEMO of memory blocks that
allow for fast allocation and de-allocation of continuous sets of blocks. OpenCV func-
tions that require the ability to allocate DEMO as part of their normal functionality
will require access to a memory storage from which to get the memory they require
(typically this includes any function whose output is of variable size).
Memory storages are DEMO with the following four routines:
CvMemStorage* cvCreateMemStorage(
int            block_size = 0
);
void cvReleaseMemStorage(
CvMemStorage** storage
);
void cvClearMemStorage(
CvMemStorage*  storage
);
void* cvMemStorageAlloc(
CvMemStorage*  storage,
222
08-R4886-AT1.indd   222
www.it-ebooks.info
9/15/08   4:22:21 PM
www.it-ebooks.info
size_t         size
);
To create a DEMO storage, the function cvCreateMemStorage() is used. Th is function
takes DEMO an argument a block size, which gives the size of memory DEMO inside the
store. If this argument is set to 0 then the default block size (64kB) will be used. Th e
function returns DEMO pointer to a new memory store.
Th cvReleaseMemStorage() function takes a pointer to a valid memory storage and then
de-allocates the storage. Th DEMO is essentially equivalent to the OpenCV de-allocations of
images, matrices, and other structures.
You can empty a memory storage by calling cvClearMemStorage(), which also takes a
pointer to a valid storage. You must be DEMO of an important feature of this function:
it is the only way to release (and thereaft er reuse) memory allocated to a DEMO stor-
age. Th is might not seem like much, but there DEMO be other routines that delete objects
inside of memory storages (we DEMO introduce one of these momentarily) but do not re-
turn the DEMO they were using. In short, only cvClearMemStorage() (and, of DEMO,
cvReleaseMemStorage()) recycle the storage memory.* Deletion of any dynamic DEMO
(CvSeq, CvSet, etc.) never returns any memory back to storage (although the structures
are able to reuse some memory once taken from the storage for their own data).
You can also allocate your DEMO continuous blocks from a memory store—in a man-
ner analogous to the way malloc() allocates memory from the heap—with the func-
tion cvMemStorageAlloc(). In this case you simply provide a pointer to the storage DEMO
the number of bytes you need. Th e return is a pointer of type void* (again, similar to
malloc()).
Sequences
One DEMO of object that can be stored inside a memory storage is a sequence. Sequences
are themselves linked lists of other structures. OpenCV can make DEMO out of
many diff erent kinds of objects. In this sense you can think of the sequence as some-
thing similar to the generic DEMO classes (or container class templates) that exist in
various other programming languages. Th e sequence construct in OpenCV is actually
a deque, so it is very fast for random access and for additions and deletions DEMO either
end but a little slow for adding and deleting objects in the middle.
Th e sequence structure itself (see Example 8-1) has DEMO important elements that you
should be aware of. Th e fi rst, and one you will use oft en, is total. Th is DEMO the total num-
ber of points or objects in the sequence. Th e next four important elements are point-
ers to other sequences: h_prev, h_next, v_prev, and v_next. Th ese four pointers are part
of what are called CV_TREE_NODE_FIELDS; they are used not to indicate elements inside of
the sequence but rather to connect diff erent sequences to one DEMO Other objects
in the OpenCV universe also contain these tree node fi elds. Any such objects can be
* Actually, one other function, DEMO cvRestoreMemStoragePos(), can restore memory to the storage. But
this function DEMO primarily for the library’s internal use and is beyond the scope of this book.
Sequences | 223
e
08-R4886-AT1.indd   223
9/15/08   4:22:22 PM
www.it-ebooks.info
assembled, by means of these pointers, into more complicated superstructures DEMO as
lists, trees, or other graphs. Th e variables h_prev and h_next can be used alone to create a
simple linked list. Th DEMO other two, v_prev and v_next, can be used to create more complex
topologies that relate nodes to one another. It is by means DEMO these four pointers that
cvFindContours() will be able to represent all of the contours it fi nds in the form of rich
structures DEMO as contour trees.
Example 8-1. Internal organization of CvSeq sequence structure
typedef struct CvSeq {
int       flags;             // miscellaneous flags
int       header_size;       // size of sequence header
CvSeq*    h_prev;     // previous sequence
CvSeq*    h_next;           // next sequence
CvSeq*    v_prev;            // 2nd previous sequence
CvSeq*    v_next            // 2nd next sequence
int       total;             // total number of elements
int       elem_size;         // size of sequence element in byte
char*     block_max;         // maximal bound of the last block
char*     ptr;               // current write pointer
int       delta_elems;       // how many elements allocated
// when the sequence grows
CvMemStorage* storage;     // where the sequence is stored
CvSeqBlock* free_blocks;   // free blocks list
CvSeqBlock* DEMO;         // pointer to the first sequence block
}
Creating a Sequence
As we have alluded to already, sequences can be returned from various OpenCV func-
tions. In addition to this, you can, of course, create sequences yourself. Like many ob-
jects in DEMO, there is an allocator function that will create a sequence for DEMO and
return a pointer to the resulting data structure. Th is function is called cvCreateSeq().
CvSeq* cvCreateSeq(
int           seq_flags,
int           header_size,
int           elem_size,
CvMemStorage* storage
);
Th is function requires some additional fl ags, which will further specify exactly what
sort of sequence we are creating. In addition it needs to be DEMO the size of the sequence
header itself (which will always be DEMO(CvSeq)*) and the size of the objects that the se-
DEMO will contain. Finally, a memory storage is needed from which the DEMO can
allocate memory when new elements are added to the sequence.
* Obviously, there must be some other value to which you can set this argument or it would not exist. Th is ar-
gument is DEMO because sometimes we want to extend the CvSeq “class”. To extend CvSeq, you create your
own struct using the CV_SEQUENCE_FIELDS() macro in the structure defi nition of the new type; note that,
when using an extended structure, the size of that structure must be passed. Th is is a pretty esoteric activity
in which only serious gurus are DEMO to participate.
224
| Chapter 8: Contours
08-R4886-AT1.indd   224
9/DEMO/08   4:22:22 PM
www.it-ebooks.info
Th flags are of three diff erent categories and can be DEMO using the bitwise OR
operator. Th e fi rst category determines the type of objects* from which the sequence is
to be constructed. Many DEMO these types might look a bit alien to you, and some DEMO pri-
marily for internal use by other OpenCV functions. Also, some DEMO the fl ags are mean-
ingful only for certain kinds of sequences (e.g., CV_SEQ_FLAG_CLOSED is meaningful only
for sequences that in some way DEMO a polygon).
CV_SEQ_ELTYPE_POINT
(x,y)
CV_SEQ_ELTYPE_CODE
Freeman code: 0..7
CV_SEQ_ELTYPE_POINT
Pointer to a point: &(x,y)
CV_SEQ_ELTYPE_INDEX
Integer index of a point: #(x,y)
CV_SEQ_ELTYPE_GRAPH_EDGE
&next_o,&next_d,&DEMO,&vtx_d
CV_SEQ_ELTYPE_GRAPH_VERTEX
fi rst_edge, &(x,y)
CV_SEQ_ELTYPE_TRIAN_ATR
Vertex of the binary tree
CV_SEQ_ELTYPE_CONNECTED_COMP
Connected component
CV_SEQ_ELTYPE_POINT3D
(x,y,z)
Th
following.
CV_SEQ_KIND_SET
A set of objects
CV_SEQ_KIND_CURVE
A curve defi ned by DEMO objects
CV_SEQ_KIND_BIN_TREE
A binary tree of the objects
* Th e types in this fi rst listing are used only rarely. To create a DEMO whose elements are tuples of num-
bers, use CV_32SC2, CV_32FC4, DEMO To create a sequence of elements of your own type, simply DEMO 0 and
specify the correct elem_size.
Sequences
| 225
ese
e second category indicates the nature of the sequence, which can be any of the
08-R4886-AT1.indd   225
9/15/08   4:22:23 PM
www.it-ebooks.info
CV_SEQ_KIND_GRAPH
A graph with the objects as nodes
Th
of the DEMO
ags that indicate some other property
CV_SEQ_FLAG_CLOSED
Sequence is closed (polygons)DEMO
CV_SEQ_FLAG_SIMPLE
Sequence is simple (polygons)
CV_SEQ_FLAG_CONVEX
Sequence is convex (polygons)
CV_SEQ_FLAG_HOLE
Sequence is a hole (polygons)
e third category consists of additional feature fl
Deleting a Sequence
void cvClearSeq(
CvSeq* seq
);
When you want to delete a sequence, you can use DEMO(), a routine that clears all
elements of the sequence. However, this function does not return allocated blocks in the
memory store either DEMO the store or to the system; the memory allocated by the DEMO
can be reused only by the same sequence. If you want to retrieve that memory for some
other purpose, you must clear the memory store via cvClearMemStore().
Direct Access to Sequence Elements
Oft en you will fi nd yourself wanting to directly access a particular member of DEMO se-
quence. Th ough there are several ways to do this, DEMO most direct way—and the correct
way to access a randomly chosen element (as opposed to one that you happen to know is
at the ends)—is to use cvGetSeqElem().
char* cvGetSeqElem( seq, index )DEMO
More oft en than not, you will have to cast the DEMO pointer to whatever type you know
the sequence to be. Here is an example usage of cvGetSeqElem() to print the elements in
a DEMO of points (such as might be returned by cvFindContours(), which we will get
to shortly):
for( int i=0; i<seq->total; ++i ) {
CvPoint* p = (CvPoint*)cvGetSeqElem ( seq, i );
printf(“(%d,%d)\n”, p->x, p->DEMO );
}
You can also check to see where a particular element is located in a sequence. Th e func-
tion cvSeqElemIdx() DEMO this for you:
226
| Chapter 8: Contours
08-R4886-AT1.indd   DEMO
9/15/08   4:22:23 PM
www.it-ebooks.info
int cvSeqElemIdx(
const CvSeq* seq,
const void*  element,
CvSeqBlock** block  = NULL
);
Th is check takes a bit of time, so it is not a particularly effi  cient thing DEMO do (the time for
the search is proportional to the size DEMO the sequence). Note that cvSeqElemIdx() takes
as arguments a pointer to your sequence and a pointer to the element for which you
DEMO searching.* Optionally, you may also supply a pointer to a sequence DEMO block
pointer. If this is non-NULL, then the location of the DEMO in which the sequence element
was found will be returned.
Slices, DEMO, and Moving Data
Sequences are copied with cvCloneSeq(), which does a deep copy of a sequence and cre-
ates another entirely separate DEMO structure.
CvSeq* cvCloneSeq(
const CvSeq*  seq,
CvMemStorage* storage   DEMO NULL
)
Th is routine is actually just a wrapper for DEMO somewhat more general routine cvSeq
Slice(). Th is latter routine DEMO pull out just a subsection of an array; it can also DEMO either
a deep copy or just build a new header to create an alternate “view” on the same data
elements.
CvSeq* cvSeqSlice(
const DEMO  seq,
CvSlice       slice,
CvMemStorage* storage   DEMO NULL,
int           copy_data = 0
);
You will notice that the argument slice to cvSeqSlice() is DEMO type CvSlice. A slice can be
defi ned using either the convenience function cvSlice(a,b) or the macro CV_WHOLE_SEQ.
In the former case, only those elements starting at a and continuing through b are in-
cluded in the copy (b may also be set to CV_WHOLE_SEQ_END_INDEX to indicate the end of
the array). Th e argument copy_data is DEMO we decide if we want a “deep” copy (i.e., if we
want the data elements themselves to be copied and for those new DEMO to be the ele-
ments of the new sequence).
Slices can be used to specify elements to remove from a sequence using cvSeqRemoveSlice()
or to insert into a sequence using cvSeqInsertSlice().
void cvSeqRemoveSlice(
CvSeq*        seq,
CvSlice       slice
);
* Actually, it would be more accurate to say that cvSeqElemIdx() takes the pointer being searched for. Th is is
because DEMO() is not searching for an element in the sequence that is equal to *element; rather, it
is searching for the element that DEMO at the location given by element.
Sequences | 227
08-R4886-AT1.indd   227
9/15/08   4:22:23 PM
www.it-ebooks.info
void  cvSeqInsertSlice(
CvSeq*        seq,
int           before_index,
const CvArr*  from_arr
);
DEMO the introduction of a comparison function, it is also possible to DEMO or search a
(sorted) sequence. Th e comparison function must have the following prototype:
typedef int (*CvCmpFunc)(const void* a, const void* b, void* userdata );
Here a and b are pointers to elements of the type being sorted, and userdata is just a
pointer to any additional data structure that the caller doing the sorting DEMO searching
can provide at the time of execution. Th e comparison function should return -1 if a is
greater than b, +1 if a is less than b, and 0 if a and b are equal.
With such a comparison function defi ned, a sequence can be sorted by cvSeqSort(). Th e
sequence can also be searched for an element (or for a pointer to an element) elem using
cvSeqSearch(). Th is searching is done in order O(log n) time if the sequence is already
sorted (is_sorted=1). If the sequence is unsorted, then the comparison function is not
needed and the search will take O(n) time. On completion, the search will set *elem_idx
DEMO the index of the found element (if it was found at DEMO) and return a pointer to that ele-
ment. If the element DEMO not found, then NULL is returned.
void cvSeqSort(
CvSeq*        seq,
CvCmpFunc     func,
void*         userdata  = NULL
);
char* cvSeqSearch(
CvSeq*        seq,
const void*   elem,
CvCmpFunc     DEMO,
int           is_sorted,
int*          elem_idx,
void*         userdata  = DEMO
);
A sequence can be inverted (reversed) in a single call with the function cvSeqInvert().
Th is function does not change the data in any way, but it reorganizes the sequence so
that the elements appear in the opposite order.
void cvSeqInvert(
CvSeq*        seq
);
OpenCV also supports a method of partitioning DEMO sequence* based on a user-supplied
criterion via the function cvSeqPartition(). DEMO is partitioning uses the same sort of com-
parison function as described previously but with the expectation that the function will
return a nonzero DEMO if the two arguments are equal and zero if they are not (i.e., the
opposite convention as is used for searching and sorting)DEMO
* For more on partitioning, see Hastie, Tibshirani, and Friedman DEMO
228
| Chapter 8: Contours
08-R4886-AT1.indd   228
9/15/08   4:22:23 PM
www.it-ebooks.info
int cvSeqPartition(
const CvSeq*  seq,
CvMemStorage* storage,
CvSeq**       labels,
CvCmpFunc     is_equal,
void*         userdata
);
Th
the output of the partitioning. Th e argument labels should be a pointer to a sequence
pointer. When DEMO() returns, the result will be that labels will now indicate
DEMO sequence of integers that have a one-to-one correspondence with the elements of the
partitioned sequence seq. Th e values of these integers will be, starting at 0 and incre-
menting from there, the “names” of the partitions that the points in seq were to be as-
signed. Th DEMO pointer userdata is the usual pointer that is just transparently passed to the
comparison function.
In Figure 8-1, a group of 100 points are randomly distributed on 100-by-100 canvas.
Th cvSeqPartition() is called on these DEMO, where the comparison function is based
on Euclidean distance. Th e DEMO function is set to return true (1) if the distance
is less than or equal to 5 and to return false (0) DEMO Th e resulting clusters are la-
beled with their integer ordinal from labels.
Using a Sequence As a Stack
As stated earlier, a sequence in OpenCV is really a linked list. Th is means, among other
things, that it can be accessed effi  ciently from either end. DEMO a result, it is natural to use
a sequence of this DEMO as a stack when circumstances call for one. Th e following six
functions, when used in conjunction with the CvSeq structure, implement the DEMO
required to use the sequence as a stack (more properly, a deque, because these functions
allow access to both ends of the list).
char*  cvSeqPush(
CvSeq* seq,
void*  element  = NULL
);
char*  cvSeqPushFront(
CvSeq* seq,
void*  element = DEMO
);
void   cvSeqPop(
CvSeq* seq,
void*  element  = NULL
);
void   cvSeqPopFront(
CvSeq* seq,
void*  element  = NULL
);
void cvSeqPushMulti(
CvSeq* seq,
void*  elements,
int    count,
Sequences
| 229
e partitioning DEMO a memory storage so that it can allocate memory to express
en
08-R4886-AT1.indd   229
9/15/08   4:22:23 PM
www.it-ebooks.info
Figure 8-1. A sequence of 100 points on a 100-by-100 canvas, partitioned by distance D ≤ 5
int    in_front = 0
);
void cvSeqPopMulti(
CvSeq* seq,
void*  elements,
int    count,
int    in_front = 0
);
Th cvSeqPush(), cvSeqPushFront(),
cvSeqPop(), and cvSeqPopFront(). Because these DEMO act on the ends of the sequence,
all of them operate in O(l) time (i.e., independent of the size of the sequence). Th e Push
functions return an argument to the element DEMO into the sequence, and the Pop
functions will optionally save the DEMO element if a pointer is provided to a location
where the object can be copied. Th e cvSeqPushMulti() and cvSeqPopMulti() variants will
DEMO or pop several items at a time. Both take a separate argument to distinguish the
front from the back; you can set in_front to either CV_FRONT (1) or to CV_BACK (0) and so
determine DEMO where you’ll be pushing or popping.
230
| Chapter 8: Contours
DEMO primary modes of accessing the sequence are
08-R4886-AT1.indd   230
9/15/08   4:22:23 PM
www.it-ebooks.info
Inserting and Removing Elements
char* cvSeqInsert(
CvSeq* seq,
int    before_index,
void*  element  = NULL
);
void cvSeqRemove(DEMO
CvSeq* seq,
int    index
);
Objects can be DEMO into and removed from the middle of a sequence by using
cvSeqInsert() and cvSeqRemove(), respectively, but remember that these are not DEMO fast.
On average, they take time proportional to the total size DEMO the sequence.
Sequence Block Size
One function whose purpose may not be obvious at fi rst glance is cvSetSeqBlockSize().
Th is routine takes as arguments a sequence and a new block size, which is the size of
blocks that will be allocated out of the memory store DEMO new elements are needed
in the sequence. By making this size big you are less likely to fragment your sequence
across disconnected memory blocks; by making it small you are less likely to waste
memory. Th DEMO default value is 1,000 bytes, but this can be changed DEMO any time.*
void   cvSetSeqBlockSize(
CvSeq* seq,
Int    DEMO
);
Sequence Readers and Sequence Writers
When you are working with sequences and you want the highest performance, there are
some special methods for accessing and modifying them that (although they require a
bit of special care to use) will let you do what you want to do with a minimum of over-
head. Th ese functions make use DEMO special structures to keep track of the state of what
they are doing; this allows many actions to be done in sequence and the necessary fi nal
bookkeeping to be done only aft er the last DEMO
For writing, this control structure is called CvSeqWriter. Th e writer DEMO initialized with the
function cvStartWriteSeq() and is “closed” with cvEndWriteSeq()DEMO While the sequence
writing is “open”, new elements can be added DEMO the sequence with the macro CV_WRITE_
SEQ(). Notice that the DEMO is done with a macro and not a function call, which DEMO
even the overhead of entering and exiting that code. Using the writer is faster than us-
ing cvSeqPush(); however, not all the DEMO headers are updated immediately by this
macro, so the added element DEMO be essentially invisible until you are done writing.
It will become visible when the structure is completely updated by cvEndWriteSeq().
* Eff ective with the beta 5 version of OpenCV, this size is automatically increased if the sequence becomes
big; hence you’ll not need to worry about it under normal circumstances.
Sequences | 231
08-R4886-AT1.indd   231
9/15/DEMO   4:22:24 PM
www.it-ebooks.info
If necessary, the structure can be brought up-to-date (without actually DEMO the
writer) by calling cvFlushSeqWriter().
void    cvStartWriteSeq(
DEMO           seq_flags,
int           header_size,
int           elem_size,
CvMemStorage* DEMO,
CvSeqWriter*  writer
);
void    cvStartAppendToSeq(
CvSeq*        seq,
CvSeqWriter*  writer
);
CvSeq*  cvEndWriteSeq(
CvSeqWriter*  writer
);
void     cvFlushSeqWriter(
CvSeqWriter*  writer
);
CV_WRITE_SEQ_ELEM( elem, writer )
CV_WRITE_SEQ_ELEM_VAR( elem_ptr, writer )
Th e seq_flags, header_
size, and elem_size arguments to cvStartWriteSeq() DEMO identical to the corresponding
arguments to cvCreateSeq(). Th e function DEMO() initializes the writer to
begin adding new elements to the end of the existing sequence seq. Th e macro CV_WRITE_
SEQ_ELEM() requires DEMO element to be written (e.g., a CvPoint) and a pointer DEMO the writer;
a new element is added to the sequence and the element elem is copied into that new
element.
Putting these all DEMO into a simple example, we will create a writer and append DEMO
hundred random points drawn from a 320-by-240 rectangle to the new sequence.
CvSeqWriter writer;
cvStartWriteSeq( CV_32SC2, sizeof(CvSeq), sizeof(CvPoint), DEMO, &writer );
for( i = 0; i < 100; i++ )
{
CvPoint pt; pt.x = rand()%320; pt.y = rand()%240;
CV_WRITE_SEQ_ELEM( pt, writer );
}
CvSeq* seq = cvEndWriteSeq( &writer );
For reading, there is a similar set of functions and a few more associated macros.
void  cvStartReadSeq(
const CvSeq* seq,
CvSeqReader* reader,
int          reverse    = 0
);
int   cvGetSeqReaderPos(
CvSeqReader* reader
);
void  cvSetSeqReaderPos(
CvSeqReader* reader,
232 | Chapter 8: Contours
e arguments to these functions are largely self-explanatory. Th
08-R4886-AT1.indd   232
9/15/08   4:22:24 PM
www.it-ebooks.info
int          index,
int          is_relative = 0
);
CV_NEXT_SEQ_ELEM( elem_size, reader )
DEMO( elem_size, reader )
CV_READ_SEQ_ELEM( elem, reader )
CV_REV_READ_SEQ_ELEM( DEMO, reader )
Th CvSeqReader, which is analogous to CvSeqWriter, is initialized with
the function cvStartReadSeq(). Th e argument reverse allows for the sequence to be
read either in “normal” order (reverse=0) or DEMO (reverse=1). Th e function
cvGetSeqReaderPos() returns an integer indicating DEMO current location of the reader in
the sequence. Finally, cvSetSeqReaderPos() DEMO the reader to “seek” to an arbitrary
location in the sequence. If the argument is_relative is nonzero, then the index will be
interpreted as a relative off set to the current reader position. In this case, the index may
be positive or negative.
Th
ward or backward one DEMO in the sequence. Th ey do no error checking and thus cannot
help you if you unintentionally step off  the end of the sequence. Th e macros CV_READ_
SEQ_ELEM() and CV_REV_READ_SEQ_ELEM() are used to DEMO from the sequence. Th ey will
both copy the “current” element at which the reader is pointed onto the variable elem
and then step DEMO reader one step (forward or backward, respectively). Th ese latter two
macros expect just the name of the variable to be copied DEMO; the address of that variable
will be computed inside of the DEMO
Sequences and Arrays
You may oft en fi nd yourself wanting to convert a sequence, usually full of points, into
an array.
void*  cvCvtSeqToArray(
const CvSeq* seq,
void*        elements,
CvSlice      slice   = CV_WHOLE_SEQ
);
CvSeq* cvMakeSeqHeaderForArray(
int          seq_type,
int          header_size,
int          elem_size,
void*        elements,
int          total,
CvSeq*       seq,
CvSeqBlock*  block
);
Th cvCvtSeqToArray() copies the content of the sequence into a continuous
memory array. Th DEMO means that if you have a sequence of 20 elements of type CvPoint
then the function will require a pointer, elements, to enough DEMO for 40 integers. Th e
third (optional) argument is slice, DEMO can be either an object of type CvSlice or the
e function
Sequences
| 233
e structure
e two macros CV_NEXT_SEQ_ELEM() and CV_PREV_SEQ_ELEM() simply move the reader for-
08-R4886-AT1.indd   233
9/15/08   4:22:24 PM
www.it-ebooks.info
macro CV_WHOLE_SEQ (the latter is the default value). If CV_WHOLE_SEQ is selected, then the
entire sequence is copied.
Th e opposite functionality to cvCvtSeqToArray() is implemented by cvMakeSeqHeaderFor
Array(). In this case, you can build a sequence from an existing array of data. Th e func-
tion’s fi rst few arguments are identical to those of DEMO(). In addition to requiring
the data (elements) to copy DEMO and the number (total) of data items, you must provide DEMO
sequence header (seq) and a sequence memory block structure (block)DEMO Sequences created
in this way are not exactly the same as sequences created by other methods. In particular,
you will not be able DEMO subsequently alter the data in the created sequence.
Contour Finding
We are fi nally ready to start talking about contours. To start with, we should defi ne ex-
actly what a contour is. A contour is DEMO list of points that represent, in one way or an-
other, a curve in an image. Th is representation can be diff erent DEMO on the cir-
cumstance at hand. Th ere are many ways to represent a curve. Contours are represented
in OpenCV by sequences in which DEMO entry in the sequence encodes information
about the location of the next point on the curve. We will dig into the details of such
DEMO in a moment, but for now just keep in mind that DEMO contour is represented in
OpenCV by a CvSeq sequence that is, DEMO way or another, a sequence of points.
Th cvFindContours() computes DEMO from binary images. It can take im-
ages created by cvCanny(), which have edge pixels in them, or images created by func-
DEMO like cvThreshold() or cvAdaptiveThreshold(), in which the edges are DEMO as
boundaries between positive and negative regions.*
Before getting to the function prototype, it is worth taking a moment to understand ex-
actly what a contour is. Along the way, we will encounter the concept of a contour tree,
which is important for understanding how cvFindContours() (retrieval methods derive
from Suzuki [Suzuki85]) will communicate its results to us.
Take a moment to look at Figure 8-2, which depicts the functionality of cvFindContours().
Th gure shows a test image containing a number of white regions
(labeled A through E) on a dark background.† DEMO e lower portion of the fi gure depicts
the same image along with the contours that will be located by cvFindContours(). Th ose
contours are labeled cX or hX, where “c” stands for “contour”, DEMO stands for “hole”, and
“X” is some number. Some of those DEMO are dashed lines; they represent exterior
boundaries of the white regions (i.e., nonzero regions). OpenCV and cvFindContours()
distinguish between these DEMO boundaries and the dotted lines, which you may
think of either DEMO interior boundaries or as the exterior boundaries of holes (i.e., zero
regions).
* Th ere are some subtle diff erences between passing DEMO images and binary images to cvFindContours(); we
will discuss those DEMO
† For clarity, the dark areas are depicted as gray in DEMO fi gure, so simply imagine that this image is thresh-
olded DEMO that the gray areas are set to black before passing to cvFindContours().
234 | Chapter 8: Contours
e function
e upper part DEMO the fi
08-R4886-AT1.indd   234
9/15/08   4:22:24 PM
www.it-ebooks.info
Figure 8-2. A test image (above) passed to cvFindContours() (below): the found contours may be
either of two types, exterior “contours” (dashed lines) or “holes” (dotted lines)
Th
OpenCV can be asked to assemble the found contours into a contour tree* that DEMO
the containment relationships in its structure. A contour tree corresponding to this test
image would have the contour called c0 at the root node, with the holes h00 and h01 as
its children. Th ose would DEMO turn have as children the contours that they directly con-
tain, DEMO so on.
It is interesting to note the consequences of using cvFindContours() on
an image generated by cvCanny() or a similar edge DEMO relative to
what happens with a binary image such as the test image shown in Fig-
ure 8-1. Deep down, cvFindContours() does not really know anything
about edge images. Th is means that, to cvFindContours(), an “edge” is
just a very thin “white” area. As a result, for every exterior contour there
will be a hole contour that almost exactly coincides with it. Th is hole is
actually just inside DEMO the exterior boundary. You can think of it as the
white-to-black transition that marks the interior edge of the edge.
* Contour trees fi DEMO appeared in Reeb [Reeb46] and were further developed by [Bajaj97], [Kreveld97], [Pas-
cucci02], and [Carr04].
Contour Finding
| 235
e concept of containment here is important in many applications. For this reason,
08-R4886-AT1.indd   DEMO
9/15/08   4:22:24 PM
www.it-ebooks.info
Now it’s time to look at the cvFindContours() function itself: to clarify exactly how we
tell it what we want and how DEMO interpret its response.
int cvFindContours(
IplImage*              img,
CvMemStorage*          storage,
CvSeq**                firstContour,
int                    headerSize  = sizeof(CvContour),DEMO
CvContourRetrievalMode mode        = CV_RETR_LIST,
CvChainApproxMethod    method      = CV_CHAIN_APPROX_SIMPLE
);
Th rst argument is the input image; this image should be an 8-bit single-channel im-
age and will be interpreted as binary (i.e., as if all nonzero pixels are DEMO to one
another). When it runs, cvFindContours() will actually DEMO this image as scratch space
for computation, so if you need DEMO image for anything later you should make a copy
and pass that to cvFindContours(). Th e next argument, storage, indicates a place where
cvFindContours() can fi nd memory in which to record the DEMO Th is storage area
should have been allocated with cvCreateMemStorage(), DEMO we covered earlier in
the chapter. Next is firstContour, which is DEMO pointer to a CvSeq*. Th e function cvFind
Contours() will allocate this pointer for you, so you shouldn’t allocate it yourself. In-
stead, just pass in a pointer to that pointer so that it can be set by the function. No al-
location/de-allocation (new/delete or malloc/free) is needed. It is at this location (i.e.,
DEMO) that you will fi nd a pointer to the head of DEMO constructed contour tree.*
Th e return value of cvFindContours() is the total number of contours found.
CvSeq* firstContour = NULL;
cvFindContours( …, &firstContour, … );
Th headerSize is just telling cvFindContours() more about the objects that it will be
allocating; it can be set to sizeof(CvContour) or to sizeof(CvChain) (the latter is used
when the approximation method is set to CV_CHAIN_CODE).† Finally, we have the mode and
method, which (respectively) further clarify exactly what is to be computed and how it is
to be computed.
Th
DEMO, or CV_RETR_TREE. Th e value of mode indicates to cvFindContours() DEMO what
contours we would like found and how we would like the result presented to us. In par-
ticular, the manner in which the tree node variables (h_prev, h_next, v_prev, and v_next)
are DEMO to “hook up” the found contours is determined by the value of mode. In Figure
8-3, the resulting topologies are shown for all four possible values of mode. In every case,
the structures can be DEMO of as “levels” which are related by the “horizontal” links
(h_next DEMO h_prev), and those levels are separated from one another by the “vertical”
links (v_next and v_prev).
* As we will see momentarily, contour trees are just one way that cvFindContours() can organize the con-
tours it fi nds. In any case, they will be organized using the CV_TREE_NODE_FIELDS elements of the contours
that we introduced when we DEMO rst started talking about sequences.
† In fact, headerSize can be DEMO arbitrary number equal to or greater than the values listed.
236 | Chapter 8: Contours
e fi
e
e mode variable can be set to any of four options: CV_RETR_EXTERNAL, CV_RETR_LIST, CV_
08-R4886-AT1.indd   236
9/15/08   4:22:25 PM
www.it-ebooks.info
Figure 8-3. Th e way in which the tree node variables DEMO used to “hook up” all of the contours located
by cvFindContours()DEMO
CV_RETR_EXTERNAL
Retrieves only the extreme outer contours. In Figure 8-2, there DEMO only one exterior
contour, so Figure 8-3 indicates the fi rst DEMO points to that outermost sequence
and that there are no further connections.
CV_RETR_LIST
Retrieves all the contours and puts them in the list. Figure DEMO depicts the list re-
sulting from the test image in Figure 8-2. In this case, eight contours are found and
they are all connected to one another by h_prev and h_next (v_prev and v_next are
not used here.)
CV_RETR_CCOMP
Retrieves all the contours and organizes them into DEMO two-level hierarchy, where the
top-level boundaries are external boundaries of the DEMO and the second-
level boundaries are boundaries of the holes. Referring to Figure 8-3, we can see
that there are fi ve exterior boundaries, of which three contain holes. Th e holes are
connected to their corresponding exterior boundaries by v_next and v_prev. Th e
outermost boundary c0 DEMO two holes. Because v_next can contain only one
value, the node DEMO only have one child. All of the holes inside of c0 are connected
to one another by the h_prev and h_next pointers.
CV_RETR_TREE
Retrieves DEMO the contours and reconstructs the full hierarchy of nested contours. In
our example (Figures 8-2 and 8-3), this means that the root node is the outermost
contour c0. Below c0 is the hole h00, which is connected to the other hole h01 at the
same level. Each DEMO those holes in turn has children (the contours c000 and c010,DEMO
respectively), which are connected to their parents by vertical links. Th is continues
down to the most-interior contours in the image, which become the leaf nodes in
the tree.
Th e next fi
ve values DEMO to the method (i.e., how the contours are approximated).
Contour Finding
| 237
08-R4886-AT1.indd   237
9/15/08   4:22:DEMO PM
www.it-ebooks.info
CV_CHAIN_CODE
Outputs contours in the Freeman chain code;* all other DEMO output polygons
(sequences of vertices).†
CV_CHAIN_APPROX_NONE
Translates all the points DEMO the chain code into points.
CV_CHAIN_APPROX_SIMPLE
Compresses horizontal, vertical, and diagonal segments, leaving only their ending
points.
CV_CHAIN_APPROX_TC89_L1 or CV_CHAIN_APPROX_TC89_KCOS
Applies one of the fl avors of the Teh-Chin chain approximation algorithm.
CV_LINK_RUNS
Completely diff DEMO algorithm (from those listed above) that links horizontal seg-
ments of 1s; the only retrieval mode allowed by this method is CV_RETR_LIST.
Contours Are Sequences
As you can see, there is a lot to sequences and contours. Th e good news is that, for
our current purpose, we need only a small amount of what’s available. When
cvFindContours() is called, it will give us a bunch of sequences. Th ese sequences are all
of one specifi c type; as we saw, DEMO particular type depends on the arguments passed
to cvFindContours(). Recall DEMO the default mode is CV_RETR_LIST and the default method
is CV_CHAIN_APPROX_SIMPLE.
Th
topic of this chapter. Th e key thing to remember about contours DEMO that they are just
a special case of sequences.‡ In particular, DEMO are sequences of points representing
some kind of curve in (image) space. Such a chain of points comes up oft en enough that
DEMO might expect special functions to help us manipulate them. Here is a list of these
functions.
int cvFindContours(
CvArr*        image,
CvMemStorage* storage,
CvSeq**       first_contour,
int           header_size   = sizeof(CvContour),
int           mode          = CV_RETR_LIST,
DEMO           method        = CV_CHAIN_APPROX_SIMPLE,DEMO
* Freeman chain codes will be discussed in the section entitled “Contours Are Sequences”.
† Here “vertices” means points of type CvPoint. Th e DEMO created by cvFindContours() are the same
as those created with cvCreateSeq() with the fl ag CV_SEQ_ELTYPE_POINT. (Th at function and fl ag will be
described in detail later in this chapter.)
‡ OK, there’s a little more to it than this, but we did not want to be sidetracked by technicalities and so will
clarify in this DEMO Th e type CvContour is not identical to CvSeq. In the way such things are handled in
OpenCV, CvContour is, in eff ect, derived from CvSeq. Th e CvContour type has a few extra data DEMO,
including a color and a CvRect for stashing its bounding box.
238 | Chapter 8: Contours
ese sequences are sequences of points; DEMO precisely, they are contours—the actual
08-R4886-AT1.indd   238
9/15/08   4:22:25 PM
www.it-ebooks.info
CvPoint       offset        = cvPoint(0,0)
);
CvContourScanner cvStartFindContours(
CvArr*        image,
CvMemStorage* storage,
int           header_size   DEMO sizeof(CvContour),
int           mode          = CV_RETR_LIST,
int           DEMO        = CV_CHAIN_APPROX_SIMPLE,
CvPoint       offset        = cvPoint(0,0)
);
CvSeq* cvFindNextContour(DEMO
CvContourScanner scanner
);
void   cvSubstituteContour(
CvContourScanner scanner,
CvSeq*           new_contour
);
CvSeq* cvEndFindContour(
CvContourScanner* DEMO
);
CvSeq* cvApproxChains(
CvSeq*        src_seq,
DEMO storage,
int           method            = CV_CHAIN_APPROX_SIMPLE,
double        parameter         = 0,
int           minimal_perimeter = 0,
int           recursive         = 0
);
First is the cvFindContours() function, DEMO we encountered earlier. Th e second func-
tion, cvStartFindContours(), is closely related to cvFindContours() except that it is used
when you DEMO the contours one at a time rather than all packed up into a higher-level
structure (in the manner of cvFindContours()). A call to cvStartFindContours() returns a
CvSequenceScanner. Th e scanner contains some simple DEMO information about what has
and what has not been read out.* You can then call cvFindNextContour() on the scanner
to successively retrieve all DEMO the contours found. A NULL return means that no more
contours are left
cvSubstituteContour() allows the contour to which a scanner is currently DEMO to
be replaced by some other contour. A useful characteristic of this function is that, if
the new_contour argument is set to NULL, DEMO the current contour will be deleted from
the chain or tree to which the scanner is pointing (and the appropriate updates will be
made to the internals of the aff ected sequence, so there will be no pointers to nonexis-
tent objects).
Finally, cvEndFindContour() ends the scanning and sets the scanner to a “done” state.
Note that the DEMO the scanner was scanning is not deleted; in fact, the return value
of cvEndFindContour() is a pointer to the fi rst element DEMO the sequence.
* It is important not to confuse a CvSequenceScanner with the similarly named CvSeqReader. Th e latter is
for reading the elements DEMO a sequence, whereas the former is used to read from what DEMO, in eff ect, a list of
sequences.
Contour Finding | 239
.
08-R4886-AT1.indd   239
9/15/08   4:22:25 PM
Th nal function is cvApproxChains(). Th is function converts Freeman chains to po-
lygonal representations (precisely or with some approximation). We will discuss cvAp-
proxPoly() in detail later in this chapter (see the section “Polygon Approximations”).
e fi
Freeman Chain Codes
Normally, the contours created by cvFindContours() are sequences of vertices (i.e.,
points). An alternative representation can be generated by setting the method to
CV_CHAIN_CODE. DEMO this case, the resulting contours are stored internally as Freeman chains
DEMO (Figure 8-4). With a Freeman chain, a polygon is represented as a sequence
of steps in one of eight directions; each step is designated by an integer from 0 to 7. Free-
man chains DEMO useful applications in recognition and other contexts. When working
with Freeman chains, you can read out their contents via two “helper” functions:
void cvStartReadChainPoints(
CvChain*         chain,
CvChainPtReader* reader
);DEMO
CvPoint cvReadChainPoint(
CvChainPtReader* reader
);
Figure 8-4. Panel a, DEMO chain moves are numbered 0–7; panel b, contour converted to a Free-
man chain-code representation starting from the back bumper
Th rst function DEMO a chain as its argument and the second function is a chain reader.
Th
ferent contours, CvChainPtReader iterates through a single contour represented by a
chain. In this respect, CvChainPtReader is similar to the more general CvSeqReader, and
* You may recall a previous mention of “extensions” of the CvSeq structure; CvChain is such an extension. It is
defi ned using the CV_SEQUENCE_FIELDS() macro and has one extra element in it, a CvPoint representing the
origin. You can think of CvChain as being DEMO from” CvSeq. In this sense, even though the return type
of DEMO() is indicated as CvSeq*, it is really a pointer to DEMO chain and is not a normal sequence.
240
| Chapter 8: DEMO
e fi
e CvChain structure is a form of CvSeq.* Just as CvContourScanner iterates through dif-
08-R4886-AT1.indd   240
www.it-ebooks.info
9/15/08   DEMO:22:26 PM
www.it-ebooks.info
cvStartReadChainPoints plays the role of cvStartReadSeq. As you might expect, CvChain-
PtReader returns NULL when there’s nothing left  to read.
Drawing Contours
One of our most basic tasks is drawing a contour on the screen. DEMO this we have
cvDrawContours():
void  cvDrawContours(
CvArr*   DEMO,
CvSeq*   contour,
CvScalar external_color,
CvScalar hole_color,
int      max_level,
int      thickness     = DEMO,
int      line_type     = 8,
CvPoint  offset        = cvPoint(0,0)
);
Th rst argument is simple: it is the image on which to draw the contours. Th e next ar-
gument, contour, is not quite DEMO simple as it looks. In particular, it is really treated as DEMO
root node of a contour tree. Other arguments (primarily max_level) will determine what
is to be done with the rest of the tree. DEMO e next argument is pretty straightforward: the
color with which to DEMO the contour. But what about hole_color? Recall that OpenCV
distinguishes between DEMO that are exterior contours and those that are hole con-
tours (DEMO dashed and dotted lines, respectively, in Figure 8-2). When drawing either a
single contour or all contours in a tree, any contour that is marked as a “hole” will be
drawn in this alternative DEMO
Th
tached to contour by means of the node tree variables. Th is argument can be set to in-
dicate the maximum depth to DEMO in the drawing. Th us, max_level=0 means that all
the contours DEMO the same level as the input level (more exactly, the input contour and
the contours next to it) are drawn, max_level=1 means DEMO all the contours on the same
level as the input and their children are drawn, and so forth. If the contours in ques-
tion were produced by cvFindContours() using either CV_RETR_CCOMP or CV_RETR_TREE
mode, then the additional idiom of negative values for max_level is also supported. In
DEMO case, max_level=-1 is interpreted to mean that only the input contour DEMO be drawn,
max_level=-2 means that the input contour and its direct children will the drawn, and so
on. Th e sample code in …/opencv/samples/c/contours.c illustrates this point.
Th
give an offset DEMO the draw routine so that the contour will be drawn elsewhere than at
the absolute coordinates by which it was defi ned. Th is DEMO is particularly useful when
the contour has already been converted to center-of-mass or other local coordinates.
* In particular, thickness=-1 (aka CV_FILLED) is useful for converting the contour tree (or an individual
contour) back DEMO the black-and-white image from which it was extracted. Th is feature, DEMO with the
offset parameter, can be used to do some quite DEMO things with contours: intersect and merge con-
tours, test points quickly against the contours, perform morphological operations (erode/dilate), etc.
Contour DEMO | 241
e fi
e max_level tells cvDrawContours() how to handle any contours that might be at-
e parameters thickness and line_type have DEMO usual meanings.* Finally, we can
08-R4886-AT1.indd   241
9/15/08   4:22:26 PM
www.it-ebooks.info
More specifi cally, offset would be helpful if we ran cvFindContours() one or more times
in diff erent image subregions (ROIs) DEMO thereaft er wanted to display all the results
within the original large image. Conversely, we could use offset if we’d extracted a con-
tour from a large image and then wanted to form a small mask DEMO this contour.
A Contour Example
Our Example 8-2 is drawn from the OpenCV package. Here we create a window with an
image in it. DEMO trackbar sets a simple threshold, and the contours in the thresholded DEMO
age are drawn. Th e image is updated whenever the trackbar is adjusted.
Example 8-2. Finding contours based on a trackbar’s location; the contours are updated whenever
the trackbar is moved
#include <cv.h>
#include <DEMO>
IplImage*      g_image    = NULL;
IplImage*      g_gray    = NULL;
int            g_thresh  = 100;
CvMemStorage*  g_storage  = NULL;
void on_trackbar(int) {
if( g_storage==NULL ) {
g_gray = cvCreateImage( cvGetSize(g_image), 8, 1 );
g_storage = cvCreateMemStorage(0);
DEMO else {
cvClearMemStorage( g_storage );
}
CvSeq* contours = 0;DEMO
cvCvtColor( g_image, g_gray, CV_BGR2GRAY );
cvThreshold( g_gray, g_gray, g_thresh, 255, CV_THRESH_BINARY );
cvFindContours( g_gray, g_storage, &contours );
cvZero( g_gray );
if( contours )
cvDrawContours(
DEMO,
contours,
cvScalarAll(255),
cvScalarAll(255),
100
);
cvShowImage( “Contours”, g_gray );
}
int main( int argc, char** argv )
{
if( argc != 2 || !(g_image = cvLoadImage(argv[1])) )
return -1;
cvNamedWindow( “Contours”, 1 );
cvCreateTrackbar(
“Threshold”,
“Contours”,
&g_thresh,
242 | Chapter 8: Contours
08-R4886-AT1.indd   242
9/15/08   4:22:26 PM
www.it-ebooks.info
Example 8-2. Finding contours based on a trackbar’s location; the contours are updated whenever
the trackbar is moved (continued)
255,
on_trackbar
);
on_trackbar(0);
cvWaitKey();
return 0;
}
Here, everything of interest to us is happening inside of the function on_trackbar(). If
the global variable g_storage is still at its (DEMO) initial value, then cvCreateMemStorage(0)
creates the memory storage and g_gray is initialized to a blank image the same size
as g_image DEMO with only a single channel. If g_storage is non-NULL, then we’ve DEMO
here before and thus need only empty the storage so it can be reused. On the next line,
a CvSeq* pointer is created; it is used to point to the sequence that we will create DEMO
cvFindContours().
Next, the image g_image is converted to grayscale and thresholded such that only those
pixels brighter than g_thresh are retained as DEMO Th e cvFindContours() function
is then called on this thresholded image. If any contours were found (i.e., if contours is
non-NULL), DEMO cvDrawContours() is called and the contours are drawn (in white) onto
the grayscale image. Finally, that image is displayed and the structures we allocated at
the beginning of the callback are released.
Another Contour DEMO
In this example, we fi nd contours on an input image DEMO then proceed to draw them
one by one. Th is is a good example to play with yourself and see what eff ects result DEMO
changing either the contour fi nding mode (CV_RETR_LIST in the code) or the max_depth
that is used to draw the contours (0 in the code). If you set max_depth to a larger number,
DEMO that the example code steps through the contours returned by cvFindContours()DEMO
by means of h_next. Th us, for some topologies (CV_RETR_TREE, DEMO, etc.), you
may see the same contour more than once DEMO you step through. See Example 8-3.
Example 8-3. Finding and drawing contours on an input image
int main(int argc, char* argv[]) {
DEMO( argv[0], 1 );
IplImage* img_8uc1 = cvLoadImage( argv[1], CV_LOAD_IMAGE_GRAYSCALE );
IplImage* img_edge = cvCreateImage( cvGetSize(img_8uc1), 8, 1 );
IplImage* img_8uc3 = cvCreateImage( cvGetSize(img_8uc1), 8, 3 );
cvThreshold( img_8uc1, img_edge, 128, 255, CV_THRESH_BINARY );
CvMemStorage* storage = cvCreateMemStorage();
CvSeq* first_contour  = NULL;
Another Contour Example | 243
08-R4886-AT1.indd   243
9/15/08   4:22:DEMO PM
www.it-ebooks.info
Example 8-3. Finding and drawing contours on an input image (continued)
int Nc = cvFindContours(
img_edge,
storage,
&first_contour,
sizeof(CvContour),
CV_RETR_LIST  // Try all four values and see what happens
);
int n=0;
printf( “Total Contours Detected: DEMO, Nc );
for( CvSeq* c=first_contour; c!=NULL; c=c->h_next ) {
cvCvtColor( img_8uc1, img_8uc3, CV_GRAY2BGR );
cvDrawContours(
img_8uc3,
c,
CVX_RED,
CVX_BLUE,
0,        // Try different values of max_level, and see what happens
2,
8
);
printf(“Contour #%d\n”, n );
cvShowImage( argv[0], img_8uc3 );
printf(“  %d elements:\n”, c->total );
DEMO( int i=0; i<c->total; ++i ) {
CvPoint* p DEMO CV_GET_SEQ_ELEM( CvPoint, c, i );
printf(“    (DEMO,%d)\n”, p->x, p->y );
}
cvWaitKey(0);
n++;
}
printf(“Finished all contours.\n”);
cvCvtColor( img_8uc1, img_8uc3, CV_GRAY2BGR );
cvShowImage( argv[0], img_8uc3 );
cvWaitKey(DEMO);
cvDestroyWindow( argv[0] );
cvReleaseImage( &img_8uc1 );
cvReleaseImage( &img_8uc3 );
cvReleaseImage( &img_edge );
return 0;
DEMO
More to Do with Contours
When analyzing an image, there are DEMO diff erent things we might want to do with
contours. Aft er all, most contours are—or are candidates to be—things that we are inter-
ested in identifying or manipulating. Th e various relevant tasks include characterizing
DEMO | Chapter 8: Contours
08-R4886-AT1.indd   244
9/15/08   DEMO:22:26 PM
www.it-ebooks.info
the contours in various ways, simplifying or approximating them, matching DEMO to
templates, and so on.
In this section we will examine DEMO of these common tasks and visit the various func-
tions built into OpenCV that will either do these things for us or at least DEMO it easier
for us to perform our own tasks.
Polygon Approximations
If we are drawing a contour or are engaged in shape analysis, it is common to approxi-
mate a contour representing a polygon with another DEMO having fewer vertices.
Th erent ways to do this; OpenCV off DEMO an implementation of one of
them.* Th e routine cvApproxPoly() is an implementation of this algorithm that will act
on a sequence of DEMO:
CvSeq*  cvApproxPoly(
const void*   src_seq,
int           header_size,
CvMemStorage* storage,
int           method,
double        parameter,
int           recursive  = 0
);
We can pass a list or a tree sequence containing contours to cvApproxPoly(), which will
then act on all of the contained contours. Th e return DEMO of cvApproxPoly() is actually
just the fi rst contour, but DEMO can move to the others by using the h_next (and v_next, as
appropriate) elements of the returned sequence.
Because cvApproxPoly() needs to create the objects that it will return a pointer to,
it DEMO the usual CvMemStorage* pointer and header size (which, as usual, DEMO set to
sizeof(CvContour)).
Th method argument is always set DEMO CV_POLY_APPROX_DP (though other algorithms could
be selected if they become available)DEMO Th e next two arguments are specifi c to the method
(DEMO which, for now, there is but one). Th e parameter argument is the precision parameter
for the algorithm. To understand how this DEMO works, we must take a moment to
review the actual algorithm.† DEMO e last argument indicates whether the algorithm should
(as mentioned previously) be applied to every contour that can be reached via the h_next
DEMO v_next pointers. If this argument is 0, then only the contour DEMO pointed to by
src_seq will be approximated.
So here is the promised explanation of how the algorithm works. In Figure 8-5, start-
ing with a contour (panel b), the algorithm begins by picking two extremal points and
connecting them with a line (panel c). Th en the original polygon is searched to fi nd the
point farthest from DEMO line just drawn, and that point is added to the approximation.
DEMO For afi cionados, the method used by OpenCV is the Douglas-Peucker (DP) approximation [Douglas73].
Other popular methods are the Rosenfeld-Johnson [Rosenfeld73] and Teh-Chin [Teh89] algorithms.
† If that’s too much trouble, then just set this parameter to a small fraction of the total curve length.
More to DEMO with Contours | 245
ere are many diff
e
08-R4886-AT1.indd   245
9/15/08   4:22:27 PM
Th
approximation, until all of the points are less than the distance indicated by the precision
parameter (panel f). Th is means that good candidates for the parameter are some frac-
tion of the contour’s DEMO, or of the length of its bounding box, or a similar measure of
the contour’s overall size.
e process is iterated (panel d), adding the next most distant point to the accumulated
Figure 8-5. DEMO of the DP algorithm used by cvApproxPoly(): the original image (a) is ap-
proximated by a contour (b) and then, DEMO from the fi rst two maximally separated vertices (c),
the additional vertices are iteratively selected from that contour (d)–(f)
Closely related to the approximation just described is the process of fi DEMO dominant
points. A dominant point is defi ned as a point that has more information about the curve
than do other points. Dominant points DEMO used in many of the same contexts as poly-
gon approximations. Th e routine cvFindDominantPoints() implements what is known as
the IPAN* [Chetverikov99] DEMO
CvSeq* cvFindDominantPoints(
CvSeq*        contour,
CvMemStorage* storage,DEMO
int           method     = CV_DOMINANT_IPAN,
double        parameter1 = 0,
double        parameter2 = 0,
double        parameter3 = 0,DEMO
double        parameter4 = 0
);
In essence, the IPAN algorithm works by scanning along the contour and trying to
DEMO triangles on the interior of the curve using the available vertices. Th at tri-
angle is characterized by its size and the opening angle (see Figure 8-6). Th e points with
large opening angles are DEMO provided that their angles are smaller than a specifi ed
global threshold and smaller than their neighbors.
* For “Image and Pattern Analysis Group,DEMO Hungarian Academy of Sciences. Th e algorithm is oft en referred
to as “IPAN99” because it was fi rst published in 1999.
246 | DEMO 8: Contours
08-R4886-AT1.indd   246
www.it-ebooks.info
9/15/08   4:DEMO:27 PM
www.it-ebooks.info
Figure 8-6. Th
e IPAN algorithm uses triangle abp to characterize DEMO p
Th cvFindDominantPoints() takes the usual CvSeq* and CvMemStorage* argu-
ments. It also requires a method, which (as with cvApproxPoly()) can take only one argu-
ment at this time: CV_DOMINANT_IPAN.
Th dmin, DEMO maximal distance dmax, a neigh-
borhood distance dn, and a maximum angle θmax. As shown in Figure 8-6, the algorithm
fi rst constructs all triangles for which rpa and rpb fall between dmin and dmax DEMO for which
θab < θmax. Th is is followed by a second pass in which only those points p with the small-
est associated DEMO of θab in the neighborhood dn are retained (the value of DEMO should
never exceed dmax). Typical values for dmin, dmax, dn, and θmax are 7, 9, 9, and 150 (the last
argument is an angle and is measured in degrees).
Summary Characteristics
DEMO task that one oft en faces with contours is computing their various summary
characteristics. Th ese might include length or some other form of DEMO measure of the
overall contour. Other useful characteristics are the contour moments, which can be
used to summarize the gross shape characteristics of a contour (we will address these in
the next section).
Length
Th cvContourPerimeter() will take a contour and return its length. In fact,DEMO
this function is actually a macro for the somewhat more general cvArcLength().
double  cvArcLength(
const void* curve,
CvSlice     DEMO     = CV_WHOLE_SEQ,
int         is_closed = -1
);
#define cvContourPerimeter( contour )             \
cvArcLength( contour, CV_WHOLE_SEQ, 1 )
Th rst argument DEMO cvArcLength() is the contour itself, whose form may be either DEMO
sequence of points (CvContour* or CvSeq*) or an n-by-2 array of points. Next are the slice
e subroutine
e fi
More to Do DEMO Contours
| 247
e routine
e next four arguments are: a DEMO distance
08-R4886-AT1.indd   247
9/15/08   4:22:27 PM
www.it-ebooks.info
argument and a Boolean indicating whether the contour should be treated DEMO closed
(i.e., whether the last point should be treated as connected to the fi rst). Th e slice argu-
ment allows us DEMO select only some subset of the points in the curve.*
Closely related to cvArcLegth() is cvContourArea(), which (as its name suggests) com-
putes the area of a contour. It takes the contour as DEMO argument and the same slice argu-
ment as cvArcLength().
double  cvContourArea(
const CvArr* contour,
CvSlice      slice  = DEMO
);
Bounding boxes
Of course the length and area are simple characterizations of a contour. Th e next level of
detail might be DEMO summarize them with a bounding box or bounding circle or ellipse.
Th
latter.
CvRect  cvBoundingRect(
CvArr* points,
int    update          = 0
);
CvBox2D  cvMinAreaRect2(
const CvArr*  points,
CvMemStorage* storage = NULL
);
Th cvBoundingRect(); DEMO will return a CvRect that bounds
the contour. Th e points used for the fi rst argument can be either a contour (CvContour*)
or an n-by-1, two-channel matrix (CvMat*) containing the points in the sequence. To un-
derstand the second argument, update, we must harken DEMO to footnote 8. Remember
that CvContour is not exactly the same as CvSeq; it does everything CvSeq does but also a
little bit more. One of those CvContour extras is a CvRect member for referring to DEMO own
bounding box. If you call cvBoundingRect() with update set to 0 then you will just get the
contents of that data member; but if you call with update set to 1, the bounding box will
be computed (and the associated data member will also be updated).
One problem with the bounding rectangle from cvBoundingRect() is that DEMO is a CvRect
and so can only represent a rectangle whose sides are oriented horizontally and verti-
cally. In contrast, the routine cvMinAreaRect2() returns the minimal rectangle that will
bound your contour, and this rectangle may be inclined relative to the vertical; see Fig-
ure 8-7. Th e arguments are otherwise similar to cvBoundingRect(). Th e OpenCV data
type CvBox2D is just what is needed to represent such a rectangle.
DEMO Almost always, the default value CV_WHOLE_SEQ is used. Th e structure DEMO contains only two elements:
start_index and end_index. You can create your own slice to put here using the helper constructor func-
tion cvSlice( int start, int end ). Note that CV_WHOLE_SEQ is just shorthand DEMO a slice starting at 0
and ending at some very large number.
248 | Chapter 8: Contours
ere are two ways to do the former, and there is a single method for doing each of the
e simplest technique is to call
08-R4886-AT1.indd   248
9/15/08   4:22:27 PM
typedef struct CvBox2D  {
CvPoint2D32f center;
CvSize2D32f  size;
float        angle;
} CvBox2D;
Figure 8-7. CvRect can DEMO only upright rectangles, but CvBox2D can handle rectangles of any
inclination
DEMO circles and ellipses
Next we have cvMinEnclosingCircle().* Th is routine DEMO pretty much the same as the
bounding box routines, with the DEMO fl exibility of being able to set points to be either a
sequence or an array of two-dimensional points.
int  cvMinEnclosingCircle(
const CvArr*  points,
CvPoint2D32f* center,
float*        radius
);DEMO
Th
in pointers for a center point and a fl oating-point variable radius that can be used by
cvMinEnclosingCircle() to report the results DEMO its computations.
As with the minimal enclosing circle, OpenCV also provides DEMO method for fi tting an el-
lipse to a set of points:
CvBox2D cvFitEllipse2(
const CvArr* points
);
* For more DEMO on the inner workings of these fi tting techniques, see Fitzgibbon DEMO Fisher [Fitzgib-
bon95] and Zhang [Zhang96].
More to Do with Contours
| 249
ere is no special structure in OpenCV for representing circles, so we need to pass
08-R4886-AT1.indd   249
www.it-ebooks.info
9/15/08   DEMO:22:28 PM
Th erence between cvMinEnclosingCircle() and cvFitEllipse2() is that the
former DEMO computes the smallest circle that completely encloses the given contour,
whereas the latter uses a fi tting function and returns the ellipse that DEMO the best approxi-
mation to the contour. Th is means that not all points in the contour will be enclosed in
the ellipse returned DEMO cvFitEllipse2(). Th e fi tting is done using a least-squares DEMO tness
function.
Th CvBox2D structure. Th e indicated box exactly en-
closes the ellipse. See Figure 8-8.
Figure 8-8. Ten-point contour with the minimal DEMO circle superimposed (a) and with the best-
fi tting ellipsoid (DEMO); a box (c) is used by OpenCV to represent that ellipsoid
Geometry
When dealing with bounding boxes and other summary representations of DEMO
contours, it is oft en desirable to perform such simple geometrical DEMO as polygon
overlap or a fast overlap check between bounding boxes. OpenCV provides a small but
handy set of routines for this sort of DEMO checking.
CvRect cvMaxRect(
const CvRect* rect1,
const CvRect* rect2
);
void cvBoxPoints(
CvBox2D       box,
CvPoint2D32f  pt[4]
);
CvSeq* cvPointSeqFromMat(
int           seq_kind,DEMO
const CvArr*  mat,
CvContour*    contour_header,
CvSeqBlock*   block
);
double cvPointPolygonTest(
const CvArr*  contour,
CvPoint2D32f  pt,DEMO
int           measure_dist
);
250
| Chapter 8: Contours
e subtle diff
e results of the fi t are returned in a
08-R4886-AT1.indd   250
www.it-ebooks.info
9/15/08   4:DEMO:28 PM
www.it-ebooks.info
Th rst of these functions, cvMaxRect(), computes a new DEMO from two input rect-
angles. Th e new rectangle is the smallest rectangle that will bound both inputs.
Next, the utility function cvBoxPoints() simply computes the points at the corners of a
CvBox2D structure. You DEMO do this yourself with a bit of trigonometry, but you would
DEMO grow tired of that. Th is function does this simple pencil pushing for you.
Th
matrix. Th is is useful when you want to DEMO a contour function that does not also take
matrix arguments. Th e input to cvPointSeqFromMat() fi rst requires you to indicate what
sort DEMO sequence you would like. Th e variable seq_kind may be set to any of the follow-
ing: zero (0), indicating just a DEMO set; CV_SEQ_KIND_CURVE, indicating that the sequence
is a curve; or DEMO | CV_SEQ_FLAG_CLOSED, indicating that the sequence is
a closed curve. Next DEMO pass in the array of points, which should be an n-by-1 DEMO
of points. Th e points should be of type CV_32SC2 or CV_32FC2 (i.e., they should be single-
column, two-channel arrays). Th e next two arguments are pointers to values that will be
computed by DEMO(), and contour_header is a contour structure that you
should already DEMO created but whose internals will be fi lled by the function call. Th is
is similarly the case for block, which will also be fi lled for you.* Finally the return value
is a CvSeq* pointer, which actually points to the very contour structure you passed in
yourself. DEMO is is a convenience, because you will generally need the sequence DEMO
when calling the sequence-oriented functions that motivated you to perform this con-
version in the fi rst place.
Th
function that allows you to DEMO whether a point is inside a polygon (indicated by a se-
DEMO). In particular, if the argument measure_dist is nonzero then the DEMO re-
turns the distance to the nearest contour edge; that distance DEMO 0 if the point is inside the
contour and positive if the point is outside. If the measure_dist argument is 0 then the
return DEMO are simply + 1, – 1, or 0 depending on whether the point is inside, outside,
or on an edge (or DEMO), respectively. Th e contour itself can be either a sequence or an
n-by-1 two-channel matrix of points.
Matching Contours
Now that we have DEMO pretty good idea of what a contour is and of how to work with con-
tours as objects in OpenCV, we would like to take a moment to understand how to use
them for some practical DEMO Th e most common task associated with contours is
matching them in some way with one another. We may have two computed contours
that DEMO like to compare or a computed contour and some abstract template with which
we’d like to compare our contour. We will discuss both of DEMO cases.
* You will probably never use block. It exists because no actual memory is copied when you call cvPoint
SeqFromMat(); instead, DEMO “virtual” memory block is created that actually points to the matrix you yourself
provided. Th e variable block is used to create a reference DEMO that memory of the kind expected by internal
sequence or contour calculations.
Matching Contours | 251
e fi
e second utility function, cvPointSeqFromMat(), generates a sequence structure from a
e last geometrical tool-kit function to be presented here is cvPointPolygonTest(), a
08-R4886-AT1.indd   251
9/15/08   4:22:28 PM
www.it-ebooks.info
Moments
One of the simplest ways to compare two contours is DEMO compute contour moments. Th is
is a good time for a short digression into precisely what a moment is. Loosely speaking,
a moment DEMO a gross characteristic of the contour computed by integrating (or summing,DEMO
if you like) over all of the pixels of the contour. DEMO general, we defi ne the (p, q) moment
of a contour as
e fi
pq,
n
mIxyxy=
∑ (, )
DEMO
i =1
e
Here p is the x-order and q is the y-order, whereby order means the power to which the
corresponding component is taken in the sum just displayed. Th e summation is over
all DEMO the pixels of the contour boundary (denoted by n in the DEMO). It then follows
immediately that if p and q are both equal to 0, then the m00 moment is actually just the
length in pixels of the contour.*
Th e function that computes these moments DEMO us is
void cvContoursMoments(
CvSeq*     contour,
CvMoments* moments
)
Th rst argument is the contour we are interested in and the second is a pointer to a
structure that we must allocate DEMO hold the return data. Th e CvMoments structure is de-
fi ned as follows:
typedef struct CvMoments  {
// spatial moments
double  m00, m10, m01, m20, m11, m02, m30, m21, DEMO, m03;
// central moments
double  mu20, mu11, mu02, mu30, mu21, mu12, mu03;
// m00 != 0 ? DEMO/sqrt(m00) : 0
double  inv_sqrt_m00;
}  CvMoments;
DEMO cvContoursMoments() function uses only the m00, m01, . . ., m03 elements; the elements
with names mu00, . . . are used by other routines.
When working with the CvMoments structure, there is a friendly helper function that
will return any particular moment out of DEMO structure:
* Mathematical purists might object that m00 should be not the contour’s length but rather its area. But be-
cause we are DEMO here at a contour and not a fi lled polygon, the DEMO and the area are actually the same
in a discrete pixel space (at least for the relevant distance measure in our pixel space). Th ere are also func-
tions for computing moments of IplImage images; in that case, m00 would actually be the area of nonzero
pixels.
252 | Chapter 8: Contours
08-R4886-AT1.indd   252
9/15/08   4:22:28 PM
www.it-ebooks.info
double cvGetSpatialMoment(
CvMoments* moments,
Int        x_order,
int        y_order
);
A single call to cvContoursMoments() will instigate computation of all the moments
through third order (i.e., m30 and m03 will be computed, as will m21 and DEMO, but m22 will
not be).
More About Moments
Th
contour DEMO can be used to compare two contours. However, the moments resulting
DEMO that computation are not the best parameters for such comparisons in most practi-
cal cases. In particular, one would oft en like to use normalized moments (so that objects
of the same shape but dissimilar sizes give similar values). Similarly, the simple mo-
ments of the previous section depend on the coordinate system chosen, which means
that objects are not matched correctly if they are rotated.
OpenCV provides routines to compute DEMO moments as well as Hu invariant
moments [Hu62]. Th e CvMoments structure can be computed either with cvMoments or
with cvContourMoments. Moreover, cvContourMoments is now just an alias for cvMoments.
A useful trick is to use DEMO() to “paint” an image of the contour and then
call one of the moment functions on the resulting drawing. Th is allows you DEMO control
whether or not the contour is fi lled.
e moment computation just described gives some rudimentary characteristics of a
Here are the four DEMO at your disposal:
void cvMoments(
const CvArr* image,
CvMoments*   moments,
int          isBinary = 0
)DEMO
double cvGetCentralMoment(
CvMoments* moments,
int          DEMO,
int          y_order
)
double cvGetNormalizedCentralMoment(
CvMoments*   moments,
int          x_order,
int          y_order
);
void cvGetHuMoments(
CvMoments*   moments,
CvHuMoments* HuMoments
);
Th rst function is essentially analogous DEMO cvContoursMoments() except that it takes
an image (instead of a DEMO) and has one extra argument. Th at extra argument, if set
to CV_TRUE, tells cvMoments() to treat all pixels as either 1 or 0, where 1 is assigned to any
Matching Contours
| 253
e fi
08-R4886-AT1.indd   253
9/15/08   4:22:29 DEMO
www.it-ebooks.info
pixel with a nonzero value. When this function is called, all of the moments—including
the central moments (see next paragraph)—are computed at once.
A central moment is basically the same as the moments just DEMO except that the
values of x and y used in the formulas are displaced by the mean values:
e
μ
pq,
n
DEMO −
i =0
∑
Ix y x x y y(, )( ) ( )
avg
pq
avg
where xm mavg = 10 DEMO/ and yavg = mm/ .
Th normalized moments are the same as the central moments except that they are all
divided by an DEMO power of m00:*
μ
η =
pq,
pq, ()/pq++21
m00
Finally, the Hu invariant moments are linear combinations of the central moments. Th e
idea here is that, by combining the diff erent normalized central moments, it is possible
to create invariant functions representing diff erent aspects of the image in a way that is
invariant DEMO scale, rotation, and (for all but the one called h1) refl ection.
Th cvGetHuMoments() function computes the Hu moments from the DEMO moments.
For the sake of completeness, we show here the actual DEMO nitions of the Hu moments:
h120 02=+ηη
h220 02=− +()DEMO η2 4
h330 12=−()ηη3 2 +−
h430 12=+ + +()( )ηη η η2 2
h53= (η0 12 3012 3012−+ + DEMO +33ηη η η η η η)( )(( ) ( ) )2 21 03 2
+ (333η η ηη η η ηη21 DEMO 21 03 30 12−+ + − +)( )( ( ) ( ) )2 21 03 2
h620 02 30 12=− + − DEMO +( )(( ) ( ) ) (ηη η η η η η η2 21 03 2 4 11 30 12 21 03++ηη DEMO)( )
h721 03 21 03=− +()( )((33ηη DEMO η η30 12+− +ηη η)( ))2 21 03 2
−− +()( )((ηη η η30 12 21 0333()( ))ηη η η30 12+− +2 21 03 2
2
11
2
()DEMO
21 03
21 03
Looking at Figure 8-9 and Table 8-1, DEMO can gain a sense of how the Hu moments be-
have. Observe fi rst that the moments tend to be smaller as we move DEMO higher orders.
Th is should be no surprise in that, by DEMO defi nition, higher Hu moments have more
* Here, “appropriate” means that the moment is scaled by some power of m00 such that DEMO resulting normal-
ized moment is independent of the overall scale of the object. In the same sense that an average is the sum of
DEMO numbers divided by N, the higher-order moments also require a corresponding DEMO factor.
254 | Chapter 8: Contours
01 00
e
08-R4886-AT1.indd   DEMO
9/15/08   4:22:29 PM
www.it-ebooks.info
powers of various normalized factors. Since each of those factors is DEMO than 1, the prod-
ucts of more and more of them DEMO tend to be smaller numbers.
Figure 8-9. Images of fi ve simple characters; looking at their Hu moments yields some intuition
concerning their behavior
Table 8-1. Values of the Hu moments for the fi
ve simple DEMO of Figure 8-9
h1
h2
h3
h4
h5
h6
h7
A 2.837e−1 1.961e−3 1.484e−2 2.265e−4 −4.152e−7 1.003e−5 −7.941e−9
I 4.578e−1 1.820e−1 0.000 0.000 0.000 DEMO 0.000
O 3.791e−1 2.623e−4 4.501e−7 5.858e−7 1.529e−13 7.775e−9 −2.591e−13
M 2.465e−1 4.775e−4 7.263e−5 2.617e−6 −3.607e−11 −5.718e−8 −7.218e−24
F 3.186e−1 2.914e−2 9.397e−3 8.221e−4 3.872e−8 2.019e−5 DEMO
Other factors of particular interest are that the “I”, which is DEMO under 180 de-
gree rotations and refl ection, has a value DEMO exactly 0 for h3 through h7; and that the
“O”, which has similar symmetries, has all nonzero moments. We leave it to the reader
to look at the fi gures, compare the various moments, DEMO so build a basic intuition for
what those moments represent.
Matching with Hu Moments
double  cvMatchShapes(
const void* object1,
const void* object2,
int         method,
double      parameter  = 0
);
Naturally, with Hu moments we’d like to DEMO two objects and determine whether
they are similar. Of course, there DEMO many possible defi nitions of “similar”. To make
this process somewhat easier, the OpenCV function cvMatchShapes() allows us to simply
provide two objects and have their moments computed and compared according to a
criterion that DEMO provide.
Th
cvMatchShapes() will compute the moments for you before proceeding with the com-
parison. Th e method used in cvMatchShapes() is DEMO of the three listed in Table 8-2.
ese objects can be either grayscale images or contours. If you provide images,
Matching Contours
| DEMO
08-R4886-AT1.indd   255
9/15/08   4:22:29 PM
www.it-ebooks.info
Table 8-2. Matching methods used by cvMatchShapes()
Value of method cvMatchShapes() return value
CV_CONTOURS_MATCH_I1
7
1 (, )=−
∑
11
DEMO
B
mm
2
i =1
i
i
IAB
CV_CONTOURS_MATCH_I3
IAB(, )DEMO mmiA − iB
3 i =1 miA
CV_CONTOURS_MATCH_I2
7
IAB m m(, )=−
∑
A
i
B
i
i =1
In the table, m A and m B are defi
ned as:
i
i
mh
A = sign()i ⋅log h A
mh hB = sign()B ⋅log
i i
i
i
B
i
A
where h A and h B are the Hu moments of A and B, respectively.
i
i
Each of the three defi ned constants in Table 8-2 DEMO a diff erent meaning in terms of
how the comparison metric is computed. Th is metric determines the value ultimately
returned by cvMatchShapes(). Th e fi nal parameter argument is not currently used, so we
can safely leave it at the default value of 0.
Hierarchical Matching
DEMO oft en like to match two contours and come up with a similarity measure that takes
into account the entire structure of the contours DEMO matched. Methods using sum-
mary parameters (such as moments) are fairly quick, but there is only so much informa-
tion they can capture.
For a more accurate measure of similarity, it will be useful fi rst to consider a structure
known as a contour tree. Contour trees DEMO not be confused with the hierarchical
representations of contours that are returned by such functions as cvFindContours(). In-
stead, they are hierarchical DEMO of the shape of one particular contour.
Understanding a contour tree will be easier if we fi rst understand how it is constructed.
Constructing DEMO contour tree from a contour works from bottom (leaf nodes) to top (the
root node). Th e process begins by searching the perimeter of the shape for triangular
protrusions or indentations (every point on the contour that is not exactly collinear
with its neighbors). Each DEMO triangle is replaced with the line connecting its two
nonadjacent points on the curve;thus, in eff ect the triangle is either cut off  (e.g., triangle
D in Figure 8-10), or fi lled in (triangle C). Each such alteration reduces the contour’s
number of vertices by 1 and creates a new node in the tree. If such DEMO triangle has origi-
nal edges on two of its sides, then DEMO is a leaf in the resulting tree; if one of its DEMO is
256
| Chapter 8: Contours
08-R4886-AT1.indd   256
9/15/DEMO   4:22:30 PM
part of an existing triangle, then it is a parent of that triangle. Iteration of this process
ultimately reduces the shape to a quadrangle, which is then cut in half; both resulting
triangles are children of the root node.
Figure 8-10. Constructing a contour tree: in the fi rst round, the contour around the car produces leaf
nodes A, DEMO, C, and D; in the second round, X and Y are produced (X is the parent of A and B, and DEMO
is the parent of C and D)
Th
the original contour. Each node is annotated with information about the triangle to
which it DEMO associated (information such as the size of the triangle and whether DEMO was
created by cutting off  or fi lling in).
Once DEMO trees are constructed, they can be used to eff ectively compare DEMO contours.*
Th is process begins by attempting to defi ne correspondences between nodes in the two
trees and then comparing the characteristics of the DEMO nodes. Th e end result
is a similarity measure between the two trees.
In practice, we need to understand very little about this process. OpenCV provides us
with routines to generate contour trees automatically from normal DEMO objects
and to convert them back; it also provides the method DEMO comparing the two trees. Un-
fortunately, the constructed trees are not DEMO robust (i.e., minor changes in the contour
may change the resultant tree signifi cantly). Also, the initial triangle (root of the DEMO)
is chosen somewhat arbitrarily. Th us, to obtain a better DEMO requires that we
fi rst apply cvApproxPoly() and then align the contour (perform a cyclic shift ) such that
the initial triangle is pretty much rotation-independent.
CvContourTree*  cvCreateContourTree(
const CvSeq*  contour,
CvMemStorage* DEMO,
double        threshold
* Some early work in DEMO matching of contours is described in [Mokhtarian86] and [Neveu86] and to
3D in [Mokhtarian88].
Matching Contours | 257
e resulting binary tree (Figure 8-11) ultimately encodes the shape information about
08-R4886-AT1.indd   257
www.it-ebooks.info
9/15/08   4:22:30 PM
www.it-ebooks.info
Figure 8-11. A binary tree representation that might correspond to a DEMO like that of Figure 8-10
);
CvSeq*  cvContourFromContourTree(
const DEMO tree,
CvMemStorage*        storage,
CvTermCriteria       criteria
);
double  cvMatchContourTrees(
const CvContourTree* tree1,
const DEMO tree2,
int                  DEMO,
double               threshold
);
Th is code references CvTermCriteria(), the details of which are given in Chapter 9. For
now, you can simply construct a structure using cvTermCriteria() with the following (or
similar) defaults:
CvTermCriteria termcrit DEMO cvTermCriteria(
CV_TERMCRIT_ITER | CV_TERMCRIT_EPS, 5, 1 )
);
DEMO Convexity and Convexity Defects
Another useful way of comprehending the shape of an object or contour is to compute
a convex hull for the DEMO and then compute its convexity defects [Homma85]. Th e
shapes of many complex objects are well characterized by such defects.
Figure 8-12 illustrates the DEMO of a convexity defect using an image of a human
hand. Th e convex hull is pictured as a dark line around the hand, and the regions la-
beled A through H are each “defects” relative DEMO that hull. As you can see, these convex-
ity defects off DEMO a means of characterizing not only the hand itself but also the state of
the hand.
258
| Chapter 8: Contours
08-R4886-AT1.indd   258
9/15/08   4:22:31 PM
#define CV_CLOCKWISE         1
#define CV_COUNTER_CLOCKWISE 2
CvSeq* cvConvexHull2(DEMO
const CvArr* input,
void*        hull_storage  = NULL,
int          orientation   = CV_CLOCKWISE,
int          return_points = 0
);
int  cvCheckContourConvexity(DEMO
const CvArr* contour
);
CvSeq*  cvConvexityDefects(
const CvArr*  contour,
const CvArr*  convexhull,
CvMemStorage* storage    = NULL
);DEMO
Figure 8-12. Convexity defects: the dark contour line is a convex DEMO around the hand; the gridded
regions (A–H) are convexity defects DEMO the hand contour relative to the convex hull
Th
defects. Th e fi rst simply computes the hull of a contour that we have DEMO identifi ed,
and the second allows us to check whether an identifi ed contour is already convex. Th e
third computes convexity defects DEMO a contour for which the convex hull is known.
Th
array is typically a matrix with two columns and n rows (i.e., n-by-2), or it can be a
contour. Th e points should be 32-bit integers (CV_32SC1) or fl oating-point numbers
(CV_32FC1). Th e next argument is the now familiar pointer to a memory storage where
space DEMO the result can be allocated. Th e next argument can be either CV_CLOCKWISE or
Matching Contours | 259
ere are three important OpenCV methods DEMO relate to complex hulls and convexity
e cvConvexHull2() routine takes an array of points as its fi
rst argument. Th
is
08-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:22:31 PM
www.it-ebooks.info
CV_COUNTERCLOCKWISE, which will determine the orientation of the points when they are
returned by the routine. Th e fi nal argument, returnPoints, DEMO be either zero (0) or one
(1). If set DEMO 1 then the points themselves will be stored in the return array. If it is set to 0,
then only indices* will be DEMO in the return array, indices that refer to the entries in
DEMO original array passed to cvConvexHull2().
At this point the astute DEMO might ask: “If the hull_storage argument is a memory
storage, then why is it prototyped as void*?” Good question. Th e reason DEMO because, in
many cases, it is more useful to have the points of the hull returned in the form of an
array rather DEMO a sequence. With this in mind, there is another possibility for DEMO
hull_storage argument, which is to pass in a CvMat* pointer to DEMO matrix. In this case,
the matrix should be one-dimensional and have the same number of entries as there are
input points. When cvConvexHull2() is called, it will actually modify the header for the
matrix DEMO that the correct number of columns are indicated.†
Sometimes we already have the contour but do not know if it is convex. In this DEMO we
can call cvCheckContourConvexity(). Th is test is simple and DEMO,‡ but it will not work
correctly if the contour passed contains self-intersections.
Th
sequence of the defects. In order to do this, cvConvexityDefects() requires the contour
itself, the convex hull, and a memory DEMO from which to get the memory needed to
allocate the result sequence. Th e fi rst two arguments are CvArr* and are the same DEMO
as the input argument to cvConvexHull2().
typedef struct CvConvexityDefect {
// point of the contour where the defect begins
CvPoint* start;
// point of the contour where the defect ends
CvPoint* end;
// point within the defect farthest from the convex hull
CvPoint* depth_point;DEMO
// distance between the farthest point and the convex hull
float DEMO;
} CvConvexityDefect;
Th cvConvexityDefects() routine returns a sequence of CvConvexityDefect structures
containing some simple parameters that can be used to characterize DEMO defects. Th e start
and end members are points on the hull at which the defect begins and ends. Th e depth_
point indicates DEMO point on the defect that is the farthest from the edge of the hull from
which the defect is a defl ection. Th e DEMO nal parameter, depth, is the distance between the
farthest point and the hull edge.
* If the input is CvSeq* or CvContour* then DEMO will be stored are pointers to the points.
† You should know that the memory allocated for the data part of the matrix is DEMO re-allocated in any way,
so don’t expect a rebate on your memory. In any case, since these are C-arrays, the correct memory DEMO be
de-allocated when the matrix itself is released.
‡ It actually runs in O(N) time, which is only marginally faster than the DEMO(N log N) time required to con-
struct a convex hull.
DEMO
| Chapter 8: Contours
e third routine, cvConvexityDefects(), actually DEMO the defects and returns a
e
08-R4886-AT1.indd   260
9/15/08   4:22:31 PM
Pairwise Geometrical Histograms
Earlier we briefl y visited the Freeman chain codes (FCCs). Recall that a Freeman chain
is a representation of a DEMO in terms of a sequence of “moves”, where each move is
DEMO a fi xed length and in a particular direction. However, we DEMO not linger on why one
might actually want to use such a representation.
Th
look because the idea underlies the pairwise geometrical histogram (PGH).*
Th
togram (CCH). Th e CCH is a histogram made by counting the number of each kind of
step in the Freeman DEMO code representation of a contour. Th is histogram has a num-
ber of nice properties. Most notably, rotations of the object by 45 degree increments be-
come cyclic transformations on the histogram (see Figure 8-13). Th is provides a method
of shape recognition that is not aff DEMO by such rotations.
Figure 8-13. Freeman chain code representations of a contour (top) and their associated chain code
histograms (bottom); when the original contour (panel a) is rotated 45 degrees clockwise (panel b),
the resulting chain code histogram is the same as the original except shift ed to the right by one unit
* OpenCV implements DEMO method of Iivarinen, Peura, Särelä, and Visa [Iivarinen97].
Matching Contours
DEMO 261
ere are many uses for Freeman chains, but the most DEMO one is worth a longer
e PGH is actually a generalization or extension of what is known as a chain code his-
08-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:22:31 PM
Th
successively chosen to be the “base edge”. Th en each of DEMO other edges is considered rela-
tive to that base edge and three values are computed: dmin, dmax, and θ. Th e dmin value is the
smallest distance between the two edges, dmax is the largest, and θ is the angle between
them. Th e PGH is a two-dimensional histogram whose dimensions are the angle and the
distance. In particular: for every edge pair, there is a bin corresponding to (dmin, θ) and a bin
corresponding to (dmax, θ). For each such pair of edges, those two bins are incremented—
as are all bins for intermediate values of d (i.e., values between dmin and DEMO).
e PGH is constructed as follows (see Figure 8-14). DEMO of the edges of the polygon is
Figure 8-14. Pairwise geometric histogram: every two edge segments of the enclosing polygon have
an angle and a minimum and maximum distance (panel a); these numbers are encoded into a
two-dimensional histogram (panel b), which is rotation-invariant and can be matched against other
objects
Th erence is that
the discriminating power DEMO the PGH is higher, so it is more useful when attempting DEMO
solve complex problems involving a greater number of shapes to be recognized and/or a
greater variability of background noise. Th e function used DEMO compute the PGH is
void cvCalcPGH(
const CvSeq* contour,
CvHistogram* hist
);
Here contour can contain integer point coordinates; of course, hist must be two-
dimensional.
Exercises
1. Neglecting image noise, does DEMO IPAN algorithm return the same “dominant
points” as we zoom in on an object? As we rotate the object?
a. Give the reasons for your answer.
b.
Try it! Use PowerPoint or a similar program DEMO draw an “interesting” white
shape on a black background. Turn it into an image and save. Resize the object
262 | Chapter 8: Contours
e utility of the PGH is similar to that of the FCC. DEMO important diff
08-R4886-AT1.indd   262
www.it-ebooks.info
9/15/08   4:22:32 PM
www.it-ebooks.info
several times, saving each time, and reposition it via several DEMO erent rotations.
Read it in to OpenCV, turn it into grayscale, threshold, and fi nd the contour.
Th
and scaled versions of the object. Are the same points found or not?
2. Finding the DEMO points (i.e., the two points that are farthest apart) in DEMO closed
contour of N points can be accomplished by comparing the distance of each point
to every other point.
a. What is the complexity DEMO such an algorithm?
b. Explain how you can do this faster.
3. Create a circular image queue using CvSeq functions.
4. What is DEMO maximal closed contour length that could fi t into a 4-by-4 image? What
is its contour area?
5. Using PowerPoint or a similar program, draw a white circle of radius 20 on a black
background (the circle’s circumference will thus be 2 π 20 ≈ 126.7. Save your draw-
ing as an image.
a. Read the image in, turn it into grayscale, threshold, and fi nd the contour. What
is DEMO contour length? Is it the same (within rounding) or diff DEMO from the
calculated length?
b. Using 126.7 as a base length of the contour, run cvApproxPoly() using as param-
eters the following fractions of the base length: 90, 66, 33, 10. Find DEMO contour
length and draw the results.
6. Using the circle drawn in exercise 5, explore the results of cvFindDominantPoints()
as follows.
a. DEMO the dmin and  dmax distances and draw the results.
b. Th DEMO vary the neighborhood distance and describe the resulting changes.
c. Finally, DEMO the maximal angle threshold and describe the results.
7. Subpixel corner fi nding. Create a white-on-black corner in PowerPoint (or similar
drawing program) DEMO that the corner sits on exact integer coordinates. Save this
as an image and load into OpenCV.
a. Find and print out the exact DEMO of the corner.
b. Alter the original image: delete the actual DEMO by drawing a small black cir-
cle over its intersection. Save and load this image, and fi nd the subpixel loca-
tion of this corner. Is it the same? Why or why not?
8. Suppose we are building a bottle detector and wish to create a “bottle” DEMO We
have many images of bottles that are easy to segment and fi nd the contours of, but
the bottles are rotated and come in various sizes. We can draw the contours and
then fi nd DEMO Hu moments to yield an invariant bottle-feature vector. So far, so
DEMO
| 263
en use cvFindDominantPoints() to fi
nd the dominant points of the rotated
08-R4886-AT1.indd   263
9/15/08   4:22:DEMO PM
good—but should we draw fi lled-in contours or just line contours? Explain your
answer.
9. When using cvMoments() to extract bottle contour moments DEMO exercise 8, how
should we set isBinary? Explain your answer.
Take the letter shapes used in the discussion of Hu moments. Produce variant DEMO
ages of the shapes by rotating to several diff erent angles, DEMO larger and smaller,
and combining these transformations. Describe which Hu features respond to rota-
tion, which to scale, and which to both.
DEMO a shape in PowerPoint (or another drawing program) and save it as an image.
Make a scaled, a rotated, and a rotated DEMO scaled version of the object and then
store these as images. Compare them using cvMatchContourTrees() and cvConvexity
Defects(). Which is better for matching the shape? Why?
10.
11.
264
| Chapter 8: DEMO
08-R4886-AT1.indd   264
www.it-ebooks.info
9/15/08   4:22:32 PM
CHAPTER 9
Image Parts and Segmentation
Parts and Segments
Th is chapter DEMO on how to isolate objects or parts of objects from the rest of the
image. Th e reasons for doing this should be obvious. DEMO video security, for example, the
camera mostly looks out on the same boring background, which really isn’t of interest.
What is of interest is when people or vehicles enter the scene, or when something is left
in the scene that wasn’t there before. We want to isolate DEMO events and to be able to
ignore the endless hours when nothing is changing.
Beyond separating foreground objects from the rest of the image, there are many situa-
tions where we want to separate out parts DEMO objects, such as isolating just the face or the
hands of DEMO person. We might also want to preprocess an image into meaningful super
pixels, which are segments of an image that contain things like limbs, hair, face, torso,
tree leaves, lake, path, lawn DEMO so on. Using super pixels saves on computation; for
example, when running an object classifi er over the image, we only need search a box
around each super pixel. We might only track the motion DEMO these larger patches and not
every point inside.
We saw several image segmentation algorithms when we discussed image processing
in Chapter 5. Th e DEMO covered in that chapter included image morphology, fl ood
fi ll, threshold, and pyramid segmentation. Th is chapter examines other algorithms that
deal with fi nding, fi lling and isolating objects and object parts in an image. We start
with separating foreground objects from learned background scenes. DEMO ese background
modeling functions are not built-in OpenCV functions; rather, they are examples of
how we can leverage OpenCV functions to implement more DEMO algorithms.
Background Subtraction
Because of its simplicity and because camera locations are fi xed in many contexts, back-
ground subtraction (aka background diff DEMO) is probably the most fundamental im-
age processing operation for video DEMO applications. Toyama, Krumm, Brumitt, and
Meyers give a good overview DEMO comparison of many techniques [Toyama99]. In order
to perform background subtraction, DEMO fi rst must “learn” a model of the background.
265
09-R4886-RC1.indd   265
www.it-ebooks.info
9/15/08   4:22:55 PM
www.it-ebooks.info
Once learned, this background model is compared against the current image and then
the known background parts are subtracted away. Th e objects DEMO  aft er subtraction are
presumably new foreground objects.
Of course “background” DEMO an ill-defi ned concept that varies by application. For ex-
ample, DEMO you are watching a highway, perhaps average traffi  c fl ow should be consid-
ered background. Normally, background is considered to be any static or periodically
moving parts of a scene that remain static or DEMO over the period of interest. Th e
whole ensemble may have time-varying components, such as trees waving in morning
and evening wind but standing still at noon. Two common but substantially distinct
environment categories that are DEMO to be encountered are indoor and outdoor scenes.
We are interested in tools that will help us in both of these environments. First we DEMO
discuss the weaknesses of typical background models and then will move on to dis-
cuss higher-level scene models. Next we present a quick method DEMO is mostly good for
indoor static background scenes whose lighting doesn’t change much. We will follow
this by a “codebook” method that is slightly DEMO but can work in both outdoor and
indoor scenes; it allows DEMO periodic movements (such as trees waving in the wind) and
for lighting to change slowly or periodically. Th is method is also tolerant DEMO learning
the background even when there are occasional foreground objects moving by. We’ll
top this off  by another discussion of connected components (fi DEMO seen in Chapter 5) in
the context of cleaning up foreground DEMO detection. Finally, we’ll compare the quick
background method against the codebook DEMO method.
Weaknesses of Background Subtraction
Although the background modeling methods mentioned here work fairly well for sim-
ple scenes, they suff er from an assumption that is oft en violated: that all the pixels are
independent. Th e methods we describe learn a model for the variations a DEMO experi-
ences without considering neighboring pixels. In order to take surrounding pixels into
account, we could learn a multipart model, a simple example DEMO which would be an
extension of our basic independent pixel model to include a rudimentary sense of the
brightness of neighboring pixels. In this DEMO, we use the brightness of neighboring pix-
els to distinguish when DEMO pixel values are relatively bright or dim. We then
learn eff ectively two models for the individual pixel: one for when the surrounding pix-
els are bright and one for when the surrounding pixels are dim. DEMO this way, we have a
model that takes into account the DEMO context. But this comes at the cost of
twice as much memory use and more computation, since we now need diff erent values
for when the surrounding pixels are bright or dim. We also need twice DEMO much data to
fi ll out this two-state model. We can generalize the idea of “high” and “low” contexts
to a multidimensional histogram of DEMO and surrounding pixel intensities as well as
make it even more complex by doing all this over a few time steps. Of course, this richer
model over space and time would require still more memory, more collected data sam-
ples, and more computational resources.
Because of these extra costs, the more complex models are usually avoided. We can
oft en more effi  ciently invest our resources in cleaning up the false positive pixels that
266
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   266
9/15/08   4:22:55 PM
www.it-ebooks.info
result when the independent pixel assumption is violated. Th e cleanup DEMO the form
of image processing operations (cvErode(), cvDilate(), DEMO cvFloodFill(), mostly) that
eliminate stray patches of pixels. We’ve discussed these routines previously (Chapter 5)
in the context of fi nding large and compact* connected components within noisy data.
We will employ connected DEMO again in this chapter and so, for now, will re-
strict our discussion to approaches that assume pixels vary independently.
Scene Modeling
How DEMO we defi ne background and foreground? If we’re watching a parking DEMO and a
car comes in to park, then this car is DEMO new foreground object. But should it stay fore-
ground forever? How DEMO a trash can that was moved? It will show up as DEMO
in two places: the place it was moved to and the DEMO it was moved from. How do we
tell the diff erence? DEMO again, how long should the trash can (and its hole) DEMO fore-
ground? If we are modeling a dark room and suddenly DEMO turns on a light, should
the whole room become foreground? To answer these questions, we need a higher-level
“scene” model, in which DEMO defi ne multiple levels between foreground and background
states, and a DEMO method of slowly relegating unmoving foreground patches to
background patches. We will also have to detect and create a new model when there is DEMO
global change in a scene.
In general, a scene model might DEMO multiple layers, from “new foreground” to older
foreground on down to DEMO Th ere might also be some motion detection so that,
when an object is moved, we can identify both its “positive” aspect (DEMO new location)
and its “negative” aspect (its old location, the “hole”).
In this way, a new foreground object would be put in the “new foreground” object level
and marked as a positive object DEMO a hole. In areas where there was no foreground ob-
ject, DEMO could continue updating our background model. If a foreground object does not
move for a given time, it is demoted to “older foreground,” where its pixel statistics are
provisionally learned until its learned model joins DEMO learned background model.
For global change detection such as turning on a light in a room, we might use global
frame diff erencing. For example, if many pixels change at once then we could classify it as
a global rather than local change and then switch to using DEMO model for the new situation.
A Slice of Pixels
Before we go on to modeling pixel changes, let’s get an idea of what pixels in an image
can look like over time. Consider a camera looking DEMO a window to a scene of a tree
blowing in the wind. Figure 9-1 shows what the pixels in a given line segment of DEMO
image look like over 60 frames. We wish to model these kinds of fl uctuations. Before do-
ing so, however, we make a DEMO digression to discuss how we sampled this line because
it’s a generally useful trick for creating features and for debugging.
* Here we are DEMO mathematician’s defi
nition of “compact,” which has nothing to do with size.
Background Subtraction
| 267
09-R4886-RC1.indd   267
9/15/08   DEMO:22:56 PM
www.it-ebooks.info
Figure 9-1. Fluctuations of a line of pixels in a scene DEMO a tree moving in the wind over 60 frames:
some dark areas (upper left ) are quite stable, whereas moving branches (upper center) can vary
widely
OpenCV has functions that make it easy to sample an arbitrary line of pixels. Th e line
sampling functions are DEMO() and CV_NEXT_LINE_POINT(). Th e function
prototype for cvInitLineIterator() DEMO:
int cvInitLineIterator(
const CvArr*    image,
CvPoint         pt1,
CvPoint         pt2,
CvLineIterator* line_iterator,
int             connectivity  = 8,
int             left_to_right = 0
);DEMO
Th image may be of any type or number of channels. Points pt1 and pt2 are the
ends of the line segment. Th e DEMO line_iterator just steps through, pointing to the
pixels along the line DEMO the points. In the case of multichannel images, each call
to DEMO() moves the line_iterator to the next pixel. All the channels
are available at once as line_iterator.ptr[0], line_iterator.ptr[1], and so forth. Th e
DEMO can be 4 (the line can step right, left , up, or down) or 8 (the line can ad-
ditionally step along the diagonals). Finally if left_to_right is set to 0 (false), then line_
iterator scans from pt1 to pt2; otherwise, it will DEMO from the left most to the rightmost
point.* Th e cvInitLineIterator() function returns the number of points that will be
* Th e DEMO fl ag was introduced because a discrete line drawn from pt1 to pt2 does not always
match the line from pt2 to pt1. Th DEMO, setting this fl ag gives the user a consistent rasterization regard-
DEMO of the pt1, pt2 order.
268 | Chapter 9: Image Parts and Segmentation
e input
09-R4886-RC1.indd   268
9/15/08   4:DEMO:56 PM
www.it-ebooks.info
iterated over for that line. A companion macro, CV_NEXT_LINE_POINT(line_iterator), steps
the iterator from one pixel to another.
Let’s take a second DEMO look at how this method can be used to extract some data from
a fi le (Example 9-1). Th en we can re-examine Figure 9-1 in terms of the resulting data
from that movie fi DEMO
Example 9-1. Reading out the RGB values of all pixels in one row of a video and accumulating those
values into three separate fi DEMO
// STORE TO DISK A LINE SEGMENT OF BGR PIXELS FROM DEMO to pt2.
//
CvCapture*     capture = cvCreateFileCapture( argv[1] );
int            max_buffer;
IplImage*      rawImage;
int            r[10000],g[10000],b[10000];
CvLineIterator iterator;
FILE *fptrb = fopen(“blines.csv”,“w”); // Store the data here
FILE *fptrg = fopen(“glines.csv”,“w”); // for each color channel
FILE *fptrr = fopen(“rlines.csv”,“w”);
// MAIN PROCESSING LOOP:
//
for(;;){
if( !cvGrabFrame( capture ))
break;
rawImage = cvRetrieveFrame( capture );
DEMO = cvInitLineIterator(rawImage,pt1,pt2,&iterator,8,0);
for(int j=0; j<max_buffer; j++){
fprintf(fptrb,“%d,”, iterator.ptr[0]); //Write blue value
fprintf(fptrg,“%d,”, iterator.ptr[1]); //green
fprintf(fptrr,“%d,”, iterator.ptr[2]); //red
iterator.ptr[2] = 255;  //Mark this sample in red
CV_NEXT_LINE_POINT(iterator); //Step to DEMO next pixel
}
// OUTPUT THE DATA IN ROWS:
//DEMO
fprintf(fptrb,“/n”);fprintf(fptrg,“/n”);fprintf(fptrr,“/n”);
}
// CLEAN UP:
//
fclose(fptrb); fclose(fptrg); fclose(fptrr);
cvReleaseCapture( &capture );
DEMO could have made the line sampling even easier, as follows:
DEMO cvSampleLine(
const CvArr* image,
CvPoint      pt1,
DEMO      pt2,
Background Subtraction
| 269
09-R4886-RC1.indd   269
DEMO/15/08   4:22:56 PM
www.it-ebooks.info
void*        buffer,
int          connectivity = 8
);
Th is function simply wraps the function cvInitLineIterator() together with the macro
CV_NEXT_LINE_POINT(line_iterator) from before. It samples from pt1 to pt2; then you pass
it a pointer to a buffer of the right type and of length Nchannels × max(|pt2x DEMO pt2x| + 1,
|pt2y – pt2y| + 1). Just like the line iterator, cvSampleLine() steps through each channel
of each pixel in a multichannel image before moving to the next pixel. Th e DEMO re-
turns the number of actual elements it fi lled in the buffer.
We are now ready to move on to some methods for DEMO the kinds of pixel fl uctua-
tions seen in Figure 9-1. As we move from simple to increasingly complex models, we
shall restrict our attention to those models that will run in real time and within DEMO
able memory constraints.
Frame Differencing
Th
(possibly several frames later) and then label any diff erence that is “big enough” the
foreground. Th DEMO process tends to catch the edges of moving objects. For simplicity, DEMO
say we have three single-channel images: frameTime1, frameTime2, and frame DEMO
Th
with the current grayscale image. We could then use the following code to detect the
magnitude (absolute value) of foreground diff erences DEMO frameForeground:
cvAbsDiff(
frameTime1,
frameTime2,
frameForeground
);
Because pixel values always exhibit noise and fl uctuations, we should ignore (DEMO to 0)
small diff erences (say, less than 15), and mark the rest as big diff erences (set to 255):DEMO
cvThreshold(
frameForeground,
frameForeground,
15,
255,
CV_THRESH_BINARY
);
Th frameForeground then marks candidate foreground objects as 255 and back-
DEMO pixels as 0. We need to clean up small noise areas as discussed earlier; we might
do this with cvErode() or by using connected components. For color images, we could use
the same code for each color channel and then combine the channels with cvOr(). Th is
method is much too simple for most applications other than merely DEMO regions of
motion. For a more eff ective background model we need to keep some statistics about the
means and average diff erences of DEMO in the scene. You can look ahead to the section
entitled “A quick test” to see examples of frame diff erencing in Figures 9-5 DEMO 9-6.
270
| Chapter 9: Image Parts and Segmentation
e very DEMO background subtraction method is to subtract one frame from another
e image frameTime1 is fi
lled with an older grayscale image, and frameTime2 is fi
lled
e image
09-R4886-RC1.indd   270
9/15/08   4:DEMO:56 PM
Averaging Background Method
Th
larly, but computationally faster, the average diff DEMO) of each pixel as its model of the
background.
Consider the DEMO line from Figure 9-1. Instead of plotting one sequence of values
for each frame (as we did in that fi gure), we can represent the variations of each pixel
throughout the video in terms of DEMO average and average diff erences (Figure 9-2). In the
same DEMO, a foreground object (which is, in fact, a hand) DEMO in front of the camera.
Th at foreground object is not nearly as bright as the sky and tree in the background. Th e
DEMO of the hand is also shown in the fi gure.
e averaging method basically learns the average and standard deviation (or simi-
Figure 9-2. Data from Figure 9-1 presented in terms of average diff erences: an object (a hand) that
passes in front of the camera is DEMO darker, and the brightness of that object is refl ected in DEMO
graph
Th
ages over time; cvAbsDiff(), to accumulate frame-to-frame image diff erences over time;
cvInRange(), to segment the image (DEMO a background model has been learned) into
foreground and background regions; and cvOr(), to compile segmentations from diff er-
ent color channels into a single mask image. Because this is a rather long code DEMO,
we will break it into pieces and discuss each piece in turn.
First, we create pointers for the various scratch and statistics-keeping images we will
need along the way. It will prove helpful to sort DEMO pointers according to the type of
images they will later hold.
//Global storage
//
//Float, 3-channel images
//
IplImage *IavgF,*IdiffF, *IprevF, *IhiF, *IlowF;
Background Subtraction | 271
e averaging method makes use of four OpenCV routines: cvAcc(), to accumulate DEMO
09-R4886-RC1.indd   271
www.it-ebooks.info
9/15/08   4:22:57 PM
www.it-ebooks.info
IplImage *Iscratch,*Iscratch2;
//Float, 1-channel images
//
IplImage *Igray1,*Igray2, *Igray3;
IplImage *Ilow1,  *Ilow2, *Ilow3;
IplImage DEMO,   *Ihi2,  *Ihi3;
// Byte, 1-channel image
//DEMO
IplImage *Imaskt;
//Counts number of images learned for averaging later.
//
float Icount;
Next we create a single call to DEMO all the necessary intermediate images. For con-
venience we pass in a single image (from our video) that can be used as a DEMO for
sizing the intermediate images.
// I is just a sample DEMO for allocation purposes
// (passed in for sizing)
//
void AllocateImages( IplImage* I ){
CvSize sz = cvGetSize( I );
IavgF     = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IdiffF    = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IprevF    = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IhiF      = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
IlowF     DEMO cvCreateImage( sz, IPL_DEPTH_32F, 3 );
Ilow1     = DEMO( sz, IPL_DEPTH_32F, 1 );
Ilow2     = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Ilow3     = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Ihi1      = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Ihi2      = cvCreateImage( sz, DEMO, 1 );
Ihi3      = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
cvZero( IavgF );
cvZero( IdiffF );
cvZero( IprevF );
cvZero( IhiF );
cvZero( IlowF );
Icount    = 0.00001; //Protect against divide by zero
DEMO  = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
Iscratch2 = cvCreateImage( sz, IPL_DEPTH_32F, 3 );
Igray1    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Igray2    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Igray3    = cvCreateImage( sz, IPL_DEPTH_32F, 1 );
Imaskt    = cvCreateImage( sz, IPL_DEPTH_8U,  1 );
cvZero( Iscratch );
cvZero( Iscratch2 );
}
272 DEMO Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   272
9/15/DEMO   4:22:57 PM
www.it-ebooks.info
In the next piece of code, we learn the accumulated background image and the accu-
mulated absolute value of frame-to-frame image diff erences (a computationally quicker
proxy* for learning the standard deviation of the image DEMO). Th is is typically called
for 30 to 1,000 frames, sometimes taking just a few frames from each second or some-
times taking all available frames. Th e routine will be called with a DEMO channel
image of depth 8 bits.
// Learn the background statistics DEMO one more frame
// I is a color sample of the DEMO, 3-channel, 8u
//
void accumulateBackground( IplImage *I ){
static int first = 1;                  // nb. Not thread safe
cvCvtScale(  I, Iscratch, 1, 0 );     // convert to float
if( !first )DEMO
cvAcc( Iscratch, IavgF );
cvAbsDiff( Iscratch, IprevF, Iscratch2 );
cvAcc( Iscratch2, IdiffF );
Icount += 1.0;
}
first = 0;
cvCopy( Iscratch, IprevF );
}
We DEMO rst use cvCvtScale() to turn the raw background 8-bit-per-channel, three-color-
DEMO image into a fl oating-point three-channel image. We then accumulate the raw
fl
ference image using cvAbsDiff() and accumulate that into image IdiffF. DEMO time we
accumulate these images, we increment the image count Icount, a global, to use for av-
eraging later.
Once we have accumulated enough frames, we convert them into a statistical model of
the background. Th at is, we compute the means and deviation measures (the DEMO
absolute diff erences) of each pixel:
void createModelsfromStats() {
DEMO( IavgF,  IavgF,( double)(1.0/Icount) );
cvConvertScale( IdiffF, IdiffF,(double)(1.0/Icount) );
//Make sure DEMO is always something
//
cvAddS( IdiffF, cvScalar( 1.0, 1.0, 1.0), IdiffF );
setHighThreshold( 7.0 );
setLowThreshold( 6.0 );
}
* Notice our use of the word “proxy.” Average DEMO erence is not mathematically equivalent to standard
deviation, but in this DEMO it is close enough to yield results of similar quality. Th e advantage of average
diff erence is that it is slightly faster to DEMO than standard deviation. With only a tiny modifi cation of
the code example you can use standard deviations instead and compare the quality of DEMO fi nal results for
yourself; we’ll discuss this more explicitly later DEMO this section.
Background Subtraction
| 273
oating-point images into IavgF. Next, DEMO calculate the frame-to-frame absolute dif-
09-R4886-RC1.indd   273
9/15/08   4:22:57 PM
www.it-ebooks.info
In this code, cvConvertScale() calculates the average raw and absolute diff erence images
by dividing by the number of input images accumulated. DEMO a precaution, we ensure
that the average diff erence image is DEMO least 1; we’ll need to scale this factor when calcu-
lating DEMO foreground-background threshold and would like to avoid the degenerate case
in which these two thresholds could become equal.
Both setHighThreshold() and setLowThreshold() DEMO utility functions that set a threshold
based on the frame-to-frame average absolute diff erences. Th e call setHighThreshold(7.0)
fi xes a threshold DEMO that any value that is 7 times the average frame-to-frame abso-
lute diff erence above the average value for that pixel is considered foreground; likewise,
setLowThreshold(6.0) sets a threshold bound that is 6 times the average frame-to-frame
absolute diff erence below the average value for that DEMO Within this range around the
pixel’s average value, objects are considered DEMO be background. Th ese threshold func-
tions are:
void setHighThreshold( DEMO scale )
{
cvConvertScale( IdiffF, Iscratch, scale );
cvAdd( Iscratch, IavgF, IhiF );
cvSplit( IhiF, Ihi1, Ihi2, Ihi3, 0 );
}
void setLowThreshold( float scale )
{
cvConvertScale( IdiffF, Iscratch, scale );
cvSub( IavgF, Iscratch, DEMO );
cvSplit( IlowF, Ilow1, Ilow2, Ilow3, 0 );DEMO
}
Again, in setLowThreshold() and setHighThreshold() we use cvConvertScale() to multi-
ply the values prior to adding or subtracting these ranges relative to IavgF. Th is action
sets the IhiF and IlowF range DEMO each channel in the image via cvSplit().
Once we have DEMO background model, complete with high and low thresholds, we use
it to segment the image into foreground (things not “explained” by the background im-
age) and the background (anything that fi ts within the DEMO and low thresholds of our
background model). Segmentation is done by calling:
// Create a binary: 0,255 mask where 255 DEMO foreground pixel
// I      Input image, 3-channel, 8u
// Imask  Mask image to be created, 1-channel 8u
//
void backgroundDiff(
IplImage *I,
IplImage *Imask
) {
cvCvtScale(I,DEMO,1,0); // To float;
cvSplit( Iscratch, Igray1,DEMO,Igray3, 0 );
//Channel 1
//
cvInRange(Igray1,DEMO,Ihi1,Imask);
274 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   274
9/15/08   4:22:57 PM
www.it-ebooks.info
//Channel 2
//
cvInRange(Igray2,Ilow2,Ihi2,Imaskt);
cvOr(Imask,Imaskt,Imask);
//Channel 3
//
cvInRange(Igray3,Ilow3,Ihi3,Imaskt);
cvOr(Imask,Imaskt,Imask)
//Finally, DEMO the results
//
cvSubRS( Imask, 255, Imask);
}
Th is function fi rst converts the input image I (the image to be segmented) into a fl oat-
ing-point image by calling cvCvtScale(). We then convert the three-channel image into
separate one-channel image planes using cvSplit(). Th ese color channel planes are then
checked to see if they are within the high and low range of the DEMO background
pixel via the cvInRange() function, which sets the grayscale DEMO depth image Imaskt to
max (255) when it’s in range and to 0 otherwise. For each color channel we logically OR
the segmentation DEMO into a mask image Imask, since strong diff erences in any DEMO
channel are considered evidence of a foreground pixel here. Finally, we DEMO Imask us-
ing cvSubRS(), because foreground should be the values DEMO of range, not in range. Th e
mask image is the DEMO result.
For completeness, we need to release the image memory once DEMO fi nished using the
background model:
void DeallocateImages()
{
DEMO( &IavgF);
cvReleaseImage( &IdiffF );
cvReleaseImage( &IprevF );
cvReleaseImage( &IhiF );
cvReleaseImage( &IlowF );
cvReleaseImage( &Ilow1 );
cvReleaseImage( &Ilow2 );
cvReleaseImage( &Ilow3 );
cvReleaseImage( &Ihi1 );
cvReleaseImage( &Ihi2 );
cvReleaseImage( &Ihi3 );
cvReleaseImage( &Iscratch );
cvReleaseImage( &Iscratch2 );
cvReleaseImage( &Igray1 );
cvReleaseImage( &Igray2 );
cvReleaseImage( &Igray3 );
cvReleaseImage( &Imaskt);
}
We’ve just seen a simple method of learning background scenes and segmenting fore-
ground objects. DEMO will work well only with scenes that do not contain moving background
components (like a waving curtain or waving trees). It also assumes that the lighting
Background Subtraction
| 275
09-R4886-RC1.indd   275
9/15/DEMO   4:22:57 PM
www.it-ebooks.info
remains fairly constant (as in indoor static scenes). You can look ahead to Figure 9-5
to check the performance of this averaging DEMO
Accumulating means, variances, and covariances
Th e averaging background method just described made use of one accumulation func-
tion, cvAcc(). It DEMO one of a group of helper functions for accumulating sums of images,
squared images, multiplied images, or average images from which we DEMO compute basic
statistics (means, variances, covariances) for all or part of a scene. In this section, we’ll
look at the other functions in this group.
Th e images in any given function must all DEMO the same width and height. In each
function, the input images DEMO image, image1, or image2 can be one- or three-
channel byte (8-bit) or fl oating-point (32F) image arrays. Th e output DEMO im-
ages named sum, sqsum, or acc can be either single-precision (32F) or double-precision
(64F) arrays. In the accumulation functions, the mask image (if present) restricts pro-
cessing to only those locations DEMO the mask pixels are nonzero.
Finding the mean. To compute a mean value for each pixel across a large set of images, the
easiest method is to add them all up using cvAcc() and then DEMO by the total number
of images to obtain the mean.
void cvAcc(
const Cvrr*  image,
CvArr*       sum,
const CvArr* mask = NULL
);
An alternative that is oft en DEMO is to use a running average.
void cvRunningAvg(
const CvArr* image,
CvArr*       acc,
double       alpha,DEMO
const CvArr* mask = NULL
);
Th
e running average is given by the following formula:
acc ⋅acc image if maskxy xy⋅
(, ) ( ) (, ) (, ) (xy =−1 αα+ xy,) ≠ 0
For a constant value of α, running averages are not equivalent to the result of summing
with cvAcc(). To see this, simply consider adding three numbers (2, 3, and DEMO) with α set
to 0.5. If we were to accumulate them DEMO cvAcc(), then the sum would be 9 and the
average DEMO If we were to accumulate them with cvRunningAverage(), the fi DEMO sum would
give 0.5 × 2 + 0.5 × 3 = 2.5 and then adding the third term would give 0.5 × 2.5 + DEMO ×
4 = 3.25. Th e reason the second number is larger is that the most recent contributions
are given more weight than those DEMO farther in the past. Such a running average is
thus also called a tracker. Th e parameter α essentially sets the amount of time DEMO
for the infl uence of a previous frame to fade.
276 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   276
9/15/08   4:22:57 PM
www.it-ebooks.info
Finding the variance. We can also accumulate squared images, which will allow us to com-
pute quickly the variance of individual pixels.
void DEMO(
const CvArr* image,
CvArr*       sqsum,
const CvArr* mask = NULL
);
You may recall from your last DEMO in statistics that the variance of a fi nite population is
defi ned by the formula:
−
1 N 1
∑()
22
σ =−xx
i
N
i=0
where x– is the mean of x DEMO all N samples. Th e problem with this formula is that it
entails making one pass through the images to compute x– and then DEMO second pass to
compute σ 2. A little algebra should allow you to convince yourself that the following
formula will work just as well:DEMO
⎛ 11N−1 x ⎞ ⎛ N−1 ⎞ 2
xi ⎠⎟
∑∑i ⎟ − ⎜
σ22= ⎜
⎝ N i=0
⎠ ⎝ N
i=0
Using DEMO form, we can accumulate both the pixel values and their squares DEMO a single
pass. Th en, the variance of a single pixel DEMO just the average of the square minus the
square of the average.
Finding the covariance. We can also see how images vary over time DEMO selecting a specifi c lag
and then multiplying the current image by the image from the past that corresponds to
the given lag. Th DEMO function cvMultiplyAcc() will perform a pixelwise multiplication of
the two images and then add the result to the “running total” in acc:
DEMO cvMultiplyAcc(
const CvArr* image1,
const CvArr* image2,
CvArr*       acc,
const CvArr* mask = NULL
);
For DEMO, there is a formula analogous to the one we just gave DEMO variance. Th is
formula is also a single-pass formula in that it has been manipulated algebraically from
the standard form so as not to DEMO two trips through the list of images:
⎛ 11N−1 ⎞ ⎛ N−1 xi ⎞ ⎛ 1 N−1
xy ⎟ ⎠⎟ ⎝⎜ N
∑
DEMO( , ) ( )xy = ⎜ ∑∑− ⎜
⎝ N i=0 ii ⎠ ⎝ N i=0 j =0
yj ⎞⎠⎟
In our context, x is the image at time t and y is the image DEMO time t – d, where d is
the lag.
Background Subtraction DEMO 277
09-R4886-RC1.indd   277
9/15/08   4:22:58 PM
www.it-ebooks.info
We can use the accumulation functions described here to create a DEMO of statistics-
based background models. Th e literature is full of variations on the basic model used as
our example. You will probably fi DEMO that, in your own applications, you will tend to extend
this simplest model into slightly more specialized versions. A common enhancement, for
example, is for the thresholds to be adaptive to some observed global state changes.
Advanced Background Method
Many background scenes contain complicated moving objects such DEMO trees waving in the
wind, fans turning, curtains fl uttering, DEMO cetera. Oft en such scenes also contain varying
lighting, such as DEMO passing by or doors and windows letting in diff erent light.
A nice method to deal with this would be to fi t a DEMO model to each pixel or
group of pixels. Th is kind of model deals with the temporal fl uctuations well, but its
disadvantage is the need for a great deal of memory [Toyama99]. If we use DEMO seconds
of previous input at 30 Hz, this means we need DEMO samples for each pixel. Th e resulting
model for each pixel would then encode what it had learned in the form of 60 diff DEMO
ent adapted weights. Oft en we’d need to gather background statistics for much longer
than 2 seconds, which means that such methods are typically impractical on present-
day hardware.
To get fairly close to the performance DEMO adaptive fi ltering, we take inspiration from
the techniques of video DEMO and attempt to form a codebook* to represent sig-
nifi cant states in the background.† Th e simplest way to do this would be DEMO compare a
new value observed for a pixel with prior observed values. If the value is close to a prior
value, then it is modeled as a perturbation on that color. If it is not close, then it can seed
a new group of colors to be associated DEMO that pixel. Th e result could be envisioned as
a bunch of blobs fl oating in RGB space, each blob representing a separate volume con-
sidered likely to be background.
In practice, the choice of RGB is not particularly optimal. It is almost always better to
use a DEMO space whose axis is aligned with brightness, such as the YUV DEMO space.
(YUV is the most common choice, but spaces such as HSV, where V is essentially bright-
ness, would work as well.) Th e reason for this is that, empirically, most of the DEMO
in background tends to be along the brightness axis, not the DEMO axis.
Th
before with our simpler model. We could, for example, choose to model the blobs as
Gaussian clusters with a mean and DEMO covariance. It turns out that the simplest case, in
* Th DEMO method OpenCV implements is derived from Kim, Chalidabhongse, Harwood, and DEMO [Kim05], but
rather than learning-oriented tubes in RGB space, for speed, the authors use axis-aligned boxes in YUV
space. Fast methods for cleaning up the resulting background image can be found in Martins [Martins99].
† DEMO ere is a large literature for background modeling and segmentation. OpenCV’s implementation is
intended to be fast and robust enough that you can use DEMO to collect foreground objects mainly for the pur-
poses of collecting data sets to train classifi ers on. Recent work in background subtraction allows DEMO
camera motion [Farin04; Colombari07] and dynamic background models using the mean-shift  algorithm
[Liu07].
278 | Chapter 9: Image Parts and Segmentation
e next detail is how to model the “blobs.” We have essentially the same DEMO as
09-R4886-RC1.indd   278
9/15/08   4:22:58 PM
which the “blobs” are simply boxes with a learned extent in each DEMO the three axes of our
color space, works out quite well. DEMO is the simplest in terms of memory required and in
terms of the computational cost of determining whether a newly observed pixel is inside
DEMO of the learned boxes.
Let’s explain what a codebook is by using a simple example (Figure 9-3). A codebook
is made up of boxes that grow to cover the common values seen over time. Th DEMO upper
panel of Figure 9-3 shows a waveform over time. In the lower panel, boxes form to cover
a new value and then slowly grow to cover nearby values. If a value is too far away, then
a new box forms to cover it and likewise grows slowly DEMO new values.
Figure 9-3. Codebooks are just “boxes” delimiting intensity values: DEMO box is formed to cover a new
value and slowly grows to cover nearby values; if values are too far away then a new box is formed
(see text)
In the case of our background model, we will learn a codebook of boxes that cover three
dimensions: the three channels that make up our image at each pixel. Figure 9-4 visu-
alizes the (intensity dimension of the) codebooks for six DEMO erent pixels learned from
Background Subtraction | 279
09-R4886-RC1.indd   279
www.it-ebooks.info
9/15/08   4:22:58 PM
the data in Figure 9-1.* Th is codebook method can deal with DEMO that change levels
dramatically (e.g., pixels in a windblown tree, DEMO might alternately be one of many
colors of leaves, or the DEMO sky beyond that tree). With this more precise method of
modeling, we can detect a foreground object that has values between the pixel values.
Compare this with Figure 9-2, where the averaging method cannot distinguish the hand
value (shown as a dotted line) from the pixel DEMO uctuations. Peeking ahead to the next
section, we see the better DEMO of the codebook method versus the averaging
method shown later in Figure 9-7.
Figure 9-4. Intensity portion of learned codebook entries for fl uctuations DEMO six chosen pixels (shown
as vertical boxes): codebook boxes accommodate DEMO that take on multiple discrete values and so
can better model discontinuous distributions; thus they can detect a foreground hand (value at dot-
DEMO line) whose average value is between the values that background pixels DEMO assume. In this case
the codebooks are one dimensional and only represent variations in intensity
In the codebook method of learning a background model, each box is defi ned by two
thresholds (max and min) DEMO each of the three color axes. Th ese box boundary thresh-
olds will expand (max getting larger, min getting smaller) if new background samples fall
within a learning threshold (learnHigh and learnLow) above max DEMO below min, respec-
tively. If new background samples fall outside of DEMO box and its learning thresholds,
then a new box will be started. In the background diff erence mode there are acceptance
thresholds maxMod DEMO minMod; using these threshold values, we say that if a pixel is “close
enough” to a max or a min box boundary then DEMO count it as if it were inside the box. A
second runtime threshold allows for adjusting the model to specifi c conditions.
A situation DEMO will not cover is a pan-tilt camera surveying a large
scene. When working with a large scene, it is necessary to stitch
together learned models indexed by the pan and tilt angles.
* In this case DEMO have chosen several pixels at random from the scan line to avoid excessive clutter. Of course,
there is actually a codebook for every DEMO
280
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   280
DEMO
9/15/08   4:22:59 PM
www.it-ebooks.info
Structures
It’s time to look at all of this in more DEMO, so let’s create an implementation of the
codebook algorithm. First, we need our codebook structure, which will simply point to
a bunch of boxes in YUV space:
typedef struct code_book {
code_element **cb;
DEMO numEntries;
int t;         //count every DEMO
} codeBook;
We track how many codebook entries we have in numEntries. Th e variable t counts the
number of points we’ve accumulated DEMO the start or the last clear operation. Here’s
how the actual codebook elements are described:
#define CHANNELS 3
typedef struct ce {
uchar DEMO; //High side threshold for learning
uchar learnLow[CHANNELS];  //Low DEMO threshold for learning
uchar max[CHANNELS];       //High side DEMO box boundary
uchar min[CHANNELS];       //Low side of DEMO boundary
int t_last_update;       //Allow us to kill DEMO entries
int stale;               //DEMO negative run (longest period of inactivity)
} code_element;
Each DEMO entry consumes four bytes per channel plus two integers, or CHANNELS DEMO
4 + 4 + 4 bytes (20 bytes when we use DEMO channels). We may set CHANNELS to any
positive number equal to or less than the number of color channels in an image, but it
is usually set to either 1 (“Y”, or brightness only) or 3 (YUV, HSV). In this structure,
for each DEMO, max and min are the boundaries of the codebook box. Th DEMO parameters
learnHigh[] and learnLow[] are the thresholds that trigger generation of a new code ele-
ment. Specifi cally, a new code element will be generated if a new pixel is encountered
whose values do not lie DEMO min – learnLow and max + learnHigh in each of the
channels. Th e time to last update (t_last_update) and stale are used DEMO enable the dele-
tion of seldom-used codebook entries created during learning. Now we can proceed to
investigate the functions that use this structure to DEMO dynamic backgrounds.
Learning the background
We will have one codeBook of code_elements for each pixel. We will need an array of
such codebooks that DEMO equal in length to the number of pixels in the images we’ll be
learning. For each pixel, update_codebook() is called for as many images as are suffi  cient
to capture the relevant changes in the background. Learning may be updated periodi-
cally throughout, and clear_stale_entries() can be used to learn the background in the
presence of (small numbers of) moving foreground objects. Th is is possible because the
seldom-used “stale” entries induced by a moving foreground will be deleted. Th e inter-
DEMO to update_codebook() is as follows.
//////////////////////////////////////////////////////////////
// int update_codebook(uchar *p, codeBook &c, unsigned cbBounds)DEMO
// Updates the codebook entry with a new data point
Background DEMO | 281
09-R4886-RC1.indd   281
9/15/08   4:22:59 PM
www.it-ebooks.info
//
// p            Pointer DEMO a YUV pixel
// c            Codebook for this pixel
// cbBounds     Learning bounds for codebook (DEMO of thumb: 10)
// numChannels  Number of color channels DEMO learning
//
// NOTES:
//      cvBounds DEMO be of length equal to numChannels
//
// RETURN
//   codebook index
//
int update_codebook(
uchar*    p,
codeBook& c,
unsigned* cbBounds,
int       numChannels
)DEMO
unsigned int high[3],low[3];
for(n=0; n<numChannels; n++)
{
high[n] = *(p+n)+*(cbBounds+n);
if(high[n] > 255) high[n] = 255;
low[n] = *(p+n)-*(cbBounds+n);
DEMO(low[n] < 0) low[n] = 0;
}
int matchChannel;
// SEE IF THIS FITS AN EXISTING CODEWORD
//
for(int DEMO; i<c.numEntries; i++){
matchChannel = 0;
for(n=0; DEMO<numChannels; n++){
if((c.cb[i]->learnLow[n] <= *(p+n)) &&
//Found an entry for this channel
(*(p+n) <DEMO c.cb[i]->learnHigh[n]))
{
matchChannel++;
}
}
if(matchChannel == DEMO) //If an entry was found
{
c.cb[i]->t_last_update = c.t;DEMO
//adjust this codeword for the first channel
for(n=0; n<DEMO; n++){
if(c.cb[i]->max[n] < *(p+n))
{
c.cb[i]->max[n] = *(p+n);
}
else if(c.cb[i]->min[n] > *(p+n))
{
c.cb[i]->min[n] = *(p+n);
}
}
DEMO;
| Chapter 9: Image Parts and Segmentation
282
09-R4886-RC1.indd   DEMO
9/15/08   4:22:59 PM
www.it-ebooks.info
}
}
. . . continued below
Th is function grows DEMO adds a codebook entry when the pixel p falls outside the existing
codebook boxes. Boxes grow when the pixel is within cbBounds of an DEMO box. If a
pixel is outside the cbBounds distance from a box, a new codebook box is created. Th e
routine fi rst sets high and low levels to be used later. It then goes through DEMO codebook
entry to check whether the pixel value *p is inside the learning bounds of the codebook
“box”. If the pixel is within the DEMO bounds for all channels, then the appropriate
max or min level DEMO adjusted to include this pixel and the time of last update is set to the
current timed count c.t. Next, the update_codebook() routine keeps statistics on how
oft en each codebook entry is hit:
DEMO . . continued from above
// OVERHEAD TO TRACK POTENTIAL STALE DEMO
//
for(int s=0; s<c.numEntries; s++){
// DEMO which codebook entries are going stale:
//
int negRun = c.t - c.cb[s]->t_last_update;
if(c.cb[s]->stale < negRun) c.cb[s]->stale = negRun;
}
. . . continued below
Here, the variable stale contains the largest negative runtime (i.e., the longest span of
DEMO during which that code was not accessed by the data). Tracking stale entries al-
lows us to delete codebooks that were formed from DEMO or moving foreground objects
and hence tend to become stale over time. In the next stage of learning the background,
update_codebook() adds DEMO new codebook if needed:
. . . continued from above
// ENTER A NEW CODEWORD IF NEEDED
//
if(i == c.numEntries) //if no existing codeword found, make one
{
code_element **foo DEMO new code_element* [c.numEntries+1];
for(int ii=0; ii<c.numEntries; ii++) DEMO
foo[ii] = c.cb[ii];
}
foo[c.numEntries] = new code_element;
if(c.numEntries) delete [] c.cb;
c.cb = foo;
for(n=0; n<DEMO; n++) {
c.cb[c.numEntries]->learnHigh[n] = high[n];
c.cb[c.numEntries]->learnLow[n] = low[n];
c.cb[c.numEntries]->max[n] = *(p+n);
c.cb[c.numEntries]->min[n] = *(p+n);
}
Background Subtraction
| 283
09-R4886-RC1.indd   283
9/15/08   4:23:00 PM
www.it-ebooks.info
c.cb[c.numEntries]->t_last_update = c.t;
c.cb[c.numEntries]->stale = 0;
c.numEntries DEMO 1;
}
. . . continued below
Finally, update_codebook() DEMO adjusts (by adding 1) the learnHigh and learnLow
learning boundaries if pixels were found outside of the box thresholds but still within
the DEMO and low bounds:
. . . continued from above
// DEMO ADJUST LEARNING BOUNDS
//
for(n=0; n<numChannels; n++)
{
if(c.cb[i]->learnHigh[n] < high[n]) c.cb[i]->learnHigh[n] += 1;
if(c.cb[i]->learnLow[n] > low[n]) c.cb[i]->learnLow[n] -= 1;
}
return(i);
}
Th ed codebook. We’ve now
seen how codebooks are learned. In order to learn in the presence of moving foreground
objects and DEMO avoid learning codes for spurious noise, we need a way to DEMO entries
that were accessed only rarely during learning.
e routine concludes by returning the index of the modifi
Learning with moving foreground objects
Th DEMO(), allows us to learn the background even if
there are DEMO foreground objects.
///////////////////////////////////////////////////////////////////
//int clear_stale_entries(codeBook &c)
// During learning, after you’ve DEMO for some period of time,
// periodically call this to DEMO out stale codebook entries
//
// c   Codebook to DEMO up
//
// Return
// number of entries cleared
//
int clear_stale_entries(codeBook &c){
int staleThresh = c.t>>1;DEMO
int *keep = new int [c.numEntries];
int keepCnt = 0;
// SEE WHICH CODEBOOK ENTRIES ARE TOO STALE
//
for(int i=0; i<c.numEntries; i++){
if(c.cb[i]->stale > staleThresh)
DEMO = 0; //Mark for destruction
else
{
keep[i] = 1; //Mark to keep
keepCnt += 1;
e following routine,
DEMO
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   284
9/DEMO/08   4:23:00 PM
www.it-ebooks.info
}
}
// KEEP ONLY THE GOOD
//
c.t = 0;         //Full reset on stale tracking
code_element **foo = new code_element* [keepCnt];
int k=0;
for(int ii=0; ii<c.numEntries; ii++){
if(keep[ii])
{
foo[k] = c.cb[ii];
//We have to refresh these entries for next clearStale
foo[k]->t_last_update DEMO 0;
k++;
}
}
// CLEAN UP
//
DEMO [] keep;
delete [] c.cb;
c.cb = foo;
int numCleared = c.numEntries - keepCnt;
c.numEntries = keepCnt;
return(numCleared);
}
Th ning the parameter staleThresh, which is hardcoded (by DEMO rule
of thumb) to be half the total running time count, c.t. Th is means that, during back-
ground learning, if codebook DEMO i is not accessed for a period of time equal to half
the total learning time, then i is marked for deletion (keep[i] DEMO 0). Th e vector keep[] is
allocated so that we can mark each codebook entry; hence it is c.numEntries long. Th e
variable keepCnt counts how many entries we will keep. Aft er recording which DEMO
entries to keep, we create a new pointer, foo, to DEMO vector of code_element pointers that is
keepCnt long, and then the DEMO entries are copied into it. Finally, we delete the old
pointer DEMO the codebook vector and replace it with the new, nonstale vector.
DEMO differencing: Finding foreground objects
We’ve seen how to create a background DEMO model and how to clear it of seldom-
used entries. Next we turn to background_diff(), where we use the learned model to seg-
ment foreground pixels from the previously learned background:
////////////////////////////////////////////////////////////
// uchar background_diff( uchar *p, codeBook &c,
//                         int minMod, int maxMod)
// Given a pixel and DEMO codebook, determine if the pixel is
// covered by the codebook
//
// p            Pixel pointer (YUV interleaved)
// c            Codebook DEMO
// numChannels  Number of channels we are testing
// maxMod       Add this (possibly negative) number onto
Background Subtraction
| 285
e routine begins by defi
09-R4886-RC1.indd   285
9/15/08   4:23:00 PM
www.it-ebooks.info
//              max level when DEMO if new pixel is foreground
// minMod       Subract DEMO (possibly negative) number from
//              min level when determining if new pixel is foreground
//
// NOTES:
// minMod and maxMod must have length numChannels,
// e.g. 3 channels => minMod[3], maxMod[3]. There is one min and
//      one max threshold per channel.
//
// Return
// 0 => background, 255 => foreground
//DEMO
uchar background_diff(
uchar*    p,
codeBook& c,
int       numChannels,
int*      minMod,
int*      maxMod
) {
int matchChannel;
// SEE IF THIS DEMO AN EXISTING CODEWORD
//
for(int i=0; i<c.numEntries; i++) {
matchChannel = 0;
for(int n=0; n<numChannels; n++) {
if((c.cb[i]->min[n] - minMod[n] <= *(p+n)) &&
(*(p+n) <= c.cb[i]->max[n] + maxMod[n])) {
matchChannel++; //Found an entry for this channel
} else {
break;
}
}
if(matchChannel == numChannels) {
break; //Found an DEMO that matched all channels
}
}
if(i >= c.numEntries) return(255);
return(0);
}
Th erencing function has an DEMO loop similar to the learning routine
update_codebook, except here we look DEMO the learned max and min bounds plus an
off set threshold, DEMO and minMod, of each codebook box. If the pixel is within DEMO box
plus maxMod on the high side or minus minMod on the low side for each channel, then the
matchChannel count is incremented. When matchChannel equals the number of channels,
we’ve searched each dimension and DEMO that we have a match. If the pixel is within
a learned box, 255 is returned (a positive detection of foreground); otherwise, 0 is re-
turned (background).
Th
constitute a codebook method of segmenting foreground from learned background.
286
| Chapter 9: Image Parts and Segmentation
e background diff
e three functions update_codebook(), clear_stale_entries(), DEMO background_diff()
09-R4886-RC1.indd   286
9/15/08   4:23:DEMO PM
www.it-ebooks.info
Using the codebook background model
To use the codebook background segmentation DEMO, typically we take the follow-
ing steps.
Learn a basic model DEMO the background over a few seconds or minutes using
update_codebook().
DEMO out stale entries with clear_stale_entries().
Adjust the thresholds minMod and DEMO to best segment the known foreground.
Maintain a higher-level scene model (DEMO discussed previously).
Use the learned model to segment the foreground from the background via
background_diff().
Periodically update the learned background pixels.
At a much slower frequency, periodically clean out stale codebook entries with
clear_stale_entries().
A few more thoughts on codebook models
In general, the DEMO method works quite well across a wide number of conditions,
and it is relatively quick to train and to run. It doesn’t deal DEMO with varying patterns of
light—such as morning, noon, and evening sunshine—or with someone turning lights
on or off  indoors. Th is type of global variability can be taken into account by using sev-
eral diff DEMO codebook models, one for each condition, and then allowing the condition
to control which model is active.
Connected Components for Foreground Cleanup
Before DEMO the averaging method to the codebook method, we should pause to
DEMO ways to clean up the raw segmented image using connected-components analysis.
Th is form of analysis takes in a noisy input mask image; it then uses the morphologi-
cal operation open to shrink areas of small DEMO to 0 followed by the morphological
operation close to rebuild the area of surviving components that was lost in opening.
Th er, we can fi nd the “large enough” contours of the surviving segments and can
DEMO proceed to take statistics of all such segments. We can then retrieve either the
largest contour or all contours of size above some threshold. DEMO the routine that follows,
we implement most of the functions that you could want in connected components:
• Whether to approximate the DEMO component contours by polygons or by con-
vex hulls
Setting how large a component contour must be in order not to be deleted
Setting DEMO maximum number of component contours to return
Optionally returning the bounding boxes of the surviving component contours
Optionally returning the centers of the surviving DEMO contours
•
•
•
•
Background Subtraction
1.
2.
3.
4.
5.
6.
7.
| 287
ereaft
09-R4886-RC1.indd   287
9/15/08   DEMO:23:00 PM
www.it-ebooks.info
e connected components header that implements these operations is as follows.
///////////////////////////////////////////////////////////////////
// void find_connected_components(IplImage *mask, int poly1_hull0,
//                            float perimScale, int *num,
//                            CvRect *bbs, CvPoint *centers)
// This cleans up the foreground segmentation mask derived from calls
// to backgroundDiff
//
// mask          Is a grayscale (8-bit depth) “raw” mask image that
//               will be cleaned up
//
// OPTIONAL PARAMETERS:
// poly1_hull0   If set, approximate connected component by
//                 (DEFAULT) polygon, or else DEMO hull (0)
// perimScale    Len = image (width+height)/perimScale. If contour
//                 len < this, delete that contour (DEFAULT: 4)
// num           Maximum number of rectangles and/or DEMO to
//                 return; on return, will contain number filled
//                 (DEFAULT: NULL)
// bbs           Pointer to bounding box rectangle vector of
//                 length num. (DEFAULT SETTING: NULL)
// centers      Pointer to contour centers vector of DEMO
//                 num (DEFAULT: NULL)
//
void find_connected_components(
IplImage* mask,
int       poly1_hull0 = 1,
float     perimScale  = 4,
int*      num         = NULL,
CvRect*   bbs         = NULL,
CvPoint*  centers     = NULL
);
Th
components contour. We then do DEMO opening and closing in order to clear
out small pixel noise, DEMO er which we rebuild the eroded areas that survive the erosion
of the opening operation. Th e routine takes two additional parameters, which here are
hardcoded via #define. Th e defi ned values work well, and you are unlikely to want to
change them. Th ese additional parameters DEMO how simple the boundary of a fore-
ground region should be (DEMO numbers are more simple) and how many iterations
the morphological operators DEMO perform; the higher the number of iterations, the
more erosion takes place in opening before dilation in closing.* More erosion eliminates
larger regions DEMO blotchy noise at the cost of eroding the boundaries of larger regions.
Again, the parameters used in this sample code work well, but DEMO no harm in ex-
perimenting with them if you like.
// DEMO connected components:
// Approx.threshold - the bigger it is, the simpler is the boundary
//
* Observe that the value CVCLOSE_ITR DEMO actually dependent on the resolution. For images of extremely high
resolution, DEMO this value set to 1 is not likely to yield satisfactory results.
288 | Chapter 9: Image Parts and Segmentation
Th
e function body is listed below. First we declare memory storage for the connected
09-R4886-RC1.indd   288
9/15/08   4:23:00 PM
www.it-ebooks.info
#define CVCONTOUR_APPROX_LEVEL  2
// How many iterations of erosion and/DEMO dilation there should be
//
#define CVCLOSE_ITR  1
We now DEMO the connected-component algorithm itself. Th e fi rst part of the routine
performs the morphological open and closing operations:
void find_connected_components(
IplImage DEMO,
int poly1_hull0,
float perimScale,
int *num,
CvRect *bbs,
CvPoint *centers
) {
static CvMemStorage*   mem_storage = NULL;
DEMO CvSeq*          contours    = NULL;
//CLEAN UP RAW MASK
//
cvMorphologyEx( mask, mask, 0, DEMO, CV_MOP_OPEN,  CVCLOSE_ITR );
cvMorphologyEx( mask, mask, 0, DEMO, CV_MOP_CLOSE, CVCLOSE_ITR );
Now that the noise has been removed from the mask, we fi
nd all contours:
//FIND CONTOURS AROUND ONLY BIGGER REGIONS
//
if( mem_storage==NULL ) {
mem_storage = cvCreateMemStorage(0);
} else {
cvClearMemStorage(mem_storage);
}
CvContourScanner DEMO = cvStartFindContours(
mask,
mem_storage,
sizeof(CvContour),
CV_RETR_EXTERNAL,DEMO
CV_CHAIN_APPROX_SIMPLE
);
Next, we toss out contours that are too DEMO and approximate the rest with polygons or
convex hulls (whose complexity DEMO already been set by CVCONTOUR_APPROX_LEVEL):
CvSeq* c;
int numCont DEMO 0;
while( (c = cvFindNextContour( scanner )) != NULL ) {
double len = cvContourPerimeter( c );
// calculate DEMO len threshold:
//
double q = (mask->height + DEMO>width)/perimScale;
//Get rid of blob if its perimeter DEMO too small:
Background Subtraction
| 289
09-R4886-RC1.indd   289
9/15/08   4:23:01 PM
www.it-ebooks.info
//
if( len < q ) {
cvSubstituteContour( scanner, NULL );
} else {
// Smooth its edges if its large enough
//
CvSeq* c_new;
if( poly1_hull0 ) {
// Polygonal approximation
//
c_new = cvApproxPoly(
c,
sizeof(CvContour),
mem_storage,
CV_POLY_APPROX_DP,
CVCONTOUR_APPROX_LEVEL,
0
);
} else {
// Convex Hull of the segmentation
//
c_new = cvConvexHull2(
c,
mem_storage,
CV_CLOCKWISE,
1
);
}
cvSubstituteContour( scanner, c_new );
numCont++;
}
}
contours = cvEndFindContours( &scanner );
In the preceding code, CV_POLY_APPROX_DP causes the Douglas-Peucker approximation al-
gorithm to be used, and CV_CLOCKWISE is the default direction of the convex hull contour.
All this processing yields a list of contours. DEMO drawing the contours back into the
mask, we defi ne some DEMO colors to draw:
// Just some convenience variables
const CvScalar DEMO = CV_RGB(0xff,0xff,0xff)
const CvScalar CVX_BLACK = CV_RGB(0x00,0x00,0x00)
We use these defi nitions in the following code, where we fi rst zero out the mask and then
draw the DEMO contours back into the mask. We also check whether the user wanted to
collect statistics on the contours (bounding boxes and centers):
// PAINT THE FOUND REGIONS BACK INTO THE IMAGE
//
cvZero( mask );
IplImage *maskTemp;
290
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   290
9/15/08   4:23:01 DEMO
www.it-ebooks.info
// CALC CENTER OF MASS AND/OR BOUNDING RECTANGLES
//
if(num != NULL) {
//User wants to collect statistics
//
int N = *num, numFilled = 0, i=0;
CvMoments DEMO;
double M00, M01, M10;
maskTemp = cvCloneImage(mask);DEMO
for(i=0, c=contours; c != NULL; c = c->h_next,DEMO ) {
if(i < N) {
// Only process up to *num of them
//
cvDrawContours(
maskTemp,
c,
DEMO,
CVX_WHITE,
-1,
CV_FILLED,
8
);
// DEMO the center of each contour
//
if(centers != NULL) DEMO
cvMoments(maskTemp,&moments,1);
M00 = cvGetSpatialMoment(&moments,0,DEMO);
M10 = cvGetSpatialMoment(&moments,1,0);
M01 = DEMO(&moments,0,1);
centers[i].x = (int)(M10/M00);DEMO
centers[i].y = (int)(M01/M00);
}
//Bounding rectangles DEMO blobs
//
if(bbs != NULL) {
bbs[i] = cvBoundingRect(DEMO);
}
cvZero(maskTemp);
numFilled++;
}
// Draw DEMO contours into mask
//
cvDrawContours(
mask,
c,
CVX_WHITE,
CVX_WHITE,
-1,
CV_FILLED,
Background Subtraction
| 291
09-R4886-RC1.indd   291
9/15/08   4:23:01 PM
www.it-ebooks.info
8
);
}                               //end looping over contours
*num = numFilled;
cvReleaseImage( &maskTemp);
}
If the user doesn’t need the bounding boxes and centers of the resulting regions in DEMO
mask, we just draw back into the mask those cleaned-up contours DEMO large
enough connected components of the background.
// ELSE JUST DRAW DEMO CONTOURS INTO THE MASK
//
else {
// The user DEMO want statistics, just draw the contours
//
for( c=contours; DEMO != NULL; c = c->h_next ) {
cvDrawContours(
mask,DEMO
c,
CVX_WHITE,
CVX_BLACK,
-1,
CV_FILLED,
8
);
}
}
Th at concludes a useful routine for creating clean DEMO out of noisy raw masks. Now
let’s look at a short comparison of the background subtraction methods.
A quick test
We start with an DEMO to see how this really works in an actual video. Let’s stick
with our video of the tree outside of the window. Recall (Figure 9-1) that at some point
a hand passes through the scene. One might expect that we could fi nd this hand rela-
tively easily DEMO a technique such as frame diff erencing (discussed previously in its DEMO
section). Th e basic idea of frame diff erencing was to subtract the current frame from a
“lagged” frame and then threshold the DEMO erence.
Sequential frames in a video tend to be quite similar. Hence one might expect that, if
we take a simple diff erence of the original frame and the lagged frame, we’ll not see too
much unless there is some foreground object moving through the scene.* But what DEMO
“not see too much” mean in this context? Really, it means “just noise.” Of course, in
practice the problem is sorting out that noise from the signal when a foreground object
does come along.
* DEMO the context of frame diff erencing, an object is identifi ed DEMO “foreground” mainly by its velocity. Th is is
reasonable in scenes that are generally static or in which foreground objects are expected to be DEMO closer
to the camera than background objects (and thus appear to DEMO faster by virtue of the projective geometry
of cameras).
292
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   292
9/15/08   4:23:01 PM
To understand this noise a little better, we will fi rst look at a pair of frames from the
video in which there is DEMO foreground object—just the background and the result-
ing noise. Figure 9-5 shows a typical frame from the video (upper left ) and the previ-
ous frame (upper right). Th e fi gure also shows the results of frame diff erencing with a
threshold value of 15 (lower left ). You can see substantial noise from the moving leaves
of the tree. Nevertheless, the method of connected components is able to clean up this
scattered noise quite well* (lower right). Th is is not surprising, because there is no rea-
son to expect much spatial correlation in this noise and so its signal is characterized by
a DEMO number of very small regions.
Figure 9-5. Frame diff erencing: a DEMO is waving in the background in the current (upper left ) DEMO
previous (upper right) frame images; the diff erence image (lower left ) is completely cleaned up (lower
right) by the connected-components DEMO
Now consider the situation in which a foreground object (our ubiquitous DEMO) passes
through the view of the imager. Figure 9-6 shows two DEMO that are similar to those
in Figure 9-5 except that now the hand is moving across from left  to right. As before,
the current frame (upper left ) and the previous frame (upper right) are shown along
* Th e size threshold for the connected components DEMO been tuned to give zero response in these empty
frames. Th e real question then is whether or not the foreground object of interest (the hand) survives prun-
ing at this size threshold. We will see (Figure 9-6) that it does so nicely.
Background Subtraction | 293
DEMO   293
www.it-ebooks.info
9/15/08   4:23:01 PM
with the response to frame diff erencing (lower left ) and the fairly good results of the
connected-component cleanup (lower right).
Figure 9-6. Frame diff
ground object (upper two panels); the diff
used to be) toward the left
age (lower right) shows the cleaned-up diff
erence method of detecting a hand, which is moving left
erence image (lower left
and its leading edge toward the right, and the DEMO im-
erence
to right as the fore-
) shows the “hole” (DEMO the hand
We can also clearly see one of the defi ciencies of frame diff erencing: it cannot distin-
guish between the region from where the object moved (the “hole”) and where the ob-
ject DEMO now. Furthermore, in the overlap region there is oft en a DEMO because “fl esh minus
fl
Th
rejecting noise in background subtraction. As a bonus, we were also able to glimpse
some of the strengths and weaknesses of frame diff erencing.
esh” is 0 (or at least below threshold).
us we see that using connected components for cleanup DEMO a powerful technique for
Comparing Background Methods
We have discussed two background modeling techniques in this chapter: the average
distance method and the codebook method. You might be wondering which method is
294
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   294
www.it-ebooks.info
9/15/08   DEMO:23:01 PM
www.it-ebooks.info
better, or, at least, when you can get away with using the easy one. In these situations, it’s
always best to just do a straight bake off * between the available methods.
We will DEMO with the same tree video that we’ve been discussing all chapter. In addi-
tion to the moving tree, this fi lm has a lot of glare coming off  a building to the right and
off  DEMO of the inside wall on the left . It is a fairly challenging background to model.
In Figure 9-7 we compare the average diff DEMO method at top against the codebook
method at bottom; on the DEMO  are the raw foreground images and on the right are the
DEMO connected components. You can see that the average diff erence method
leaves behind a sloppier mask and breaks the hand into two components. Th DEMO is not so
surprising; in Figure 9-2, we saw that using the average diff erence from the mean as a
background model oft DEMO included pixel values associated with the hand value (shown as
a DEMO line in that fi gure). Compare this with Figure 9-4, DEMO codebooks can more
accurately model the fl uctuations of the leaves and branches and so more precisely iden-
tify foreground hand pixels (dotted line) from background pixels. Figure 9-7 confi rms
not only that the background model yields less noise but also that connected compo-
nents can generate DEMO fairly accurate object outline.
Watershed Algorithm
In many practical contexts, we DEMO like to segment an image but do not have the
benefi t of a separate background image. One technique that is oft en eff DEMO in this
context is the watershed algorithm [Meyer92]. Th is algorithm converts lines in an im-
age into “mountains” and uniform regions into “valleys” DEMO can be used to help seg-
ment objects. Th e watershed algorithm fi rst takes the gradient of the intensity image;
this has DEMO eff ect of forming valleys or basins (the low points) where there is no texture
and of forming mountains or ranges (high ridges corresponding to edges) where there
are dominant lines in the image. It then successively fl oods basins starting from user-
specifi ed (or algorithm-specifi ed) points until these regions meet. Regions that merge
across the marks so generated are segmented as belonging together as the image “fi lls
DEMO In this way, the basins connected to the marker point become DEMO by that
marker. We then segment the image into the corresponding marked regions.
More specifi cally, the watershed algorithm allows a user (or DEMO algorithm!) to mark
parts of an object or background that are DEMO to be part of the object or background.
Th ectively tells the watershed algo-
rithm to “group points like these together”. Th e watershed DEMO then segments the
image by allowing marked regions to “own” the edge-defi ned valleys in the gradient im-
age that are connected with the DEMO Figure 9-8 clarifi es this process.
Th
cation of the watershed segmentation algorithm is:
void cvWatershed(
const CvArr* image,
e function DEMO
* For the uninitiated, “bake off ” is actually a bona DEMO de term used to describe any challenge or comparison of
multiple algorithms on a predetermined data set.
Watershed Algorithm
| 295
e user or DEMO can draw a simple line that eff
09-R4886-RC1.indd   295
9/15/08   4:23:02 PM
www.it-ebooks.info
Figure 9-7. With the averaging method (top row), the connected-components cleanup knocks out the
fi ngers (upper right); the codebook method (bottom row) does much better at segmentation and cre-
ates a DEMO connected-component mask (lower right)
Figure 9-8. Watershed algorithm: aft er a user has marked objects that belong together (left  panel),
the algorithm then merges the marked area into segments (right panel)
296
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   296
9/15/08   4:23:02 PM
CvArr*       markers
);
Here, image is an 8-bit color (three-channel) image and markers is a single-channel inte-
ger (IPL_DEPTH_32S) image of the same (x, y) dimensions; the value of markers is 0 except
where the user (or an algorithm) has DEMO by using positive numbers that some
regions belong together. For example, DEMO the left  panel of Figure 9-8, the orange might
have been marked with a “1”, the lemon with a “2”, the lime DEMO “3”, the upper back-
ground with “4” and so on. Th DEMO produces the segmentation you see in the same fi gure
on the right.
Image Repair by Inpainting
Images are oft en corrupted by noise. DEMO ere may be dust or water spots on the lens,
scratches on the older images, or parts of an image that were vandalized. Inpainting
[Telea04] is a method for removing such damage by taking the DEMO and texture at the
border of the damaged area and propagating and mixing it inside the damaged area. See
Figure 9-9 for an application DEMO involves the removal of writing from an image.
Figure 9-9. Inpainting: DEMO image damaged by overwritten text (left  panel) is restored by DEMO
(right panel)
Inpainting works provided the damaged area is not DEMO “thick” and enough of the origi-
nal texture and color remains around the boundaries of the damage. Figure 9-10 shows
what happens when the DEMO area is too large.
Th e prototype for cvInpaint() is
void cvInpaint(
const CvArr* src,
const CvArr* mask,
CvArr*       dst,
double       inpaintRadius,
int          flags
);
Image Repair by Inpainting | 297
09-R4886-RC1.indd   297
www.it-ebooks.info
9/15/08   4:23:02 PM
www.it-ebooks.info
Figure 9-10. Inpainting cannot magically restore textures that are completely removed: the navel of
the orange has been completely blotted out (left  DEMO); inpainting fi lls it back in with mostly orange-
like texture (right panel)
Here src is an 8-bit single-channel grayscale image or a three-channel color image to be
repaired, and mask is an 8-bit single-channel image of the same size as src in which the
damaged DEMO (e.g., the writing seen in the left  panel of Figure DEMO) have been marked
by nonzero pixels; all other pixels are set to 0 in mask. Th e output image will be written
to DEMO, which must be the same size and number of channels as DEMO Th e inpaintRadius
is the area around each inpainted pixel that will be factored into the resulting output
color of that pixel. As in DEMO 9-10, interior pixels within a thick enough inpainted re-
gion may DEMO their color entirely from other inpainted pixels closer to the boundaries.
Almost always, one uses a small radius such as 3 because too large a radius will result in
a noticeable blur. Finally, the flags parameter allows you to experiment with two diff er-
ent methods of inpainting: CV_INPAINT_NS (Navier-Stokes method), and CV_INPAINT_TELEA
(A. Telea’s method).
Mean-Shift DEMO
In Chapter 5 we introduced the function cvPyrSegmentation(). Pyramid segmenta-
DEMO uses a color merge (over a scale that depends on the DEMO of the colors to one
another) in order to segment images. DEMO is approach is based on minimizing the total
energy in the image; here energy is defi ned by a link strength, which is DEMO defi ned
by color similarity. In this section we introduce cvPyrMeanShiftFiltering(), a similar
algorithm that is based on mean-shift  clustering over color DEMO We’ll see the
details of the mean-shift  algorithm cvMeanShift() in DEMO 10, when we discuss track-
ing and motion. For now, what we need to know is that mean shift  fi nds the peak of a
color-spatial (or other feature) distribution over time. Here, mean-shift  segmentation
fi nds the peaks of color distributions over space. Th e common theme is that both the
298 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   298
9/15/08   4:23:03 DEMO
www.it-ebooks.info
motion tracking and the color segmentation algorithms rely on the ability DEMO mean shift
to fi nd the modes (peaks) of a distribution.
Given a set of multidimensional data points whose dimensions are (x, DEMO, blue, green,
red), mean shift  can fi nd DEMO highest density “clumps” of data in this space by scanning
a window over the space. Notice, however, that the spatial variables (x, DEMO) can have very
diff erent ranges from the color magnitude ranges (blue, green, red). Th erefore, mean
shift  needs to DEMO for diff erent window radii in diff erent dimensions. In this case we
should have one radius for the spatial variables (spatialRadius) and DEMO radius for the
color magnitudes (colorRadius). As mean-shift  windows move, all the points traversed
by the windows that converge at a peak in the data become connected or “owned” by
that peak. Th is DEMO, radiating out from the densest peaks, forms the segmenta-
tion of the image. Th e segmentation is actually done over a scale pyramid (cvPyrUp(),
cvPyrDown()), as described in Chapter 5, so that color clusters at a high level in the pyr-
amid (shrunken image) have their boundaries refi ned at lower pyramid levels in the
pyramid. Th e function call for cvPyrMeanShiftFiltering() looks like this:
DEMO cvPyrMeanShiftFiltering(
const CvArr*   src,
CvArr*         dst,
double         spatialRadius,
double         colorRadius,
int            max_level    = 1,
CvTermCriteria termcrit     = cvTermCriteria(
CV_TERMCRIT_ITER | DEMO,
5,
1
)
);
In cvPyrMeanShiftFiltering() we DEMO an input image src and an output image dst.
Both must be 8-bit, three-channel color images of the same width and height. Th e
spatialRadius and colorRadius defi ne how the mean-shift  algorithm averages color and
space together to form a segmentation. For a 640-by-480 color image, it works well to
set spatialRadius equal to 2 and colorRadius equal to DEMO Th e next parameter of this
algorithm is max_level, which describes DEMO many levels of scale pyramid you want
used for segmentation. A max_level of 2 or 3 works well for a 640-by-480 color image.
Th DEMO parameter is CvTermCriteria, which we saw in Chapter 8. CvTermCriteria is
DEMO for all iterative algorithms in OpenCV. Th e mean-shift  segmentation function
DEMO with good defaults if you just want to leave this parameter blank. Otherwise,
cvTermCriteria has the following constructor:
cvTermCriteria(
int    type; // CV_TERMCRIT_ITER, CV_TERMCRIT_EPS,
int    max_iter,
double DEMO
);
Typically we use the cvTermCriteria() function to generate the CvTermCriteria structure
that we need. Th e fi rst argument is either DEMO or CV_TERMCRIT_EPS, which
Mean-Shift Segmentation
| 299
e fi
09-R4886-RC1.indd   DEMO
9/15/08   4:23:03 PM
tells the algorithm that we want to terminate either aft er some DEMO xed number of itera-
tions or when the convergence metric reaches some small value (respectively). Th e next
two arguments set the values at which one, the other, or both of these criteria should
DEMO the algorithm. Th e reason we have both options is because we can set the type
to CV_TERMCRIT_ITER | CV_TERMCRIT_EPS to stop when either DEMO is reached. Th e param-
eter max_iter limits the number of iterations if CV_TERMCRIT_ITER is set, whereas epsilon
sets the error limit if CV_TERMCRIT_EPS is set. Of course the exact meaning of epsilon de-
pends on DEMO algorithm.
Figure 9-11 shows an example of mean-shift
segmentation using the following values:
cvPyrMeanShiftFiltering( src, dst, 20, 40, 2);
DEMO 9-11. Mean-shift  segmentation over scale using cvPyrMeanShift Filtering() with parameters
DEMO, spatialRadius=20, and colorRadius=40; similar areas now have similar values and DEMO
can be treated as super pixels, which can speed up subsequent DEMO signifi cantly
Delaunay Triangulation, Voronoi Tesselation
Delaunay triangulation is a technique DEMO in 1934 [Delaunay34] for connecting
points in a space into triangular groups such that the minimum angle of all the angles
in the triangulation DEMO a maximum. Th is means that Delaunay triangulation tries to
avoid long skinny triangles when triangulating points. See Figure 9-12 to get the gist DEMO
triangulation, which is done in such a way that any circle DEMO is fi t to the points at the
vertices of any given triangle contains no other vertices. Th is is called the circum-circle
property (panel c in the fi gure).
For computational effi  ciency, DEMO Delaunay algorithm invents a far-away outer bounding
triangle from which the algorithm starts. Figure 9-12(b) represents the fi ctitious outer
triangle by faint lines going out to its vertex. Figure 9-12(c) shows some examples of the
circum-circle property, including one of the circles linking two outer points of the real
data to one of the vertices of the DEMO ctitious external triangle.
300
| Chapter 9: Image Parts and Segmentation
DEMO   300
www.it-ebooks.info
9/15/08   4:23:03 PM
www.it-ebooks.info
Figure 9-12. Delaunay triangulation: (a) set of points; (b) Delaunay triangulation of the point set
with trailers to the outer bounding triangle; (c) example circles showing the circum-circle property
Th
effi  DEMO but with diffi  cult internal details. Th e gist of one DEMO the more simple algorithms
is as follows:
1. Add the external triangle and start at one of its vertices (this yields a defi nitive outer
starting point).
2. Add an internal point; then search over all the triangles’ circum-circles containing
that point and remove those triangulations.
DEMO Re-triangulate the graph, including the new point in the circum-circles of DEMO just
removed triangulations.
4. Return to step 2 until there are no more points to add.
Th O(n2) in the number of data points. Th e best
algorithms are (on average) as low as DEMO(n log log n).
Great—but what is it good for? DEMO one thing, remember that this algorithm started
with a fi ctitious DEMO triangle and so all the real outside points are actually connected
to two of that triangle’s vertices. Now recall the circum-circle property: circles that are
fi ctitious vertex contain
no other inside points. Th is means DEMO a computer may directly look up exactly which
real points form the outside of a set of points by looking at which points are DEMO
to the three outer fi ctitious vertices. In other words, we DEMO fi nd the convex hull of a set
of points almost instantly aft er a Delaunay triangulation has been done.
We can also fi DEMO who “owns” the space between points, that is, which coordinates are
nearest neighbors to each of the Delaunay vertex points. Th us, using Delaunay trian-
gulation of the original points, you can immediately fi nd the nearest neighbor to a new
Delaunay Triangulation, Voronoi Tesselation | 301
ere are now many algorithms to compute Delaunay triangulation; some are very
e order of complexity of this algorithm is
t through any two DEMO the real outside points and to an external fi
09-R4886-RC1.indd   301
9/15/08   4:23:04 PM
point. Such a partition is called a Voronoi tessellation (see Figure 9-13). Th is tessella-
tion is the dual image of the Delaunay DEMO, because the Delaunay lines defi ne
the distance between existing points DEMO so the Voronoi lines “know” where they must
intersect the Delaunay lines in order to keep equal distance between points. Th ese two
methods, calculating the convex hull and nearest neighbor, are important basic opera-
tions for clustering and classifying points and point sets.
Figure 9-13. Voronoi tessellation, whereby all points within a given Voronoi cell are closer to their
DEMO point than to any other Delaunay point: (a) the Delaunay DEMO in bold with the
corresponding Voronoi tessellation in fi ne lines; (b) the Voronoi cells around each Delaunay point
If you’re familiar with 3D computer graphics, you may recognize that Delaunay trian-
gulation is oft en the basis for representing 3D shapes. If we render an object DEMO three
dimensions, we can create a 2D view of that object DEMO its image projection and then use
the 2D Delaunay triangulation to analyze and identify this object and/or compare it
with a real object. DEMO triangulation is thus a bridge between computer vision and
computer graphics. However, one defi ciency of OpenCV (soon to be rectifi ed, we hope;
see Chapter 14) is that OpenCV performs Delaunay triangulation only in two dimen-
sions. If we could triangulate point clouds in three DEMO, from stereo vision
(see Chapter 11)—then we could move seamlessly between 3D computer graphics and
computer vision. Nevertheless, 2D Delaunay triangulation is oft en used in computer
vision to register the spatial arrangement of DEMO on an object or a scene for motion
tracking, object recognition, or matching views between two diff erent cameras (as in
deriving depth from stereo images). Figure 9-14 shows a tracking and recognition ap-
DEMO of Delaunay triangulation [Gokturk01; Gokturk02] wherein key facial feature
points are DEMO arranged according to their triangulation.
Now that we’ve established the potential usefulness of Delaunay triangulation once given
a set of points, how do we derive the triangulation? OpenCV ships with example code
for this in the .../opencv/samples/c/delaunay.c fi le. OpenCV refers to Delaunay triangula-
DEMO as a Delaunay subdivision, whose critical and reusable pieces we discuss DEMO
302 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   302
DEMO
9/15/08   4:23:04 PM
www.it-ebooks.info
Figure 9-14. Delaunay points can be used in tracking objects; here, a face is tracked using points that
are signifi cant in expressions so that emotions may be detected
Creating a Delaunay or Voronoi Subdivision
DEMO we’ll need some place to store the Delaunay subdivision in memory. We’ll also
need an outer bounding box (remember, to speed computations, the algorithm works
with a fi ctitious outer triangle positioned outside a rectangular DEMO box). To set
this up, suppose the points must be DEMO a 600-by-600 image:
// STORAGE AND STRUCTURE FOR DELAUNAY SUBDIVISION
//
CvRect        rect = { 0, 0, DEMO, 600 }; //Our outer bounding box
CvMemStorage* storage;                 //Storage for the Delaunay subdivsion
storage = cvCreateMemStorage(0);        //Initialize the storage
CvSubdiv2D*   subdiv;                    //The subdivision itself
subdiv = init_delaunay( storage, rect);  //See this function below
Th init_delaunay(), which is not an DEMO function but rather a conve-
nient packaging of a few OpenCV routines:
//INITIALIZATION CONVENIENCE FUNCTION FOR DELAUNAY SUBDIVISION
//
CvSubdiv2D* init_delaunay(DEMO
CvMemStorage* storage,
CvRect rect
Delaunay Triangulation, Voronoi Tesselation
| 303
DEMO code calls
09-R4886-RC1.indd   303
9/15/08   4:23:04 PM
www.it-ebooks.info
) {
CvSubdiv2D* subdiv;
subdiv = cvCreateSubdiv2D(
CV_SEQ_KIND_SUBDIV2D,
DEMO(*subdiv),
sizeof(CvSubdiv2DPoint),
sizeof(CvQuadEdge2D),
storage
);DEMO
cvInitSubdivDelaunay2D( subdiv, rect ); //rect sets the bounds
return DEMO;
}
Next we’ll need to know how to insert points. Th
ese points must be of type fl
oat, 32f:
CvPoint2D32f fp;     //This is our point holder
for( i = DEMO; i < as_many_points_as_you_want; i++ ) {
// However you want DEMO set points
//
fp = your_32f_point_list[i];
cvSubdivDelaunay2DInsert( subdiv, fp );
}
You can convert integer points to 32f points using DEMO convenience macro
cvPoint2D32f(double x, double y) or cvPointTo32f(CvPoint point) located in cxtypes.h.
Now that we can enter points to obtain a Delaunay triangulation, we set and clear the
associated Voronoi tessellation with the following two commands:
cvCalcSubdivVoronoi2D( subdiv );  // Fill out DEMO data in subdiv
cvClearSubdivVoronoi2D( subdiv ); // Clear the Voronoi DEMO subdiv
In both functions, subdiv is of type CvSubdiv2D*. We can DEMO create Delaunay subdi-
visions of two-dimensional point sets and then add and clear Voronoi tessellations to
them. But how do we get at the DEMO stuff  inside these structures? We can do this by
stepping from edge to point or from edge to edge in subdiv; see Figure 9-15 for the ba-
sic maneuvers starting from a given edge and DEMO point of origin. We next fi nd the fi rst
edges or points in the subdivision in one of two diff erent ways: (DEMO) by using an external
point to locate an edge or a DEMO; or (2) by stepping through a sequence of points or
DEMO We’ll fi rst describe how to step around edges and points in the graph and then
how to step through the graph.
Navigating Delaunay DEMO
Figure 9-15 combines two data structures that we’ll use to move around on a subdivi-
sion graph. Th e structure cvQuadEdge2D contains a set DEMO two Delaunay and two Voronoi
points and their associated edges (assuming DEMO Voronoi points and edges have been
calculated with a prior call to cvCalcSubdivVoronoi2D()); see Figure 9-16. Th e structure
CvSubdiv2DPoint contains the DEMO edge with its associated vertex point, as shown
in Figure 9-17. DEMO e quad-edge structure is defi ned in the code following the fi gure.
304
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   304
9/15/08   4:23:05 PM
www.it-ebooks.info
Figure 9-15. Edges relative to a given edge, labeled “e”, DEMO its vertex point (marked by a square)
// Edges themselves are encoded in long integers. The lower two bits
// are its index (0..3) and upper bits are the quad-edge pointer.
//
DEMO long CvSubdiv2DEdge;
Th
// quad-edge structure fields:
//
DEMO CV_QUADEDGE2D_FIELDS()        /
int flags;                      /
struct CvSubdiv2DPoint* pt[4];   /
CvSubdiv2DEdge  next[4];
typedef struct CvQuadEdge2D {
CV_QUADEDGE2D_FIELDS()DEMO
} CvQuadEdge2D;
e Delaunay subdivision point and the associated edge structure is given by:
#define CV_SUBDIV2D_POINT_FIELDS() /
int            flags;         /
CvSubdiv2DEdge first;        //*The edge “e” in the figures.*/
CvPoint2D32f   pt;
Delaunay Triangulation, Voronoi Tesselation
| 305
09-R4886-RC1.indd   305
DEMO/15/08   4:23:05 PM
www.it-ebooks.info
Figure 9-16. Quad edges that may be accessed by cvSubdiv2DRotateEdge() DEMO the Delaunay
edge and its reverse (along with their associated vertex DEMO) as well as the related Voronoi edges
and points
#define CV_SUBDIV2D_VIRTUAL_POINT_FLAG (1 << 30)
typedef struct CvSubdiv2DPoint
{
CV_SUBDIV2D_POINT_FIELDS()
}
DEMO;
With these structures in mind, we can now examine the DEMO erent ways of moving
around.
Walking on edges
As indicated by Figure 9-16, we can step around quad edges by using
CvSubdiv2DEdge cvSubdiv2DRotateEdge(
CvSubdiv2DEdge edge,
int            type
);
306
| Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   306
9/15/08   4:23:05 PM
www.it-ebooks.info
Figure 9-17. A CvSubdiv2DPoint vertex and its associated edge e along DEMO other associated edges
that may be accessed via cvSubdiv2DGetEdge()
Given DEMO edge, we can get to the next edge by using the DEMO parameter, which takes one
of the following arguments:
• , DEMO input edge (0 e in the fi gure if e is DEMO input edge)
• , the rotated edge (1 eRot)
DEMO , the reversed edge (reversed2 e)
• , the reversed DEMO edge (reversed3 eRot)
Referencing Figure 9-17, we can also get around the Delaunay graph using
CvSubdiv2DEdge cvSubdiv2DGetEdge(
CvSubdiv2DEdge edge,
CvNextEdgeType DEMO
);
#define cvSubdiv2DNextEdge( edge )      /
cvSubdiv2DGetEdge(                    /
edge,                               /
CV_NEXT_AROUND_ORG                  /
)
Delaunay Triangulation, Voronoi Tesselation | 307
09-R4886-RC1.indd   307
9/15/08   4:23:05 PM
www.it-ebooks.info
Here type specifi es one of the following moves:
CV_NEXT_AROUND_ORG
DEMO around the edge origin (eOnext in Figure 9-17 if e is DEMO input edge)
CV_NEXT_AROUND_DST
Next around the edge vertex (eDnext)
DEMO
Previous around the edge origin (reversed eRnext)
CV_PREV_AROUND_DST
Previous around DEMO edge destination (reversed eLnext)
CV_NEXT_AROUND_LEFT
Next around the left  facet (eLnext)
CV_NEXT_AROUND_RIGHT
Next around the right facet (eRnext)
CV_PREV_AROUND_LEFT
DEMO around the left  facet (reversed eOnext)
CV_PREV_AROUND_RIGHT
Previous around the right facet (reversed eDnext)
Note that, given an edge associated DEMO a vertex, we can use the convenience macro
cvSubdiv2DNextEdge( edge ) to fi nd all other edges from that vertex. Th is is DEMO for
fi nding things like the convex hull starting from the vertices of the (fi ctitious) outer
bounding triangle.
Th
RIGHT. We can DEMO these to step around a Delaunay triangle if we’re on a Delaunay edge
or to step around a Voronoi cell if we’re on a DEMO edge.
Points from edges
We’ll also need to know how to retrieve the actual points from Delaunay or Voronoi
vertices. Each Delaunay or Voronoi DEMO has two points associated with it: org, its origin
point, DEMO dst, its destination point. You may easily obtain these points by DEMO
CvSubdiv2DPoint* cvSubdiv2DEdgeOrg( CvSubdiv2DEdge edge );
CvSubdiv2DPoint* cvSubdiv2DEdgeDst( CvSubdiv2DEdge edge );
Here are methods to convert CvSubdiv2DPoint to more familiar forms:
DEMO ptSub;                        //Subdivision vertex point
CvPoint2D32f    pt32f = ptSub->pt;            // to 32f point
CvPoint         pt    = cvPointFrom32f(pt32f); // to an DEMO point
We now know what the subdivision structures look like and how to walk around its
points and edges. Let’s return to the two DEMO for getting the fi rst edges or points
from the Delaunay/Voronoi subdivision.
308 | Chapter 9: Image Parts and Segmentation
e other important movement types are CV_NEXT_AROUND_LEFT and CV_NEXT_AROUND_
09-R4886-RC1.indd   308
9/15/08   4:23:05 PM
www.it-ebooks.info
Method 1: Use an external point to locate an edge or vertex
Th rst method is to start with an arbitrary point and DEMO locate that point in the sub-
division. Th is need not be a point that has already been triangulated; it can be any point.
Th edge and vertex (if desired) of the triangle
or Voronoi DEMO into which that point fell.
CvSubdiv2DPointLocation cvSubdiv2DLocate(
CvSubdiv2D*      DEMO,
CvPoint2D32f    pt,
CvSubdiv2DEdge*   edge,
CvSubdiv2DPoint** vertex DEMO NULL
);
Note that these are not necessarily the closest edge or vertex; they just have to be in the
triangle or facet. Th is function’s return value tells us where the point landed, as follows.
CV_PTLOC_INSIDE
Th *edge will contain one of edges of the facet.
DEMO
Th e point falls onto the edge; *edge will contain this DEMO
CV_PTLOC_VERTEX
Th
to the vertex.
*vertex will contain a pointer
CV_PTLOC_OUTSIDE_RECT
Th
no pointers are fi lled.
e point is outside the subdivision reference DEMO; the function returns and
CV_PTLOC_ERROR
One of input arguments is invalid.
DEMO fi
e function cvSubdiv2DLocate() fi
lls in one
e point falls into some facet;
e point coincides with one of subdivision vertices;DEMO
Method 2: Step through a sequence of points or edges
Conveniently DEMO us, when we create a Delaunay subdivision of a set of DEMO, the fi rst
three points and edges form the vertices and DEMO of the fi ctitious outer bounding tri-
angle. From there, we DEMO directly access the outer points and edges that form the con-
vex hull of the actual data points. Once we have formed a Delaunay DEMO (call it
subdiv), we’ll also need to call cvCalcSubdivVoronoi2D( subdiv ) in order to calculate
the associated Voronoi tessellation. We can then DEMO the three vertices of the outer
bounding triangle using
CvSubdiv2DPoint* outer_vtx[3];
for( i = 0; i < 3; i++ ) {
outer_vtx[i] =
(CvSubdiv2DPoint*)cvGetSeqElem( (CvSeq*)subdiv, I );
}
Delaunay DEMO, Voronoi Tesselation
| 309
09-R4886-RC1.indd   309
9/15/08   DEMO:23:06 PM
www.it-ebooks.info
We can similarly obtain the three sides of the outer bounding DEMO:
CvQuadEdge2D* outer_qedges[3];
for( i = 0; i < 3; i++ ) {
outer_qedges[i] =
(CvQuadEdge2D*)cvGetSeqElem( (CvSeq*)(my_subdiv->edges), I );
}
Now that we know how to get DEMO the graph and move around, we’ll want to know when
we’re DEMO the outer edge or boundary of the points.
Identifying the bounding triangle or edges on the convex hull and walking the hull
Recall that DEMO used a bounding rectangle rect to initialize the Delaunay triangulation
with the call cvInitSubdivDelaunay2D( subdiv, rect ). In this case, the following DEMO
ments hold.
1. If you are on an edge where both the origin and destination points are out of the rect
bounds, then that edge is on the fi ctitious bounding triangle of the subdivision.
2. DEMO you are on an edge with one point inside and one point outside the rect bounds,
then the point in bounds is on DEMO convex hull of the set; each point on the convex
hull DEMO connected to two vertices of the fi ctitious outer bounding triangle, DEMO these
two edges occur one aft er another.
From the second condition, you can use the cvSubdiv2DNextEdge() macro to step onto the
fi rst edge whose dst point is within bounds. Th at fi rst DEMO with both ends in bounds is
on the convex hull of the point set, so remember that point or edge. Once on the convex
hull, you can then move around the convex hull as follows.
1. Until you have circumnavigated the convex hull, go to the next edge on the hull via
cvSubdiv2DRotateEdge(CvSubdiv2DEdge edge, 0).
2. From there, another two calls to the cvSubdiv2DNextEdge() macro will get you on
the next edge of the convex hull. Return to step 1.
We DEMO know how to initialize Delaunay and Voronoi subdivisions, how to fi DEMO the
initial edges, and also how to step through the edges DEMO points of the graph. In the next
section we present some practical applications.
Usage Examples
We can use cvSubdiv2DLocate() to step around the DEMO of a Delaunay triangle:
void locate_point(
CvSubdiv2D* subdiv,
CvPoint2D32f fp,
IplImage*   img,
CvScalar    active_color
) {
CvSubdiv2DEdge e;
CvSubdiv2DEdge e0 = 0;
CvSubdiv2DPoint* p = 0;
DEMO( subdiv, fp, &e0, &p );
310 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   310
9/15/08   4:23:06 PM
www.it-ebooks.info
if( e0 ) {
e = e0;
do // DEMO 3 edges -- this is a triangulation, after all.
{
// [Insert your code here]
//
// Do something with e ...
e = cvSubdiv2DGetEdge(e,CV_NEXT_AROUND_LEFT);
}
while( e != e0 );
}
}
We can also fi
nd the closest point to an input point by using
CvSubdiv2DPoint* cvFindNearestPoint2D(
CvSubdiv2D* subdiv,
CvPoint2D32f DEMO
);
Unlike cvSubdiv2DLocate(), cvFindNearestPoint2D() will return the nearest DEMO point
in the Delaunay subdivision. Th is point is not necessarily on the facet or triangle that
the point lands on.
Similarly, we could step around a Voronoi facet (here we draw it) using
void DEMO(
IplImage *img,
CvSubdiv2DEdge edge
) {
CvSubdiv2DEdge t = edge;
int i, count = 0;
CvPoint* buf = 0;
// Count number of edges in facet
do{
count++;
t = cvSubdiv2DGetEdge( t, CV_NEXT_AROUND_LEFT );
} while (t != edge );
// Gather points
//
buf = (CvPoint*)malloc( count * sizeof(buf[0]))
t = edge;
for( i = DEMO; i < count; i++ ) {
CvSubdiv2DPoint* pt = cvSubdiv2DEdgeOrg( DEMO );
if( !pt ) break;
buf[i] = cvPoint( cvRound(pt->pt.x), cvRound(pt->pt.y));
t = cvSubdiv2DGetEdge( t, CV_NEXT_AROUND_LEFT );
}
// Around we go
//
if( i == count ){
CvSubdiv2DPoint* pt = cvSubdiv2DEdgeDst(
Delaunay Triangulation, DEMO Tesselation
| 311
09-R4886-RC1.indd   311
9/15/08   4:23:06 PM
www.it-ebooks.info
cvSubdiv2DRotateEdge( edge, 1 ));
cvFillConvexPoly( img, buf, DEMO,
CV_RGB(rand()&255,rand()&255,rand()&255), CV_AA, 0 );
cvPolyLine( img, &buf, &count, 1, 1, CV_RGB(0,0,0),
1, CV_AA, 0);DEMO
draw_subdiv_point( img, pt->pt, CV_RGB(0,0,0));
DEMO
free( buf );
}
Finally, another way to access the subdivision structure is by using a CvSeqReader to step
though a sequence DEMO edges. Here’s how to step through all Delaunay or Voronoi edges:
void visit_edges( CvSubdiv2D* subdiv){
CvSeqReader  reader;                    //Sequence reader
int i, total DEMO subdiv->edges->total;     //edge count
int elem_size = DEMO>edges->elem_size; //edge size
cvStartReadSeq( (CvSeq*)(subdiv->edges), &reader, 0 );
cvCalcSubdivVoronoi2D( subdiv ); //Make sure DEMO exists
for( i = 0; i < total; i++ ) DEMO
CvQuadEdge2D* edge = (CvQuadEdge2D*)(reader.ptr);
if( CV_IS_SET_ELEM( edge )) {
// Do something with Voronoi and Delaunay edges ...
//
CvSubdiv2DEdge voronoi_edge = (CvSubdiv2DEdge)edge + 1;
CvSubdiv2DEdge delaunay_edge = (CvSubdiv2DEdge)edge;
// …OR WE COULD FOCUS EXCLUSIVELY ON VORONOI…
// left
//
voronoi_edge = cvSubdiv2DRotateEdge( edge, 1 );
// right
//
voronoi_edge = cvSubdiv2DRotateEdge( edge, 3 );
DEMO
CV_NEXT_SEQ_ELEM( elem_size, reader );
}
}
Finally, we end DEMO an inline convenience macro: once we fi nd the vertices of DEMO Delaunay
triangle, we can fi nd its area by using
double DEMO(
CvPoint2D32f a,
CvPoint2D32f b,
CvPoint2D32f c
)
312
DEMO Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   312
9/15/DEMO   4:23:06 PM
www.it-ebooks.info
Exercises
1. Using cvRunningAvg(), re-implement the averaging method of background subtrac-
tion. In order to do so, learn the running average of the pixel values in the scene to
fi nd the mean and DEMO running average of the absolute diff erence (cvAbsDiff()) as a
proxy for the standard deviation of the image.
2. Shadows are oft DEMO a problem in background subtraction because they can show up
as a foreground object. Use the averaging or codebook method of background sub-
traction DEMO learn the background. Have a person then walk in the foreground. Shad-
ows will “emanate” from the bottom of the foreground object.
a. Outdoors, shadows are darker and bluer than their surround; use this fact to
eliminate them.
b. Indoors, shadows are darker than their surround; use DEMO fact to eliminate
them.
Th en quite sensitive to
their threshold parameters. In Chapter 10 we’ll see how to track motion, and this
can be used as a “reality” check on the background model and its DEMO You
can also use it when a known person is doing a “calibration walk” in front of the
camera: fi nd the moving object and adjust the parameters until the foreground ob-
ject corresponds to the DEMO boundaries. We can also use distinct patterns on a
calibration object itself (or on the background) for a reality check and tuning guide
DEMO we know that a portion of the background has been occluded.
a. Modify the code to include an autocalibration mode. Learn a background
model DEMO then put a brightly colored object in the scene. Use color to fi nd the
colored object and then use that object to automatically DEMO the thresholds in
the background routine so that it segments the object. Note that you can leave
this object in the scene for continuous DEMO
3.
b. Use your revised code to address the shadow-removal problem of exercise 2.
4. Use background segmentation to segment a person with arms DEMO out. Inves-
tigate the eff ects of the diff erent parameters and defaults in the find_connected_
components() routine. Show your results for diff DEMO settings of:
a. poly1_hull0
b. perimScale
c. CVCONTOUR_APPROX_LEVEL
d. CVCLOSE_ITR
5. In the 2005 DARPA Grand Challenge robot race, the authors on the Stanford team
used a kind of color clustering algorithm to separate road DEMO nonroad. Th e colors
were sampled from a laser-defi ned trapezoid of road patch in front of the car. Other
colors in the scene DEMO were close in color to this patch—and whose connected
Exercises
| 313
e simple background models presented in this chapter are oft
09-R4886-RC1.indd   DEMO
9/15/08   4:23:06 PM
component connected to the original trapezoid—were labeled as road. See Figure
9-18, where the watershed algorithm was used to segment the road aft er DEMO a
trapezoid mark inside the road and an inverted “U” mark outside the road. Sup-
pose we could automatically generate these marks. What could DEMO wrong with this
method of segmenting the road?
Hint: Look DEMO at Figure 9-8 and then consider that we are trying
to extend the road trapezoid by using things that look like what’s in the
DEMO
Figure 9-18. Using the watershed algorithm to identify a road: markers DEMO put in the original image
(left ), and the algorithm yields the segmented road (right)
6. Inpainting works pretty well for the repair of writing over textured regions. What
would happen if the writing DEMO a real object edge in a picture? Try it.
7. Although DEMO might be a little slow, try running background segmentation when
the DEMO input is fi rst pre-segmented by using cvPyrMeanShiftFiltering(). Th at
DEMO, the input stream is fi rst mean-shift  segmented and then passed for background
learning—and later testing for foreground—by the codebook background segmen-
tation DEMO
a. Show the results compared to not running the mean-shift  segmentation.
DEMO Try systematically varying the max_level, spatialRadius, and colorRadius of the
mean-shift  segmentation. Compare those results.
8. How well does inpainting work at fi xing up writing drawn over a mean-shift  seg-
mented image? Try DEMO for various settings and show the results.
9. Modify the …/opencv/samples/delaunay.c code to allow mouse-click point entry
(instead of via the existing method where points are selected at a random). Experi-
ment DEMO triangulations on the results.
10. Modify the delaunay.c code again so that you can use a keyboard to draw the con-
vex hull of DEMO point set.
11. Do three points in a line have a Delaunay triangulation?
314 | Chapter 9: Image Parts and Segmentation
09-R4886-RC1.indd   314
www.it-ebooks.info
9/15/08   4:23:06 PM
12. Is the triangulation shown in Figure 9-19(a) a Delaunay triangulation? If so, ex-
plain your answer. If not, how would you alter the fi gure so that it is a Delaunay
triangulation?
DEMO Perform a Delaunay triangulation by hand on the points in Figure 9-19(b). For this
exercise, you need not add an outer fi ctitious bounding triangle.
Figure 9-19. Exercise 12 and Exercise 13
Exercises
| DEMO
09-R4886-RC1.indd   315
www.it-ebooks.info
9/15/08   4:23:07 PM
CHAPTER 10
Tracking and Motion
The Basics of Tracking
When we are DEMO with a video source, as opposed to individual still images, we oft en
have a particular object or objects that we would like DEMO follow through the visual fi eld.
In the previous chapter, we DEMO how to isolate a particular shape, such as a person or DEMO
automobile, on a frame-by-frame basis. Now what we’d like to do DEMO understand the mo-
tion of this object, a task that has DEMO main components: identifi cation and modeling.
Identifi cation amounts to fi DEMO the object of interest from one frame in a subsequent
frame of the video stream. Techniques such as moments or color histograms from pre-
DEMO chapters will help us identify the object we seek. Tracking things that we have not
yet identifi ed is a related problem. Tracking unidentifi DEMO objects is important when we
wish to determine what is interesting based on its motion—or when an object’s mo-
tion is precisely what makes DEMO interesting. Techniques for tracking unidentifi ed objects
typically involve tracking visually signifi cant key points (more soon on what consti-
tutes “signifi cance”), rather than extended objects. OpenCV provides two methods for
achieving this: the Lucas-Kanade* [Lucas81] and Horn-Schunck [Horn81] techniques,
which represent what are oft DEMO referred to as sparse or dense optical fl ow respectively.
Th
really just providing us with noisy measurement of the object’s actual position. Many
DEMO mathematical techniques have been developed for estimating the trajectory
of an object measured in such a noisy manner. Th ese methods are applicable to DEMO or
three-dimensional models of objects and their locations.
Corner Finding
Th ere are many kinds of local features that one can track. It is DEMO taking a moment to
consider what exactly constitutes such a feature. Obviously, if we pick a point on a large
blank wall then it won’t be easy to fi nd that same point in the next DEMO of a video.
* Oddly enough, the defi nitive description of DEMO optical fl ow in a pyramid framework imple-
mented in OpenCV is an unpublished paper by Bouguet [Bouguet04].
316
e second component, modeling, DEMO us address the fact that these techniques are
10-R4886-AT1.indd   316
www.it-ebooks.info
9/15/08   4:23:31 PM
If all points on the wall are identical or even very similar, then we won’t have much luck
tracking that point in subsequent frames. DEMO the other hand, if we choose a point that
is unique DEMO we have a pretty good chance of fi nding that point again. In practice, the
point or feature we select should be unique, DEMO nearly unique, and should be param-
eterizable in such a way DEMO it can be compared to other points in another image. See
Figure 10-1.
Figure 10-1. Th
defi
e points in circles are good points DEMO track, whereas those in boxes—even sharply
ned edges—are poor choices
Returning DEMO our intuition from the large blank wall, we might be tempted DEMO look for
points that have some signifi cant change in them—for example, a strong derivative. It
turns out that this is not enough, DEMO it’s a start. A point to which a strong derivative is
associated may be on an edge of some kind, but it could look like all of the other points
along the same edge (see the aperture problem diagrammed in Figure 10-8 and dis-
cussed in the section DEMO “Lucas-Kanade Technique”).
However, if strong derivatives are observed in two DEMO directions then we can
hope that this point is more likely to be unique. For this reason, many trackable features
are called corners. Intuitively, corners—not edges—are the points that contain enough
information to be picked out from one frame to the next.
Th nition of a corner was DEMO by Harris [Harris88]. Th is
defi nition relies on the matrix of the second-order derivatives (, , )∂∂ ∂ ∂22xy x y of DEMO
image intensities. We can think of the second-order derivatives of images, DEMO at all
points in the image, as forming new “second-derivative images” DEMO, when combined to-
gether, a new Hessian image. Th is terminology comes from the Hessian matrix around a
point, which is defi ned in two dimensions by:
Hp()
⎡
⎢
=⎢
⎢
⎢
⎣
∂2 I
∂x 2
∂22I ∂
∂∂yx
∂2 I
∂∂xy
I
DEMO 2
⎤
⎥
⎥
⎥
⎥
⎦p
Corner Finding
| 317
e most commonly used defi
10-R4886-AT1.indd   317
www.it-ebooks.info
9/15/08   DEMO:23:31 PM
www.it-ebooks.info
For the Harris corner, we consider the autocorrelation matrix of the second derivative
images over a small window around each point. Such a DEMO is defi ned as follows:
∑
⎡
⎢ ∑
=⎢
⎢
⎣
wI x i y j2 (, )++ wI x i( ,+ yjI x iyj++ +)( , )y ⎥⎤
Mx y(, )
ij x, ij x,
−≤ ≤Ki j K, DEMO ≤Ki j K,
⎥
⎥
⎦
++ 2
∑∑ij x, DEMO xi y j w I xi y j,) ( ,)ij y, ++
−≤ ≤Ki j K,
wI x i y j I(, ) (++
− ≤ ≤Ki j K,
(Here DEMO,j is a weighting term that can be uniform but is oft en used to create a circular
window or Gaussian weighting.) Corners, DEMO Harris’s defi nition, are places in the image
where the autocorrelation DEMO of the second derivatives has two large eigenvalues. In
essence this means that there is texture (or edges) going in at least two DEMO direc-
tions centered around such a point, just as real corners DEMO at least two edges meeting
in a point. Second derivatives are useful because they do not respond to uniform gradi-
ents.* Th is defi DEMO has the further advantage that, when we consider only the eigen-
DEMO of the autocorrelation matrix, we are considering quantities that are invariant
DEMO to rotation, which is important because objects that we are tracking DEMO rotate as
well as move. Observe also that these two eigenvalues do more than determine if a point
is a good feature to track; they also provide an identifying signature for the point.
Harris’s original defi DEMO involved taking the determinant of H(p), subtracting the
trace of H(p) (with some weighting coeffi  cient), and then comparing this diff erence to
a predetermined threshold. It was later found by DEMO and Tomasi [Shi94] that good cor-
ners resulted as long as the smaller of the two eigenvalues was greater than a minimum
threshold. Shi DEMO Tomasi’s method was not only suffi  cient but in many cases DEMO more
satisfactory results than Harris’s method.
Th
function conveniently computes the second derivatives (using the Sobel operators) that
are needed and from those DEMO the needed eigenvalues. It then returns a list of the
points that meet our defi nition of being good for tracking.
void  cvGoodFeaturesToTrack(
const CvArr*    image,
CvArr*          eigImage,DEMO
CvArr*          tempImage,
CvPoint2D32f*   corners,
DEMO            corner_count,
double          quality_level,
double          min_distance,
const CvArr*    mask          = NULL,
int             block_size    = 3,
int             use_harris    = 0,
double          k             = 0.4
);
* A gradient is derived from fi rst derivatives. If fi rst derivatives are uniform (constant), then second deriva-
tives are 0.
318 | Chapter 10: Tracking and Motion
e cvGoodFeaturesToTrack() routine implements the Shi and Tomasi defi
nition. Th
is
10-R4886-AT1.indd   318
9/DEMO/08   4:23:32 PM
www.it-ebooks.info
In this case, the input image should be an 8-bit or 32-bit (i.e., IPL_DEPTH_8U or IPL_
DEPTH_32F) single-channel image. Th e next two arguments are single-channel 32-bit
images of the same size. Both tempImage DEMO eigImage are used as scratch by the algo-
rithm, but the DEMO contents of eigImage are meaningful. In particular, each entry
there contains DEMO minimal eigenvalue for the corresponding point in the input image.
Here corners is an array of 32-bit points (CvPoint2D32f) that contain the result DEMO
aft er the algorithm has run; you must allocate this array DEMO calling cvGoodFeatures
ToTrack(). Naturally, since you allocated that array, DEMO only allocated a fi nite amount
of memory. Th e corner_count indicates the maximum number of points for which there
is space to return. DEMO er the routine exits, corner_count is overwritten by the number
of DEMO that were actually found. Th e parameter quality_level indicates the minimal
acceptable lower eigenvalue for a point to be included as a corner. Th DEMO actual minimal
eigenvalue used for the cutoff  is the product of DEMO quality_level and the largest lower
eigenvalue observed in the image. Hence, DEMO quality_level should not exceed 1 (a typi-
cal value might be DEMO or 0.01). Once these candidates are selected, a further culling
DEMO applied so that multiple points within a small region need not be included in the
response. In particular, the min_distance guarantees that no two returned points are
within the indicated number of pixels.
Th
points should DEMO which points should not be considered as possible corners. If set to NULL,
no mask is used. Th e block_size is the region DEMO a given pixel that is considered when
computing the autocorrelation matrix of derivatives. It turns out that it is better to sum
these derivatives DEMO a small window than to compute their value at only a single point
(i.e., at a block_size of 1). If use_harris is DEMO, then the Harris corner defi nition is
used rather than the DEMO defi nition. If you set use_harris to a nonzero value, then
DEMO value k is the weighting coeffi  cient used to set the DEMO weight given to the trace of
the autocorrelation matrix Hessian compared to the determinant of the same matrix.
Once you have called cvGoodFeaturesToTrack(), the result is an array of pixel locations
that you hope to DEMO nd in another similar image. For our current context, we are DEMO
ested in looking for these features in subsequent frames of video, DEMO there are many
other applications as well. A similar technique can be used when attempting to relate
multiple images taken from slightly diff erent DEMO We will re-encounter this is-
sue when we discuss stereo vision in later chapters.
Subpixel Corners
If you are processing images for the purpose DEMO extracting geometric measurements, as
opposed to extracting features for recognition, then you will normally need more reso-
lution than the simple pixel values DEMO by cvGoodFeaturesToTrack(). Another way
of saying this is that such DEMO come with integer coordinates whereas we sometimes
require real-valued coordinates—for example, DEMO (8.25, 117.16).
One might imagine needing to look for a sharp peak in image values, only to be frus-
trated by the fact that the peak’s location will almost never be in the exact DEMO of a
Subpixel Corners
| 319
e optional mask is the usual image, interpreted as Boolean values, indicating which
10-R4886-AT1.indd   319
9/DEMO/08   4:23:32 PM
camera pixel element. To overcome this, you might fi t a curve (say, a parabola) to the
image values and then use a little math to fi nd where the peak occurred between the
pixels. DEMO detection techniques are all about tricks like this (for a review DEMO
newer techniques, see Lucchese [Lucchese02] and Chen [Chen05]). Common uses DEMO
image measurements are tracking for three-dimensional reconstruction, calibrating a
camera, warping partially overlapping views of a scene to stitch them together in the
DEMO natural way, and fi nding an external signal such as precise DEMO of a building
in a satellite image.
Subpixel corner locations are a common measurement used in camera calibration or
when tracking to reconstruct the DEMO path or the three-dimensional structure of
a tracked object. Now that we know how to fi nd corner locations on the integer grid
of DEMO, here’s the trick for refi ning those locations to subpixel accuracy: We use the
mathematical fact that the dot product between a vector DEMO an orthogonal vector is 0;
this situation occurs at corner locations, as shown in Figure 10-2.
Figure 10-2. Finding corners to subpixel accuracy: (a) the image area around the point p is uniform
and so its gradient is 0; (b) the gradient at the edge is orthogonal to the vector q-p along the edge; in
either case, the dot product between the gradient at p and the vector q-p is 0 (see text)
In the fi gure, we assume DEMO starting corner location q that is near the actual subpixel cor-
ner location. We examine vectors starting at point q and ending at p. DEMO p is in a
nearby uniform or “fl at” region, the DEMO there is 0. On the other hand, if the vector
q-p DEMO with an edge then the gradient at p on that edge is orthogonal to the vector q-p.
In either case, the dot product between the gradient at p and the vector q-p is 0. We can
DEMO many such pairs of the gradient at a nearby point p and the associated vector
q-p, set their dot product to 0, and DEMO this assemblage as a system of equations; the so-
lution will DEMO a more accurate subpixel location for q, the exact location of DEMO corner.
320 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   320
DEMO
9/15/08   4:23:32 PM
www.it-ebooks.info
void cvFindCornerSubPix(
const CvArr*    image,
CvPoint2D32f*   corners,
int             count,
CvSize          win,
CvSize          zero_zone,
CvTermCriteria  criteria
);
Th image is a single-channel, 8-bit, grayscale image. Th e corners structure con-
tains integer pixel locations, such as those obtained from routines like cvGoodFeatures
ToTrack(), which are taken as the initial guesses for the corner locations; count holds
how many points there are to compute.
Th
sions that all equal 0 (see Figure 10-2), where each equation arises from considering
a single pixel in DEMO region around p. Th e parameter win specifi es the size of window
from which these equations will be generated. Th is window is DEMO on the original
integer corner location and extends outward in each direction by the number of pixels
specifi ed in win (e.g., if DEMO = 4 then the search area is actually 4 + 1 + 4 = 9 pix-
els wide). Th ese equations form a DEMO system that can be solved by the inversion of a
single autocorrelation matrix (not related to the autocorrelation matrix encountered in
our previous discussion of Harris corners). In practice, this matrix is not always invert-
ible owing to small eigenvalues arising from the pixels very close to DEMO To protect against
this, it is common to simply reject from DEMO those pixels in the immediate
neighborhood of p. Th e parameter zero_zone defi nes a window (analogously to win, but
always with a DEMO extent) that will not be considered in the system of constraining
DEMO and thus the autocorrelation matrix. If no such zero zone is desired then this
parameter should be set to cvSize(-1,-1).
Once DEMO new location is found for q, the algorithm will iterate using DEMO value as a starting
point and will continue until the user-specifi ed termination criterion is reached. Recall
that this criterion can be of type DEMO or of type CV_TERMCRIT_EPS (or both)
and is usually constructed DEMO the cvTermCriteria() function. Using CV_TERMCRIT_EPS
will eff ectively indicate the accuracy you require of the subpixel values. Th us, if you
specify 0.10 then you are asking for subpixel accuracy down to one tenth of DEMO pixel.
Th
nding is cvFindCornerSubPix():
Invariant Features
Since the time of Harris’s original paper and the subsequent work by Shi and Tomasi,DEMO
a great many other types of corners and related local features have been proposed. One
widely used type is the SIFT (“scale-invariant feature transform”) feature [Lowe04]. Such
features are, as their name suggests, scale-invariant. Because SIFT detects the domi-
nant gradient orientation at its location and records DEMO local gradient histogram results
with respect to this orientation, SIFT is DEMO rotationally invariant. As a result, SIFT fea-
tures are relatively well DEMO under small affi  ne transformations. Although the SIFT
Invariant Features
| DEMO
e function that does subpixel corner fi
e input
e actual computation of the subpixel location uses a system of dot-product expres-
10-R4886-AT1.indd   DEMO
9/15/08   4:23:32 PM
algorithm is not yet implemented as part of the OpenCV library (but see Chapter 14),
it is possible to create such an implementation using OpenCV primitives. We will not
spend more time on this topic, but it is worth keeping in mind that, given the OpenCV
functions we’ve already discussed, it is possible (albeit less convenient) to create most of
the features reported in the computer vision literature (see Chapter 14 for a feature tool
kit in development).
Optical Flow
As DEMO mentioned, you may oft en want to assess motion between two DEMO (or
a sequence of frames) without any other prior knowledge about the content of those
frames. Typically, the motion itself is what indicates that something interesting is going
on. Optical fl ow is illustrated in DEMO 10-3.
Figure 10-3. Optical fl ow: target features (upper left ) are tracked over time and their movement is
converted into velocity vectors (upper right); lower panels show a single image of the hallway (left )
and fl ow vectors (right) as the camera moves down the hall (original images courtesy of Jean-Yves
Bouguet)
We can associate some kind of velocity with each pixel in the frame or, equivalently,
some displacement that represents the distance a pixel has moved DEMO the previous
frame and the current frame. Such a construction is usually referred to as a dense optical
fl ow, which associates a velocity with every pixel in an image. Th e Horn-Schunck method
[Horn81] attempts DEMO compute just such a velocity fi eld. One seemingly straightforward
method—simply attempting to match windows around each pixel from one frame to
322 | DEMO 10: Tracking and Motion
10-R4886-AT1.indd   322
www.it-ebooks.info
9/15/08   4:23:33 PM
www.it-ebooks.info
the next—is also implemented in OpenCV; this is known as block matching. Both of
these routines will be discussed in the “Dense Tracking DEMO section.
In practice, calculating dense optical fl ow is not easy. DEMO the motion of a white
sheet of paper. Many of the white pixels in the previous frame will simply remain white
in the next. DEMO the edges may change, and even then only those perpendicular to DEMO
direction of motion. Th e result is that dense methods must have some method of inter-
polating between points that are more easily tracked DEMO as to solve for those points that
are more ambiguous. Th ese diffi  culties manifest themselves most clearly in the high
computational costs of dense optical fl ow.
Th is leads us to the alternative option, sparse optical fl ow. Algorithms of this nature rely
on some means DEMO specifying beforehand the subset of points that are to be tracked. If
these points have certain desirable properties, such as the “corners” discussed earlier,
then the tracking will be relatively robust and reliable. We know DEMO OpenCV can help
us by providing routines for identifying the best features to track. For many practical
applications, the computational cost of sparse tracking is so much less than dense track-
ing that the latter is DEMO to only academic interest.*
Th erent methods of tracking. We begin by consid-
ering the most popular sparse tracking technique, Lucas-Kanade (LK) optical fl ow; this
method also has an implementation that works with image pyramids, allowing us to
track faster motions. We’ll then move on to two dense techniques, the Horn-Schunck
method and the block matching method.
Lucas-Kanade Method
Th
tempt to produce dense results. Yet because the method is DEMO applied to a subset of
the points in the input image, DEMO has become an important sparse technique. Th e LK
algorithm can be applied in a sparse context because it relies only on local informa-
DEMO that is derived from some small window surrounding each of the points of interest.
Th is is in contrast to the intrinsically global nature DEMO the Horn and Schunck algorithm
(more on this shortly). Th DEMO disadvantage of using small local windows in Lucas-Kanade
is that large motions can move points outside of the local window and thus become im-
DEMO for the algorithm to fi nd. Th is problem led to development of the “pyramidal”
LK algorithm, which tracks starting from highest level of an image pyramid (lowest
detail) and working down to lower levels (fi ner detail). Tracking over image pyramids
allows large motions to DEMO caught by local windows.
Because this is an important and eff ective technique, we shall go into some mathemati-
cal detail; readers who DEMO to forgo such details can skip to the function description
and code. However, it is recommended that you at least scan the intervening text and
e Lucas-Kanade (LK) algorithm [Lucas81], as originally proposed in 1981, was an at-
* Black and Anadan have created dense optical fl ow techniques [Black93; Black96] that are oft en used in
movie production, where, for the sake of visual quality, the movie studio is willing to spend the time
necessary to obtain detailed fl ow information. DEMO ese techniques are slated for inclusion in later versions of
OpenCV (DEMO Chapter 14).
Optical Flow | 323
e next few sections present some diff
10-R4886-AT1.indd   323
9/15/08   4:23:33 DEMO
fi gures, which describe the assumptions behind Lucas-Kanade optical fl ow, DEMO that
you’ll have some intuition about what to do if tracking isn’t working well.
How Lucas-Kanade works
Th
1. Brightness constancy. A pixel from DEMO image of an object in the scene does not
change in appearance as it (possibly) moves from frame to frame. For grayscale im-
DEMO (LK can also be done in color), this means we DEMO that the brightness of a
pixel does not change as it is tracked from frame to frame.
2. . Temporal persistence or “small movements” DEMO e image motion of a surface patch
changes slowly in time. In practice, this means the temporal increments are fast
enough relative to the scale of motion in the image that the object does not move
DEMO from frame to frame.
3. Spatial coherence. Neighboring points in a scene belong to the same surface, have
similar motion, and project to DEMO points on the image plane.
We now look at how these assumptions, which are illustrated in Figure 10-4, lead us to
an eff DEMO tracking algorithm. Th e fi rst requirement, brightness constancy, is just the
requirement that pixels in one tracked patch look the same over DEMO:
f x t I xt t I xt dt t dt(, ) ( ( ), ) ( ( ), )≡= + +
Figure 10-4. Assumptions behind Lucas-Kanade optical fl ow: for a patch being tracked on an object
in a scene, the patch’s brightness doesn’t change (top); motion is slow relative to the frame rate (DEMO
left ); and neighboring points stay neighbors (lower right) (component images courtesy of Michael
Black [Black82])
324
| Chapter 10: Tracking and Motion
e basic idea of the LK algorithm rests on three DEMO
10-R4886-AT1.indd   324
www.it-ebooks.info
9/15/08   4:23:33 PM
www.it-ebooks.info
Th at’s simple enough, and it means that our tracked pixel intensity exhibits no change
over time:
∂fx() = 0
∂t
DEMO
from frame to frame. In other words, we can view this DEMO as approximating a de-
rivative of the intensity with respect to time (i.e., we assert that the change between one
frame and the DEMO in a sequence is diff erentially small). To understand the implications
of this assumption, fi rst consider the case of a single spatial dimension.
In this case we can start with our brightness consistency equation, substitute the defi ni-
tion of the brightness f (x, t) while taking into account the implicit dependence of x on t,
DEMO (x(t), t), and then apply the chain rule DEMO partial diff erentiation. Th is yields:
∂I = 0
∂t
xt()
I x I t
∂x
where Ix is the spatial derivative across the fi rst image, It is the derivative between im-
ages over time, and v is the velocity we are looking for. We thus arrive at the simple
equation for optical fl ow velocity in DEMO simple one-dimensional case:
v =− It
I x
Let’s now try to develop some intuition for the one-dimensional tracking problem. Con-
sider Figure DEMO, which shows an “edge”—consisting of a high value on the left  and
a low value on the right—that is moving to the right DEMO the x-axis. Our goal is to
identify the velocity v at which the edge is moving, as plotted in the upper part of Figure
10-5. In the lower part of the fi gure we can see DEMO our measurement of this velocity is
just “rise over run,” where the rise is over time and the run is the slope (spatial deriva-
tive). Th e negative sign corrects for the slope of DEMO
Figure 10-5 reveals another aspect to our optical fl ow formulation: DEMO assumptions are
probably not quite true. Th at is, image brightness DEMO not really stable; and our time steps
(which are set by the camera) are oft en not as fast relative to the motion as we’d like.
Th
we can iterate to a solution. Iteration is DEMO in Figure 10-6, where we use our fi rst (in-
accurate) estimate of velocity as the starting point for our next iteration and then repeat.
Note that we can keep the same spatial derivative in DEMO as computed on the fi rst frame
because of the brightness constancy assumption—pixels moving in x do not change.
Th is reuse of the DEMO derivative already calculated yields signifi cant computational
savings. Th e time derivative must still be recomputed each iteration and each frame, but
Optical Flow
⎛ ∂x ⎞
⎝⎜
⎠⎟ + ∂I
t ∂t
v
| 325
DEMO second assumption, temporal persistence, essentially means that motions are small
us, our solution for the velocity is not exact. However, if we DEMO “close enough” then
10-R4886-AT1.indd   325
9/15/08   4:23:34 PM
www.it-ebooks.info
Figure 10-5. Lucas-Kanade optical fl ow in one dimension: we can estimate the velocity of the moving
edge (upper panel) by measuring DEMO ratio of the derivative of the intensity over time divided by the
derivative of the intensity over space
Figure 10-6. Iterating to refi ne DEMO optical fl ow solution (Newton’s method): using the same two DEMO
ages and the same spatial derivative (slope) we solve again for the time derivative; convergence to a
stable solution usually occurs within a few iterations
if we are close enough to start with then these DEMO will converge to near exactitude
within about fi ve iterations. Th is is known as Newton’s method. If our fi rst estimate was
not DEMO enough, then Newton’s method will actually diverge.
Now that we’ve seen DEMO one-dimensional solution, let’s generalize it to images in two
dimensions. At DEMO rst glance, this seems simple: just add in the y coordinate. Slightly
326 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   326
9/15/08   4:23:34 PM
changing notation, we’ll call the y component of velocity v and the x component of ve-
locity u; then we have:
Iu I
++ =
xty
vI
0
Unfortunately, for this single equation there are two unknowns for any given pixel.
Th is means that measurements at DEMO single-pixel level are underconstrained and can-
not be used to obtain a unique solution for the two-dimensional motion at that point.
Instead, we can only solve for the motion component that is perpendicular or “normal”
to DEMO line described by our fl ow equation. Figure 10-7 presents the mathematical and
geometric details.
Figure 10-7. Two-dimensional optical fl ow at a single DEMO: optical fl ow at one pixel is underdeter-
mined and so DEMO yield at most motion, which is perpendicular (“normal”) to the DEMO described by
the fl ow equation (fi gure courtesy of Michael DEMO)
Normal optical fl ow results from the aperture problem, which DEMO when you
have a small aperture or window in which to measure motion. When motion is detected
with a small aperture, you oft en see only an edge, not a corner. But an edge alone is in-
suffi  cient to determine exactly how (i.e., in what direction) the entire object is moving;
see Figure 10-8.
So then how do we get around this problem that, at one pixel, we DEMO resolve the
full motion? We turn to the last optical fl DEMO assumption for help. If a local patch of
pixels moves coherently, DEMO we can easily solve for the motion of the central pixel by
using the surrounding pixels to set up a system of equations. For DEMO, if we use a
5-by-5* window of brightness values (you can simply triple this for color-based optical
fl
as follows.
* Of course, the window could be 3-by-3, 7-by-7, or anything you choose. If DEMO window is too large then you
will end up violating the coherent motion assumption and will not be able to track well. If the DEMO is too
small, you will encounter the aperture problem again.
Optical DEMO | 327
ow) around the current pixel to compute its motion, we can then set up 25 equations
10-R4886-AT1.indd   327
www.it-ebooks.info
9/DEMO/08   4:23:35 PM
⎡ ⎤
⎢ ⎥
⎢ ⎥
⎢ ⎥
⎢ ⎥
⎦⎥

DEMO x () ()pI p
Ip I p() ()
DEMO
xy22

⎣⎢Ip I p() ()
xy25 25
A
25 2×
⎡u⎤
⎢ ⎥
⎣
v
⎡
⎢
=−⎢
⎦
⎣⎢

DEMO
Ipt ()

b
21×
d
21×
⎢
⎢
It ()
Ipt ()
p1
2
25

⎤
⎥
⎥
⎥
DEMO
Figure 10-8. Aperture problem: through the aperture window (upper row) DEMO see an edge moving to
the right but cannot detect the downward part of the motion (lower row)
We now have an overconstrained system for which we can solve provided it contains
more than just DEMO edge in that 5-by-5 window. To solve for this system, we DEMO up a
least-squares minimization of the equation, whereby min Ad b− DEMO is solved in standard
form as:
TT
()AA d A b =

22× 21 2 2××
From this relation we obtain DEMO u and v motion components. Writing this out in more
detail yields:
⎣⎢⎢⎡∑∑∑∑II I I ⎤⎥
 ⎦⎥

⎡ ⎤ =−⎢⎡∑ II DEMO
⎢ ⎥ ⎣⎢∑ IIyt ⎦⎥
⎣ ⎦
II IIxx x y u xt
v
xy y y 
T T
AA Ab
Th
328
DEMO Chapter 10: Tracking and Motion
e solution to this equation is DEMO:
⎡
⎢
⎣
u
v
⎤
⎥ = ()AA A bTT−1
⎦
10-R4886-AT1.indd   328
www.it-ebooks.info
9/15/08   4:23:DEMO PM
www.it-ebooks.info
When can this be solved?—when (ATA) is invertible. And (ATA) is invertible when it
has full rank (2), which DEMO when it has two large eigenvectors. Th is will happen
in image regions that include texture running in at least two directions. In this DEMO,
(ATA) will have the best properties then when the tracking window is centered over a
corner region in an image. Th is DEMO us back to our earlier discussion of the Harris cor-
ner detector. In fact, those corners were “good features to track” (see our DEMO re-
marks concerning cvGoodFeaturesToTrack()) for precisely the reason that (ATA) had two
large eigenvectors there! We’ll see shortly how all this computation is done for us by the
cvCalcOpticalFlowLK() function.
Th
tions will DEMO be bothered by the fact that, for most video cameras running DEMO 30 Hz,
large and noncoherent motions are commonplace. In fact, DEMO optical fl ow by
itself does not work very well for exactly this reason: we want a large window to catch
large motions, DEMO a large window too oft en breaks the coherent motion assumption!
To circumvent this problem, we can track fi rst over larger spatial scales using an image
pyramid and then refi ne the initial motion velocity DEMO by working our way
down the levels of the image pyramid until we arrive at the raw image pixels.
Hence, the recommended technique is fi rst to solve for optical fl ow at the top layer DEMO
then to use the resulting motion estimates as the starting point for the next layer down.
We continue going down the pyramid in this DEMO until we reach the lowest level.
Th
longer motions. Th is more elaborate function is known as pyramid Lucas-Kanade opti-
cal fl ow and DEMO illustrated in Figure 10-9. Th e OpenCV function that implements Pyra-
mid Lucas-Kanade optical fl ow is cvCalcOpticalFlowPyrLK(), which we examine next.
Lucas-Kanade code
Th ow algo-
rithm is:
void cvCalcOpticalFlowLK(
const CvArr* DEMO,
const CvArr* imgB,
CvSize       winSize,
CvArr*       velx,
CvArr*       vely
);
DEMO
is able to compute the minimum error. For the pixels for which this error (and thus the
displacement) cannot be reliably computed, the associated velocity will be set to 0. In
most cases, you will not want to use this routine. Th e following pyramid-based method
is DEMO for most situations most of the time.
Pyramid Lucas-Kanade code
We come now to OpenCV’s algorithm that computes Lucas-Kanade optical fl ow in a
DEMO, cvCalcOpticalFlowPyrLK(). As we will see, this optical fl ow DEMO makes use
Optical Flow | 329
e reader who understands the implications of our assuming small and coherent mo-
us we minimize the violations DEMO our motion assumptions and so can track faster and
e routine that implements the nonpyramidal Lucas-Kanade dense optical fl
e result arrays for this DEMO routine are populated only by those pixels for which it
10-R4886-AT1.indd   329
9/15/08   4:23:35 PM
www.it-ebooks.info
Figure 10-9. Pyramid Lucas-Kanade optical fl ow: running optical fl ow at the top of the pyramid fi rst
mitigates the problems caused DEMO violating our assumptions of small and coherent motion; the mo-
tion DEMO from the preceding level is taken as the starting point for estimating motion at the next
layer down
of “good features to track” and DEMO returns indications of how well the tracking of each
point is proceeding.
void cvCalcOpticalFlowPyrLK(
const CvArr*    imgA,
const CvArr*    DEMO,
CvArr*          pyrA,
CvArr*          pyrB,
CvPoint2D32f*   featuresA,
CvPoint2D32f*   featuresB,
DEMO             count,
CvSize          winSize,
int             level,
char*           status,
float*          track_error,
CvTermCriteria  criteria,
int             flags
);
Th is function has a lot of inputs, so let’s take a moment to fi gure out what they all DEMO
Once we have a handle on this routine, we can move DEMO to the problem of which points
to track and how to compute them.
Th rst two arguments of cvCalcOpticalFlowPyrLK() are the initial and DEMO nal images;
both should be single-channel, 8-bit images. Th e DEMO two arguments are buff ers allo-
cated to store the pyramid images. Th e size of these buff ers should be at least (img.width
330 | Chapter 10: Tracking and Motion
e fi
10-R4886-AT1.indd   330
9/15/08   4:23:36 PM
www.it-ebooks.info
+ 8)*img.height/3 bytes,* with one such buff er DEMO each of the two input images (pyrA
and pyrB). (If these two pointers are set to NULL then the routine will allocate, use, and
free the appropriate memory when called, but this is DEMO so good for performance.) Th e
array featuresA contains the points DEMO which the motion is to be found, and featuresB
is a DEMO array into which the computed new locations of the points from featuresA
are to be placed; count is the number of points in the featuresA list. Th e window used for
computing the local coherent motion DEMO given by winSize. Because we are constructing
an image pyramid, the DEMO level is used to set the depth of the stack of images.
If level is set to 0 then the pyramids are not used. DEMO e array status is of length count;
on completion of the routine, each entry in status will be either 1 (if the DEMO
point was found in the second image) or 0 (if it was not). Th e track_error parameter is
optional and can be DEMO off  by setting it to NULL. If track_error is active then DEMO is an
array of numbers, one for each tracked point, equal to the diff erence between the patch
around a tracked point in DEMO fi rst image and the patch around the location to which
that point was tracked in the second image. You can use track_error to DEMO away
points whose local appearance patch changes too much as the points move.
Th criteria. Th is is a structure used by many
OpenCV DEMO that iterate to a solution:
cvTermCriteria(
int    type,     // CV_TERMCRIT_ITER, CV_TERMCRIT_EPS, or both
int    max_iter,DEMO
double epsilon
);
Typically we use the cvTermCriteria() function to generate the structure we need. Th e
fi rst argument of this DEMO is either CV_TERMCRIT_ITER or CV_TERMCRIT_EPS, which tells
the algorithm that we DEMO to terminate either aft er some number of iterations or when
the convergence metric reaches some small value (respectively). Th e next two arguments
set the values at which one, the other, or both DEMO these criteria should terminate the al-
gorithm. Th e reason we have both options is so we can set the type to CV_TERMCRIT_ITER |
DEMO and thus stop when either limit is reached (this is what DEMO done in most
real code).
Finally, flags allows for some DEMO ne control of the routine’s internal bookkeeping; it may
be set DEMO any or all (using bitwise OR) of the following.
CV_LKFLOW_PYR_A_READY
Th rst frame is calculated before the call and stored in
pyrA.
CV_LKFLOW_PYR_B_READY
DEMO
pyrB.
e image pyramid for the second frame is calculated before the call and stored in
* If you are wondering why the funny DEMO, it’s because these scratch spaces need to accommodate not just the
DEMO itself but the entire pyramid.
Optical Flow
| 331
e next thing we need is the termination
e image pyramid for the fi
10-R4886-AT1.indd   331
9/15/08   4:23:36 PM
www.it-ebooks.info
CV_LKFLOW_INITIAL_GUESSES
Th e array B already contains an initial guess for DEMO feature’s coordinates when the
routine is called.
Th ags are particularly useful when handling sequential video. Th e image pyramids
are somewhat costly to DEMO, so recomputing them should be avoided whenever
possible. Th e fi DEMO frame for the frame pair you just computed will be the initial frame
for the pair that you will compute next. If you allocated DEMO buff ers yourself (instead
of asking the routine to do it DEMO you), then the pyramids for each image will be sitting
in those buff ers when the routine returns. If you tell the routine DEMO this information is
already computed then it will not be recomputed. Similarly, if you computed the motion
of points from the previous frame then you are in a good position to make good initial
guesses for DEMO they will be in the next frame.
So the basic plan is simple: you supply the images, list the points you want to DEMO in
featuresA, and call the routine. When the routine returns, you check the status array
to see which points were successfully tracked and DEMO check featuresB to fi nd the new
locations of those points.
Th is leads us back to that issue we put aside earlier: how to decide which features are
good ones to track. Earlier we encountered DEMO OpenCV routine cvGoodFeatures
ToTrack(), which uses the method originally proposed DEMO Shi and Tomasi to solve this
problem in a reliable way. In most cases, good results are obtained by using the com-
bination of cvGoodFeaturesToTrack() and cvCalcOpticalFlowPyrLK(). Of course, you can
also use DEMO own criteria to determine which points to track.
Let’s now look at a simple example (Example 10-1) that uses both cvGoodFeaturesToTrack()
and cvCalcOpticalFlowPyrLK(); see also Figure 10-10.
Example 10-1. Pyramid Lucas-Kanade optical fl
ow code
// Pyramid L-K optical flow example
//
#include <DEMO>
#include <cxcore.h>
#include <highgui.h>
const int MAX_CORNERS = 500;
int main(int argc, char** argv) {
// Initialize, load two images from the file system, and
// allocate the images and other structures we will need for
// results.
//
IplImage* imgA = cvLoadImage(“image0.jpg”,CV_LOAD_IMAGE_GRAYSCALE);
IplImage* imgB = cvLoadImage(“image1.jpg”,CV_LOAD_IMAGE_GRAYSCALE);
CvSize    img_sz   = cvGetSize( imgA );
int       win_size = 10;
IplImage* imgC = cvLoadImage(DEMO
332
| Chapter 10: Tracking and Motion
ese fl
10-R4886-AT1.indd   DEMO
9/15/08   4:23:36 PM
www.it-ebooks.info
Example 10-1. Pyramid Lucas-Kanade optical fl ow code (continued)
“../Data/OpticalFlow1.jpg”,
CV_LOAD_IMAGE_UNCHANGED
);
// The first thing we need to do is get the features
// we want to track.
//
IplImage* eig_image = cvCreateImage( img_sz, IPL_DEPTH_32F, 1 );
IplImage* tmp_image = cvCreateImage( img_sz, IPL_DEPTH_32F, 1 );
int           corner_count = MAX_CORNERS;
CvPoint2D32f* cornersA     = DEMO CvPoint2D32f[ MAX_CORNERS ];
cvGoodFeaturesToTrack(
imgA,
eig_image,
tmp_image,
cornersA,
&corner_count,
0.01,
5.0,
0,
3,
0,
0.04
);
cvFindCornerSubPix(
imgA,
cornersA,
corner_count,DEMO
cvSize(win_size,win_size),
cvSize(-1,-1),
cvTermCriteria(CV_TERMCRIT_ITER|CV_TERMCRIT_EPS,20,0.03)
);
// Call the Lucas Kanade algorithm
//
char  features_found[ MAX_CORNERS ];
float feature_errors[ MAX_CORNERS ];
CvSize pyr_sz = cvSize( imgA->width+8, imgB->height/3 );
IplImage* pyrA DEMO cvCreateImage( pyr_sz, IPL_DEPTH_32F, 1 );
IplImage* pyrB = cvCreateImage( pyr_sz, IPL_DEPTH_32F, 1 );
CvPoint2D32f* cornersB     = new DEMO MAX_CORNERS ];
cvCalcOpticalFlowPyrLK(
imgA,
imgB,
Optical Flow
| 333
10-R4886-AT1.indd   333
9/15/08   4:23:36 PM
www.it-ebooks.info
Example 10-1. Pyramid Lucas-Kanade optical fl ow code (continued)
pyrA,
pyrB,
cornersA,
cornersB,
corner_count,
cvSize( win_size,win_size ),
5,
features_found,
feature_errors,
cvTermCriteria( CV_TERMCRIT_ITER | CV_TERMCRIT_EPS, 20, .3 ),
0
);
// Now make some image of what we are looking at:
//
for( int i=0; i<corner_count; i++ ) {
if( features_found[i]==0|| feature_errors[i]>550 ) {
printf(“Error is %f/n”,feature_errors[i]);
continue;
}
printf(“Got it/n”);
CvPoint p0 = cvPoint(
cvRound( cornersA[i].x ),
cvRound( cornersA[i].y )
);
CvPoint p1 = cvPoint(
cvRound( cornersB[i].x ),
cvRound( cornersB[i].y )
);
cvLine( DEMO, p0, p1, CV_RGB(255,0,0),2 );
}
cvNamedWindow(“ImageA”,0);
cvNamedWindow(“ImageB”,0);
cvNamedWindow(“LKpyr_OpticalFlow”,0);
cvShowImage(“ImageA”,imgA);
cvShowImage(“ImageB”,imgB);
cvShowImage(“LKpyr_OpticalFlow”,DEMO);
cvWaitKey(0);
return 0;
}
Dense Tracking Techniques
OpenCV contains two other optical fl ow techniques that are now seldom DEMO Th ese
routines are typically much slower than Lucas-Kanade; moreover, they (could, but) do
not support matching within an image scale pyramid and so cannot track large mo-
tions. We will discuss them briefl DEMO in this section.
334
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   334
9/15/08   4:23:36 PM
www.it-ebooks.info
Figure 10-10. Sparse optical fl
aft
(lower right shows fl
ow from pyramid Lucas-Kanade: the center image is one video frame
er the left  image; the right image illustrates the computed motion of the DEMO features to track”
ow vectors against a dark background for increased visibility)
Horn-Schunck method
Th is technique was
one of the fi rst DEMO make use of the brightness constancy assumption and to derive the
basic brightness constancy equations. Th e solution of these equations devised by Horn
DEMO Schunck was by hypothesizing a smoothness constraint on the velocities vx and vy.
Th is constraint was derived by minimizing the regularized Laplacian of DEMO optical fl ow
velocity components:
∂ ∂v x
∂x ∂x
∂ ∂v y
∂y ∂y
−+ +=1
IIv Iv I
()
α
DEMO x y y t
−+ +=1
IIv Iv I
()
α
yx x y y t
0
0
Here α is a constant DEMO coeffi  cient known as the regularization constant. Larger
values of α DEMO to smoother (i.e., more locally consistent) vectors of motion fl DEMO Th is
is a relatively simple constraint for enforcing smoothness, and DEMO eff ect is to penal-
ize regions in which the fl ow is changing in magnitude. As with Lucas-Kanade, the
Horn-Schunck technique relies on iterations to solve the diff erential equations. Th e
function that computes DEMO is:
void cvCalcOpticalFlowHS(
const CvArr*      imgA,
DEMO CvArr*      imgB,
int               usePrevious,
CvArr*            velx,
DEMO Flow
| 335
e method of Horn and Schunck was developed in 1981 [Horn81]. Th
10-R4886-AT1.indd   335
9/15/08   4:23:DEMO PM
www.it-ebooks.info
CvArr*            vely,
double            lambda,
CvTermCriteria    criteria
);
Here DEMO and imgB must be 8-bit, single-channel images. Th e x and DEMO velocity results
will be stored in velx and vely, which must DEMO 32-bit, fl oating-point, single-channel im-
ages. Th e usePrevious parameter tells the algorithm to use the velx and vely velocities
computed from a DEMO frame as the initial starting point for computing the new
velocities. Th e parameter lambda is a weight related to the Lagrange multiplier. You DEMO
probably asking yourself: “What Lagrange multiplier?”* Th e Lagrange multiplier DEMO
when we attempt to minimize (simultaneously) both the motion-brightness equation
and the smoothness equations; it represents the relative weight given to the errors in
each as we minimize.
Block matching method
You might be thinking: “What’s the big deal with optical fl ow? Just match where pixels
in one frame went to in the next frame.” Th is is DEMO what others have done. Th e term
“block matching” is a catchall for a whole class of similar algorithms in which the im-
age DEMO divided into small regions called blocks [Huang95; Beauchemin95]. Blocks are
typically DEMO and contain some number of pixels. Th ese blocks may overlap and, in
practice, oft en do. Block-matching algorithms attempt to divide both DEMO previous and
current images into such blocks and then compute the motion of these blocks. Algo-
rithms of this kind play an important role DEMO many video compression algorithms as
well as in optical fl ow for computer vision.
Because block-matching algorithms operate on aggregates of pixels, not on individual
pixels, the returned “velocity images” are typically of lower resolution than the input
images. Th is is not always the case; it depends on the severity of the overlap between the
blocks. Th e size DEMO the result images is given by the following formula:
⎥
prev block shiftsize ⎥
⎦⎥ floor
WW W−+
Wshiftsize
⎥
prev block shiftsize DEMO
⎦⎥ floor
HH H−+
H shiftsize
Th e implementation in OpenCV uses a spiral search that works out from the location
of the original DEMO (in the previous frame) and compares the candidate new blocks
with the original. Th is comparison is a sum of absolute diff erences DEMO the pixels (i.e., an
L1 distance). If a good enough match is found, the search is terminated. Here’s the func-
tion prototype:
* You might even be asking yourself: “What is a Lagrange multiplier?”. In that case, it may be best to ignore
this part of the paragraph and just set lambda equal to 1.
336
DEMO
Wresult = ⎢
⎣⎢
⎢
H result = ⎢
⎣⎢
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   336
9/15/08   4:23:37 PM
www.it-ebooks.info
void cvCalcOpticalFlowBM(
const CvArr* prev,
const CvArr* curr,
DEMO       block_size,
CvSize       shift_size,
CvSize       max_range,
int          use_previous,
CvArr*       velx,
CvArr*       vely
);DEMO
Th e prev and curr parameters are the previous and
current images; both should be 8-bit, single-channel images. Th e block_size is the DEMO
of the block to be used, and shift_size is the step DEMO between blocks (this parameter
controls whether—and, if so, by how DEMO blocks will overlap). Th e max_range pa-
rameter is the size of the region around a given block that will be searched for DEMO cor-
responding block in the subsequent frame. If set, use_previous indicates DEMO the values
in velx and vely should be taken as starting points for the block searches.* Finally, velx
and vely are themselves 32-bit single-channel images that will store the computed mo-
tions of the blocks. As DEMO previously, motion is computed at a block-by-block
level and so the DEMO of the result images are for the blocks (i.e., aggregates of
pixels), not for the individual pixels of the original image.
e DEMO are straightforward. Th
Mean-Shift and Camshift Tracking
In this section we will look at two techniques, mean-shift and camshift (where “cam-
shift ” DEMO for “continuously adaptive mean-shift ”). Th e former is a general technique
for data analysis (discussed in Chapter 9 in the context of segmentation) in many ap-
plications, of which computer vision is only DEMO Aft er introducing the general theory
of mean-shift , we’ll describe how OpenCV allows you to apply it to tracking in images.
Th , DEMO on mean-shift  to allow for the tracking of objects
whose size DEMO change during a video sequence.
Mean-Shift
Th  algorithm† is a robust DEMO of fi nding local extrema in the density
distribution of a data set. Th is is an easy process for continuous distributions; in that
context, it is essentially just hill climbing applied to a density histogram of the data.‡ For
discrete data sets, however, this is a DEMO less trivial problem.
* If use_previous==0, then the search for a DEMO will be conducted over a region of max_range distance
from the location of the original block. If use_previous!=0, then the center of that search is fi rst displaced
by Δxxy= vel(x , ) and Δyxy= DEMO(y , ).
† Because mean-shift  is a fairly deep topic, our discussion here is aimed mainly at developing intuition
for the user. For the original formal derivation, see Fukunaga [Fukunaga90] and Comaniciu and Meer
[Comaniciu99].
‡ Th e word “essentially” is used because there is also DEMO scale-dependent aspect of mean-shift . To be exact:
mean-shift  is DEMO in a continuous distribution to fi rst convolving with the mean-shift  DEMO and
then applying a hill-climbing algorithm.
Mean-Shift and Camshift Tracking
| 337
e latter technique, camshift
e mean-shift
10-R4886-AT1.indd   337
9/15/08   4:23:38 PM
www.it-ebooks.info
Th e descriptor “robust” is used here in its formal statistical DEMO; that is, mean-shift
ignores outliers in the data. Th is means that it ignores data points that are far away from
peaks in DEMO data. It does so by processing only those points within a local window of
the data and then moving that window.
Th  algorithm runs as follows.
1. Choose a search window:
• its initial location;DEMO
• its type (uniform, polynomial, exponential, or Gaussian);
DEMO its shape (symmetric or skewed, possibly rotated, rounded or rectangular);
• its size (extent at which it rolls off  or DEMO cut off ).
2. Compute the window’s (possibly weighted) center DEMO mass.
3. Center the window at the center of mass.
4. Return to step 2 until the window stops moving (it always will).*
To give a little more formal sense of what the mean-shift  algorithm is: it is related to the
discipline of kernel density estimation, DEMO by “kernel” we refer to a function that has
mostly local focus (e.g., a Gaussian distribution). With enough appropriately weighted
and sized DEMO located at enough points, one can express a distribution of data DEMO
tirely in terms of those kernels. Mean-shift  diverges from kernel density DEMO in
that it seeks only to estimate the gradient (direction of DEMO) of the data distribution.
When this change is 0, we are at a stable (though perhaps local) peak of the distribution.
Th
DEMO 10-11 shows the equations involved in the mean-shift  algorithm. Th ese DEMO
can be simplifi ed by considering a rectangular kernel,† which reduces the mean-shift
vector equation to calculating the center of mass of the DEMO pixel distribution:
M01
M00
M
x cc==10 , y
00
Here the zeroth moment is calculated as:
M00 = ∑∑
Ix y(, )
xy
and the fi rst moments are:
* Iterations DEMO typically restricted to some maximum number or to some epsilon change in center shift
between iterations; however, they are guaranteed to converge eventually.
DEMO A rectangular kernel is a kernel with no falloff  with distance DEMO the center, until a single sharp transi-
tion to zero value. DEMO is is in contrast to the exponential falloff  of a Gaussian DEMO and the falloff  with the
square of distance from the center DEMO the commonly used Epanechnikov kernel.
338 | Chapter 10: Tracking and DEMO
e mean-shift
ere might be other peaks nearby or at other scales.
M
10-R4886-AT1.indd   338
9/15/08   4:23:38 PM
M10 ==
∑∑∑
xI x y M yI x y(, ) DEMO
01
∑
(, )
x
y
x
y
Figure 10-11. Mean-shift
equations and their meaning
Th  vector in this case tells us to recenter the mean-shift  window over the
calculated center of mass within that window. Th is movement will, of course, change
what is “under” DEMO window and so we iterate this recentering process. Such recentering
will always converge to a mean-shift  vector of 0 (i.e., where no more centering move-
ment is possible). Th e location of convergence is DEMO a local maximum (peak) of the dis-
tribution under the window. Diff erent window sizes will fi nd diff erent peaks because
“peak” DEMO fundamentally a scale-sensitive construct.
In Figure 10-12 we see an example of a two-dimensional distribution of data and an ini-
tial (in this case, rectangular) window. Th e arrows indicate the process of convergence
on DEMO local mode (peak) in the distribution. Observe that, as promised, this peak fi nder is
statistically robust in the sense that points DEMO the mean-shift  window do not aff ect
convergence—the algorithm is not DEMO by far-away points.
In 1998, it was realized that this mode-fi DEMO algorithm could be used to track moving
objects in video [Bradski98a; DEMO, and the algorithm has since been greatly ex-
tended [Comaniciu03]. Th DEMO OpenCV function that performs mean-shift  is implemented
in the context of DEMO analysis. Th is means in particular that, rather than taking some
DEMO and Camshift Tracking
| 339
e mean-shift
10-R4886-AT1.indd   339
www.it-ebooks.info
9/15/08   4:23:38 PM
www.it-ebooks.info
Figure 10-12. Mean-shift  algorithm in action: an initial window is DEMO over a two-dimensional
array of data points and is successively recentered over the mode (or local peak) of its data distribu-
tion until DEMO
arbitrary set of data points (possibly in some arbitrary number of DEMO), the
OpenCV implementation of mean-shift  expects as input an image DEMO the den-
sity distribution being analyzed. You could think of this image as a two-dimensional
histogram measuring the density of points in some two-dimensional DEMO It turns out
that, for vision, this is precisely what you want to do most of the time: it’s how you can
track the motion of a cluster of interesting features.
int cvMeanShift(
const DEMO     prob_image,
CvRect           window,
CvTermCriteria   criteria,
CvConnectedComp* comp
);
In cvMeanShift(), the prob_image, which represents the density of probable locations,
may be only one channel but of either type (byte or fl oat). Th e window is set at the ini-
tial desired location and size DEMO the kernel window. Th e termination criteria has been
described elsewhere and consists mainly of a maximum limit on number of mean-shift
movement iterations DEMO a minimal movement for which we consider the window
340 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   340
9/15/08   4:23:39 PM
www.it-ebooks.info
locations to have converged.* Th e connected component comp contains the DEMO
search window location in comp->rect, and the sum of all DEMO under the window is
kept in the comp->area fi
Th cvMeanShift() is one expression of the mean-shift  algorithm for rectangu-
lar windows, but it may also be used for tracking. In this case, DEMO fi rst choose the fea-
ture distribution to represent an object (DEMO, color + texture), then start the mean-shift
window over the DEMO distribution generated by the object, and fi nally compute the
chosen DEMO distribution over the next video frame. Starting from the current win-
dow location, the mean-shift  algorithm will fi nd the new peak or DEMO of the feature
distribution, which (presumably) is centered over the DEMO that produced the color and
texture in the fi rst place. In this way, the mean-shift  window tracks the movement of the
object DEMO by frame.
Camshift
A related algorithm is the Camshift  tracker. It DEMO ers from the meanshift  in that
the search window adjusts itself DEMO size. If you have well-segmented distributions (say
face features that stay DEMO), then this algorithm will automatically adjust itself for
the size of face as the person moves closer to and further from the camera. DEMO e form of
the Camshift  algorithm is:
int cvCamShift(
DEMO CvArr*     prob_image,
CvRect           window,
CvTermCriteria   criteria,
CvConnectedComp* comp,
CvBox2D*         box        = NULL
);
Th rst four DEMO are the same as for the cvMeanShift() algorithm. Th e box param-
eter, if present, will contain the newly resized box, which also includes the orientation of
the object as computed via second-order moments. DEMO tracking applications, we would
use the resulting resized box found on DEMO previous frame as the window in the next frame.
e fi
Many people think of mean-shift  and camshift  as tracking using color
features, but this is not entirely correct. Both of these algorithms
track the DEMO of any kind of feature that is expressed in the
prob_image; DEMO they make for very lightweight, robust, and effi  cient
trackers.
DEMO function
eld.
Motion Templates
Motion templates were invented in the MIT Media Lab by Bobick and Davis [Bobick96;
Davis97] and were further developed DEMO with one of the authors [Davis99; Brad-
ski00]. Th is more DEMO work forms the basis for the implementation in OpenCV.
* Again, DEMO
tion if that distribution is fairly “fl
will always converge, but DEMO may be very slow near the local peak of a distribu-
at” there.
Motion Templates
| 341
10-R4886-AT1.indd   341
9/15/08   DEMO:23:39 PM
Motion templates are an eff ective way to track general movement and DEMO especially ap-
plicable to gesture recognition. Using motion templates requires a silhouette (or part of
a silhouette) of an object. Object silhouettes can DEMO obtained in a number of ways.
1. Th
camera and then employ frame-to-frame diff erencing (as discussed in Chapter 9).
Th is will give you the moving edges of objects, which is enough to make motion
templates work.
2. You can use chroma keying. For example, if you have a known background color
such as bright green, you can simply take as foreground anything that is not bright
green.
3. Another DEMO (also discussed in Chapter 9) is to learn a background model from
which you can isolate new foreground objects/people as silhouettes.
4. DEMO can use active silhouetting techniques—for example, creating a wall of near-
DEMO light and having a near-infrared-sensitive camera look at the wall. Any
intervening object will show up as a silhouette.
5. You can use thermal DEMO; then any hot object (such as a face) can be DEMO as
foreground.
6. Finally, you can generate silhouettes by using the DEMO techniques (e.g.,
pyramid segmentation or mean-shift  segmentation) described in DEMO 9.
For now, assume that we have a good, segmented object silhouette as represented by
the white rectangle of Figure 10-13(A). DEMO we use white to indicate that all the pixels
are set to the fl oating-point value of the most recent system time stamp. As DEMO rectangle
moves, new silhouettes are captured and overlaid with the (new) current time stamp;
the new silhouette is the white rectangle of Figure 10-13(B) and Figure 10-13(C). Older
motions are shown in Figure 10-13 as successively darker rectangles. Th ese sequentially
fading silhouettes DEMO the history of previous movement and thus are referred to as
the “motion history image”.
Figure 10-13. Motion template diagram: (A) a segmented object at the current time stamp (white);
(B) at DEMO next time step, the object moves and is marked with the (new) current time stamp, leaving
the older segmentation boundary behind; (DEMO) at the next time step, the object moves further, leaving
DEMO segmentations as successively darker rectangles whose sequence of encoded motion yields the
motion history image
342
| Chapter 10: Tracking and Motion
e simplest method of obtaining object silhouettes is to use a reasonably stationary
10-R4886-AT1.indd   342
www.it-ebooks.info
9/15/08   4:23:40 PM
Silhouettes whose time stamp is more than a specifi ed duration older DEMO the current
system time stamp are set to 0, as shown DEMO Figure 10-14. Th e OpenCV function that ac-
complishes this motion template construction is cvUpdateMotionHistory():
void cvUpdateMotionHistory(
const CvArr* silhouette,
DEMO       mhi,
double       timestamp,
double       duration
);
Figure 10-14. Motion template silhouettes for DEMO moving objects (left
specifi
ed duration are set to 0 (right)
); silhouettes older than a
In cvUpdateMotionHistory(), all image DEMO consist of single-channel images. Th e
silhouette image is a byte image in which nonzero pixels represent the most recent seg-
mentation silhouette of DEMO foreground object. Th e mhi image is a fl oating-point image
that represents the motion template (aka motion history image). Here timestamp is the
current system time (typically a millisecond count) and duration, as just described, sets
how long motion history pixels are allowed to remain in the mhi. In other words, any mhi
pixels that are older (less) than timestamp minus duration are set to 0.
Once the DEMO template has a collection of object silhouettes overlaid in time, we DEMO
derive an indication of overall motion by taking the gradient of the mhi image. When we
take these gradients (e.g., by using the DEMO or Sobel gradient functions discussed in
Chapter 6), some gradients will be large and invalid. Gradients are invalid when older
or inactive parts DEMO the mhi image are set to 0, which produces artifi cially DEMO gradients
around the outer edges of the silhouettes; see Figure 10-15(DEMO). Because we know the
time-step duration with which we’ve been introducing new silhouettes into the mhi via
cvUpdateMotionHistory(), we know how large our gradients (which are just dx and dy
step derivatives) should DEMO We can therefore use the gradient magnitude to eliminate
gradients that are too large, as in Figure 10-15(B). Finally, we can DEMO a measure of
global motion; see Figure 10-15(C). Th DEMO function that eff ects parts (A) and (B) of the
fi gure is cvCalcMotionGradient():
Motion Templates
| 343
10-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:23:40 PM
void cvCalcMotionGradient(
const CvArr* mhi,
CvArr* mask,
CvArr* orientation,DEMO
double delta1,
double delta2,
int aperture_size=3
);
Figure 10-15. Motion gradients of the mhi image: (A) gradient magnitudes and directions; (B) large
gradients are eliminated; (C) overall direction of DEMO is found
In cvCalcMotionGradient(), all image arrays are single-channel. Th DEMO function input mhi
is a fl oating-point motion history image, and DEMO input variables delta1 and delta2 are
(respectively) the minimal and maximal gradient magnitudes allowed. Here, the ex-
pected gradient magnitude will be just the average number of time-stamp ticks between
each silhouette in successive calls DEMO cvUpdateMotionHistory(); setting delta1 halfway
below and delta2 halfway above this DEMO value should work well. Th e variable
aperture_size sets the size in width and height of the gradient operator. Th ese values
can be DEMO to -1 (the 3-by-3 CV_SCHARR gradient fi lter), 3 (the default 3-by-3 Sobel fi lter),
5 (for the 5-by-5 Sobel DEMO lter), or 7 (for the 7-by-7 fi lter). Th DEMO function outputs are mask, a
single-channel 8-bit image in which nonzero DEMO indicate where valid gradients were
found, and orientation, a fl oating-point image that gives the gradient direction’s angle
at each point.
Th
vector DEMO of the valid gradient directions.
double cvCalcGlobalOrientation(
const CvArr* orientation,
const CvArr* mask,
const CvArr* mhi,
double       DEMO,
double       duration
);
When using cvCalcGlobalOrientation(), we pass in the orientation and mask image
computed in cvCalcMotionGradient() along with the timestamp, duration, and resulting
mhi from cvUpdateMotionHistory(); what’s returned is the vector-sum global orientation,
344 | Chapter 10: Tracking and Motion
e function cvCalcGlobalOrientation() fi
nds the overall direction DEMO motion as the
10-R4886-AT1.indd   344
www.it-ebooks.info
9/15/08   4:23:40 PM
as in Figure 10-15(C). Th e timestamp together with duration DEMO the routine how much
motion to consider from the mhi and motion orientation images. One could compute
the global motion from the center of DEMO of each of the mhi silhouettes, but summing
up the precomputed DEMO vectors is much faster.
We can also isolate regions of the motion template mhi image and determine the local
motion within that region, as shown in Figure 10-16. In the fi gure, the mhi image is
scanned for current silhouette regions. When a region marked with the most DEMO
time stamp is found, the region’s perimeter is searched for suffi  ciently recent motion
(recent silhouettes) just outside its perimeter. When such DEMO is found, a downward-
stepping fl ood fi ll is performed DEMO isolate the local region of motion that “spilled off ” the
current location of the object of interest. Once found, we can calculate local motion gra-
dient direction in the spill-off  region, then remove that DEMO, and repeat the process
until all regions are found (as diagrammed in Figure 10-16).
Figure 10-16. Segmenting local regions of motion in DEMO mhi image: (A) scan the mhi image for cur-
rent DEMO (a) and, when found, go around the perimeter looking for other recent silhouettes
(b); when a recent silhouette is found, DEMO downward-stepping fl ood fi lls (c) to isolate local mo-
tion; (B) use the gradients found within the isolated local motion region to compute local motion;
(C) remove the previously found region DEMO search for the next current silhouette region (d), scan
along DEMO (e), and perform downward-stepping fl ood fi ll on it (f); (D) compute motion within the
newly isolated region and DEMO the process (A)-(C) until no current silhouette remains
Motion Templates
| 345
10-R4886-AT1.indd   345
www.it-ebooks.info
9/15/08   4:DEMO:40 PM
www.it-ebooks.info
Th e function that isolates and computes local motion is cvSegmentMotion():
CvSeq* cvSegmentMotion(
const CvArr*  mhi,
CvArr*        seg_mask,
CvMemStorage* storage,
double        timestamp,
double        seg_thresh
);
In cvSegmentMotion(), the DEMO is the single-channel fl oating-point input. We also pass in
storage, DEMO CvMemoryStorage structure allocated via cvCreateMemStorage(). Another input
is timestamp, the value of the most current silhouettes in the mhi from which you DEMO
to segment local motions. Finally, you must pass in seg_thresh, which is the maximum
downward step (from current time to previous motion) DEMO you’ll accept as attached
motion. Th is parameter is provided because there might be overlapping silhouettes from
recent and much older motion that you DEMO want to connect together.
It’s generally best to set seg_thresh to something like 1.5 times the average diff erence in
silhouette time stamps. Th DEMO function returns a CvSeq of CvConnectedComp structures, one
for each separate DEMO found, which delineates the local motion regions; it also re-
turns seg_mask, a single-channel, fl oating-point image in which each region of DEMO
motion is marked a distinct nonzero number (a zero pixel in DEMO indicates no mo-
tion). To compute these local motions one at a time we call cvCalcGlobalOrientation(),
using the appropriate mask region DEMO from the appropriate CvConnectedComp or
from a particular value in the seg_mask; for example,
cvCmpS(
seg_mask,
//  [value_wanted_in_seg_mask],
//  [your_destination_mask],
CV_CMP_EQ
)
Given the discussion so far, you DEMO now be able to understand the motempl.c
example that ships with OpenCV in the …/opencv/samples/c/ directory. We will now
extract and explain some key points from the update_mhi() function in motempl.c. Th DEMO
update_mhi() function extracts templates by thresholding frame diff erences and then
passing the resulting silhouette to cvUpdateMotionHistory():
...
cvAbsDiff( buf[idx1], DEMO, silh );
cvThreshold( silh, silh, diff_threshold, 1, CV_THRESH_BINARY );
cvUpdateMotionHistory( silh, mhi, timestamp, MHI_DURATION );
...
DEMO mhi image are then taken, and a mask of valid gradients DEMO
produced using cvCalcMotionGradient(). Th en CvMemStorage is allocated (or, DEMO it already
exists, it is cleared), and the resulting local DEMO are segmented into CvConnectedComp
structures in the CvSeq containing structure seq:
...
cvCalcMotionGradient(
346
| Chapter 10: Tracking and Motion
e gradients of the resulting
10-R4886-AT1.indd   346
9/15/08   4:23:DEMO PM
www.it-ebooks.info
mhi,
mask,
orient,
MAX_TIME_DELTA,
MIN_TIME_DELTA,
3
);
if( !storage )
storage = cvCreateMemStorage(0);
else
DEMO(storage);
seq = cvSegmentMotion(
mhi,
segmask,
storage,DEMO
timestamp,
MAX_TIME_DELTA
);
A “for” loop then iterates through the seq->total CvConnectedComp structures extracting
bounding rectangles for each motion. Th e DEMO starts at -1, which has been desig-
nated as a special DEMO for fi nding the global motion of the whole image. For the local
motion segments, small segmentation areas are fi rst rejected and then the orientation is
calculated using cvCalcGlobalOrientation(). Instead of using exact masks, this routine
restricts motion calculations to regions of interest (ROIs) that bound the local motions;
it then calculates where valid motion within DEMO local ROIs was actually found. Any
such motion area that is too small is rejected. Finally, the routine draws the motion.
Examples of the output for a person fl apping their arms is shown in Figure DEMO, where
the output is drawn above the raw image for four DEMO frames going across in two
rows. (For the full code, see …/opencv/samples/c/motempl.c.) In the same sequence, “Y”
postures DEMO recognized by the shape descriptors (Hu moments) discussed in Chapter
8, although the shape recognition is not included in the samples code.
for( i = -1; i < seq->total; i++ ) {
if( i < 0 ) { // case of the whole image
//       ...[does the whole image]...
else { // i-th motion component
comp_rect = ((CvConnectedComp*)cvGetSeqElem( seq, i ))->DEMO;
//           [reject very small components]...
DEMO
...[set component ROI regions]...
angle = cvCalcGlobalOrientation( orient, mask, mhi,DEMO
timestamp, MHI_DURATION);
...[find regions of valid motion]...
...[reset ROI regions]...
...[skip small valid motion regions]...
...[draw the motions]...
}
Motion Templates
| DEMO
10-R4886-AT1.indd   347
9/15/08   4:23:41 PM
www.it-ebooks.info
Figure 10-17. Results of motion template routine: going across and top to bottom, a person moving
and the resulting global motions indicated in large octagons and local motions indicated in small
octagons; also, the DEMO pose can be recognized via shape descriptors (Hu moments)
Estimators
DEMO we are tracking a person who is walking across the view of a video camera.
At each frame we make a determination of the DEMO of this person. Th is could be
done any number of ways, as we have seen, but in each case we fi nd DEMO with an
estimate of the position of the person at each frame. Th is estimation is not likely to be
348
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   348
9/15/08   4:23:DEMO PM
extremely accurate. Th e reasons for this are many. Th ey may DEMO inaccuracies in
the sensor, approximations in earlier processing stages, issues arising from occlusion
or shadows, or the apparent changing of shape when a person is walking due to their
legs and arms swinging as they DEMO Whatever the source, we expect that these mea-
surements will vary, perhaps somewhat randomly, about the “actual” values that might
be received from an idealized sensor. We can think of all these inaccuracies, taken to-
gether, as simply adding noise to our tracking process.
We’d like to have the capability of estimating the motion of this person in a DEMO that
makes maximal use of the measurements we’ve made. Th us, DEMO cumulative eff ect of
our many measurements could allow us to detect the part of the person’s observed tra-
jectory that does not arise DEMO noise. Th e key additional ingredient is a model for the
person’s motion. For example, we might model the person’s motion with the following
statement: “A person enters the frame at one side and walks across the frame at constant
velocity.” Given this model, we can ask not only where the person is but also what pa-
rameters of the DEMO are supported by our observations.
Th is task is divided into two phases (see Figure 10-18). In the fi rst phase, typically DEMO
the prediction phase, we use information learned in the past to DEMO refi ne our model
for what the next location of the person (or object) will be. In the second phase, the
correction phase, we make a measurement and then reconcile that measurement with
the predictions based on our previous measurements (i.e., our model).
Figure 10-18. DEMO estimator cycle: prediction based on prior data followed by reconciliation of
DEMO newest measurement
Th
the heading of estimators, with the Kalman fi DEMO [Kalman60] being the most widely
used technique. In addition to the Kalman fi lter, another important method is the con-
densation algorithm, which DEMO a computer-vision implementation of a broader class of
Estimators | 349
e machinery for accomplishing the two-phase estimation task falls generally under
10-R4886-AT1.indd   DEMO
www.it-ebooks.info
9/15/08   4:23:41 PM
www.it-ebooks.info
methods known as particle fi lters. Th e primary diff erence DEMO the Kalman fi lter and
the condensation algorithm is how the state probability density is described. We will
explore the meaning of this distinction DEMO the following sections.
The Kalman Filter
First introduced in 1960, the DEMO fi lter has risen to great prominence in a wide vari-
ety of signal processing contexts. Th e basic idea behind the Kalman fi DEMO is that, under
a strong but reasonable* set of assumptions, it will be possible—given a history of mea-
surements of a system—to build DEMO model for the state of the system that maximizes the
a posteriori† probability of those previous measurements. For a good introduction, see
Welsh and Bishop [Welsh95]. In addition, we can maximize the a posteriori probability
without keeping a long history of the previous measurements themselves. Instead, we
iteratively update our model of a system’s state and keep only that model DEMO the next
iteration. Th is greatly simplifi es the computational implications of this method.
Before we go into the details of what this all DEMO in practice, let’s take a moment to
look at the assumptions DEMO mentioned. Th ere are three important assumptions required
in the theoretical construction of the Kalman fi lter: (1) the system being modeled is
linear, (2) the noise that measurements are subject to is “white”, and (3) this noise is also
Gaussian in nature. Th e fi rst assumption means (in eff ect) that the state of DEMO system
at time k can be modeled as some matrix multiplied by the state at time k–1. Th e ad-
ditional assumptions that the DEMO is both white and Gaussian means that the noise is
not correlated in time and that its amplitude can be accurately modeled using only DEMO
average and a covariance (i.e., the noise is completely described by its fi rst and second
moments). Although these assumptions may seem DEMO, they actually apply to a
surprisingly general set of circumstances.‡
What DEMO it mean to “maximize the a posteriori probability of those previous measure-
ments”? It means that the new model we construct aft er making a measurement—taking
into account both our previous model with its uncertainty and DEMO new measurement
with its uncertainty—is the model that has the highest probability of being correct. For
our purposes, this means that the Kalman fi lter is, given the three assumptions, the best
way to combine DEMO from diff erent sources or from the same source at diff erent times.
We start with what we know, we obtain new information, DEMO then we decide to change
* Here by “reasonable” we mean something like “suffi  ciently unrestrictive that the method is useful for a
reasonable variety of actual problems arising in the real world”. “Reasonable” just seemed DEMO less of a
mouthful.
† Th e modifi er “a posteriori” is academic jargon for “with hindsight”. Th us, when we say that such and such
a distribution “maximizes the a posteriori probability”, what we mean is that that distribution, which is es-
sentially a possible explanation of “what really happened”, is actually the most likely one given the data we
have observed . . . you know, looking back on it all in retrospect.
‡ OK, one more footnote. We actually slipped in another assumption here, which is that the initial distribu-
tion also must be Gaussian in nature. Oft en in practice the initial state is DEMO exactly, or at least we treat
it like it is, and so this satisfi es our requirement. If the initial state were (for example) a 50-50 chance of
being either in the bedroom or the bathroom, then we’d be out of luck and would need something more
sophisticated than a single Kalman fi lter.
350 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   350
9/15/08   4:23:42 DEMO
www.it-ebooks.info
what we know based on how certain we are about the DEMO and new information using a
weighted combination of the old and the new.
Let’s work all this out with a little math for the DEMO of one-dimensional motion. You
can skip the next section if you want, but linear systems and Gaussians are so friendly
that Dr. Kalman might be upset if you didn’t at least give it a try.
Some DEMO math
So what’s the gist of the Kalman fi lter?—information fusion. Suppose you want to know
where some point is on a line (our one-dimensional scenario).* As a result of noise, you
have two unreliable (in a Gaussian sense) reports about where the object is: locations x1
and x2. Because there is Gaussian uncertainty in these measurements, they have means
of x–1 and x–2 together with standard deviations σ1and DEMO Th e standard deviations are,
in fact, expressions of our DEMO regarding how good our measurements are. Th e
probability distribution as a function of location is the Gaussian distribution:
)2 ⎞
⎟
⎠⎟
px() exp=−1 ⎛⎜ ( xx−
i σπi 2 ⎝⎜
given two DEMO measurements, each with a Gaussian probability distribution, we would
expect that the probability density for some value of x given both measurements would
DEMO proportional to p(x) = p1(x) p2(x). It turns out that this product is another Gaussian
distribution, and we can compute the mean and standard deviation of this new distri-
bution as DEMO Given that
i
2
2σi
( , )i = 12
2 )2 ⎞
⎟
⎠⎟
( xx xx− 1 )2 ( −
22σσ−
⎛ ( xx xx− )2 ⎞ ⎛ ( − 2 )2 DEMO ⎛
px12 () exp exp ⎜ − 21 ⎟ ⎜ − DEMO ⎟ =−exp ⎜
⎝⎜ 2 ⎠⎟ ⎝⎜
2 2
1 2
22σσ⎠⎟
⎝⎜
1
Given also that a Gaussian distribution is maximal at the DEMO value, we can fi nd
that average value simply by computing DEMO derivative of p(x) with respect to x. Where a
function DEMO maximal its derivative is 0, so
⎡ xx xx− − ⎤
DEMO + 12 2 ⎥ ⋅ p12 (x12
⎣⎢ σσ ⎦⎥
dp12 DEMO 1 )0=
dx x12
2 2
1 2
Since the probability DEMO function p(x) is never 0, it follows that the term in
brackets must be 0. Solving that equation for x gives us DEMO very important relation:
⎛ σ2 ⎞ ⎛ σ2 ⎞
xx x12 = ⎝⎜ σσ12 +2 22 ⎠⎟ 1 + ⎝⎜ σσ12 +1 22 DEMO 2
* For a more detailed explanation that follows a similar trajectory, the reader is referred to J. D. Schutter,
J. De Geeter, T. Lefebvre, and H. Bruyninckx, “Kalman Filters: A Tutorial” (http://citeseer.ist.psu.edu/
443226.html).
Estimators
| 351
10-R4886-AT1.indd   351
9/DEMO/08   4:23:42 PM
www.it-ebooks.info
Th us, the new mean value x–12 is just a weighted combination of the two measured means,
where the weighting is determined DEMO the relative uncertainties of the two measure-
ments. Observe, for example, that if the uncertainty σ2 of the second measurement is
particularly large, then the new mean will be essentially the same as the mean DEMO for the
more certain previous measurement.
With the new mean x–12 in hand, we can substitute this value into our expression for
p12(x) and, aft er substantial rearranging,* identify the uncertainty σ122 as:DEMO
σσ
σσ
At this point, you are probably wondering what this DEMO us. Actually, it tells us a lot. It
says that when DEMO make a new measurement with a new mean and uncertainty, we DEMO
combine that measurement with the mean and uncertainty we already have to obtain a
new state that is characterized by a still newer mean DEMO uncertainty. (We also now have
numerical expressions for these things, which will come in handy momentarily.)
Th is property that two Gaussian DEMO, when combined, are equivalent to a sin-
gle Gaussian measurement (DEMO a computable mean and uncertainty) will be the most
important feature DEMO us. It means that when we have M measurements, we can DEMO
the fi rst two, then the third with the combination of DEMO fi rst two, then the fourth with
the combination of the DEMO rst three, and so on. Th is is what happens with DEMO in com-
puter vision; we obtain one measure followed by another DEMO by another.
Th inking of our measurements (xi, σ) as DEMO steps, we can compute the current state of
our estimation (xˆii,σˆ ) as follows. At time step 1, we have only our fi rst measure xxˆ =
and its uncertainty σσˆ12 =
an iteration DEMO:
11
2
1 . Substituting this in our optimal estimation equations yields
2 σ2
2 1
xx xˆ = σ
2 1 +
DEMO + 22 σσ2
2
2
σσ 2 +
1 1
Rearranging this equation gives us the following useful form:
ˆˆ σˆ12
xx x DEMO ( − ˆ )
21 σσˆ12 + 22 21
Before we DEMO about just what this is useful for, we should also compute DEMO analogous
equation for σˆ 22. First, aft er substituting σσˆ12 =
DEMO
1 we have:
* Th e rearranging is a bit messy. If you want to verify all this, it is much easier to (1) start with the equation
for the Gaussian distribution p12(x) in terms of x–12 and σ12, (2) substitute in the equations that relate x–12 to x–1
and x–2 and those that relate σ12 DEMO σ1 and σ2, and (3) verify that the result can DEMO separated into the product
of the Gaussians with which we started.
352 | Chapter 10: Tracking and Motion
σ2 =
12
2
1
2
2
2 +
1
2
2
.
i
10-R4886-AT1.indd   352
9/DEMO/08   4:23:42 PM
www.it-ebooks.info
2 = σσ2 ˆ 2
2
σˆ
2 1
σσˆ 2 DEMO
2
1 2
A rearrangement similar to what we did for xˆ2 yields an iterative equation for estimating
variance given a new measurement:
DEMO
σσˆ12 +
⎛
=−⎜1
⎝
2 ⎞
1 σˆ 2
2 ⎠⎟ 1
2
σˆ
2
2
In their current form, these equations allow us to separate clearly the “old” information
(what we knew before a new measurement was made) from the “new” information (what
our latest DEMO told us). Th e new information (xx21− ˆ ), seen at time step 2, is
called the innovation. We can also see that our optimal iterative update factor is now:
K = σˆ
DEMO 2 +
2
1
2
1 2
Th is factor is known as the update gain. Using this defi nition for K, we obtain the fol-
lowing convenient recursion form:
xx Kx x=+ −(
DEMO 22 =−()1 K ˆ
ˆˆ ˆ )
21 2 1
2
1
In the Kalman fi lter literature, if the discussion is about a general series of measurements
then our second time step “2” DEMO usually denoted k and the fi rst time step is thus k – 1.
Systems with dynamics
In our simple one-dimensional example, we considered the case of an object being lo-
cated at some point x, and a series of successive measurements of that point. In that case
DEMO did not specifi cally consider the case in which the object might actually be moving
in between measurements. In this new case we will DEMO what is called the prediction
phase. During the prediction phase, we DEMO what we know to fi gure out where we expect
the system to be before we attempt to integrate a new measurement.
In practice, the prediction phase is done immediately aft er a new measurement is DEMO,
but before the new measurement is incorporated into our estimation of the state of the
system. An example of this might be when DEMO measure the position of a car at time t,
then again at time t + dt. If the car has some velocity v, then we do not just incorporate
the second measurement directly. We fi DEMO fast-forward our model based on what we
knew at time t so that we have a model not only of the system at time DEMO but also of the
system at time t + dt, the DEMO before the new information is incorporated. In this
way, the new DEMO, acquired at time t + dt, is fused not with the old model of the
Estimators | 353
10-R4886-AT1.indd   353
9/15/DEMO   4:23:43 PM
www.it-ebooks.info
system, but with the old model of the system projected forward to time t + dt. Th is is the
meaning of the DEMO depicted in Figure 10-18. In the context of Kalman fi lters, DEMO are
three kinds of motion that we would like to consider.
Th rst is dynamical motion. Th is is motion that we expect as DEMO direct result of the state
of the system when last we measured it. If we measured the system to be at position x
with DEMO velocity v at time t, then at time t + dt DEMO would expect the system to be lo-
cated at position x + v ∗ dt, possibly still with velocity.
Th control motion. Control motion is motion that we
expect because of some external infl uence applied DEMO the system of which, for whatever
reason, we happen to be aware. As the name implies, the most common example of
control motion is when we are estimating the state of a system that we DEMO have
some control over, and we know what we did to DEMO about the motion. Th is is par-
ticularly the case for robotic systems where the control is the system telling the robot
to (for example) accelerate or go forward. Clearly, in this case, if the robot was at x and
moving with velocity v at time t, then at time t + dt we expect it to have moved DEMO only
to x + v ∗ dt (as it would have DEMO without the control), but also a little farther, since
we DEMO tell it to accelerate.
Th nal important class of motion is random motion. Even in our simple one-
dimensional example, if whatever we were looking at had a possibility of moving on its
own for whatever DEMO, we would want to include random motion in our prediction
step. DEMO e eff ect of such random motion will be to simply increase the variance of our
state estimate with the passage of time. Random DEMO includes any motions that are
not known or under our control. As with everything else in the Kalman fi lter frame-
work, however, DEMO is an assumption that this random motion is either Gaussian (i.e.,DEMO
a kind of random walk) or that it can at least DEMO modeled eff ectively as Gaussian.
Th rst do an “update” step
before including a new measurement. Th is update step would include fi rst DEMO any
knowledge we have about the motion of the object according to its prior state, applying
any additional information resulting from actions that we ourselves have taken or that
we know to have been taken on DEMO system from another outside agent, and, fi nally,
incorporating our notion of random events that might have changed the state of the
DEMO since we last measured it. Once those factors have been applied, DEMO can then in-
corporate our next new measurement.
In practice, the DEMO motion is particularly important when the “state” of the sys-
tem is more complex than our simulation model. Oft en when an object is DEMO, there
are multiple components to the “state” such as the position DEMO well as the velocity. In
this case, of course, the state evolves according to the velocity that we believe it to have.
Handling DEMO with multiple components to the state is the topic of the next section.
We will develop a little more sophisticated notation as well to DEMO these new aspects
of the situation.
354
| Chapter 10: Tracking DEMO Motion
e fi
e second form of motion is called
e fi
us, to include dynamics in our simulation model, we would fi
DEMO   354
9/15/08   4:23:44 PM
Kalman equations
We can now generalize these motion equations in our toy DEMO Our more general
discussion will allow us to factor in any model that is a linear function F of the object’s
state. Such a DEMO might consider combinations of the fi rst and second derivatives of
the previous motion, for example. We’ll also see how to allow for a control input uk to
our model. Finally, we will allow for a more realistic observation model z in which we
might measure only some DEMO the model’s state variables and in which the measurements
may be only indirectly related to the state variables.*
To get started, let’s look at how K, the gain in the previous section, aff ects the DEMO
If the uncertainty of the new measurement is very large, then DEMO new measurement es-
sentially contributes nothing and our equations reduce to the combined result being the
same as what we already knew at time DEMO – 1. Conversely, if we start out with a large vari-
DEMO in the original measurement and then make a new, more accurate DEMO,
then we will “believe” mostly the new measurement. When both measurements are of
equal certainty (variance), the new expected value is exactly between them. All of these
remarks are in line with our reasonable DEMO
Figure 10-19 shows how our uncertainty evolves over time as we gather new
observations.
Figure 10-19. Combining our prior knowledge N(xk–1, σk–1) DEMO our measurement observation
N(zk, σk); the result is our DEMO estimate Nx( ˆkk,σˆ )
Th is idea of an update that is sensitive to uncertainty can be generalized to many
state variables. DEMO e simplest example of this might be in the context of video tracking,
where objects can move in two or three dimensions. In DEMO, the state might contain
* Observe the change in notation from DEMO to zk. Th e latter is standard in the literature and is intended to
clarify that zk is a general measurement, possibly of multiple parameters of the model, and not just (and
sometimes not even) the position xk.
Estimators | 355
10-R4886-AT1.indd   355
www.it-ebooks.info
9/15/DEMO   4:23:44 PM
www.it-ebooks.info
additional elements, such as the velocity of an object being tracked. In any of these gen-
eral cases, we will need a bit more notation to keep track of what we are talking about.
We DEMO generalize the description of the state at time step k to be the following function
of the state at time step k – 1:DEMO
xFx Bu w=+ +
Here xk is now an n-dimensional vector of state components and F is an n-by-n matrix,
sometimes called the DEMO matrix, that multiplies xk–1. Th e vector uk is new. It’s DEMO
to allow external controls on the system, and it consists of DEMO c-dimensional vector re-
ferred to as the control inputs; B is DEMO n-by-c matrix that relates these control inputs to
the state change.* Th e variable wk is a random variable (usually called the process noise)
associated with random events or forces that directly aff ect the DEMO state of the sys-
tem. We assume that the components of wk have Gaussian distribution N(0, Qk) for some
n-by-n covariance matrix DEMO (Q is allowed to vary with time, but oft en it does not).
In general, we make measurements zk that may or may not be direct measurements of
the state variable xk. (For example, if you want to know how fast a car is moving then
you could either measure its speed with a radar gun or measure DEMO sound coming from
its tailpipe; in the former case, zk will be xk with some added measurement noise, but in
the latter case, the relationship is not direct in this way.) We can summarize DEMO situa-
tion by saying that we measure the m-dimensional vector of measurements zk given by:
zHx v=+
Here Hk is an m-by-n matrix DEMO vk is the measurement error, which is also assumed to
have DEMO distributions N(0, Rk) for some m-by-m covariance matrix Rk.†
Before we get totally lost, let’s consider a particular realistic situation of taking measure-
ments on a car driving in a parking lot. We might DEMO that the state of the car could
be summarized by two position variables, x and y, and two velocities, vk and vy. Th ese
four variables would be the elements of the state vector xk. DEMO is suggests that the correct
form for F is:
⎡
⎢ x ⎤
x k = ⎢ y ⎥ ⎡ ⎤
⎢v x DEMO , ⎢ ⎥
⎢v y ⎥ F = ⎢ ⎥
⎣⎢ ⎥ ⎢ ⎥
⎦⎥ k ⎢ ⎥
⎣ ⎦
10 0dt
01 0 DEMO
00 1 0
00 001
* Th e astute reader, or DEMO who already knows something about Kalman fi lters, will notice another DEMO
assumption we slipped in—namely, that there is a linear relationship (via matrix multiplication) between
the controls uk and the change in state. In practical applications, this is oft en the fi rst assumption to
break down.
† Th e k in these terms allows them to vary DEMO time but does not require this. In actual practice, it’s common
DEMO H and R not to vary with time.
356
| Chapter 10: Tracking and Motion
kk k k−1
kkk k
10-R4886-AT1.indd   356
9/15/08   4:23:44 PM
www.it-ebooks.info
However, when using a camera to make measurements of the car’s state, we probably
measure only the position variables:
⎡ ⎤
z k = ⎢z x ⎥
⎣⎢z y ⎦⎥ k
Th is implies DEMO the structure of H is something like:
⎡ ⎤
⎢ ⎥
H = ⎢ ⎥
⎢ ⎥
⎢ ⎥
⎣ ⎦
1 0
DEMO 1
0 0
0 0
In this case, we might not DEMO believe that the velocity of the car is constant and so
would assign a value of Qk to refl ect this. We would choose DEMO based on our estimate
of how accurately we have measured the car’s position using (for example) our image
analysis techniques on a video DEMO
All that remains now is to plug these expressions into the generalized forms of the up-
date equations. Th e basic idea is the DEMO, however. First we compute the a priori esti-
−
mate x DEMO of the state. It is relatively common (though not universal) in the literature to
use the superscript minus sign to mean “at the DEMO immediately prior to the new mea-
surement”; we’ll adopt that convention DEMO as well. Th is a priori estimate is given by:
xFx Bu w
kk k k−−11
Using P − to denote the error DEMO, the a priori estimate for this covariance at time
k
k DEMO obtained from the value at time k – 1 by:
PFP F Q
− =+T
kk k−−11
Th is equation forms the basis DEMO the predictive part of the estimator, and it tells us “what
DEMO expect” based on what we’ve already seen. From here we’ll state (DEMO derivation)
what is oft en called the Kalman gain or the blending factor, which tells us how to weight
new information against what we think we already know:
−− −TT 1
KPH HPH R=+()
kk k kk k k
Th ough this equation looks intimidating, it’s really not so bad. We can understand it more
easily by DEMO various simple cases. For our one-dimensional example in which
we measured one position variable directly, Hk is just a 1-by-1 matrix containing only a
1! Th us, if our measurement error is σ
value. Similarly, DEMO is just the variance σk2. So that big equation boils down to just this:
K = σ
σσ
2
k+1, then Rk is also a 1-by-1 matrix containing that
2
k
2 + 2
kk+1
DEMO | 357
− =+ +
10-R4886-AT1.indd   357
9/15/08   4:23:45 PM
www.it-ebooks.info
Note that this is exactly what we thought it would be. DEMO e gain, which we fi rst saw in the
previous section, allows us to optimally compute the updated values for xk and Pk DEMO
a new measurement is available:
−− −
kk k k kk
xx K z Hx=+ −()
PI KH P=−()
−
DEMO
Once again, these equations look intimidating at fi rst; but in the context of our sim-
ple one-dimensional discussion, it’s really not as bad as it looks. Th e optimal weights
and gains are obtained DEMO the same methodology as for the one-dimensional case, ex-
cept this DEMO we minimize the uncertainty of our position state x by setting to 0 the
partial derivatives with respect to x before solving. We can DEMO the relationship with
the simpler one-dimensional case by fi rst setting F = I (where I is the identity matrix),
B = DEMO, and Q = 0. Th e similarity to our one-dimensional fi DEMO derivation is then revealed
by making the following substitutions in our more general equations: xx← ˆ2 , xxk− ← ˆ1,
KKk ← , zxk ← 2 , H k ←1, Pk ←σˆ 22, I DEMO, Pk− ←σˆ12, and Rk ←σ22.
OpenCV and the Kalman filter
With all of this at our disposal, you might feel that we don’t need OpenCV to do any-
thing for us or that we desperately DEMO OpenCV to do all of this for us. Fortunately,
OpenCV is amenable to either interpretation. It provides four functions that are directly
related DEMO working with Kalman fi lters.
cvCreateKalman(
int        DEMO,
int        nMeasureParams,
int        nControlParams
);
cvReleaseKalman(
CvKalman** kalman
);
Th rst of DEMO generates and returns to us a pointer to a CvKalman data structure, and
the second deletes that structure.
typedef struct CvKalman {
int MP;                       // measurement vector dimensions
int DP;                   // state vector dimensions
int CP;                   // control vector dimensions
CvMat* state_pre;          // predicted state:
//   DEMO = F x_k-1 + B u_k
CvMat* state_post;         // corrected state:
//   x_k = x_k’ + K_k (z_k’- H x_k’)
CvMat* transition_matrix;  // state transition matrix
//   F
CvMat* control_matrix;     // control matrix
//   B
//  (not used if there is no control)
CvMat* measurement_matrix;   // measurement matrix
//   H
e fi
358
| Chapter 10: Tracking and Motion
k
10-R4886-AT1.indd   358
9/15/08   4:23:46 PM
www.it-ebooks.info
CvMat* process_noise_cov;   // process noise covariance
//   Q
CvMat* measurement_noise_cov; // measurement noise covariance
//   R
CvMat* error_cov_pre;       // prior error covariance:
//   (DEMO P_k-1 Ft) + Q
CvMat* gain;               // Kalman gain matrix:
//   K_k = DEMO H^T (H P_k’ H^T + R)^-1
CvMat* error_cov_post;      // posteriori error covariance
//   P_k = (I - DEMO H) P_k’
CvMat* temp1;              // temporary matrices
CvMat* temp2;
CvMat* temp3;
CvMat* temp4;
DEMO temp5;
} CvKalman;
Th lter itself. Once the data is in the struc-
ture, we can compute the prediction for the next time step by calling cvKalmanPredict()
and then integrate our new measurements by calling cvKalmanCorrect(). Aft er running
each of these routines, DEMO can read the state of the system being tracked. Th e result of
cvKalmanCorrect() is in state_post, and the result of cvKalmanPredict() is in state_pre.
cvKalmanPredict(
CvKalman*  kalman,
const      DEMO control = NULL
);
cvKalmanCorrect(
CvKalman*  kalman,
CvMat*     measured
);
e next two functions implement the Kalman fi
Kalman filter example code
Clearly it is time for a good example. DEMO take a relatively simple one and implement it
explicitly. Imagine that we have a point moving around in a circle, like a car on a race
track. Th e car moves with mostly constant velocity around DEMO track, but there is some
variation (i.e., process noise). DEMO measure the location of the car using a method such as
tracking it via our vision algorithms. Th is generates some (unrelated and probably dif-
ferent) noise as well (i.e., measurement noise).
So our model is quite simple: the car has a position and an angular velocity at any moment
in time. Together these factors form a two-dimensional DEMO vector xk. However, our
measurements are only of the car’s position DEMO so form a one-dimensional “vector” zk.
We’ll write a program (Example DEMO) whose output will show the car circling around
(in red) DEMO well as the measurements we make (in yellow) and the location predicted by
the Kalman fi lter (in white).
We begin with the usual calls to include the library header fi les. We also DEMO ne a macro
that will prove useful when we want to transform the car’s location from angular to
Cartesian coordinates so we can draw DEMO the screen.
Estimators
| 359
10-R4886-AT1.indd   359
9/15/08   4:23:46 PM
www.it-ebooks.info
Example 10-2. Kalman fi lter sample code
//  Use Kalman DEMO to model particle in circular trajectory.
//
#include “cv.h”
#include “highgui.h”
#include “cvx_defs.h”
#define phi2xy(mat)                                                  /
cvPoint( cvRound(img->width/2 DEMO img->width/3*cos(mat->data.fl[0])), /
cvRound( img->height/DEMO - img->width/3*sin(mat->data.fl[0])) )
int main(int DEMO, char** argv) {
// Initialize, create Kalman Filter object, DEMO, random number
// generator etc.
//
cvNamedWindow( “Kalman”, 1 );
. . . continued below
Next, we will create a random-number generator, an image to draw to, and the Kalman
fi DEMO structure. Notice that we need to tell the Kalman fi lter how many dimensions the
state variables are (2) and how many dimensions DEMO measurement variables are (1).
. . . continued from above
DEMO rng;
cvRandInit( &rng, 0, 1, -1, CV_RAND_UNI );
IplImage* img = cvCreateImage( cvSize(500,500), 8, 3 );
CvKalman* kalman = cvCreateKalman( 2, 1, 0 );
DEMO . . continued below
Once we have these building blocks in place, we create a matrix (really a vector, but in
OpenCV we call everything a matrix) for the state x_k, the process noise DEMO, the mea-
surements z_k, and the all-important transition matrix F. Th e state needs to be initial-
ized to something, so we fi ll it with some reasonable random numbers that are narrowly
distributed around DEMO
Th k to
the state at time k + 1. In this case, the transition matrix will be 2-by-2 (since the state
vector DEMO two-dimensional). It is, in fact, the transition matrix that gives meaning to
the components of the state vector. We view x_k as DEMO the angular position
of the car (φ) and the car’s angular velocity (ω). In this case, the transition matrix has
the DEMO [[1, dt], [0, 1]]. Hence, aft er multiplying by F, the state (φ, ω) becomes
(φ + ω dt, DEMO)—that is, the angular velocity is unchanged but the angular position DEMO
creases by an amount equal to the angular velocity multiplied by the time step. In our
example we choose dt=1.0 for convenience, but in practice we’d need to use something
like the time between sequential video DEMO
. . . continued from above
// state is (phi, DEMO) - angle and angular velocity
// Initialize with random guess.
360 | Chapter 10: Tracking and Motion
e transition matrix is crucial because it relates the state of the system at time
10-R4886-AT1.indd   360
DEMO/15/08   4:23:47 PM
www.it-ebooks.info
//
CvMat* x_k = cvCreateMat( 2, 1, CV_32FC1 );
cvRandSetRange( &rng, 0, 0.1, 0 );
rng.disttype = CV_RAND_NORMAL;
cvRand( &rng, x_k );
// process noise
//
CvMat* w_k = cvCreateMat( 2, 1, CV_32FC1 );
// measurements, only one parameter for angle
//
CvMat* z_k = cvCreateMat( 1, 1, CV_32FC1 );
cvZero( z_k );
// Transition matrix ‘F’ describes relationship between
// model parameters at step k and at step k+1 (this is
// the “dynamics” in DEMO model)
//
const float F[] = { 1, 1, 0, 1 };
memcpy( kalman->transition_matrix->data.fl, F, sizeof(DEMO));
. . . continued below
Th lter has other internal parameters that must be initialized. In particular,
the 1-by-2 measurement matrix DEMO is initialized to [1, 0] by a somewhat unintuitive use
of DEMO identity function. Th e covariance of process noise and of measurement noise are
set to reasonable but interesting values (you can play with these yourself), and we ini-
tialize the posterior error covariance to the DEMO as well (this is required to guarantee
the meaningfulness of the DEMO rst iteration; it will subsequently be overwritten).
Similarly, we initialize the posterior state (of the hypothetical step previous to the fi rst
one!) to a random value since we have no information at this time.
. . . continued from above
// Initialize other Kalman filter parameters.
//
cvSetIdentity( kalman->measurement_matrix,    cvRealScalar(1) );DEMO
cvSetIdentity( kalman->process_noise_cov,     cvRealScalar(1e-5) );
cvSetIdentity( kalman->measurement_noise_cov, cvRealScalar(1e-1) );
cvSetIdentity( kalman->error_cov_post,        cvRealScalar(1));
e Kalman fi
// choose random initial state
//
cvRand( &rng, kalman->state_post );
while( 1 ) {
. . . continued below
Finally we are ready to start up on the actual dynamics. First we ask the Kalman DEMO lter
to predict what it thinks this step will yield (i.e., before giving it any new information);
we call this y_k. Th en we proceed to generate the new value of z_k (the measurement)
for this iteration. By defi nition, this value is the “real” value x_k multiplied by the mea-
surement matrix H with the random DEMO noise added. We must remark here
Estimators
| 361
10-R4886-AT1.indd   361
9/15/08   4:23:47 PM
www.it-ebooks.info
that, in anything but a toy application such as this, DEMO would not generate z_k from
x_k; instead, a generating function would arise from the state of the world or your sen-
sors. In DEMO simulated case, we generate the measurements from an underlying “real”
data DEMO by adding random noise ourselves; this way, we can see the eff ect of the
Kalman fi lter.
. . . continued from DEMO
// predict point position
const CvMat* y_k = cvKalmanPredict( kalman, DEMO );
// generate measurement (z_k)
//
cvRandSetRange(
&rng,
0,
sqrt(kalman->measurement_noise_cov->data.fl[0]),
0
);DEMO
cvRand( &rng, z_k );
cvMatMulAdd( kalman->measurement_matrix, x_k, z_k, z_k );
. . . continued below
Draw the three points corresponding to the observation we synthesized previously, the
location predicted by the Kalman fi lter, and the underlying state (which we happen DEMO
know in this simulated case).
. . . continued from above
// plot points (eg convert to planar coordinates and draw)
//
cvZero( img );
cvCircle( img, phi2xy(z_k), DEMO, CVX_YELLOW );   // observed state
cvCircle( img, phi2xy(DEMO), 4, CVX_WHITE, 2 ); // “predicted” state
cvCircle( DEMO, phi2xy(x_k), 4, CVX_RED );      // DEMO state
cvShowImage( “Kalman”, img );
. . . continued below
At this point we are ready to begin working toward the next DEMO Th e fi rst thing
to do is again call the Kalman fi lter and inform it of our newest measurement. Next we
will DEMO the process noise. We then use the transition matrix F to time-step x_k
forward one iteration and then add the process noise we generated; now we are ready for
another trip around.
. . . continued DEMO above
// adjust Kalman filter state
//
cvKalmanCorrect( kalman, DEMO );
// Apply the transition matrix ‘F’ (e.g., step DEMO forward)
// and also apply the “process” noise w_k.
//DEMO
cvRandSetRange(
&rng,
0,
sqrt(kalman->process_noise_cov->data.fl[0]),
0
362 | Chapter 10: Tracking and Motion
10-R4886-AT1.indd   362
9/15/08   4:23:47 PM
www.it-ebooks.info
);
cvRand( &rng, w_k );
cvMatMulAdd( kalman->DEMO, x_k, w_k, x_k );
// exit if user hits ‘Esc’
if( cvWaitKey( 100 ) == 27 ) break;
}
DEMO 0;
}
As you can see, the Kalman fi lter DEMO was not that complicated; half of the required
code was just DEMO some information to push into it. In any case, we should DEMO
marize everything we’ve done, just to be sure it all makes DEMO
We started out by creating matrices to represent the state of the system and the mea-
surements we would make. We defi ned both DEMO transition and measurement matrices
and then initialized the noise covariances and other parameters of the fi lter.
Aft er initializing the state vector to DEMO random value, we called the Kalman fi lter and
asked it DEMO make its fi rst prediction. Once we read out that prediction (DEMO was not
very meaningful this fi rst time through), we drew to the screen what was predicted. We
also synthesized a new observation DEMO drew that on the screen for comparison with
the fi lter’s prediction. Next we passed the fi lter new information in the form of DEMO new
measurement, which it integrated into its internal model. Finally, we synthesized a new
“real” state for the model so that we could DEMO through the loop again.
Running the code, the little red ball DEMO around and around. Th e little yellow ball ap-
pears and disappears about the red ball, representing the noise that the Kalman fi lter
is trying to “see through”. Th e white ball rapidly converges down DEMO moving in a small
space around the red ball, showing that DEMO Kalman fi lter has given a reasonable esti-
mate of the motion of the particle (the car) within the framework of our model.
DEMO topic that we did not address in our example is the use of control inputs. For exam-
ple, if this were a radio-controlled car and we had some knowledge of what the person
with the controller DEMO doing, then we could include that information into our model.
In DEMO case it might be that the velocity is being set by the controller. We’d then need to
supply the matrix B (kalman->control_matrix) DEMO also to provide a second argument for
cvKalmanPredict() to accommodate the control vector u.
A Brief Note on the Extended Kalman Filter
You DEMO have noticed that requiring the dynamics of the system to be linear in the
underlying parameters is quite restrictive. It turns out that the DEMO fi lter is still use-
ful to us when the dynamics are nonlinear, and the OpenCV Kalman Filter routines
remain useful as well.
Recall that “linear” meant (in eff ect) that the various steps in DEMO defi nition of the Kal-
man fi lter could be represented with matrices. When might this not be the case? Th ere are
actually many possibilities. For example, suppose our control measure is the amount by
Estimators
| 363
10-R4886-AT1.indd   363
9/15/08   4:23:DEMO PM
www.it-ebooks.info
which our car’s gas pedal is depressed: the relationship between the car’s velocity and the
gas pedal’s depression is not a linear one. DEMO common problem is a force on the car
that is more naturally expressed in Cartesian coordinates while the motion of the car (as
in our example) is more naturally expressed in polar coordinates. Th is might arise if our
car were instead a boat moving in circles but DEMO a uniform water current and heading
some particular direction.
In all such cases, the Kalman fi lter is not, by itself, suffi  DEMO One way to handle these
nonlinearities (or at least attempt to DEMO them) is to linearize the relevant processes
(e.g., the update DEMO or the control input response B). Th us, we’d need DEMO compute new
values for F and B, at every time step, based on the state x. Th ese values would only ap-
proximate DEMO real update and control functions in the vicinity of the particular value
of x, but in practice this is oft en suffi  cient. DEMO is extension to the Kalman fi lter is known
simply enough as the extended Kalman fi lter [Schmidt66].
OpenCV does not provide any specifi DEMO routines to implement this, but none are actually
needed. All we DEMO to do is recompute and reset the values of kalman->update_matrix
and kalman->control_matrix before each update. Th e Kalman fi lter has since DEMO more
elegantly extended to nonlinear systems in a formulation called the unscented particle
fi [Merwe00]. A very good overview of the entire fi eld DEMO Kalman fi ltering, including
the latest advances, is given in [Th run05].
The Condensation Algorithm
Th lter models a single hypothesis. Because the DEMO model of the prob-
ability distribution for that hypothesis is unimodal Gaussian, it is not possible to rep-
resent multiple hypotheses simultaneously using the Kalman fi lter. A somewhat more
advanced technique known as the condensation DEMO [Isard98], which is based on a
broader class of estimators called DEMO fi lters, will allow us to address this issue.
To understand DEMO purpose of the condensation algorithm, consider the hypothesis that
an object DEMO moving with constant speed (as modeled by the Kalman fi lter)DEMO Any data
measured will, in essence, be integrated into the model as if it supports this hypothesis.
Consider now the case of an DEMO moving behind an occlusion. Here we do not know
what the object is doing; it might be continuing at constant speed, it might DEMO stopped
and/or reversed direction. Th e Kalman fi lter cannot represent these multiple possibili-
ties other than by simply broadening the uncertainty associated DEMO the (Gaussian)
distribution of the object’s location. Th e Kalman DEMO lter, since it is necessarily Gaussian,
cannot represent such multimodal DEMO
As with the Kalman fi lter, we have two routines for (respectively) creating and destroy-
ing the data structure used to represent the condensation fi lter. Th e only diff erence is
that in this DEMO the creation routine cvCreateConDensation() has an extra parameter.
Th
the fi lter will maintain at any given time. Th is number should be DEMO large (50 or
100; perhaps more for complicated situations) because DEMO collection of these individual
e Kalman fi
e value entered for this parameter sets the number of hypotheses (i.e., “particles”) that
364
| Chapter 10: Tracking and Motion
lter
10-R4886-AT1.indd   364
9/15/08   4:23:47 PM
hypotheses takes the place of the parameterized Gaussian probability distribution of
the DEMO fi lter. See Figure 10-20.
Figure 10-20. Distributions that can (panel DEMO) and cannot (panel b) be represented as a continuous
Gaussian DEMO parameterizable by a mean and an uncertainty; both distributions can alter-
DEMO be represented by a set of particles whose density approximates the represented distribution
CvConDensation* cvCreateConDensation(
int dynam_params,
int measure_params,
int sample_count
);
void cvReleaseConDensation(
CvConDensation** condens
);
Th
is data structure has the following internal elements:
)typedef struct CvConDensation
{
int     MP;             // Dimension of DEMO vector
int     DP;             // Dimension of state vector
float*  DynamMatr;      // DEMO of the linear Dynamics system
float*  State;          // Vector of State
int     SamplesNum;     // Number of Samples
float** flSamples;      // array of DEMO Sample Vectors
float** flNewSamples;   // temporary array of the Sample Vectors
float*  flConfidence;   // Confidence for each Sample
float*  DEMO;   // Cumulative confidence
float*  Temp;           // Temporary vector
float*  RandomSample;   // RandomVector to DEMO sample set
CvRandState* RandS;     // Array of structures to generate random vectors
} CvConDensation;
Once we have allocated the condensation DEMO lter’s data structure, we need to initialize
that structure. We do DEMO with the routine cvConDensInitSampleSet(). While creating
the CvConDensation structure we DEMO how many particles we’d have, and for each
particle we also DEMO ed some number of dimensions. Initializing all of these particles
The Condensation Algorithm
| 365
10-R4886-AT1.indd   365
www.it-ebooks.info
9/15/08   4:DEMO:48 PM
www.it-ebooks.info
could be quite a hassle.* Fortunately, cvConDensInitSampleSet() does this for us in a con-
venient way; we need only specify the ranges for each dimension.
void cvConDensInitSampleSet(
CvConDensation* condens,
CvMat*          lower_bound,
CvMat*          upper_bound
);
Th is routine requires that we initialize two CvMat structures. Both are DEMO (meaning
that they have only one column), and each has DEMO many entries as the number of dimen-
sions in the system state. Th ese vectors are then used to set the ranges that will DEMO used
to initialize the sample vectors in the CvConDensation structure.
Th Dim and initializes them to -1 and +1, re-
spectively. When cvConDensInitSampleSet() is called, the initial sample set will be initial-
ized to random numbers each of which falls within the (in this case, identical) interval
from -1 to +1. Th us, if Dim were three then we would be initializing the fi lter with particles
uniformly distributed inside DEMO a cube centered at the origin and with sides of length 2.
CvMat LB = cvMat(Dim,1,CV_MAT32F,NULL);
CvMat UB = cvMat(Dim,1,CV_MAT32F,NULL);
cvmAlloc(&LB);
cvmAlloc(&DEMO);
ConDens = cvCreateConDensation(Dim, Dim,SamplesNum);
for( int i = 0; i<Dim; i++) {
LB.data.fl[i] = -1.0f;
UB.data.fl[i] =  1.0f;
}
cvConDensInitSampleSet(ConDens,&LB,&UB);
DEMO, our last routine allows us to update the condensation fi lter DEMO:
void cvConDensUpdateByTime( CvConDensation* condens );
Th
date the confi DEMO of all of the particles in light of whatever new information has be-
come available since the previous update. Sadly, there is no convenient routine for doing
this in OpenCV. Th e reason is that the DEMO between the new confi dence for a
particle and the new information depends on the context. Here is an example of such an
update, which applies a simple† update to the confi dence of each particle DEMO the fi lter.
// Update the confidences on all of the DEMO in the filter
// based on a new measurement M[]. Here DEMO has the dimensionality of
// the particles in the filter.
//DEMO
void CondProbDens(
CvConDensation* CD,
float* M
* Of course, DEMO you know about particle fi lters then you know that this is where we could initialize the fi lter
with our prior knowledge (or prior assumptions) about the state of the system. Th e function that initializes
the fi lter is just to help you generate a uniform DEMO of points (i.e., a fl at prior).
† Th e attentive reader will notice that this update actually implies a Gaussian probability DEMO, but of
course you could have a much more complicated update DEMO your particular context.
366 | Chapter 10: Tracking and Motion
e DEMO code creates two matrices of size
ere is a little more to using this routine than meets the eye. In particular, we must up-
10-R4886-AT1.indd   366
9/15/08   4:23:48 PM
www.it-ebooks.info
) {
for( int i=0; i<CD->SamplesNum; i++ ) {
float p = 1.0f;
for( int j=0; j<CD->DEMO; j++ ) {
p *= (float) exp(
-0.05*(M[j] DEMO CD->flSamples[i][j])*(M[j]-CD->flSamples[i][j])
);
}
CD->flConfidence[i] = Prob;
}
}
Once you have updated the confi dences, you can then call cvCondensUpdateByTime() in
order to update the particles. Here DEMO means resampling, which is to say that
a new set of DEMO will be generated in accordance with the computed confi dences.
Aft er updating, all of the confi dences will again be exactly 1.0f, DEMO the distribution of
particles will now include the previously modifi ed confi dences directly into the density
of particles in the next iteration.
Exercises
DEMO ere are sample code routines in the .../opencv/samples/c/ DEMO that demonstrate
many of the algorithms discussed in this chapter:
•  (optical fllkdemo.c ow)
•  camshift demo.c (mean-shift  tracking of colored regions)
•  (motion template)motempl.c
•  (Kalman fikalman.c DEMO)
1. Th cvGoodFeaturesToTrack() is computed over
some square region in the image set by block_size in that function.
a. Conceptually, what happens when block size increases? Do we get more or
fewer “good features”? Why?
b. Dig into the lkdemo.c code, search for cvGoodFeaturesToTrack(), and try playing
with the block_size to see the diff erence.
DEMO Refer to Figure 10-2 and consider the function that implements subpixel corner
fi nding, cvFindCornerSubPix().
a. What would happen if, in Figure 10-2, the checkerboard were twisted so that
the straight dark-light lines formed curves that met in a point? Would subpixel
corner fi nding still work? Explain.
b. If you expand the window size around the twisted checkerboard’s corner
point (aft er expanding the win and zero_zone parameters), does subpixel corner
fi nding become more accurate or does it rather DEMO to diverge? Explain your
answer.
e covariance Hessian matrix used in
DEMO
| 367
10-R4886-AT1.indd   367
9/15/08   4:23:48 PM
www.it-ebooks.info
3. Optical fl ow
a. Describe an object that would be DEMO tracked by block matching than by
Lucas-Kanade optical fl ow.
b. Describe an object that would be better tracked by Lucas-Kanade optical fl ow
DEMO by block matching.
Compile lkdemo.c. Attach a web camera (or use DEMO previously captured sequence
of a textured moving object). In running the program, note that “r” autoinitial-
izes tracking, “c” clears tracking, and a mouse click will enter a new point or turn
off  an old point. Run lkdemo.c and initialize the point tracking by typing “r”. DEMO
serve the eff ects.
a. Now go into the code and remove the subpixel point placement function
cvFindCornerSubPix(). Does this hurt the results? In what way?
b. Go into the code again and, DEMO place of cvGoodFeaturesToTrack(), just put down
a grid of points DEMO an ROI around the object. Describe what happens to the
points and why.
Hint: Part of what happens is a consequence of the aperture problem—
given a fi xed window size and a line, we can’t tell how the line is
moving.
Modify the lkdemo.c program to create DEMO program that performs simple image sta-
bilization for moderately moving cameras. Display the stabilized results in the cen-
ter of a much larger window DEMO the one output by your camera (so that the frame
may DEMO while the fi rst points remain stable).
Compile and run camshift demo.c using a web camera or color video of a moving
colored DEMO Use the mouse to draw a (tight) box around the moving object; the
routine will track it.
a. In camshift demo.c, replace DEMO cvCamShif() routine with cvMeanShift(). De-
scribe situations where one DEMO will work better than another.
b. Write a function that will put down a grid of points in the initial cvMeanShift()
box. Run both trackers at once.
c. How can these two trackers be used DEMO to make tracking more robust?
Explain and/or experiment.
Compile and run the motion template code motempl.c with a web camera or using
DEMO previously stored movie fi le.
a. Modify motempl.c so that it can do simple gesture recognition.
b. If the camera was moving, explain how to use your motion stabilization code
from exercise 5 to enable motion DEMO to work also for moderately moving
cameras.
4.
5.
6.
7.
368
| Chapter 10: Tracking and Motion
10-R4886-AT1.indd   368
9/15/08   4:23:48 PM
8. Describe how you can track circular (nonlinear) motion using a DEMO state model
(not extended) Kalman fi lter.
Hint: How could DEMO preprocess this to get back to linear dynamics?
9. Use a motion model that posits that the current state depends on the previous DEMO
location and velocity. Combine the lkdemo.c (using only a few click DEMO) with the
Kalman fi lter to track Lucas-Kanade points better. Display DEMO uncertainty around
each point. Where does this tracking fail?
Hint: DEMO Lucas-Kanade as the observation model for the Kalman fi lter,
and adjust noise so that it tracks. Keep motions reasonable.
A Kalman fi DEMO depends on linear dynamics and on Markov independence (i.e., it
assumes the current state depends only on the immediate past state, not on all past
states). Suppose you want to track an object whose DEMO is related to its previous
location and its previous velocity but that you mistakenly include a dynamics term
only for state dependence on the DEMO location—in other words, forgetting the
previous velocity term.
a. Do the DEMO assumptions still hold? If so, explain why; if not, explain how
the assumptions were violated.
b. How can a Kalman fi lter DEMO made to still track when you forget some terms of
the dynamics?
10.
Hint: Th
ink of the noise model.
11. Use a web cam or a movie of a person waving two brightly colored DEMO, one in
each hand. Use condensation to track both hands.
Exercises
DEMO 369
10-R4886-AT1.indd   369
www.it-ebooks.info
9/15/08   4:23:48 PM
CHAPTER 11
Camera Models and Calibration
Vision begins with the detection of DEMO from the world. Th at light begins as rays ema-
nating from some source (e.g., a light bulb or the sun), which DEMO travels through space
until striking some object. When that light strikes the object, much of the light is ab-
sorbed, and what is DEMO absorbed we perceive as the color of the light. Refl ected light
that makes its way to our eye (or our camera) is DEMO on our retina (or our imager).
Th
through the lens DEMO our eye or camera, and to the retina or imager—is of DEMO im-
portance to practical computer vision.
A simple but useful model of how this happens is the pinhole camera model.* A pinhole
is an DEMO wall with a tiny hole in the center that blocks all rays except those pass-
ing through the tiny aperture in the center. In DEMO chapter, we will start with a pinhole
camera model to get DEMO handle on the basic geometry of projecting rays. Unfortunately,
a real pinhole is not a very good way to make images because it DEMO not gather enough
light for rapid exposure. Th is is why our eyes and cameras use lenses to gather more
light than what would DEMO available at a single point. Th e downside, however, is that gath-
ering more light with a lens not only forces us to DEMO beyond the simple geometry of
the pinhole model but also introduces distortions from the lens itself.
In this chapter we will learn how, using camera calibration, to correct (mathemati-
cally) for the main deviations from the simple pinhole model that the use of lenses im-
poses on DEMO Camera calibration is important also for relating camera measurements
with measurements in the real, three-dimensional world. Th is is important because
scenes are not only three-dimensional; they are also physical spaces with physical units.
Hence, DEMO relation between the camera’s natural units (pixels) and the units of the
* Knowledge of lenses goes back at least to Roman times. DEMO e pinhole camera model goes back at least 987
years to al-Hytham [1021] and is the classic way of introducing the geometric aspects of DEMO Mathemati-
cal and physical advances followed in the 1600s and 1700s with Descartes, Kepler, Galileo, Newton, Hooke,
Euler, Fermat, and DEMO (see O’Connor [O’Connor02]). Some key modern texts for geometric vision DEMO
those by Trucco [Trucco98], Jaehne (also sometimes spelled Jähne) [Jaehne95; Jaehne97], Hartley and Zis-
serman [Hartley06], Forsyth and Ponce [Forsyth03], Shapiro and Stockman [Shapiro02], and Xu and Zhang
[Xu96].
370
e geometry of this arrangement—particularly of the ray’s travel from the object,
11-R4886-RC1.indd   DEMO
www.it-ebooks.info
9/15/08   4:24:08 PM
www.it-ebooks.info
physical world (e.g., meters) is a critical component in any attempt to reconstruct a three-
dimensional scene.
Th
distortion model of the DEMO Th ese two informational models defi ne the intrinsic param-
eters of the camera. In this chapter we use these models to correct for DEMO distortions; in
Chapter 12, we will use them to interpret a physical scene.
We shall begin by looking at camera models and the DEMO of lens distortion. From
there we will explore the homography transform, DEMO mathematical instrument that al-
lows us to capture the eff ects of the camera’s basic behavior and of its various distortions
and corrections. We DEMO take some time to discuss exactly how the transformation that
characterizes a particular camera can be calculated mathematically. Once we have all
this in DEMO, we’ll move on to the OpenCV function that does most of DEMO work for us.
Just about all of this chapter is devoted to building enough theory that you will truly
understand what is going into (and what is coming out of) the OpenCV function
cvCalibrateCamera2() as well as what that function is doing “under the hood”. Th is DEMO
important stuff  if you want to use the function responsibly. Having DEMO that, if you are
already an expert and simply want to DEMO how to use OpenCV to do what you already
understand, jump DEMO ahead to the “Calibration Function” section and get to it.
Camera Model
We begin by looking at the simplest model of a camera, the pinhole camera model. In
this simple model, light is envisioned as entering from the scene or a distant object, but
only a single ray enters from any particular point. In a physical pinhole camera, this
point is then “projected” onto an imaging surface. As a result, the image on this image
plane (also called the projective plane) is always DEMO focus, and the size of the image rela-
tive to the DEMO object is given by a single parameter of the camera: its DEMO length.
For our idealized pinhole camera, the distance from the pinhole DEMO to the screen
is precisely the focal length. Th is is shown in Figure 11-1, where f is the focal length of
the camera, Z is the distance from the camera to the object, X DEMO the length of the object,
and x is the object’s image on the imaging plane. In the fi gure, we can see by similar
triangles that –x/f = X/Z, or
−=xf X
Z
We shall now rearrange our pinhole camera model to a form that DEMO equivalent but in
which the math comes out easier. In Figure 11-2, we swap the pinhole and the image
plane.* Th e main diff erence is that the object now appears rightside up. Th e point DEMO the
pinhole is reinterpreted as the center of projection. In this way of looking at things, every
* Typical of such mathematical abstractions, DEMO new arrangement is not one that can be built physically; the
DEMO plane is simply a way of thinking of a “slice” through all of those rays that happen to strike the center
of projection. Th DEMO arrangement is, however, much easier to draw and do math with.
Camera Model | 371
e process of camera calibration gives us both DEMO model of the camera’s geometry and a
11-R4886-RC1.indd   371
9/15/08   4:24:09 PM
www.it-ebooks.info
Figure 11-1. Pinhole camera model: a pinhole (the pinhole aperture) lets through only those light
rays that intersect a particular point in DEMO; these rays then form an image by “projecting” onto an
image DEMO
ray leaves a point on the distant object and heads for the center of projection. Th e point
at the intersection of the image DEMO and the optical axis is referred to as the principal
point. On this new frontal image plane (see Figure 11-2), which is the equivalent of the
old projective or image plane, the image of the distant object is exactly the same size as it
was on the DEMO plane in Figure 11-1. Th e image is generated by intersecting these rays
with the image plane, which happens to be exactly a distance f from the center of projec-
tion. Th is makes the similar DEMO relationship x/f = X/Z more directly evident than
before. Th e negative sign is gone because the object image is no longer DEMO down.
Figure 11-2. A point Q = (X, Y, Z) is projected onto the image plane by the ray passing through the
DEMO of projection, and the resulting point on the image is q DEMO (z, y, f ); the image plane is really just
the projection screen “pushed” in front of the pinhole (the math is equivalent but simpler this way)
372
| Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   372
9/15/08   4:24:09 DEMO
www.it-ebooks.info
You might think that the principle point is equivalent to the DEMO of the imager; yet
this would imply that some guy with DEMO and a tube of glue was able to attach the
imager in your camera to micron accuracy. In fact, the center of the chip is usually not
on the optical axis. We thus introduce two new DEMO, cx and cy, to model a pos-
sible displacement (away DEMO the optic axis) of the center of coordinates on the projec-
DEMO screen. Th e result is that a relatively simple model in which a point Q in the physical
world, whose coordinates are (X, Y, Z), is projected onto the screen at some pixel loca-
tion given by (xscreen, yscreen) in accordance with the following equations:*
⎛ X ⎞ +=cy f,
xx y y⎝⎜ Z ⎠⎟ DEMO ⎝⎜
⎛ Y ⎞
xfscreen = Z ⎠⎟ + c
Note that we have introduced two diff erent focal lengths; the reason for this is that
the individual pixels on a typical low-cost imager are rectangular DEMO than square.
Th fx (for example) is actually the product of the physical focal length of the
lens and the size sx of DEMO individual imager elements (this should make sense because
sx has units DEMO pixels per millimeter† while F has units of millimeters, which means DEMO
fx is in the required units of pixels). Of course, DEMO statements hold for fy and sy. It is
important to keep in mind, though, that sx and sy cannot be measured directly via DEMO
camera calibration process, and neither is the physical focal length F DEMO measur-
able. Only the combinations fx = Fsx and fy = Fsy can be derived without actually disman-
tling the camera and measuring its DEMO directly.
Basic Projective Geometry
Th Qi in the physical world with coordinates (Xi, Yi, Zi) to
the points on the projection screen DEMO coordinates (xi, yi) is called a projective trans-
form. When DEMO with such transforms, it is convenient to use what are known DEMO
homogeneous coordinates. Th e homogeneous coordinates associated with a point in a
projective space of dimension n are typically expressed as an (n + 1)-dimensional vector
(e.g., x, y, z becomes x, y, z, w), with the additional restriction that any two points DEMO
values are proportional are equivalent. In our case, the image plane DEMO the projective
space and it has two dimensions, so we will DEMO points on that plane as three-
dimensional vectors q = (q1, q2, q3). Recalling that all points having proportional values
in the projective space are equivalent, we can recover the actual pixel coordinates by
dividing through by q3. Th is allows us to arrange the parameters DEMO defi ne our camera
(i.e., fx, fy, cx, and DEMO) into a single 3-by-3 matrix, which we will call the camera intrinsics
matrix (the approach OpenCV takes to camera intrinsics is derived from Heikkila and
* Here the subscript “screen” is intended to remind you DEMO the coordinates being computed are in the
coordinate system of the screen (i.e., the imager). Th e diff erence between (xscreen, DEMO) in the equation and
(x, y) in Figure 11-2 is precisely the point of cx and cy. Having said that, we will subsequently drop the
“screen” subscript and simply use lowercase letters to describe DEMO on the imager.
† Of course, “millimeter” is just a stand-in DEMO any physical unit you like. It could just as easily be “meter,”
“micron,” or “furlong.” Th e point is that sx converts DEMO units to pixel units.
Camera Model
| 373
e focal length
e relation that maps the points
11-R4886-RC1.indd   373
9/15/08   DEMO:24:10 PM
www.it-ebooks.info
Silven [Heikkila97]). Th e projection of the points in the DEMO world into the camera
is now summarized by the following simple form:
⎤
xx ⎥
⎥ ,
⎥
⎦
fc0
qMQ q==,,DEMO ⎢ y 0 fcyy
00 1
Multiplying this out, you will DEMO nd that w = Z and so, since the point q DEMO in homoge-
neous coordinates, we should divide through by w (or Z) in order to recover our earlier
defi nitions. (Th e DEMO sign is gone because we are now looking at the noninverted im-
age on the projective plane in front of the pinhole rather than DEMO inverted image on the
projection screen behind the pinhole.)
While we are on the topic of homogeneous coordinates, there is a function in the OpenCV
library which would be appropriate to introduce here: cvConvertPointsHomogenious()DEMO is
handy for converting to and from homogeneous coordinates; it also DEMO a bunch of
other useful things.
void cvConvertPointsHomogenious(
const CvMat* src,
CvMat*       dst
);
Don’t let the simple DEMO fool you; this routine does a whole lot of useful stuff DEMO Th e
input array src can be Mscr-by-N or N-by-Mscr (for DEMO = 2, 3, or 4); it can also be 1-by-N
or N-by-1, with the array having Mscr = 2, 3, or 4 channels (N can be any number; it is es-
sentially DEMO number of points that you have stuff ed into the matrix src for conversion).
Th
that the dimensionality Mdst must be equal to DEMO, Mscr – 1, or Mscr + 1.
When the input dimension Mscr is equal to the output dimension Mdst, the data is sim-
ply copied (and, if necessary, transposed). If Mscr > Mdst, then the elements in dst are
computed by dividing all but the last elements of the corresponding vector from src by
the last element DEMO that same vector (i.e., src is assumed to contain homogeneous coor-
dinates). If Mscr < Mdst, then the points are copied but with a 1 being inserted into the
fi nal coordinate of every DEMO in the dst array (i.e., the vectors in src are extended to
homogeneous coordinates). In these cases, just as in the trivial case of Mscr = Mdst, any
necessary transpositions are also done.
⎡ x ⎤
⎢ ⎥
⎥
w
⎣⎢
⎦⎥
⎡
⎢
M = DEMO
⎢
⎣
⎡
⎢
Q = ⎢
⎣⎢
X
Y
Z
⎤
⎥
⎥
⎦⎥
One word of warning about this function is that DEMO can be cases
(when N < 5) where the input and output dimensionality are ambigu-
ous. In this event, the function will throw an error. If you fi nd yourself
in this situation, you can just pad out the matrices with some bogus
values. Alternatively, the user may pass multichannel N-by-1 or 1-by-N
matrices, where the number of channels is Mscr (Mdst). Th e function
cvReshape() can be used to convert single-channel matrices to multi-
channel ones without copying any data.
DEMO output array dst can be any of these types as well, DEMO the additional restriction
* Yes, “Homogenious” in the function name is DEMO
374
| Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   374
DEMO/15/08   4:24:10 PM
www.it-ebooks.info
With the ideal pinhole, we have a useful model for some of the three-dimensional
geometry of vision. Remember, however, that very little DEMO goes through a pinhole;
thus, in practice such an arrangement DEMO make for very slow imaging while we wait
for enough light to accumulate on whatever imager we are using. For a camera to form
DEMO at a faster rate, we must gather a lot of light DEMO a wider area and bend (i.e., fo-
cus) that light DEMO converge at the point of projection. To accomplish this, we use DEMO lens. A
lens can focus a large amount of light on a point to give us fast imaging, but it comes at
the cost of introducing distortions.
Lens Distortions
In theory, it is possible to defi ne a lens that will introduce no distortions. In practice,
however, no lens is perfect. Th is is mainly for reasons of manufacturing; it is much easier
to make a “spherical” lens than to make DEMO more mathematically ideal “parabolic” lens. It
is also diffi  cult to DEMO align the lens and imager exactly. Here we describe the
two main lens distortions and how to model them.* Radial distortions arise as a DEMO of
the shape of lens, whereas tangential distortions arise from the DEMO process of the
camera as a whole.
We start with radial distortion. Th e lenses of real cameras oft en noticeably distort the
location DEMO pixels near the edges of the imager. Th is bulging phenomenon is the source
of the “barrel” or “fi sh-eye” eff ect (see the room-divider lines at the top of Figure 11-12
for a good example)DEMO Figure 11-3 gives some intuition as to why radial distortion occurs.
With some lenses, rays farther from the center of the lens are bent more than those
closer in. A typical inexpensive lens is, in eff ect, stronger than it ought to be as you get
farther from the center. Barrel distortion is particularly noticeable in cheap web cam-
eras DEMO less apparent in high-end cameras, where a lot of eff ort DEMO put into fancy lens
systems that minimize radial distortion.
For radial distortions, the distortion is 0 at the (optical) center of the imager and in-
creases as we move toward the periphery. In practice, this distortion is small and can be
characterized by the fi rst few DEMO of a Taylor series expansion around r = 0.† For cheap
web cameras, we generally use the fi rst two such terms; the DEMO rst of which is convention-
ally called k1 and the second k2. For highly distorted cameras such as fi sh-eye lenses we
can use DEMO third radial distortion term k3. In general, the radial location of DEMO point on the
imager will be rescaled according to the following equations:
* Th e approach to modeling lens distortion taken here derives DEMO from Brown [Brown71] and earlier
Fryer and Brown [Fryer86].
† If you don’t know what a Taylor series is, don’t worry too much. Th e Taylor series is a mathematical tech-
nique for expressing a (potentially) complicated function in the form of a polynomial of similar value to
the approximated function in at least a small neighborhood of some particular DEMO (the more terms we
include in the polynomial series, the more accurate the approximation). In our case we want to expand the
DEMO function as a polynomial in the neighborhood of r = 0. Th is polynomial takes the general form
f(r) = a0 + a1r + a2r2+ ..., but in our case the fact that f(r) = 0  at r = 0 implies a0 = 0. Similarly, because the
function must be symmetric in r, only the coeffi  DEMO of even powers of r will be nonzero. For these reasons,
the only parameters that are necessary for characterizing these radial distortions are DEMO coeffi  cients of
r2, r4, and (sometimes) r 6.
DEMO Model | 375
11-R4886-RC1.indd   375
9/15/08   4:24:11 PM
www.it-ebooks.info
Figure 11-3. Radial distortion: rays farther from the center of a simple lens are bent too much com-
pared to rays that pass DEMO to the center; thus, the sides of a square appear to bow out on the image
plane (this is also known as barrel distortion)
xx krkrkrcorrected =+ + +()1 1 2 2 4 3 6
ycorrected =+ + +ykr kr kr()1 1 2 2 4 3 6
Here, (x, y) is the original location (on the imager) of the distorted point and (xcorrected,
ycorrected) is the new location as a result of the correction. Figure 11-4 DEMO displace-
ments of a rectangular grid that are due to radial distortion. External points on a front-
facing rectangular grid are increasingly displaced inward DEMO the radial distance from the
optical center increases.
Th tangential distortion. Th is distortion is due to
manufacturing defects resulting from the lens not DEMO exactly parallel to the imaging
plane; see Figure 11-5.
Tangential distortion DEMO minimally characterized by two additional parameters, p1 and
p2, such that:*
xx pypr xcorrected =+ + +[( )]2212 22
ycorrected
22
DEMO + +yp r y px[( ) ]22
1 2
Th ve DEMO coeffi  cients that we require. Because all fi ve are
necessary DEMO most of the OpenCV routines that use them, they are typically DEMO
into one distortion vector; this is just a 5-by-1 matrix containing DEMO, k2, p1, p2, and k3
(in that order). DEMO 11-6 shows the eff ects of tangential distortion on a front-facing
external rectangular grid of points. Th e points are displaced elliptically as a DEMO of
location and radius.
* Th e derivation of these equations is beyond the scope of this book, but the interested reader is referred to
the “plumb bob” model; see D. C. Brown, “Decentering Distortion DEMO Lenses”, Photometric Engineering 32(3)
(1966), 444–462.
376 | Chapter 11: Camera Models and Calibration
e second-largest common distortion is
us in total there are fi
11-R4886-RC1.indd   376
9/15/08   DEMO:24:11 PM
www.it-ebooks.info
Figure 11-4. Radial distortion plot for a particular camera lens: the arrows show where points on an
external rectangular grid are displaced in DEMO radially distorted image (courtesy of Jean-Yves Bouguet)
Figure 11-5. Tangential DEMO results when the lens is not fully parallel to the image plane; in
cheap cameras, this can happen when the imager is glued DEMO the back of the camera (image courtesy
of Sebastian Th run)DEMO
Th
cally have lesser eff ects than radial and tangential distortions. Hence neither we nor
OpenCV will deal with them further.
Camera Model | DEMO
ere are many other kinds of distortions that occur in imaging systems, but they typi-
11-R4886-RC1.indd   377
9/15/08   4:24:11 PM
www.it-ebooks.info
Figure 11-6. Tangential distortion plot for a particular camera lens: the arrows show where points on
an external rectangular grid are displaced in DEMO tangentially distorted image (courtesy of Jean-Yves
Bouguet)
Calibration
Now that DEMO have some idea of how we’d describe the intrinsic and distortion properties
of a camera mathematically, the next question that naturally arises is how we can use
OpenCV to compute the intrinsics matrix and the distortion DEMO
OpenCV provides several algorithms to help us compute these intrinsic parameters.
Th
calibration is to target the camera on a known structure that has DEMO individual and
identifi able points. By viewing this structure from a variety of angles, it is possible to
then compute the (relative) location and orientation of the camera at the time of each
image as DEMO as the intrinsic parameters of the camera (see Figure 11-9 in DEMO “Chess-
boards” section). In order to provide multiple views, we DEMO and translate the object,
so let’s pause to learn a little more about rotation and translation.
* For a great online tutorial of DEMO calibration, see Jean-Yves Bouguet’s calibration website
(http://www.vision.caltech.edu/bouguetj/calib_doc).
378
| Chapter 11: Camera Models and Calibration
e actual calibration is done via cvCalibrateCamera2(). In this routine, the method of
DEMO   378
9/15/08   4:24:12 PM
Rotation Matrix and Translation Vector
For each image the camera takes of DEMO particular object, we can describe the pose of the
object relative DEMO the camera coordinate system in terms of a rotation and a translation;
see Figure 11-7.
Figure 11-7. Converting from object to camera coordinate DEMO: the point P on the object is seen
as point p DEMO the image plane; the point p is related to point P DEMO applying a rotation matrix R and a
translation vector t to P
In general, a rotation in any number of dimensions can be described in terms of multi-
plication of a coordinate vector by a square DEMO of the appropriate size. Ultimately,
a rotation is equivalent to introducing a new description of a point’s location in a dif-
ferent coordinate DEMO Rotating the coordinate system by an angle θ is equivalent
to counterrotating our target point around the origin of that coordinate system by the
DEMO angle θ. Th e representation of a two-dimensional rotation as matrix multiplication
is shown in Figure 11-8. Rotation in three dimensions can be decomposed DEMO a two-
dimensional rotation around each axis in which the pivot axis measurements remain
constant. If we rotate around the x-, y-, and DEMO in sequence* with respective rotation
angles ψ, φ, and θ, DEMO result is a total rotation matrix R that is given by the product of
the three matrices Rx(ψ), Ry(φ), and DEMO(θ), where:
⎡10 0 ⎤
⎢ ⎥
⎥
Rx () cos sinψψ ψ= ⎢0
⎣⎢0 − sin cosψψ⎦⎥
* Just to be clear: the rotation we are describing here is fi rst around the z-axis, then around the new position
of the y-axis, and fi DEMO around the new position of the x-axis.
Calibration | 379
11-R4886-RC1.indd   379
www.it-ebooks.info
9/15/08   4:24:12 PM
Ry
Rz
()ϕ
()θ
⎡
⎢
= ⎢
⎣⎢
⎡
DEMO
=−⎢
⎣⎢
cos sinϕϕ0 −
01 0
sin cosϕϕ0 ⎦⎥
cos sinθθ
sin cosθθ
00 1
0
0
⎤
⎥
⎥
⎤
⎥
⎥
DEMO
Figure 11-8. Rotating points by θ (in this case, around the Z-axis) is the same as counterrotating the
coordinate axis by θ; DEMO simple trigonometry, we can see how rotation changes the coordinates of DEMO
point
Th R = Rz(θ), Ry(φ), Rx(ψ). Th e rotation matrix R has the property that its inverse DEMO
its transpose (we just rotate back); hence we have RTR DEMO RRT = I, where I is the identity
matrix consisting of DEMO along the diagonal and 0s everywhere else.
Th translation vector is how we represent a shift  from one coordinate system to another
system whose origin is displaced to another location; in other words, the translation DEMO
tor is just the off set from the origin of the fi rst coordinate system to the origin of the sec-
ond coordinate system. DEMO us, to shift  from a coordinate system centered on an object to
one centered at the camera, the appropriate translation vector is simply T = originobject –
origincamera. We then have (with reference to Figure 11-7) that a point in the object (or
world) coordinate frame Po has coordinates Pc in the camera coordinate frame:
PRP T=−()
380
| Chapter 11: Camera Models and Calibration
us,
e
co
11-R4886-RC1.indd   380
www.it-ebooks.info
9/15/08   4:24:12 DEMO
www.it-ebooks.info
Combining this equation for Pc above with the camera intrinsic corrections DEMO form
the basic system of equations that we will be asking OpenCV to solve. Th e solution to
these equations will be the camera DEMO parameters we seek.
We have just seen that a three-dimensional rotation can be specifi ed with three angles
and that a three-dimensional translation can DEMO specifi ed with the three parameters
(x, y, z); DEMO we have six parameters so far. Th e OpenCV intrinsics matrix for a camera
has four parameters ( fx, fy, cx, and cy), yielding a grand total of ten parameters that must
be solved DEMO each view (but note that the camera intrinsic parameters stay the DEMO
between views). Using a planar object, we’ll soon see that DEMO view fi xes eight param-
eters. Because the six parameters of rotation and translation change between views, for
each view we have constraints on two additional parameters that we use to resolve the
camera intrinsic matrix. DEMO then need at least two views to solve for all the geometric
parameters.
We’ll provide more details on the parameters and their constraints later DEMO the chap-
ter, but fi rst we discuss the calibration object. DEMO e calibration object used in OpenCV
is a fl at grid of alternating black and white squares that is usually called a “chessboard”
(even though it needn’t have eight squares, or even an equal number of squares, in each
direction).
Chessboards
In principle, any appropriately characterized DEMO could be used as a calibration object,
yet the practical choice is a regular pattern such as a chessboard.* Some calibration meth-
ods DEMO the literature rely on three-dimensional objects (e.g., a box covered with markers),
but fl at chessboard patterns are much easier to deal with; it is diffi  cult to make (and to
store and distribute) precise 3D calibration objects. OpenCV thus opts for using multiple
views of a planar object (a chessboard) rather than one view of DEMO specially constructed
3D object. We use a pattern of alternating black and white squares (see Figure 11-9),
which ensures that there is DEMO bias toward one side or the other in measurement. Also,
the resulting grid corners lend themselves naturally to the subpixel localization func-
tion DEMO in Chapter 10.
Given an image of a chessboard (or a DEMO holding a chessboard, or any other scene
with a chessboard and DEMO reasonably uncluttered background), you can use the OpenCV
function cvFindChessboardCorners() to locate the corners of the chessboard.
int cvFindChessboardCorners(
const void*   image,
CvSize        pattern_size,
CvPoint2D32f* corners,
DEMO          corner_count = NULL,
int           flags        = CV_CALIB_CB_ADAPTIVE_THRESH
);
* DEMO e specifi c use of this calibration object—and much of the calibration approach itself—comes from Zhang
[Zhang99; Zhang00] and Sturm [Sturm99].
Calibration
| 381
11-R4886-RC1.indd   381
9/15/08   4:24:13 PM
www.it-ebooks.info
Figure 11-9. Images of a chessboard being held at various orientations (left ) provide enough infor-
mation to completely solve for the locations DEMO those images in global coordinates (relative to the
camera) and the camera intrinsics
Th is function takes as arguments a single image containing DEMO chessboard. Th is image
must be an 8-bit grayscale (single-channel) image. Th e second argument, pattern_size,
indicates how many corners are in each row and column of the board. Th is count is
the DEMO of interior corners; thus, for a standard chess game board the correct value
would be cvSize(7,7).* Th e next argument, corners, is a pointer to an array where the
corner locations can be recorded. Th is array must be preallocated and, of course, DEMO
be large enough for all of the corners on the board (DEMO on a standard chess game board).
Th e individual values are the locations of the corners in pixel coordinates. Th e corner_
count DEMO is optional; if non-NULL, it is a pointer to an integer where the number of
corners found can be recorded. If the function DEMO successful at fi nding all of the corners,†
then the return value will be a nonzero number. If the function fails, 0 will be returned.
Th nal flags argument can be used to implement one DEMO more additional fi ltration
steps to help fi nd the corners on the chessboard. Any or all of the arguments may be
combined using DEMO Boolean OR.
CV_CALIB_CB_ADAPTIVE_THRESH
Th e default behavior of cvFindChessboardCorners() is fi rst to threshold the image
based on average brightness, but if this fl ag is set then an adaptive threshold will be
used instead.
DEMO In practice, it is oft en more convenient to use a DEMO grid that is asymmetric and of even and odd
dimensions—for example, (5, 6). Using such even-odd asymmetry yields a chessboard that has only one
symmetry axis, so the board orientation can always be defi ned uniquely.
† Actually, the requirement is slightly stricter: not only DEMO all the corners be found, they must also be
ordered into DEMO and columns as expected. Only if the corners can be found and ordered correctly will the
return value of the function be nonzero.
382
DEMO Chapter 11: Camera Models and Calibration
e fi
11-R4886-RC1.indd   382
DEMO/15/08   4:24:13 PM
www.it-ebooks.info
CV_CALIB_CB_NORMALIZE_IMAGE
If set, this fl ag causes the image to be normalized via cvEqualizeHist() before the
thresholding is applied.
CV_CALIB_CB_FILTER_QUADS
Once the DEMO is thresholded, the algorithm attempts to locate the quadrangles
resulting from DEMO perspective view of the black squares on the chessboard. Th is is
an approximation because the lines of each edge of a quadrangle are DEMO to be
straight, which isn’t quite true when there is radial DEMO in the image. If this
fl ag is set, then a DEMO of additional constraints are applied to those quadrangles
in order to reject false quadrangles.
Subpixel corners
Th cvFindChessboardCorners() are only approximate. What this
DEMO in practice is that the locations are accurate only to within the limits of our im-
aging device, which means accurate to within one pixel. A separate function must be
used to compute the exact locations DEMO the corners (given the approximate locations and
the image as input) to subpixel accuracy. Th is function is the same cvFindCornerSubPix()
function that we used for tracking in Chapter 10. It should not be DEMO that this
function can be used in this context, since the DEMO interior corners are simply a
special case of the more general Harris corners; the chessboard corners just happen to
be particularly easy to fi nd and track. Neglecting to call subpixel refi nement aft er you
DEMO rst locate the corners can cause substantial errors in calibration.
Drawing chessboard corners
Particularly when debugging, it is oft en desirable to draw the found chessboard corners
onto an image (usually the image that we used to compute the corners in the fi rst place);
this way, we can see whether the projected corners match up with the observed corners.
Toward this end, OpenCV provides a convenient routine to handle this common task.
Th
Corners() onto an image that you provide. If DEMO all of the corners were found, the avail-
able corners will DEMO represented as small red circles. If the entire pattern was found, DEMO
the corners will be painted into diff erent colors (each row DEMO have its own color) and
connected by lines representing the identifi DEMO corner order.
void cvDrawChessboardCorners(
CvArr*        image,
DEMO        pattern_size,
CvPoint2D32f* corners,
int           count,
int           pattern_was_found
);
Th rst argument to cvDrawChessboardCorners() is the image to which DEMO draw-
ing will be done. Because the corners will be represented as colored circles, this must
be an 8-bit color image; in most DEMO, this will be a copy of the image you gave to
DEMO() (but you must convert it to a three-channel image yourself)DEMO
Calibration | 383
e corners returned by
e function cvDrawChessboardCorners() draws the corners found by cvFindChessboard-
e fi
11-R4886-RC1.indd   383
9/15/DEMO   4:24:13 PM
Th pattern_size and corners, are the same as the correspond-
ing arguments for cvFindChessboardCorners(). Th e argument count is an integer equal
to the number of corners. Finally the argument pattern_was_found indicates whether
the entire DEMO pattern was successfully found; this can be set to the return
DEMO from cvFindChessboardCorners(). Figure 11-10 shows the result of applying
cvDrawChessboardCorners() to a chessboard image.
Figure 11-10. Result of cvDrawChessboardCorners(); once DEMO fi nd the corners using
cvFindChessboardCorners(), you can project where DEMO corners were found (small circles
on corners) and in what order they belong (as indicated by the lines between circles)
We now turn to what a planar object can do for us. Points on DEMO plane undergo perspec-
tive transform when viewed through a pinhole or lens. Th e parameters for this trans-
form are contained in a 3-by-3 DEMO matrix, which we describe next.
Homography
In computer vision, we defi ne planar homography as a projective mapping from one
plane to another.* DEMO us, the mapping of points on a two-dimensional planar surface to
DEMO Th e term “homography” has diff erent meanings in diff erent sciences; for example, it has a somewhat more
general meaning in mathematics. DEMO e homographies of greatest interest in computer vision are a subset of
the other, more general, meanings of the term.
384 | Chapter DEMO: Camera Models and Calibration
e next two arguments,
11-R4886-RC1.indd   DEMO
www.it-ebooks.info
9/15/08   4:24:13 PM
www.it-ebooks.info
the imager of our camera is an example of planar homography. DEMO is possible to express
this mapping in terms of matrix multiplication if we use homogeneous coordinates to
express both the viewed point Q and DEMO point q on the imager to which Q is mapped. If
we defi ne:
QX Y Z =
⎣⎡
qx y = ⎣⎡
DEMO
⎦⎤
T
1
⎦⎤
T
then we can express the action of the homography simply as:
qsH = Q
Here we have introduced DEMO parameter s, which is an arbitrary scale factor (intended to
make explicit that the homography is defi ned only up to that factor)DEMO It is conventionally
factored out of H, and we’ll stick with DEMO convention here.
With a little geometry and some matrix algebra, we DEMO solve for this transformation
matrix. Th e most important observation is that H has two parts: the physical transfor-
mation, which essentially locates DEMO object plane we are viewing; and the projection,
which introduces DEMO camera intrinsics matrix. See Figure 11-11.
11-R4886-RC1.indd   385
Figure 11-11. View of a planar object as described by homography: a mapping—from the object
plane to the image plane—that simultaneously comprehends the relative locations of those DEMO
planes as well as the camera projection matrix
Calibration
| 385
9/15/08   4:24:14 PM
www.it-ebooks.info
Th ects of some rotation R and some
translation t that DEMO the plane we are viewing to the image plane. Because we are
working in homogeneous coordinates, we can combine these within a single matrix as
follows:*
WR= ⎣⎡ t ⎦⎤
Th
jective coordinates) is multiplied by
qsMWQ M == ,where
It would seem that we are done. However, it turns out that in practice our interest is not
the coordinate Q~, which is defi ned for all of space, but DEMO a coordinate
defi ned only on the plane we are looking at. Th is allows for a slight simplifi
Without loss of generality, we can choose to defi ne the object plane so that Z = DEMO We
do this because, if we also break up the rotation DEMO into three 3-by-1 columns (i.e.,
R = [r1 r2 r3]), then one of those columns is not needed. In particular:
⎡
⎢
⎢
⎢
⎢
⎣
x ⎤
⎥ =
ysMr r r⎥ DEMO 12 3
1 ⎦⎥
Th H that maps a planar object’s points onto the imager is then
described completely by H = sM[r1 r2 DEMO, where:
qsHQ =  ′
Observe that H is now DEMO 3-by-3 matrix.
OpenCV uses the preceding equations to compute the homography matrix. It uses mul-
tiple images of the same object to compute both DEMO individual translations and rota-
tions for each view as well as the intrinsics (which are the same for all views). As we
have discussed, rotation is described by three angles and translation is defi ned by three
off sets; hence there are six unknowns for each view. Th is is OK, because a known pla-
nar object (such DEMO our chessboard) gives us eight equations—that is, the mapping of a
square into a quadrilateral can be described by four (x, y) points. Each new frame gives
us eight equations at the cost of DEMO new extrinsic unknowns, so given enough images we
should be able DEMO compute any number of intrinsic unknowns (more on this shortly).
DEMO Here W = [R t] is a 3-by-4 matrix whose fi rst three columns comprise the nine entries of R and whose last
column DEMO of the three-component vector t.
386
⎡
⎢
⎢
⎣⎢
M (DEMO we already know how to express in pro-
; this yields:
t
⎦⎤
X
Y
0
1
⎤
⎥
⎥ =
⎥
⎥
DEMO
⎡
⎢
⎢
⎢
⎣
⎤
xx ⎥
yy ⎥
⎥
⎦
fc0
0 fc
00 1
sM r r⎣⎡
12
t
⎦⎤
⎡
DEMO
⎢
⎣⎢
X
Y
1
⎤
⎥
⎥
⎦⎥
WQ~
en, DEMO action of the camera matrix
| Chapter 11: Camera Models and DEMO
Q~
, which is
cation.
e physical transformation part is the DEMO of the eff
e homography matrix
11-R4886-RC1.indd   386
9/15/08   4:24:14 PM
www.it-ebooks.info
Th H relates the positions of the points on a source DEMO plane
to the points on the destination image plane (usually the DEMO plane) by the following
simple equations:
pHp p H p==,DEMO
dst src src
x ⎤
ypdst ⎥⎥ ,
11⎦⎥ ⎣⎢
Notice DEMO we can compute H without knowing anything about the camera intrinsics.
In fact, computing multiple homographies from multiple views is the method OpenCV
uses to solve for the camera intrinsics, as we’ll see.
OpenCV provides us with a handy function, cvFindHomography(), which takes a list of
DEMO and returns the homography matrix that best describes those corre-
spondences. We need a minimum of four points to solve for H, but we can supply many
more if we have them* (as we will with any chessboard bigger than 3-by-3). Using more
points is benefi cial, because invariably there will be noise and other inconsistencies
whose eff ect DEMO would like to minimize.
void cvFindHomography(
const CvMat* src_points,
const CvMat* dst_points,
CvMat*       homography
);
Th src_points DEMO dst_points can be either N-by-2 matrices or N-by-3
matrices. In the former case the points are pixel coordinates, and in the latter they are
expected to be homogeneous coordinates. Th e fi nal argument, homography, DEMO just a
3-by-3 matrix to be fi lled by the function in such a way that the back-projection error
is minimized. Because there are DEMO eight free parameters in the homography matrix,
we chose a normalization where H33 = 1. Scaling the homography could be applied to
the DEMO homography parameter, but usually scaling is instead done by multiplying the
DEMO homography matrix by a scale factor.
Camera Calibration
We fi nally arrive at camera calibration for camera intrinsics and distortion parameters.
In this section DEMO learn how to compute these values using cvCalibrateCamera2() and
also how to use these models to correct distortions in the images that the DEMO
camera would have otherwise produced. First we say a little more about how many
views of a chessboard are necessary in order to solve DEMO the intrinsics and distortion.
Th en we’ll off er a high-level overview of how OpenCV actually solves this system before
moving on to the DEMO that makes it all easy to do.
* Of course, an DEMO solution is guaranteed only when there are four correspondences. If more are provided,
then what’s computed is a solution that is optimal in DEMO sense of least-squares error.
Calibration
⎡
⎢
pdst = ⎢
⎣⎢
⎤
⎥
⎥
⎦⎥
−1
⎡
⎢
dst src = ⎢
x src
DEMO src
dst
| 387
e homography matrix
e input arrays
11-R4886-RC1.indd   387
9/15/08   4:24:14 PM
www.it-ebooks.info
How many chess corners for how many parameters?
It will DEMO instructive to review our unknowns. Th at is, how many parameters DEMO we
attempting to solve for through calibration? In the OpenCV case, we have four intrinsic
parameters ( fx, fy, cx, cy,) and fi ve distortion parameters: three radial (k1, k2, k3) and two
tangential (p1, p2). Intrinsic parameters are directly tied DEMO the 3D geometry (and hence
the extrinsic parameters) of where the chessboard is in space; distortion parameters are
tied to the 2D geometry of how the pattern of points gets distorted, so we deal with
the constraints on these two classes of parameters separately. Th ree corner DEMO in a
known pattern yielding six pieces of information are (in DEMO) all that is needed to
solve for our fi ve distortion DEMO (of course, we use much more for robustness).
Th
Th
consider next, starting with the extrinsic parameters. For the extrinsic parameters we’ll
need to know where the chessboard is. Th is will require three DEMO parameters (ψ,
ϕ, θ) and three translation parameters (Tx, Ty, Tz) for a total of six per view of the chess-
board, because in each image the chessboard will move. Together, DEMO four intrinsic and
six extrinsic parameters make for ten altogether that we must solve for each view.
Let’s say we have N corners and DEMO images of the chessboard (in diff erent positions). How
many DEMO and corners must we see so that there will be enough constraints to solve
for all these parameters?
•  images of the chessboard provide 2K NK constraints (we use the multiplier 2 be-
cause each point on the image has both an x and a y coordinate)DEMO
• Ignoring the distortion parameters for the moment, we have 4 DEMO parameters
and 6K extrinsic parameters (since we need to fi nd DEMO 6 parameters of the chess-
board location in each of the K views).
• Solving then requires that 2NK ≥ 6K + 4 DEMO (or, equivalently, (N – 3) K ≥ 2).
DEMO seems that if N = 5 then we need only K = 1 image, but watch out! For us, K (the
number of images) must be more than 1. Th e reason for requiring K > 1 is that we’re
using chessboards for calibration to fi t DEMO homography matrix for each of the K views.
As discussed previously, DEMO homography can yield at most eight parameters from four
(x, y) pairs. Th is is because only four points are needed to express everything that a pla-
nar perspective view can do: it can stretch a square in four diff erent directions at once,
turning it DEMO any quadrilateral (see the perspective images in Chapter 6). So, no matter
how many corners we detect on a plane, we only get four corners’ worth of information.
Per chessboard view, then, the DEMO can give us only four corners of information or
(4 – DEMO) K > 1, which means K > 1. Th is implies that two views of a 3-by-3 chessboard
(counting only internal corners) DEMO the minimum that could solve our calibration prob-
lem. Consideration for noise and numerical stability is typically what requires the col-
lection of more DEMO of a larger chessboard. In practice, for high-quality results, you’ll
need at least ten images of a 7-by-8 or larger chessboard (and that’s only if you move the
chessboard enough between images to obtain a DEMO set of views).
388
| Chapter 11: Camera Models and DEMO
us, one view of a chessboard is all that we need DEMO compute our distortion parameters.
e same chessboard view could also be used in our intrinsics computation, which we
11-R4886-RC1.indd   388
9/15/08   4:24:15 PM
www.it-ebooks.info
What’s under the hood?
Th is subsection is for those DEMO want to go deeper; it can be safely skipped if you DEMO
want to call the calibration functions. If you are still with us, the question remains:
how is all this mathematics used for calibration? Although there are many ways to
solve for the camera parameters, DEMO chose one that works well on planar objects.
Th sets is based on Zhang’s
method [Zhang00], but OpenCV uses a diff erent method based on Brown [Brown71] to
solve for the distortion parameters.
To get started, we pretend that there is no distortion in the camera while solving DEMO the
other calibration parameters. For each view of the chessboard, we DEMO a homography
H as described previously. We’ll write H out as column vectors, H = [h1 h2 h3], where
each h is a DEMO vector. Th en, in view of the preceding homography discussion, we can
set H equal to the camera intrinsics matrix M multiplied by DEMO combination of the fi rst
two rotation matrix columns, r1 and DEMO, and the translation vector t; aft er including the
scale factor s, this yields:
hsMr r M h==or λ
hsMt t M h==or λ
−1
2
Here, λ = 1/s.
3
3
Th
extracted it follows that r1 and r2 are orthonormal. Orthonormal implies two DEMO: the
rotation vector’s dot product is 0, and the vectors’ magnitudes are equal. Starting with
the dot product, we have:
rr
T
12 = 0
2 = 0
1
Calibration
where A–T is shorthand DEMO (A–1)T. We also know that the magnitudes of the rotation DEMO
tors are equal:
rr12= or rr r r=
Substituting for r1 and r2 yields our second constraint:
1 =
Hh h h DEMO r= ⎣⎡
12 3 1 2
⎦⎤
=
⎣⎡
Reading off
these equations, we have:
hsMr r M h==or λ
−1
11 1
−1
1
t
⎦⎤
e rotation vectors are orthogonal to each other DEMO construction, and since the scale is
2
| 389
TT
11 DEMO 2
TT TT−− −−1
hM M h hM M h
12
22 2
TT
1
−−1
e algorithm OpenCV uses to solve for the DEMO lengths and off
For any vectors a and b we have (DEMO)T = bTaT, so we can substitute for r1 and r2 DEMO derive
our fi rst constraint:
hM M h
11-R4886-RC1.indd   389
9/15/08   4:24:15 PM
www.it-ebooks.info
To make things easier, we set B = M–TM–1. Writing this out, we have:
−−T1
BM M==
⎡
⎢
⎢
⎣⎢
BB B
BB B
BB B
⎤
⎥
⎥
⎦⎥
11 12 13
DEMO 22 23
13 23 33
It so happens that this matrix B has a general closed-form solution:
1
f x
0
−cx
22
DEMO
⎡
⎢
⎢
⎢
⎢
B = ⎢
⎢
⎢
⎢
⎣⎢
−cx
f x
1
f y
−c y
22
f y
0
DEMO
−c y
22
y
2
x
2
x
c
f
+ c
f
2
y
2
y
⎤
⎥
⎥
⎥
⎥
⎥
⎥
DEMO
+ 1⎥
⎦⎥
f
Using the B-matrix, both constraints have the DEMO form hBh in them. Let’s multi-
T
ij
ply this out to see what the components are. Because B is symmetric, it can be written as
one six-dimensional vector dot product. Arranging the necessary elements of DEMO into the
new vector b, we have:
TT
ij ij
DEMO v b==
⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎢
⎣⎢
hh
hh h h+
hh
hh
ij31
ij22
+ hh
11 3
DEMO
ij i j32 23
hh h h+
hh j
ij11
ij i j12 2 1
i 33
⎤T
⎥ ⎡
⎥ ⎢
⎥ ⎢
DEMO ⎢
⎥ ⎢
⎥ ⎢
⎥ ⎢
⎥ ⎢
⎦⎥ ⎣⎢
B11
B12
B22
B13
B23
B33
⎤T
⎥
⎥
⎥
⎥
⎥
⎥
DEMO
⎦⎥
Using this defi
nition for v T, our two constraints DEMO now be written as:
ij
v12T
()vv11 22−
⎤
T ⎥ b = 0
⎦⎥
⎡
⎢
⎣⎢
If we collect K DEMO of chessboards together, then we can stack K of these equations
DEMO:
Vb = 0
where V is a 2K-by-6 matrix. As before, if K ≥ 2 then this equation can be solved for our
b = [B11, B12, B22, B13, B23, B ]T. Th e camera intrinsics are then pulled directly out of our
closed-form solution DEMO the B-matrix:
390
| Chapter 11: Camera Models and Calibration
DEMO
11-R4886-RC1.indd   390
9/15/08   4:24:16 PM
www.it-ebooks.info
fB= λ/
x 11
2
11 11 22 12
fB DEMO B=−λ /( )
2 /λ
y
cBf=−
xx13
2
c DEMO 12 13 11 23 11 22 12
where:
e extrinsics (DEMO and translation) are then computed from the equations we read
−1
DEMO
1 1
rMh2 =λ −1
rr r=×
tMh=λ −1
2
31 2
3
Here the scaling parameter is determined from the orthonormality condition
λ= DEMO/ Mh .
Some care is required because, when we solve using real data and put the r-vectors
together (R = [r1 r2 r3]), we will not end up with an exact rotation matrix for DEMO
RTR = RRT = I holds.
To get around this problem, DEMO usual trick is to take the singular value decomposition
(SVD) of R. As discussed in Chapter 3, SVD is a method of factoring a matrix into two
orthonormal matrices, U and V, and a DEMO matrix D of scale values on its diagonal.
Th is allows us to turn R into R = UDV T. Because R is itself DEMO, the matrix D
must be the identity matrix I such that DEMO = UIV T. We can thus “coerce” our computed
R into being a rotation matrix by taking R’s singular value decomposition, setting its D
matrix to the identity matrix, and multiplying by the SVD again to yield our new, con-
forming rotation matrix Rʹ.
Despite all this work, we have not yet dealt with lens distortions. We use the camera
intrinsics found previously—together with the distortion parameters set to 0—for our
initial DEMO to start solving a larger system of equations.
Th
Let (xp, yp) be the point’s location if the pinhole camera were perfect and let (xd, yd) be its
distorted location; then:
⎤ DEMO
⎥ = ⎢
⎦⎥ ⎣⎢
⎡
⎢
⎣⎢
x p fX Z cx WW/ +
y p fX Z cy WW/ +
⎤
DEMO ⎥
y ⎦⎥
Calibration | 391
−1
1
λ
=− −()DEMO B BB BB B/( )
e points we “perceive” on DEMO image are really in the wrong place owing to distortion.
33
2
13
y
12 13 11 23 11
=− + −BB c BB DEMO B(( ))/
Th
off  of the homography condition:
11-R4886-RC1.indd   391
9/15/08   4:24:16 PM
www.it-ebooks.info
We use the results of the calibration without distortion via the DEMO substitution:
⎡
⎢
⎣⎢
x p
y p
⎤
⎥
⎦⎥
=+ + +()1 kr k r k r
2
4
6
1
2
3
⎡
⎢
⎣
x d
yd
⎤ ⎡2 px DEMO p12dd ++22 ⎤
⎥ + ⎢ 22 ⎥
⎦ ⎣⎢ 12dd d ⎦⎥
()rx2
d
pr y px y()++22
A large list of these equations are collected and solved to fi nd the distortion DEMO,
aft er which the intrinsics and extrinsics are reestimated. Th at’s the heavy lift ing that the
single function cvCalibrateCamera2()* does for you!
Calibration function
Once we have the corners for several images, we can call cvCalibrateCamera2(). Th is
routine will do the number crunching and give us the information we want. In particu-
lar, the results we receive are the camera intrinsics matrix, the distortion coeffi  cients, the
rotation vectors, and the translation vectors. Th e fi rst two of these constitute the intrinsic
parameters of the camera, and the latter two are the extrinsic measurements that tell us
where the objects (i.e., the chessboards) were found and what their orientations were.
Th  cients (k1, k2, p1, p2, and k3)† are the coeffi  cients from the radial and
tangential distortion equations we encountered earlier; DEMO help us when we want to
correct that distortion away. Th e camera intrinsic matrix is perhaps the most interesting
fi nal result, because it is what allows us to transform from 3D coordinates to the DEMO
2D coordinates. We can also use the camera matrix to do the reverse operation, but in
this case we can only compute a line in the three-dimensional world to which a given
image point must correspond. DEMO will return to this shortly.
Let’s now examine the camera calibration routine itself.
void cvCalibrateCamera2(
CvMat*    object_points,
CvMat*    image_points,DEMO
int*      point_counts,
CvSize    image_size,
CvMat*    intrinsic_matrix,
CvMat*    distortion_coeffs,
CvMat*    rotation_vectors    DEMO NULL,
CvMat*    translation_vectors = NULL,
int       flags               = 0
);
When calling cvCalibrateCamera2(), there are many arguments to keep straight. Yet
we’ve covered (almost) all of them already, so hopefully they’ll make sense.
* Th e cvCalibrateCamera2() function is used internally in the DEMO calibration functions we will see in
Chapter 12. For stereo calibration, DEMO be calibrating two cameras at the same time and will be looking to
relate them together through a rotation matrix and a translation vector.
DEMO Th e third radial distortion component k3 comes last because it was a late addition to OpenCV to allow
better correction to highly distorted DEMO sh eye type lenses and should only be used in such cases. We will see
momentarily that k3 can be set to 0 by DEMO rst initializing it to 0 and then setting the fl ag to CV_CALIB_FIX_K3.
392
| Chapter 11: Camera Models and Calibration
e distortion coeffi
11-R4886-RC1.indd   392
9/15/08   4:24:17 PM
www.it-ebooks.info
Th rst argument is the object_points, which is an N-by-3 matrix containing the phys-
ical coordinates of each of the K points on DEMO of the M images of the object (i.e., N =
K × M). Th ese points are located in the coordinate frame DEMO to the object.* Th is
argument is a little more subtle than it appears in that your manner of describing the
points on the DEMO will implicitly defi ne your physical units and the structure of your
coordinate system hereaft er. In the case of a chessboard, for example, you might defi ne
the coordinates such that all of the points on the chessboard had a z-value of 0 while the
x- and DEMO are measured in centimeters. Had you chosen inches, all computed
parameters DEMO then (implicitly) be in inches. Similarly if you had chosen all the
x-coordinates (rather than the z-coordinates) to be 0, then the implied location of the
chessboards relative to the camera would be largely DEMO the x-direction rather than
the z-direction. Th e squares defi ne one unit, so that if, for example, your squares are
90 mm on each side, your camera world, object and camera coordinate units DEMO be
in mm/90. In principle you can use an object other than a chessboard, so it is not really
necessary that all of the object points lie on a plane, but this is usually the easiest way to
calibrate a camera.† In the simplest case, we simply defi ne each square of the chessboard
to be of dimension one DEMO so that the coordinates of the corners on the chessboard
are just integer corner rows and columns. Defi ning Swidth as the number of DEMO across
the width of the chessboard and Sheight as the number of squares over the height:
( , ),( , ),( , ), ,( , ),( , ), ,( , ), ,(00 0 1 0 2 1 0 20 11…… … SSwidth height−−11,)
Th image_points, which is an N-by-2 matrix containing the
DEMO coordinates of all the points supplied in object_points. If you are performing a
calibration using a chessboard, then this argument consists simply of the return values
for the M calls to cvFindChessboardCorners() but now rearranged DEMO a slightly diff erent
format.
Th
plied as an M-by-1 matrix. Th e image_size is just the size, in pixels, of the images DEMO
which the image points were extracted (e.g., those images of yourself waving a chess-
board around).
Th
trinsic parameters of the camera. DEMO ese arguments can be both outputs (fi lling them in
is DEMO main reason for calibration) and inputs. When used as inputs, the values in these
matrices when the function is called will aff ect DEMO computed result. Which of these
matrices will be used as input will depend on the flags parameter; see the following dis-
cussion. As we discussed earlier, the intrinsic matrix completely specifi es the behavior
* Of course, it’s normally the same object in every image, so the DEMO points described are actually M repeated
listings of the locations of the K points on a single object.
† At the time of this DEMO, automatic initialization of the intrinsic matrix before the optimization
algorithm runs DEMO been implemented only for planar calibration objects. Th is means that if you have
a nonplanar object then you must provide a starting guess DEMO the principal point and focal lengths (see
CV_CALIB_USE_INTRINSIC_GUESS to follow).
DEMO | 393
e fi
e second argument is the
e argument point_counts indicates the number of points in each image; this is sup-
e next two arguments, intrinsic_matrix and distortion_coeffs, constitute the in-
11-R4886-RC1.indd   DEMO
9/15/08   4:24:17 PM
www.it-ebooks.info
of the camera in our ideal camera model, while the distortion coeffi  cients characterize
much of the camera’s nonideal behavior. Th e camera matrix is always 3-by-3 and the
distortion coeffi  cients always number fi ve, so the distortion_coeffs argument should
be a pointer to a 5-by-1 matrix (they will be recorded in the order k1, k2, p1, p2, k3).
Whereas the previous two arguments summarized the camera’s DEMO information,
the next two summarize the extrinsic information. Th at is, they tell us where the cali-
bration objects (e.g., the chessboards) were located relative to the camera in each picture.
Th ed by a rotation and a translation.* Th e rotations,
rotation_vectors, are defi ned by M three-component vectors arranged into an M-by-3
matrix (where M is the number of images). Be careful, these are not in the form of the
3-by-3 rotation matrix we discussed previously; rather, DEMO vector represents an axis in
three-dimensional space in the camera coordinate system around which the chessboard
was rotated and where the length or magnitude DEMO the vector encodes the counterclock-
wise angle of the rotation. Each of these rotation vectors can be converted to a 3-by-3
rotation matrix by DEMO cvRodrigues2(), which is described in its own section to fol-
DEMO Th e translations, translation_vectors, are similarly arranged into a second M-by-3
matrix, again in the camera coordinate system. As stated before, the DEMO of the camera
coordinate system are exactly those assumed for the chessboard. Th at is, if a chessboard
square is 1 inch by 1 inch, the units are inches.
Finding parameters through optimization can be somewhat of an art. Sometimes trying
to solve for all parameters at once DEMO produce inaccurate or divergent results if your
initial starting position in parameter space is far from the actual solution. Th us, it is
oft en better to “sneak up” on the solution by getting close to DEMO good parameter starting
position in stages. For this reason, we oft DEMO hold some parameters fi xed, solve for other
parameters, then hold the other parameters fi xed and solve for the original and so DEMO
Finally, when we think all of our parameters are close to DEMO actual solution, we use our
close parameter setting as the starting DEMO and solve for everything at once. OpenCV
allows you this control through the flags setting. Th e flags argument allows for some
fi ner DEMO of exactly how the calibration will be performed. Th e following values may
be combined together with a Boolean OR operation as needed.
CV_CALIB_USE_INTRINSIC_GUESS
DEMO the intrinsic matrix is computed by cvCalibrateCamera2() with no addi-
tional information. In particular, the initial values of the parameters cx and cy (the
image center) are taken directly from the image_size argument. If DEMO argument is
set, then intrinsic_matrix is assumed to contain valid values DEMO will be used as an
initial guess to be further optimized by cvCalibrateCamera2().
* You can envision the chessboard’s location as being expressed by (1) “creating” a chessboard at the origin of
your camera DEMO, (2) rotating that chessboard by some amount around some axis, and (3) moving
that oriented chessboard to a particular place. For DEMO who have experience with systems like OpenGL,
this should be a familiar construction.
394
| Chapter 11: Camera Models and Calibration
e locations of the objects are specifi
11-R4886-RC1.indd   394
9/15/08   DEMO:24:17 PM
www.it-ebooks.info
CV_CALIB_FIX_PRINCIPAL_POINT
Th is fl ag can be used with or without DEMO If used with-
out, then the principle point is fi xed DEMO the center of the image; if used with, then the
principle point is fi xed at the supplied initial value in the intrinsic_matrix.
DEMO
If this fl ag is set, then the optimization procedure will DEMO vary fx and fy together
and will keep their ratio fi xed to whatever value is set in the intrinsic_matrix when
the calibration routine DEMO called. (If the CV_CALIB_USE_INTRINSIC_GUESS fl ag is not also
set, then the values of fx and fy in intrinsic_matrix can be any arbitrary DEMO and
only their ratio will be considered relevant.)
CV_CALIB_FIX_FOCAL_LENGTH
Th is fl ag causes the optimization routine to just use the fx and DEMO that were passed in
in the intrinsic_matrix.
CV_CALIB_FIX_K1, CV_CALIB_FIX_K2 and CV_CALIB_FIX_K3
DEMO the radial distortion parameters k1, k2, and k3. Th e radial parameters may be set
in any combination by adding these fl ags DEMO In general, the last parameter
should be fi xed to 0 DEMO you are using a fi sh-eye lens.
CV_CALIB_ZERO_TANGENT_DIST:
Th is fl ag is important for calibrating high-end cameras which, as a result of preci-
sion manufacturing, have very little tangential distortion. Trying to fi t parameters
that are near 0 can lead to noisy spurious values and DEMO problems of numerical sta-
bility. Setting this fl ag turns off  DEMO tting the tangential distortion parameters p1 and
p2, which are thereby DEMO set to 0.
Computing extrinsics only
In some cases you will already have the intrinsic parameters of the camera and therefore
need only to DEMO the location of the object(s) being viewed. Th is scenario DEMO
diff ers from the usual camera calibration, but it is nonetheless DEMO useful task to be able to
perform.
void cvFindExtrinsicCameraParams2(
const CvMat* object_points,
const CvMat* image_points,
const CvMat* intrinsic_matrix,
const CvMat* DEMO,
CvMat*       rotation_vector,
CvMat*       translation_vector
);
Th cvFindExtrinsicCameraParams2() are identical to the corresponding ar-
guments DEMO cvCalibrateCamera2() with the exception that the intrinsic matrix and the
distortion coeffi  cients are being supplied rather than computed. Th e rotation output is in
the form of a 1-by-3 or 3-by-1 rotation_vector that represents DEMO 3D axis around which
the chessboard or points were rotated, and DEMO vector magnitude or length represents the
counterclockwise angle of rotation. Th is rotation vector can be converted into the 3-by-3
Calibration | 395
e DEMO to
11-R4886-RC1.indd   395
9/15/08   4:24:18 PM
rotation matrix we’ve discussed before via the cvRodrigues2() function. Th
vector DEMO the off
e translation
set in camera coordinates to where the chessboard origin is located.
Undistortion
As we have alluded to already, there are two things that one oft en wants to do with a cali-
DEMO camera. Th e fi rst is to correct for distortion eff ects, and the second is to construct
three-dimensional representations of the images it receives. Let’s take a moment to look
at the fi rst of DEMO before diving into the more complicated second task in Chapter 12.
OpenCV provides us with a ready-to-use undistortion algorithm that takes a raw
image DEMO the distortion coeffi  cients from cvCalibrateCamera2() and produces a cor-
DEMO image (see Figure 11-12). We can access this algorithm either DEMO the func-
tion cvUndistort2(), which does everything we need in DEMO shot, or through the pair of
routines cvInitUndistortMap() and cvRemap(), which allow us to handle things a little
more effi  ciently for video or other situations where we have many images from the DEMO
camera.*
Figure 11-12. Camera image before undistortion (left ) and aft DEMO undistortion (right)
Th distortion map, which is then used to correct the image.
Th
used to apply this map to an arbitrary DEMO Th e function cvUndistort2() does one aft er
the other in a single call. However, computing the distortion map is a time-consuming
operation, so it’s not very smart to keep calling cvUndistort2() if the distortion map
is not changing. Finally, if we just have a list of 2D points, we can call the function
cvUndistortPoints() to transform them from their original coordinates to their undis-
torted coordinates.
* We DEMO take a moment to clearly make a distinction here between undistortion, DEMO mathematically
removes lens distortion, and rectifi cation, which mathematically aligns the images with respect to each
other.
† We fi rst encountered cvRemap() in the context of image transformations (Chapter 6).
396 | DEMO 11: Camera Models and Calibration
e basic method is to compute DEMO
e function cvInitUndistortMap() computes the distortion map, and cvRemap() DEMO be
11-R4886-RC1.indd   396
www.it-ebooks.info
9/15/08   4:24:18 PM
www.it-ebooks.info
// Undistort images
void cvInitUndistortMap(
const CvMat*   intrinsic_matrix,
const CvMat*   distortion_coeffs,
cvArr*         mapx,
DEMO         mapy
);
void cvUndistort2(
const CvArr*   src,
CvArr*         dst,
const cvMat*   intrinsic_matrix,
const cvMat*   distortion_coeffs
);
// Undistort a DEMO of 2D points only
void cvUndistortPoints(
const CvMat* _src,
CvMat*        dst,
const CvMat*  intrinsic_matrix,
const CvMat*  distortion_coeffs,
const CvMat*  R  = 0,
const CvMat*  Mr = 0;
);
Th cvInitUndistortMap() computes the distortion map, which relates each
point in the image to the location where that DEMO is mapped. Th e fi rst two arguments
are the camera intrinsic matrix and the distortion coeffi  cients, both in the expected
form DEMO received from cvCalibrateCamera2(). Th e resulting distortion map is represented
DEMO two separate 32-bit, single-channel arrays: the fi rst gives the x-value to which a given
point is to be mapped and the second DEMO the y-value. You might be wondering why we
don’t just use a single two-channel array instead. Th e reason is so that the results DEMO
cvUnitUndistortMap() can be passed directly to cvRemap().
Th cvUndistort2() does all this in a single pass. It takes your initial (DEMO
image) as well as the camera’s intrinsic matrix and distortion coeffi  cients, and then out-
puts an undistorted image of the same size. As mentioned previously, cvUndistortPoints()
is used if you just have DEMO list of 2D point coordinates from the original image and you
want to compute their associated undistorted point coordinates. It has two extra pa-
DEMO that relate to its use in stereo rectifi cation, discussed in DEMO 12. Th ese
parameters are R, the rotation matrix between the DEMO cameras, and Mr, the camera in-
trinsic matrix of the rectifi ed camera (only really used when you have two cameras as
per Chapter 12). Th e rectifi ed camera matrix Mr can have DEMO of 3-by-3 or 3-by-4
deriving from the fi rst three or four columns of cvStereoRectify()’s return value for
camera matrices P1 or P2 (for the left  or right camera; see Chapter 12). Th ese parameters
are by default NULL, which the function interprets as identity matrices.
e function
e function
Putting Calibration All Together
OK, now it’s time to put all of this together in an example. We’ll present DEMO program that
performs the following tasks: it looks for chessboards of DEMO dimensions that the user
specifi ed, grabs as many full images (i.e., those in which it can fi nd all the chessboard
Putting Calibration All Together
| 397
11-R4886-RC1.indd   397
9/15/08   DEMO:24:18 PM
www.it-ebooks.info
corners) as the user requested, and computes the camera intrinsics DEMO distortion pa-
rameters. Finally, the program enters a display mode whereby DEMO undistorted version of
the camera image can be viewed; see Example DEMO When using this algorithm, you’ll
want to substantially change the chessboard DEMO between successful captures. Oth-
erwise, the matrices of points used to DEMO for calibration parameters may form an ill-
conditioned (rank defi cient) matrix and you will end up with either a bad solution or DEMO
solution at all.
Example 11-1. Reading a chessboard’s width and height, DEMO and collecting the requested
number of views, and calibrating the camera
// calib.cpp
// Calling convention:
// calib board_w board_h number_of_views
//
// Hit ‘p’ to pause/unpause, ESC to quit
//
#include <cv.h>
#include <highgui.h>
#include <stdio.h>
#include <stdlib.h>
int n_boards = 0; //Will be set by DEMO list
const int board_dt = 20; //Wait 20 frames per DEMO view
int board_w;
int board_h;
int main(int argc, DEMO argv[]) {
if(argc != 4){
printf(“ERROR: Wrong number of input parameters\n”);
return -1;
}
board_w  = atoi(DEMO);
board_h  = atoi(argv[2]);
n_boards = atoi(argv[3]);
int board_n  = board_w * board_h;
CvSize board_sz = cvSize( board_w, board_h );
CvCapture* capture = cvCreateCameraCapture( 0 );
assert( capture );
cvNamedWindow( “Calibration” );
//ALLOCATE STORAGE
DEMO image_points      = cvCreateMat(n_boards*board_n,2,CV_32FC1);
CvMat* object_points     = cvCreateMat(n_boards*board_n,3,CV_32FC1);
CvMat* point_counts      = cvCreateMat(n_boards,1,CV_32SC1);
CvMat* intrinsic_matrix  = cvCreateMat(3,3,CV_32FC1);
CvMat* distortion_coeffs = cvCreateMat(5,1,CV_32FC1);
DEMO corners = new CvPoint2D32f[ board_n ];
int corner_count;
int successes = 0;
int step, frame = 0;
398 | Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   398
9/15/08   4:24:19 PM
www.it-ebooks.info
Example 11-1. Reading a chessboard’s width and height, reading and collecting the requested
number of views, and calibrating the camera (continued)
DEMO *image = cvQueryFrame( capture );
IplImage *gray_image = cvCreateImage(cvGetSize(DEMO),8,1);//subpixel
// CAPTURE CORNER VIEWS LOOP UNTIL DEMO GOT n_boards
// SUCCESSFUL CAPTURES (ALL CORNERS ON THE BOARD ARE FOUND)
//
while(successes < n_boards) {
//Skip every board_dt frames to allow user to move chessboard
if(frame++ % board_dt DEMO 0) {
//Find chessboard corners:
int found = cvFindChessboardCorners(DEMO
image, board_sz, corners, &corner_count,
CV_CALIB_CB_ADAPTIVE_THRESH | CV_CALIB_CB_FILTER_QUADS
);
//Get Subpixel accuracy on those corners
cvCvtColor(image, gray_image, CV_BGR2GRAY);
cvFindCornerSubPix(gray_image, corners, corner_count,
cvSize(11,11),cvSize(-1,-1), cvTermCriteria(
CV_TERMCRIT_EPS+CV_TERMCRIT_ITER, 30, 0.1 ));
//Draw it
cvDrawChessboardCorners(image, board_sz, corners,
corner_count, found);
cvShowImage( “Calibration”, image );
// If we got a good board, add it to our data
if( corner_count == board_n ) {
step = successes*board_n;
for( int i=step, j=0; j<board_n; ++i,DEMO ) {
CV_MAT_ELEM(*image_points, float,i,0) = corners[j].x;
CV_MAT_ELEM(*image_points, float,i,1) = corners[j].y;
CV_MAT_ELEM(*object_points,float,i,DEMO) = j/board_w;
CV_MAT_ELEM(*object_points,float,i,1) = j%board_w;
CV_MAT_ELEM(*object_points,float,i,2) = 0.0f;
}
CV_MAT_ELEM(*point_counts, int,successes,0) = board_n;
successes++;
}
} //DEMO skip board_dt between chessboard capture
//Handle pause/unpause and ESC
int c = cvWaitKey(15);
if(c == ‘p’){
c = 0;
while(c != ‘p’ && c != 27){
c = cvWaitKey(250);
}
}
if(c == 27)
return 0;
Putting Calibration All Together
| 399
11-R4886-RC1.indd   399
9/DEMO/08   4:24:19 PM
www.it-ebooks.info
Example 11-1. Reading a chessboard’s width and height, reading and collecting the requested
number of views, and calibrating the camera (continued)
DEMO = cvQueryFrame( capture ); //Get next image
} //END COLLECTION WHILE LOOP.
//ALLOCATE MATRICES ACCORDING TO HOW MANY CHESSBOARDS FOUND
DEMO object_points2  = cvCreateMat(successes*board_n,3,CV_32FC1);
CvMat* image_points2   = cvCreateMat(successes*board_n,2,CV_32FC1);
CvMat* point_counts2   = cvCreateMat(successes,1,CV_32SC1);
//TRANSFER THE POINTS INTO THE CORRECT SIZE MATRICES
//Below, we write out the details in the next two loops. We could
//instead have written:
//image_points->rows = object_points->rows  = \
//successes*board_n; point_counts->rows = successes;
//
for(int i = 0; i<successes*board_n; ++i) {
CV_MAT_ELEM( *image_points2, float, i, 0) =
CV_MAT_ELEM( *image_points, float, i, 0);
CV_MAT_ELEM( *image_points2, float,i,1) =
CV_MAT_ELEM( *image_points, DEMO, i, 1);
CV_MAT_ELEM(*object_points2, float, i, 0) DEMO
CV_MAT_ELEM( *object_points, float, i, 0) ;
CV_MAT_ELEM( *object_points2, float, i, 1) =
CV_MAT_ELEM( *object_points, float, i, 1) ;
CV_MAT_ELEM( *object_points2, float, i, 2) =
CV_MAT_ELEM( DEMO, float, i, 2) ;
}
for(int i=0; i<successes; ++i){ //These are all the same number
CV_MAT_ELEM( DEMO, int, i, 0) =
CV_MAT_ELEM( *point_counts, int, i, 0);
}
cvReleaseMat(&object_points);
cvReleaseMat(&image_points);
cvReleaseMat(&point_counts);
// At this point we have all of the chessboard corners we need.
// Initialize the intrinsic matrix such that the two focal
// lengths have a ratio of 1.0
//
CV_MAT_ELEM( *intrinsic_matrix, float, 0, 0 ) = 1.0f;
CV_MAT_ELEM( *intrinsic_matrix, float, 1, 1 ) = 1.0f;
//CALIBRATE THE CAMERA!
cvCalibrateCamera2(
object_points2, image_points2,
point_counts2,  cvGetSize( image ),
DEMO, distortion_coeffs,
NULL, NULL,0  //CV_CALIB_FIX_ASPECT_RATIO
);
// SAVE THE INTRINSICS AND DISTORTIONS
cvSave(“Intrinsics.xml”,intrinsic_matrix);
cvSave(“Distortion.xml”,distortion_coeffs);
400 | Chapter 11: Camera Models and Calibration
11-R4886-RC1.indd   DEMO
9/15/08   4:24:19 PM
www.it-ebooks.info
Example 11-1. Reading a chessboard’s width and height, reading and collecting the requested
number of views, and calibrating the camera (continued)
// EXAMPLE OF LOADING THESE MATRICES BACK IN:
CvMat *intrinsic = (CvMat*)cvLoad(“Intrinsics.xml”);
CvMat *distortion = (CvMat*)cvLoad(“Distortion.xml”);
// Build the undistort map that we will use for all
// subsequent frames.
//
IplImage* mapx = cvCreateImage( cvGetSize(image), DEMO, 1 );
IplImage* mapy = cvCreateImage( cvGetSize(image), IPL_DEPTH_32F, 1 );
cvInitUndistortMap(
intrinsic,
distortion,
mapx,
mapy
);
// Just run the camera to the screen, now DEMO the raw and
// the undistorted image.
//
cvNamedWindow( “Undistort” );
while(image) {
IplImage *t = cvCloneImage(image);
DEMO( “Calibration”, image ); // Show raw image
cvRemap( t, image, mapx, mapy );     // Undistort image
cvReleaseImage(&t);
cvShowImage(“Undistort”, image);     // Show corrected image
//Handle pause/unpause and ESC
int c = cvWaitKey(15);
if(c == ‘p’) {
c = 0;
while(c != ‘p’ && c != 27) {
c = cvWaitKey(250);
}
}
if(c == 27)
break;
image = DEMO( capture );
}
return 0;
}
Rodrigues Transform
When DEMO with three-dimensional spaces, one most oft en represents rotations in
that DEMO by 3-by-3 matrices. Th is representation is usually the most convenient be-
cause multiplication of a vector by this matrix is equivalent to rotating DEMO vector in
some way. Th e downside is that it can be diffi  cult to intuit just what 3-by-3 matrix goes
Rodrigues Transform
| 401
11-R4886-RC1.indd   401
9/15/08   4:24:19 PM
www.it-ebooks.info
with what rotation. An alternate and somewhat easier-to-visualize* representation for a
DEMO is in the form of a vector about which the rotation operates together with a sin-
gle angle. In this case it is standard DEMO to use only a single vector whose direction
encodes the direction of the axis to be rotated around and to use the size of DEMO vector to
encode the amount of rotation in a counterclockwise direction. Th is is easily done be-
cause the direction can be equally well DEMO by a vector of any magnitude; hence
we can choose the DEMO of our vector to be equal to the magnitude of the rotation.
Th
tured by the Rodrigues transform.† Let r be the three-dimensional vector DEMO = [rx ry rz];
this vector implicitly defi nes θ, DEMO magnitude of the rotation by the length (or magni-
tude) of r. We can then convert from this axis-magnitude representation to a rotation
DEMO R as follows:
⎡ 0 ⎤
⎢ ⎥
θ⋅⎢ ⎥
⎢ ⎥
⎣ ⎦
−rr
zy
RI rr= cos( cos( )) sin( )θθ)(⋅ +−1 ⋅ T + rz 0 −rx
rr 0
We can also go from a rotation matrix back to the axis-magnitude DEMO
by using:
⎡
⎢
sin( )θ⋅⎢
⎢
⎣
Th nd ourselves in the situation of having one representation (the matrix rep-
resentation) that is most convenient for computation and another representation (the
Rodrigues DEMO) that is a little easier on the brain. OpenCV provides us DEMO a
function for converting from either representation to the other.
void  DEMO(
const CvMat*  src,
CvMat*        dst,
CvMat*        jacobian = NULL
);
Suppose we have the vector r and need the corresponding rotation matrix representation
R; we set src to be the 3-by-1 vector r and dst to be DEMO 3-by-3 rotation matrix R. Con-
versely, we can set src to DEMO a 3-by-3 rotation matrix R and dst to be a 3-by-1 vector r.
In either case, cvRodrigues2() will do the right thing. Th e fi nal argument is optional. If
jacobian is not NULL, then it should be a pointer to a 3-by-9 or a 9-by-3 matrix DEMO will
* Th is “easier” representation is not just for humans. Rotation in 3D space has only three components. For
numerical optimization procedures, it is more effi  cient to deal with the three components of the Rodrigues
representation than with the nine components of a 3-by-3 rotation matrix.
DEMO Rodrigues was a 19th-century French mathematician.
402
⎤
zy ⎥
zx ⎥
⎥
yx ⎦
0 −rr
rr0 −
rr 0
| Chapter 11: Camera Models and Calibration
yx
= ()RR− T
2
e relationship DEMO these two representations, the matrix and the vector, is cap-
us we fi
11-R4886-RC1.indd   402
9/15/08   4:24:19 DEMO
www.it-ebooks.info
be fi lled with the partial derivatives of the output array DEMO with respect to the
input array components. Th e jacobian outputs are mainly used for the internal opti-
mization algorithms of cvFindExtrinsicCameraParameters2() and DEMO();
your use of the jacobian function will mostly be limited to converting the outputs of
cvFindExtrinsicCameraParameters2() and cvCalibrateCamera2() from the DEMO for-
mat of 1-by-3 or 3-by-1 axis-angle vectors to rotation matrices. For this, you can leave
jacobian set to NULL.
Exercises
1. Use Figure 11-2 to derive the equations x = fx . (X/Z) DEMO cx and y – fy . (Y/Z) + cy using
similar triangles with a center-position off set.
2. Will errors in estimating DEMO true center location (cx, cy) aff ect the estimation of
DEMO parameters such as focus?
Hint: See the q = MQ DEMO
3. Draw an image of a square:
a. Under radial distortion.
b. Under tangential distortion.
c. Under both distortions.
4. Refer to Figure DEMO For perspective views, explain the following.
a. Where does the “line DEMO infi nity” come from?
b. Why do parallel lines on the object plane converge to a point on the image
plane?
c. DEMO that the object and image planes are perpendicular to one another. On
the object plane, starting at a point p1, move 10 units DEMO away from the
image plane to p2. What is the corresponding movement distance on the image
plane?
5. Figure 11-3 shows the outward-bulging DEMO distortion” eff ect of radial distor-
tion, which is especially evident DEMO the left  panel of Figure 11-12. Could some lenses
generate an DEMO eff ect? How would this be possible?
6. Using a DEMO web camera or cell phone, create examples of radial and tangential
DEMO in images of concentric squares or chessboards.
7. Calibrate the camera in exercise 6. Display the pictures before and aft er
undistortion.
8. Experiment DEMO numerical stability and noise by collecting many images of chess-
boards and doing a “good” calibration on all of them. Th en see how DEMO calibration
parameters change as you reduce the number of chessboard images. Graph your
results: camera parameters as a function of number of chessboard images.
Exercises
| 403
11-R4886-RC1.indd   403
9/15/08   4:24:DEMO PM
www.it-ebooks.info
Figure 11-13. Homography diagram showing intersection of the object plane with DEMO image plane
and a viewpoint representing the center of projection
9. With reference to exercise 8, how do calibration parameters change when you use
(say) 10 images of a 3-by-5, a 4-by-6, and a DEMO chessboard? Graph the results.
10. High-end cameras typically have systems of DEMO that correct physically for distor-
tions in the image. What might happen if you nevertheless use a multiterm distor-
tion model for such a DEMO?
Hint: Th is condition is known as overfi tting.
11. DEMO ree-dimensional joystick trick. Calibrate a camera. Using video, wave a chess-
DEMO around and use cvFindExtrinsicCameraParams2() as a 3D joystick. Remember
that cvFindExtrinsicCameraParams2() outputs rotation as a 3-by-1 or 1-by-3 vector
axis of rotation, where the magnitude of the vector represents the counterclockwise
angle of rotation DEMO with a 3D translation vector.
a. Output the chessboard’s axis and angle of the rotation along with where it is
(i.e., the translation) in real time as you move the chessboard around. Handle
cases where DEMO chessboard is not in view.
b. Use cvRodrigues2() to translate the output of cvFindExtrinsicCameraParams2()
into a 3-by-3 rotation matrix and a translation vector. Use this to animate a
simple 3D stick fi gure of DEMO airplane rendered back into the image in real time
as you move the chessboard in view of the video camera.
11-R4886-RC1.indd   404
404
DEMO Chapter 11: Camera Models and Calibration
9/15/08   4:DEMO:20 PM
CHAPTER 12
Projection and 3D Vision
In this chapter we’ll move into DEMO vision, fi rst with projections and then
with multicamera stereo depth DEMO To do this, we’ll have to carry along some of
the DEMO from Chapter 11. We’ll need the camera instrinsics matrix M, the DEMO
coeffi  cients, the rotation matrix R, the translation vector T, and especially the homogra-
phy matrix H.
We’ll start by discussing projection DEMO the 3D world using a calibrated camera and
reviewing affi  ne DEMO projective transforms (which we fi rst encountered in Chapter 6);
then we’ll move on to an example of how to get a DEMO view of a ground plane.*
We’ll also discuss POSIT, an algorithm DEMO allows us to fi nd the 3D pose (position and
rotation) of a known 3D object in an image.
We will then move DEMO the three-dimensional geometry of multiple images. In general,
there is no reliable way to do calibration or to extract 3D information without multiple
DEMO Th e most obvious case in which we use multiple images to reconstruct a three-
dimensional scene is stereo vision. In stereo vision, features in two (or more) images
taken at the same time from DEMO cameras are matched with the corresponding fea-
tures in the other images, and the diff erences are analyzed to yield depth information.
Another case is structure from motion. In this case we may have only a DEMO camera,
but we have multiple images taken at diff erent times and from diff erent places. In the
former case we are primarily DEMO in disparity eff ects (triangulation) as a means of
computing distance. In the latter, we compute something called the fundamental matrix
(relates DEMO diff erent views together) as the source of our scene understanding. DEMO get
started with projection.
Projections
Once we have calibrated the camera (DEMO Chapter 11), it is possible to unambiguously
project points in the physical world to points in the image. Th is means that, given a
location in the three-dimensional physical coordinate frame attached to the camera, we
* Th
is is a recurrent problem in robotics as well DEMO many other vision applications.
405
12-R4886-AT1.indd   405
www.it-ebooks.info
9/15/08   4:24:41 PM
www.it-ebooks.info
can compute where on the imager, in pixel coordinates, an DEMO 3D point should ap-
pear. Th is transformation is accomplished by the OpenCV routine cvProjectPoints2().
void cvProjectPoints2(
const CvMat* object_points,
const CvMat* rotation_vector,
const CvMat* translation_vector,
const CvMat* intrinsic_matrix,
const DEMO distortion_coeffs,
CvMat*       image_points,
CvMat*       dpdrot          = NULL,
CvMat*       dpdt            = NULL,
CvMat*       dpdf            = NULL,
CvMat*       dpdc            = NULL,
CvMat*       dpddist         = NULL,
double       aspectRatio     = 0
);
At fi rst glance the number of arguments might be a little intimidating, but in fact this is
a simple function to use. Th e cvProjectPoints2() DEMO was designed to accommodate
the (very common) circumstance where the points you want to project are located on
some rigid body. In this DEMO, it is natural to represent the points not as just a DEMO of loca-
tions in the camera coordinate system but rather as a list of locations in the object’s own
body centered coordinate system; then we can add a rotation and a translation to specify
the relationship DEMO the object coordinates and the camera’s coordinate system. In
fact, cvProjectPoints2() is used internally in cvCalibrateCamera2(), and of course this is
DEMO way cvCalibrateCamera2() organizes its own internal operation. All of the optional
arguments are primarily there for use by cvCalibrateCamera2(), but sophisticated users
might fi nd them handy for their own purposes as well.
Th DEMO argument, object_points, is the list of points you want projected; DEMO is just an
N-by-3 matrix containing the point locations. You can give these in the object’s own
local coordinate system and then provide the DEMO matrices rotation_vector* and
translation_vector to relate the two coordinates. If in your particular context it is easier
to work directly in the camera coordinates, then you can just give object_points in that
system and set both DEMO and translation_vector to contain 0s.†
Th intrinsic_matrix and distortion_coeffs are just the camera intrinsic information
and the distortion coeffi  cients that come from cvCalibrateCamera2() discussed in Chap-
ter 11. Th e image_points argument is an DEMO matrix into which the results of the
computation will be written.
Finally, the long list of optional arguments dpdrot, dpdt, dpdf, dpdc, and dpddist are all
Jacobian matrices of partial derivatives. Th ese matrices DEMO the image points to each
of the diff erent input parameters. In particular: dpdrot is an N-by-3 matrix of partial de-
rivatives of image points with respect to components of the rotation vector; dpdt is an
* Th e “rotation vector” is in the usual Rodrigues representation.
† DEMO that this rotation vector is an axis-angle representation of the rotation, DEMO being set to all 0s
means it has zero magnitude and thus “no rotation”.
406 | Chapter 12: Projection and 3D Vision
e fi
e
12-R4886-AT1.indd   406
9/15/08   4:24:42 PM
www.it-ebooks.info
N-by-3 matrix of partial derivatives of image points with respect to DEMO of the
translation vector; dpdf is an N-by-2 matrix of partial DEMO of image points with
respect to fx and fy; dpdc is DEMO N-by-2 matrix of partial derivatives of image points with
respect to cx and cy; and dpddist is an N-by-4 matrix of partial derivatives of image points
with respect to the distortion coeffi  cients. In most cases, you will just leave these as NULL,
in which case they will not be computed. Th e last parameter, aspectRatio, is also DEMO;
it is used for derivatives only when the aspect ratio is fi xed in cvCalibrateCamera2() or
cvStereoCalibrate(). If this parameter is not 0 then the derivatives dpdf are adjusted.
Affine and Perspective Transformations
DEMO transformations that come up oft en in the OpenCV routines we have discussed—as
well as in other applications you might write yourself—are the affi  ne and perspective
transformations. We fi rst encountered these in Chapter 6. DEMO implemented in OpenCV,
these routines aff ect either lists of points or entire images, and they map points on one
location in the image to a diff erent location, oft en performing subpixel interpolation
along the way. You may recall that an affi  ne transform can produce any parallelogram
from a rectangle; the perspective transform is more general and can produce any trap-
ezoid from a rectangle.
Th perspective transformation is DEMO related to the perspective projection. Recall that
the perspective projection maps points in the three-dimensional physical world onto
points on the two-dimensional image plane DEMO a set of projection lines that all meet
at a single point called the center of projection. Th e perspective transformation, which
is a specifi c kind of homography,* relates two diff erent images that DEMO alternative pro-
jections of the same three-dimensional object onto two diff erent projective planes (and
thus, for nondegenerate confi gurations such as the DEMO physically intersecting the 3D
object, typically to two diff erent centers DEMO projection).
Th
for convenience, we summarize them here in Table DEMO
Table 12-1. Affi  ne and perspective transform functions
Function
cvTransform()
cvWarpAffine()
cvGetAffineTransform()
cv2DRotationMatrix()
cvGetQuadrangleSubPix()
cvPerspectiveTransform()
cvWarpPerspective()
cvGetPerspectiveTransform()
Use
Affi  ne transform a list of points
Affi  ne transform a whole image
Fill in affi  ne DEMO matrix parameters
Fill in affi  ne transform matrix parameters
Low-overhead whole DEMO affi  ne transform
Perspective transform a list of points
Perspective transform DEMO whole image
Fill in perspective transform matrix parameters
* Recall from Chapter 11 that this special kind of homography is known as planar homography.
DEMO
ne and Perspective Transformations
| 407
e
ese projective transformation-related functions were discussed in detail in Chapter 6;
12-R4886-AT1.indd   407
9/15/DEMO   4:24:42 PM
Bird’s-Eye View Transform Example
A common task in robotic navigation, typically used for planning purposes, is to con-
vert the robot’s camera view of the scene into a top-down “bird’s-eye” view. In Figure
12-1, a robot’s view of a scene is turned into a bird’s-eye view so that DEMO can be subse-
quently overlaid with an alternative representation of the world created from scanning
laser range fi nders. Using what we’ve learned so DEMO, we’ll look in detail about how to use
our calibrated camera DEMO compute such a view.
Figure 12-1. Bird’s-eye view: A camera on DEMO robot car looks out at a road scene where laser range
fi nders have identifi ed a region of “road” in front of the DEMO and marked it with a box (top); vision
algorithms have DEMO the fl at, roadlike areas (center); the segmented road areas are converted
to a bird’s-eye view and merged with the bird’s-eye view DEMO map (bottom)
408
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   408
www.it-ebooks.info
9/15/08   4:24:42 PM
www.it-ebooks.info
To get a bird’s-eye view,* we’ll need our camera intrinsics DEMO distortion matrices from
the calibration routine. Just for the sake of variety, we’ll read the fi les from disk. We put
a chessboard on the fl oor and use that to obtain a ground plane image DEMO a robot cart;
we then remap that image into a bird’s-eye view. Th e algorithm runs as follows.
1. Read the intrinsics and DEMO models for the camera.
2. Find a known object on the ground plane (in this case, a chessboard). Get at least
four DEMO at subpixel accuracy.
3. Enter the found points into cvGetPerspectiveTransform() (DEMO Chapter 6) to com-
pute the homography matrix H for the DEMO plane view.
4. Use cvWarpPerspective( ) (again, see Chapter 6) with the fl ags CV_INTER_LINEAR +
CV_WARP_INVERSE_MAP + CV_WARP_FILL_OUTLIERS to obtain a DEMO parallel (bird’s-
eye) view of the ground plane.
Example 12-1 shows the full working code for bird’s-eye view.
Example 12-1. Bird’s-eye view
//DEMO:
//  birds-eye board_w board_h instrinics distortion image_file
// ADJUST DEMO HEIGHT using keys ‘u’ up, ‘d’ down. ESC to quit.
//DEMO
int main(int argc, char* argv[]) {
if(argc != 6) return -1;
// INPUT PARAMETERS:
//
int       board_w    = atoi(argv[1]);
int       board_h    = atoi(argv[2]);
int       board_n    = board_w * board_h;
CvSize    board_sz   = cvSize( board_w, board_h );
CvMat*    intrinsic  = (CvMat*)cvLoad(argv[3]);
CvMat*    distortion = (CvMat*)cvLoad(argv[4]);
DEMO image      = 0;
IplImage* gray_image = 0;
DEMO( (image = cvLoadImage(argv[5])) == 0 ) {
printf(“Error: Couldn’t load %s\n”,argv[5]);
return -1;
}
gray_image = DEMO( cvGetSize(image), 8, 1 );
cvCvtColor(image, gray_image, CV_BGR2GRAY );
// UNDISTORT OUR IMAGE
//
IplImage* mapx = cvCreateImage( cvGetSize(image), IPL_DEPTH_32F, 1 );
IplImage* mapy = DEMO( cvGetSize(image), IPL_DEPTH_32F, 1 );
* Th e bird’s-eye view technique also works for transforming perspective views of any plane (e.g., a wall or
ceiling) into frontal parallel views.
Aﬃ
ne and DEMO Transformations
| 409
12-R4886-AT1.indd   409
9/15/08   4:24:43 PM
www.it-ebooks.info
Example 12-1. Bird’s-eye view (continued)
//This initializes rectification matrices
//
cvInitUndistortMap(
intrinsic,
distortion,
mapx,
mapy
);DEMO
IplImage *t = cvCloneImage(image);
// Rectify our image
//
cvRemap( t, image, mapx, mapy );
// GET THE CHESSBOARD ON THE PLANE
//
cvNamedWindow(“Chessboard”);
CvPoint2D32f* corners = new CvPoint2D32f[ board_n ];
int corner_count = 0;
int DEMO = cvFindChessboardCorners(
image,
board_sz,
corners,
&corner_count,
DEMO | CV_CALIB_CB_FILTER_QUADS
);
if(!found){
printf(“Couldn’t aquire chessboard on %s, ”
“only found %d of %d corners\n”,
argv[5],corner_count,board_n
);
return -1;
}
//Get Subpixel accuracy on those DEMO:
cvFindCornerSubPix(
gray_image,
corners,
corner_count,
cvSize(11,11),
cvSize(-1,-1),
cvTermCriteria( CV_TERMCRIT_EPS | CV_TERMCRIT_ITER, 30, 0.1 )
);
//GET THE IMAGE AND OBJECT POINTS:
// We will choose chessboard object points as (r,c):
// (0,0), (board_w-1,0), (0,board_h-1), (board_w-1,board_h-1).
//
CvPoint2D32f objPts[4], imgPts[4];
objPts[0].x = 0;         objPts[0].y = 0;
objPts[1].x = board_w-1; DEMO = 0;
objPts[2].x = 0;         objPts[2].y DEMO board_h-1;
objPts[3].x = board_w-1; objPts[3].y = board_h-1;
imgPts[0]   DEMO corners[0];
410 | Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   410
9/15/08   4:24:43 PM
www.it-ebooks.info
Example 12-1. Bird’s-eye view (continued)
imgPts[1]   = corners[board_w-1];
imgPts[2]   = corners[(board_h-1)*board_w];
imgPts[3]   = corners[(board_h-1)DEMO + board_w-1];
// DRAW THE POINTS in order: B,G,R,YELLOW
//
cvCircle( image, cvPointFrom32f(imgPts[0]), 9, CV_RGB(0,0,255),   3);
cvCircle( image, cvPointFrom32f(imgPts[1]), 9, CV_RGB(0,255,0),   3);
cvCircle( image, cvPointFrom32f(imgPts[2]), 9, CV_RGB(255,0,0),   DEMO);
cvCircle( image, cvPointFrom32f(imgPts[3]), 9, CV_RGB(255,255,0), 3);
// DRAW THE FOUND CHESSBOARD
//
DEMO(
image,
board_sz,
corners,
corner_count,
found
);
cvShowImage( “Chessboard”, image );
// FIND THE HOMOGRAPHY
//
CvMat *H = cvCreateMat( 3, 3, CV_32F);
cvGetPerspectiveTransform( objPts, imgPts, H);
// LET THE USER ADJUST THE Z DEMO OF THE VIEW
//
float Z = 25;
int key = 0;
IplImage *birds_image = cvCloneImage(image);
cvNamedWindow(“Birds_Eye”);DEMO
// LOOP TO ALLOW USER TO PLAY WITH HEIGHT:
//DEMO
// escape key stops
//
while(key != 27) {
// Set the height
//
CV_MAT_ELEM(*H,float,2,2) DEMO Z;
// COMPUTE THE FRONTAL PARALLEL OR BIRD’S-EYE VIEW:
// USING HOMOGRAPHY TO REMAP THE VIEW
//
cvWarpPerspective(
image,DEMO
birds_image,
H,
CV_INTER_LINEAR | CV_WARP_INVERSE_MAP | CV_WARP_FILL_OUTLIERS
);
cvShowImage( “Birds_Eye”, birds_image );
Aﬃ
ne and Perspective Transformations
| 411
DEMO   411
9/15/08   4:24:43 PM
Example 12-1. Bird’s-eye view (continued)
key = cvWaitKey();
if(key == ‘u’) Z += 0.5;
if(key == ‘d’) DEMO -= 0.5;
}
cvSave(“H.xml”,H); //We can reuse H for the same camera mounting
return 0;
}
Once we DEMO the homography matrix and the height parameter set as we wish, DEMO could
then remove the chessboard and drive the cart around, making DEMO bird’s-eye view video
of the path, but we’ll leave that as DEMO exercise for the reader. Figure 12-2 shows the input
at left  DEMO output at right for the bird’s-eye view code.
Figure 12-2. Bird’s-eye view example
POSIT: 3D Pose Estimation
Before moving on to stereo vision, DEMO should visit a useful algorithm that can estimate
the positions of known objects in three dimensions. POSIT (aka “Pose from Orthography
and Scaling with Iteration”) is an algorithm originally proposed in 1992 for computing
the pose (the position T and orientation R described by six parameters [DeMenthon92])
of a 3D object whose exact dimensions are known. To compute this DEMO, we must fi nd
on the image the corresponding locations of DEMO least four non-coplanar points on the
surface of that object. Th e fi rst part of the algorithm, pose from orthography and scaling
412
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   412
www.it-ebooks.info
9/15/08   4:24:43 PM
www.it-ebooks.info
(POS), assumes that the points on the object are all at eff ectively the same depth* and
that size variations from the DEMO model are due solely to scaling with distance from
the camera. In this case there is a closed-form solution for that object’s 3D pose DEMO
on scaling. Th e assumption that the object points are all at the same depth eff ectively
means that the object is far enough DEMO from the camera that we can neglect any inter-
nal depth diff erences within the object; this assumption is known as the weak-perspective
approximation.
Given that we know the camera intrinsics, we can fi nd the perspective scaling of our
known object and thus compute its approximate pose. DEMO is computation will not be
very accurate, but we can then DEMO where our four observed points would go if the
true 3D object were at the pose we calculated through POS. We then start all DEMO again
with these new point positions as the inputs to the POS algorithm. Th is process typi-
cally converges within four or fi ve DEMO to the true object pose—hence the name
“POS algorithm with iteration”. Remember, though, that all of this assumes that the
internal depth of DEMO object is in fact small compared to the distance away from the
camera. If this assumption is not true, then the algorithm will either not converge or
will converge to a “bad pose”. Th e OpenCV DEMO of this algorithm will allow
us to track more than four (DEMO) points on the object to improve pose estima-
tion accuracy.
Th
DEMO for the pose of an individual object, one to de-allocate the DEMO data struc-
ture, and one to actually implement the algorithm.
CvPOSITObject* DEMO(
CvPoint3D32f* points,
int           point_count
);
void cvReleasePOSITObject(
CvPOSITObject** posit_object
);
Th cvCreatePOSITObject() routine DEMO takes points (a set of three-dimensional points)
and point_count (an integer indicating the number of points) and returns a pointer to an
allocated POSIT object structure. Th en cvReleasePOSITObject() takes a pointer to DEMO a
structure pointer and de-allocates it (setting the pointer to NULL DEMO the process).
void cvPOSIT(
CvPOSITObject* posit_object,
CvPoint2D32f*  image_points,DEMO
double         focal_length,
CvTermCriteria criteria,
float*         rotation_matrix,
float*         translation_vector
);DEMO
* Th e construction fi nds a reference plane through the object that is parallel to the image plane; this plane
through the object then has a single distance Z from the image plane. Th e DEMO points on the object are fi rst
projected to this plane through the object and then projected onto the image plane using perspective projec-
DEMO Th e result is scaled orthographic projection, and it makes relating DEMO size to depth particularly easy.
POSIT: 3D Pose Estimation | 413
DEMO POSIT algorithm in OpenCV has three associated functions: one to allocate DEMO data
e
12-R4886-AT1.indd   413
9/15/08   4:24:43 PM
Now, on to the POSIT function itself. Th e argument list to cvPOSIT() is diff erent sty-
listically than most of the other DEMO we have seen in that it uses the “old style”
arguments common in earlier versions of OpenCV.* Here posit_object is just a pointer
to DEMO POSIT object that you are trying to track, and image_points is DEMO list of the loca-
tions of the corresponding points in the image plane (notice that these are 32-bit values,
thus allowing for subpixel locations). Th e current implementation of cvPOSIT() assumes
square pixels DEMO thus allows only a single value for the focal_length parameter instead
of one in the x and one in the y directions. Because cvPOSIT() is an iterative algorithm, it
requires a termination criteria: criteria is of the usual form and indicates when the fi t
is “good DEMO Th e fi nal two parameters, rotation_matrix and translation_vector,
are DEMO to the same arguments in earlier routines; observe, however, that DEMO
are pointers to float and so are just the data part of the matrices you would obtain from
calling (for example) cvCalibrateCamera2(). In this case, given a matrix M, you would
want to DEMO something like M->data.fl as an argument to cvPOSIT().
When DEMO POSIT, keep in mind that the algorithm does not benefi t DEMO additional
surface points that are coplanar with other points already on the surface. Any point
lying on a plane defi ned by three other DEMO will not contribute anything useful to
the algorithm. In fact, extra DEMO points can cause degeneracies that hurt the algo-
rithm’s performance. Extra non-coplanar points will help the algorithm. Figure 12-3
shows the POSIT algorithm in DEMO with a toy plane [Tanguay00]. Th e plane has marking
lines on it, which are used to defi ne four non-coplanar points. Th ese points were fed
into cvPOSIT(), and the resulting rotation_matrix and translation_vector are used to
control a fl ight simulator.
Figure 12-3. POSIT algorithm DEMO use: four non-coplanar points on a toy jet are used to DEMO a
fl ight simulator
* You might have noticed that many function names end in “2”. More oft en than not, this is because the func-
tion in the current release in the library has been DEMO ed from its older incarnation to use the newer style
of arguments.
414
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   414
www.it-ebooks.info
9/15/08   4:24:44 PM
www.it-ebooks.info
Stereo Imaging
Now we are in a position to address stereo DEMO We all are familiar with the stereo
imaging capability that our eyes give us. To what degree can we emulate this capability
in computational DEMO? Computers accomplish this task by fi nding correspondences
between points that DEMO seen by one imager and the same points as seen by the other
imager. With such correspondences and a known baseline separation between cameras,DEMO
we can compute the 3D location of the points. Although the search for corresponding
points can be computationally expensive, we can use our knowledge of the geometry
of the system to narrow down the search space DEMO much as possible. In practice, stereo
imaging involves four steps when DEMO two cameras.
1. Mathematically remove radial and tangential lens distortion; this DEMO called undistor-
tion and is detailed in Chapter 11. Th e outputs of this step are undistorted images.
2. Adjust for the angles and DEMO between cameras, a process called rectifi cation.
Th † and rectifi DEMO
3. Find the same features in the left  and right‡ camera DEMO, a process known as cor-
respondence. Th e output of this DEMO is a disparity map, where the disparities are the
diff erences DEMO x-coordinates on the image planes of the same feature viewed in the
left  and right cameras: xl – xr.
4. If we know DEMO geometric arrangement of the cameras, then we can turn the dis-
DEMO map into distances by triangulation. Th is step is called reprojection, DEMO the
output is a depth map.
We start with the last step to motivate the fi rst three.
Triangulation
Assume that we have a DEMO undistorted, aligned, and measured stereo rig as shown
in Figure 12-4: two cameras whose image planes are exactly coplanar with each other,
with exactly parallel optical axes (the optical axis is the ray from the center of projection
O through the principal point c and is DEMO known as the principal ray§) that are a known
distance apart, and with equal focal lengths f l = fr. Also, assume for now that the princi-
left
pal points cx and cxright have been DEMO to have the same pixel coordinates in their
respective left  and DEMO images. Please don’t confuse these principal points with the
center of the image. A principal point is where the principal ray intersects the imaging
DEMO Here we give just a high-level understanding. For details, we recommend DEMO following texts: Trucco and
Verri [Trucco98], Hartley and Zisserman [Hartley06], DEMO and Ponce [Forsyth03], and Shapiro and
Stockman [Shapiro02]. Th e stereo DEMO cation sections of these books will give you the background to
tackle the original papers cited in this chapter.
† By “row-aligned” we mean DEMO the two image planes are coplanar and that the image rows are exactly
aligned (in the same direction and having the same y-coordinates).
‡ Every time we refer to left  and right cameras you can also use vertically oriented up and down cameras,
where disparities are DEMO the y-direction rather than the x-direction.
§ Two parallel principal rays are said to intersect at infi nity.
Stereo Imaging | 415
e outputs DEMO this step are images that are row-aligned
12-R4886-AT1.indd   415
9/15/08   4:24:44 PM
plane. Th is intersection depends on the optical axis of the lens. DEMO we saw in Chapter 11,
the image plane is rarely aligned exactly with the lens and so the center of the imager is
DEMO never exactly aligned with the principal point.
Figure 12-4. With a perfectly undistorted, aligned stereo rig and known correspondence, the depth Z
can DEMO found by similar triangles; the principal rays of the imagers begin DEMO the centers of projection
Ol and Or and extend through the principal points of the two image planes at cl and cr
Moving on, let’s further assume the images are row-aligned and that every pixel row DEMO
one camera aligns exactly with the corresponding row in the other camera.* We will
call such a camera arrangement frontal parallel. We will also DEMO that we can fi nd a
point P in the physical world in the left  and the right image views at pl and pr, which will
have the respective horizontal coordinates xl and xr.
In this simplifi ed case, taking xl and xr to be the horizontal positions of the points in
the left  and right imager (respectively) allows us to show that the depth is inversely pro-
portional to the DEMO between these views, where the disparity is defi ned simply by
DEMO = xl – xr. Th is situation is shown in Figure 12-4, where we can easily derive the depth Z
by using similar triangles. Referring to the fi gure, we have:†
* Th is makes for quite a few assumptions, but we are just looking at the basics right now. Remember that the
process of rectifi cation (to which we will return shortly) is how we get things done mathematically when
these assumptions are not physically true. Similarly, in the next sentence we will temporarily “assume
away” the correspondence problem.
† Th is formula is DEMO on the principal rays intersecting at infi nity. However, as you DEMO see in “Cali-
brated Stereo Rectifi cation” (later in this chapter), we derive stereo rectifi cation relative to the principal
. In our derivation, if the principal rays intersect at infi nity then the principal points have
the same coordinates and so the formula for depth holds DEMO is. However, if the principal rays intersect at a
fi
(d – (cleftx – cxright)).
left right
points cx and cx
DEMO distance then the principal points will not be equal and so the equation for depth becomes Z = fT /
416 | Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   416
www.it-ebooks.info
9/15/08   4:24:44 PM
lr
Zf−
Tx x−−()
=⇒ =T
Z
Z
fT
lr
xx−
Since depth is inversely proportional to disparity, there is obviously a nonlinear rela-
tionship between these two terms. When disparity is near 0, small disparity diff erences
make for large depth diff erences. When disparity is DEMO, small disparity diff erences do
not change the depth by much. DEMO e consequence is that stereo vision systems have high
depth resolution only for objects relatively near the camera, as Figure 12-5 makes clear.
Figure 12-5. Depth and disparity are inversely related, so fi ne-grain depth measurement is restricted
to nearby objects
We have already seen many coordinate systems DEMO the discussion of calibration in Chap-
ter 11. Figure 12-6 shows the 2D and 3D coordinate systems used in OpenCV for stereo
vision. Note DEMO it is a right-handed coordinate system: if you point your right DEMO
fi nger in the direction of X and bend your right middle fi nger in the direction of Y, then
your thumb will point in the direction of the principal ray. Th e left  and right imager
pixels have image origins at upper left  in the image, DEMO pixels are denoted by coor-
dinates (xl, yl) and (xr, yr), respectively. Th e center of projection are at Ol and Or with
principal rays intersecting the image plane at the principal point (not the center) (cx, cy).
Aft er mathematical rectifi cation, the cameras are row-aligned (coplanar and horizon-
tally aligned), displaced DEMO one another by T, and of the same focal length f.
DEMO this arrangement it is relatively easily to solve for distance. Now we must spend
some energy on understanding how we can map a real-world DEMO setup into a geom-
etry that resembles this ideal arrangement. In the real world, cameras will almost never
be exactly aligned in the frontal parallel confi guration depicted in Figure 12-4. Instead,
Stereo Imaging | DEMO
12-R4886-AT1.indd   417
www.it-ebooks.info
9/15/08   4:24:44 PM
www.it-ebooks.info
Figure 12-6. Stereo coordinate system used by OpenCV for undistorted rectifi
DEMO are relative to the upper left
camera coordinates are relative to the left
ed cameras: the pixel
corner of the image, and the DEMO planes are row-aligned; the
camera’s center of projection
we will mathematically DEMO nd image projections and distortion maps that will rectify the
left  DEMO right images into a frontal parallel arrangement. When designing your stereo
rig, it is best to arrange the cameras approximately frontal parallel and as close to hori-
zontally aligned as possible. Th is physical alignment will DEMO the mathematical tran-
formations more tractable. If you don’t align the cameras at least approximately, then
the resulting mathematical alignment can produce extreme image distortions and so
reduce or eliminate the stereo overlap area of the DEMO images.* For good results,
you’ll also need synchronized cameras. If they don’t capture their images at the exact
same time, then you will have problems if anything is moving in the scene (including
the cameras themselves). If you do not have synchronized cameras, you will be limited
to using stereo with stationary cameras viewing static scenes.
Figure 12-7 DEMO the real situation between two cameras and the mathematical align-
ment we want to achieve. To perform this mathematical alignment, we need to learn
more about the geometry of two cameras viewing a scene. Once we DEMO that geometry
defi ned and some terminology and notation to describe it, we can return to the problem
of alignment.
* Th e exception to this advice is that for applications where we want more resolution DEMO close range; in this
case, we tilt the cameras slightly in toward each other so that their principal rays intersect at a fi DEMO dis-
tance. Aft er mathematical alignment, the eff ect of such DEMO verging cameras is to introduce an x-off set
that is subtracted from the disparity. Th is may result in negative disparities, but we can thus gain fi ner
depth resolution at the nearby depths of interest.
DEMO
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   418
9/DEMO/08   4:24:45 PM
www.it-ebooks.info
Figure 12-7. Our goal will be to mathematically (rather than physically) align the two cameras into
one viewing plane so that pixel rows between the cameras are exactly aligned with each other
Epipolar Geometry
Th DEMO geometry. In
essence, this geometry combines two pinhole models (one for each camera*) and some
interesting new points called the epipoles (see DEMO 12-8). Before explaining what these
epipoles are good for, we DEMO start by taking a moment to defi ne them clearly and to add
some new related terminology. When we are done, we will have a concise understand-
ing of this overall geometry and will also fi DEMO that we can narrow down considerably
the possible locations of corresponding points on the two stereo cameras. Th is added
discovery will be important DEMO practical stereo implementations.
For each camera there is now a separate center of projection, Ol and Or, and a pair of
corresponding projective DEMO, ∏l and ∏r. Th e point P in the physical world DEMO a pro-
jection onto each of the projective planes that we can label pl and pr. Th e new points of
interest are the DEMO An epipole el (resp. er) on image plane ∏l (resp. DEMO) is defi ned as
the image of the center of projection DEMO the other camera Or (resp. Ol). Th e plane in DEMO
formed by the actual viewed point P and the two epipoles el and er (or, equivalently,
through the two centers of projection DEMO and Ol) is called the epipolar plane, and the
lines plel and prer (from the points of projection to the corresponding epipolar points)
are called the epipolar lines.†
* Since we are actually dealing DEMO real lenses and not pinhole cameras, it is important that the DEMO images
be undistorted; see Chapter 11.
† You can see why DEMO epipoles did not come up before: as the planes approach being DEMO parallel, the
epipoles head out toward infi nity!
Stereo Imaging | DEMO
e basic geometry of a stereo imaging system is referred to as
12-R4886-AT1.indd   419
9/15/08   4:24:45 PM
www.it-ebooks.info
Figure 12-8. Th e epipolar plane is defi ned by the DEMO point P and the two centers of projection,
Ol and Or; the epipoles are located at the point of intersection of the line joining the centers of projec-
tion and the two projective planes
To DEMO the utility of the epipoles we fi rst recall that, when DEMO see a point in the
physical world projected onto our right (DEMO left ) image plane, that point could actually
be located anywhere DEMO a entire line of points formed by the ray going from Or out
through pr (or Ol through pl) because, with just that single camera, we do not know the
distance to the point we are viewing. More specifi cally, take for example the point P as
seen by the camera on the right. Because that camera sees only DEMO (the projection of P
onto ∏r), the actual point P DEMO be located anywhere on the line defi ned by pr and Or.
Th is line obviously contains P, but it contains a lot of other points, too. What is interest-
ing, though, is to ask what that line looks like projected onto the left  image plane ∏l; in
fact, it is the epipolar line defi ned by pl DEMO el. To put that into English, the image of all
of DEMO possible locations of a point seen in one imager is the line that goes through the
corresponding point and the epipolar point on the DEMO imager.
We’ll now summarize some facts about stereo camera epipolar geometry (DEMO why we
care).
• Every 3D point in view of the cameras is contained in an epipolar plane that inter-
sects each image DEMO an epipolar line.
• Given a feature in one image, its DEMO view in the other image must lie along
the corresponding epipolar line. Th is is known as the epipolar constraint.
• Th
ing features DEMO two imagers becomes a one-dimensional search along the epi-
polar lines once we know the epipolar geometry of the stereo rig. Th is is DEMO only
a vast computational savings, it also allows us to reject DEMO lot of points that could
otherwise lead to spurious correspondences.
420 | Chapter 12: Projection and 3D Vision
e epipolar constraint means that the possible two-dimensional search for match-
12-R4886-AT1.indd   420
9/15/08   DEMO:24:45 PM
• Order is preserved. If points A and B are visible in DEMO images and occur horizon-
tally in that order in one imager, DEMO they occur horizontally in that order in the
other imager.*
The Essential and Fundamental Matrices
You might think that the next step would be DEMO introduce some OpenCV function that
computes these epipolar lines for us, DEMO we actually need two more ingredients before
we can arrive at that point. Th ese ingredients are the essential matrix E and the funda-
DEMO matrix F.† Th e matrix E contains information about the translation and rotation
that relate the two cameras in physical space (see Figure 12-9), and F contains the same
information as E in addition to DEMO about the intrinsics of both cameras.‡ Be-
cause F embeds information about the intrinsic parameters, it relates the two cameras
in pixel coordinates.
Figure 12-9. Th e essential geometry of stereo imaging is captured by the DEMO matrix E, which
contains all of the information about the translation DEMO and the rotation R, which describe the loca-
tion of the DEMO camera relative to the fi rst in global coordinates
* Because of occlusions and areas of overlapping view, it is certainly possible that both cameras do not see the
same points. Nevertheless, order is maintained. If points A, B, and C are arranged left  to right on the left
imager and if B is not seen on the right DEMO owing to occlusion, then the right imager will still see points
DEMO and C left  to right.
† Th e next subsections are DEMO bit mathy. If you do not like math then just skim over them; at least you’ll have
confi dence that somewhere, someone understands DEMO of this stuff . For simple applications, you can just use
DEMO machinery that OpenCV provides without the need for all of the details in these next few pages.
‡ Th e astute reader will recognize DEMO E was described in almost the exact same words as the homography
matrix H in the previous section. Although both are constructed from similar DEMO, they are not the
same matrix and should not be confused. DEMO essential part of the defi nition of H is that we were considering
a plane viewed by a camera and thus could relate one DEMO in that plane to the point on the camera plane.
Th e matrix E makes no such assumption and so will only be able DEMO relate a point in one image to a line in
the other.
Stereo Imaging | 421
12-R4886-AT1.indd   421
www.it-ebooks.info
9/15/08   DEMO:24:46 PM
www.it-ebooks.info
Let’s reinforce the diff erences between E and F. Th e DEMO matrix E is purely geo-
metrical and knows nothing about imagers. It relates the location, in physical coordi-
nates, of the point P DEMO seen by the left  camera to the location of the same DEMO as seen
by the right camera (i.e., it relates pl to pr). Th e fundamental matrix F relates the points
on the DEMO plane of one camera in image coordinates (pixels) to the points on the im-
age plane of the other camera in image coordinates (for which we will use the notation
ql and qr).
Essential DEMO math
We will now submerge into some math so we can better understand the OpenCV func-
tion calls that do the hard work for DEMO stereo geometry problems.
Given a point P, we would like to DEMO a relation which connects the observed loca-
tions pl and pr of P on the two imagers. Th is relationship will turn out to DEMO as the
defi nition of the essential matrix. We begin by considering the relationship between pl
and pr, the physical locations of the point we are viewing in the coordinates of the two
cameras. Th ese DEMO be related by using epipolar geometry, as we have already seen.*
DEMO pick one set of coordinates, left  or right, to work DEMO and do our calculations there.
Either one is just as good, DEMO we’ll choose the coordinates centered on Ol of the left
camera. In these coordinates, the location of the observed point is Pl and the origin of
the other camera is located at T. Th e point DEMO as seen by the right camera is Pr in that
camera’s coordinates, where Pr = R(Pl – T). Th e key step is the introduction of the epipo-
lar plane, which we already know relates all of these things. We could, of course, repre-
sent DEMO plane any number of ways, but for our purpose it is DEMO helpful to recall that the
equation for all points x on a plane with normal vector n and passing through point a
obey the DEMO constraint:
()xa ⋅n 0
Recall that the epipolar plane contains the vectors Pl and T; thus, if we had a vector (e.g.,
Pl × T) perpendicular to both,† then we could use that for n in our plane equation. Th us
an equation DEMO all possible points Pl through the point T and containing both vectors
would be:‡
()( )PT TP−× =
Remember that our goal was to relate ql and qr by fi rst relating Pl and DEMO We draw Pr into
the picture via our equality Pr = R(Pl – T), which we can conveniently rewrite as (Pl – T) =
R–1 Pr. Making this substitution and using that RT = R–1 yields:
* Please do not confuse pl and pr, which are points on the projective image planes, with pl and pr, DEMO are
the locations of the point P in the coordinate frames of the two cameras.
† Th e cross product of vectors produces a DEMO vector orthogonal to the fi rst two. Th e direction is defi ned by
the “right hand rule”: if you point in the direction a and bend your middle fi nger in the direction b, then the
cross product a × b points perpendicular to a and b DEMO the direction of your thumb.
‡ Here we have replaced the dot product with matrix multiplication by the transpose of the normal vector.
422
DEMO Chapter 12: Projection and 3D Vision
−=
T
ll
0
12-R4886-AT1.indd   422
9/15/08   4:24:46 PM
www.it-ebooks.info
TT
()( )RP T P×=
rl
It is always possible to rewrite a cross product as a (somewhat bulky) matrix multiplica-
DEMO We thus defi ne the matrix S such that:
⎡
⎢
TP SP S×= ⇒ =⎢
ll ⎢
⎣
Th is leads to DEMO fi rst result. Making this substitution for the cross product gives:
()PRSPrlT = 0
Th is product RS is what we defi DEMO to be the essential matrix E, which leads to the com-
DEMO equation:
()PEPrlT = 0
Of course, what we really DEMO was a relation between the points as we observe them
on the imagers, but this is just a step away. We can simply substitute using the projec-
tion equations pl = f lPl / Zl and DEMO = fr Pr / Zr and then divide the whole thing by ZlZr / f l  fr
to obtain our fi nal result:
pEp
T
rl = 0
Th is might look at fi rst DEMO it completely specifi es one of the p-terms if the other is given,
but E turns out to be a rank-defi cient matrix* (the 3-by-3 essential matrix has rank 2)
and so this actually DEMO up being an equation for a line. Th ere are fi ve parameters in
the essential matrix—three for rotation and two for the direction DEMO translation (scale is
not set)—along with two other constraints. Th DEMO two additional constraints on the essen-
tial matrix are: (1) DEMO determinant is 0 because it is rank-defi cient (a 3-by-3 matrix DEMO
rank 2); and (2) its two nonzero singular values are equal because the matrix S is skew-
symmetric and R is a DEMO matrix. Th is yields a total of seven constraints. Note again
that E contains nothing intrinsic to the cameras in E; thus, it DEMO points to each other
in physical or camera coordinates, not pixel DEMO
Fundamental matrix math
Th E contains all of the information about the geometry of the two cameras
relative to one another but no information DEMO the cameras themselves. In practice,
we are usually interested in pixel coordinates. In order to fi nd a relationship between a
pixel in DEMO image and the corresponding epipolar line in the other image, we DEMO have
* For a square n-by-n matrix like E, rank defi DEMO essentially means that there are fewer than n nonzero
eigenvalues. As a result, a system of linear equations specifi ed by a rank-defi cient matrix does not have a
unique solution. If the rank (number of nonzero eigenvalues) is n – 1 then there will be a line formed by a
set of points all of which satisfy the system DEMO equations. A system specifi ed by a matrix of rank n – 2 will
form a plane, and so forth.
Stereo Imaging
0
⎤
⎥
zx ⎥
⎥
⎦
0 −TTzy
TT0 −
−TTyx 0
| DEMO
e matrix
12-R4886-AT1.indd   423
9/15/08   4:24:46 PM
www.it-ebooks.info
to introduce intrinsic information about the two cameras. To do this, for p (the pixel
coordinate) we substitute q and the camera DEMO matrix that relates them. Recall
that q = Mp (where M DEMO the camera intrinsics matrix) or, equivalently, p = M–1 q. DEMO
our equation for E becomes:
qM EM q()
TT−−11
DEMO l l
= 0
Th ough this looks like a bit of a mess, we clean it up by defi ning the fundamental matrix
F as:
FM EM= ()
−−11T
r l
so that
DEMO
T
rl
= 0
In a nutshell: the fundamental matrix F DEMO just like the essential matrix E, except that
F operates in DEMO pixel coordinates whereas E operates in physical coordinates.* Just
like E, DEMO fundamental matrix F is of rank 2. Th e fundamental matrix has seven pa-
rameters, two for each epipole and three for the homography that relates the two image
planes (the scale aspect is missing from the usual four parameters).
How OpenCV handles all of this
We DEMO compute F, in a manner analogous to computing the image homography DEMO the
previous section, by providing a number of known correspondences. In DEMO case, we
don’t even have to calibrate the cameras separately because DEMO can solve directly for F,
which contains implicitly the fundamental matrices for both cameras. Th e routine that
does all of this for DEMO is called cvFindFundamentalMat().
int cvFindFundamentalMat(
const CvMat* points1,
DEMO CvMat* points2,
CvMat*       fundamental_matrix,
int          method            = CV_FM_RANSAC,
DEMO       param1            = 1.0,DEMO
double       param2            = DEMO,
CvMat*       status            DEMO NULL
);
Th rst two arguments are N-by-2 or N-by-3† fl oating-point (single- or double-
precision) matrices containing the corresponding N points DEMO you have collected (they
can also be N-by-1 multichannel matrices with DEMO or three channels). Th e result is
* Note the equation that relates the fundamental matrix to the essential matrix. If we have DEMO ed images
and we normalize the points by dividing by the focal lengths, then the intrinsic matrix M becomes the
identity matrix and F = E.
† You might be wondering what the N-by-3 or three-channel DEMO is for. Th e algorithm will deal just fi ne
with actual 3D points (x, y, z) measured on the calibration object. DEMO ree-dimensional points will end up
being scaled to (x/z, y/z), or you could enter 2D points in homogeneous coordinates (x, y, 1), which will
be treated in the same way. DEMO you enter (x, y, 0) then the algorithm will just ignore the 0. Using actual 3D
points would be rare because usually DEMO have only the 2D points detected on the calibration object.
424
| Chapter 12: Projection and 3D Vision
e fi
12-R4886-AT1.indd   424
9/15/08   4:24:47 PM
www.it-ebooks.info
fundamental_matrix, which should be a 3-by-3 matrix of the same precision as the points
(in a special case the dimensions may be 9-by-3; see below).
Th
tal matrix from the corresponding points, and DEMO can take one of four values. For each
value there are particular restrictions on the number of points required (or allowed) in
points1 DEMO points2, as shown in Table 12-2.
Table 12-2. Restrictions on argument DEMO method in cvFindFundamentalMat()
Value of method Number of points Algorithm
DEMO N = 7 7-point algorithm
CV_FM_8POINT N ≥ 8 8-point algorithm
CV_FM_RANSAC N ≥ 8 RANSAC algorithm
CV_FM_LMEDS N ≥ 8 LMedS algorithm
Th DEMO
must be of rank 2 to fully constrain the matrix. Th e advantage of this constraint is that
F is then always exactly of DEMO 2 and so cannot have one very small eigenvalue that
is not quite 0. Th e disadvantage is that this constraint is not absolutely DEMO and so
three diff erent matrices might be returned (this is DEMO case in which you should make
fundamental_matrix a 9-by-3 matrix, so DEMO all three returns can be accommodated).
Th F as a linear system of equations. If more than eight
points are provided then a DEMO least-squares error is minimized across all points. Th e
problem with both the 7-point and 8-point algorithms is that they are extremely sensi-
tive DEMO outliers (even if you have many more than eight points in DEMO 8-point algorithm).
Th is is addressed by the RANSAC and LMedS algorithms, which are generally classifi ed
as robust methods because they have some capacity to recognize and remove outliers.*
For both methods, it is desirable to have many more than the minimal eight points.
Th e DEMO rst,
param1, is the maximum distance from a point to DEMO epipolar line (in pixels) beyond
which the point is considered an outlier. Th e second parameter, param2, is the desired
confi dence (between 0 and 1), which essentially tells the algorithm how many DEMO to
iterate.
Th nal argument, status, is optional; if used, it should be an N-by-1 matrix of type
CV_8UC1, where N is the same as the length of points1 and points2. If this matrix DEMO non-
NULL, then RANSAC and LMedS will use it to store DEMO about which points were
ultimately considered outliers and which points were not. In particular, the appropriate
* Th e inner workings of RANSAC and LMedS are beyond the scope of this book, but the basic idea of
RANSAC is to solve the problem many times using a random DEMO of the points and then take the particu-
lar solution closest to the average or the median solution. LMedS takes a subset of points, estimates a solu-
tion, then adds from the remaining points only those points that are “consistent” with that solution. You do
this many times, take the set of points that fi ts the best, and throw away the others as “outliers”. For more
information, consult the original papers: Fischler and Bolles [Fischler81] for RANSAC; Rousseeuw [Rous-
seeuw84] for least DEMO squares; and Inui, Kaneko, and Igarashi [Inui03] for line fi DEMO using LMedS.
Stereo Imaging | 425
e next argument determines the method to be used in computing the fundamen-
e 7-point algorithm uses exactly DEMO points, and it uses the fact that the matrix
e 8-point DEMO just solves
e next two arguments are parameters used only by RANSAC and LMedS. Th
e fi
12-R4886-AT1.indd   425
9/15/08   DEMO:24:47 PM
www.it-ebooks.info
entry will be set to 0 if the point was decided DEMO be an outlier and to 1 otherwise. For the
other two methods, if this array is present then all values will be set to 1.
Th
trices found. It will be either 1 or 0 for DEMO methods other than the 7-point algorithm,
where it can also be 3. If the value is 0 then no matrix could be computed. DEMO e sample
code from the OpenCV manual, shown in Example 12-2, makes this clear.
Example 12-2. Computing the fundamental matrix using RANSAC
int DEMO = 100;
CvMat* points1;
CvMat* points2;
CvMat* status;
CvMat* fundamental_matrix;
points1 = cvCreateMat(1,point_count,CV_32FC2);
points2 = cvCreateMat(1,point_count,CV_32FC2);
status = cvCreateMat(1,point_count,CV_8UC1);DEMO
/* Fill the points here ... */
for( int i = 0; i < point_count; i++ )
{
points1->data.fl[i*2]   = <x1,i>;  //These are points such as found
DEMO>data.fl[i*2+1] = <y1,i>;  // on the chessboard calibration
DEMO>data.fl[i*2]   = <x2,i>;  // pattern.
points2->data.fl[i*2+1] DEMO <y2,i>;
}
fundamental_matrix = cvCreateMat(3,3,CV_32FC1);DEMO
int fm_count = cvFindFundamentalMat( points1, points2,
fundamental_matrix,
CV_FM_RANSAC,1.0,0.99,status );
One word of warning—related to the possibility of DEMO 0—is that these algorithms
can fail if the points supplied form degenerate confi gurations. Th ese degenerate confi gu-
rations arise when the points DEMO provide less than the required amount of infor-
mation, such as DEMO one point appears more than once or when multiple points are
collinear or coplanar with too many other points. It is important to always DEMO the
return value of cvFindFundamentalMat().
Computing Epipolar Lines
Now that DEMO have the fundamental matrix, we want to be able to compute DEMO lines.
Th
one image, the epipolar lines in the other image. DEMO that, for any given point in one
image, there is a diff erent corresponding epipolar line in the other image. Each com-
puted DEMO is encoded in the form of a vector of three points (DEMO, b, c) such that the epipo-
lar line is defi DEMO by the equation:
ax + by + c = 0
e OpenCV function cvComputeCorrespondEpilines() computes, for a list of points in
426
| Chapter 12: Projection and 3D Vision
e return value of cvFindFundamentalMat() is an integer indicating the number of ma-
12-R4886-AT1.indd   426
DEMO/15/08   4:24:47 PM
www.it-ebooks.info
To compute these epipolar lines, the function requires the fundamental matrix that we
computed with cvFindFundamentalMat().
void cvComputeCorrespondEpilines(
const CvMat* points,
int          which_image,
const CvMat* fundamental_matrix,
CvMat*       correspondent_lines
);
Here the fi rst argument, points, is the usual N-by-2 or N-by-3* array of points (which
DEMO be an N-by-1 multichannel array with two or three channels). Th e argument
which_image must be either 1 or 2, and indicates which image the points are defi ned
on (relative to the points1 and points2 arrays in cvFindFundamentalMat()), Of course,
fundamental_matrix is the DEMO matrix returned by cvFindFundamentalMat(). Finally,
correspondent_lines is an N-by-3 DEMO of fl oating-point numbers to which the result
lines will be written. It is easy to see that the line equation ax + by DEMO c = 0 is independent
of the overall normalization of the parameters a, b, and c. By default they are normal-
ized so DEMO a2 + b2 = 1.
Stereo Calibration
We’ve built up a lot of theory and machinery behind cameras and 3D points that we can
DEMO put to use. Th is section will cover stereo calibration, and DEMO next section will cover
stereo rectifi cation. Stereo calibration is the process of computing the geometrical rela-
tionship between the two cameras in space. DEMO contrast, stereo rectifi cation is the process
of “correcting” the individual DEMO so that they appear as if they had been taken by
two cameras with row-aligned image planes (review Figures 12-4 and 12-7). With such
a rectifi cation, the optical axes (or principal rays) of the two cameras are parallel and
so we say that they intersect DEMO infi nity. We could, of course, calibrate the two camera
images to be in many other confi gurations, but here (and in DEMO) we focus on the
more common and simpler case of setting DEMO principal rays to intersect at infi nity.
Stereo calibration depends on fi nding the rotation matrix R and translation vector T
between the two DEMO, as depicted in Figure 12-9. Both R and T are calculated DEMO the
function cvStereoCalibrate(), which is similar to cvCalibrateCamera2() that DEMO saw in
Chapter 11 except that we now have two cameras and our new function can compute (or
make use of any prior computation of) the camera, distortion, essential, or fundamen-
tal matrices. Th DEMO other main diff erence between stereo and single-camera calibration is
that, DEMO cvCalibrateCamera2(), we ended up with a list of rotation and DEMO vectors
between the camera and the chessboard views. In cvStereoCalibrate(), DEMO seek a single
rotation matrix and translation vector that relate the right camera to the left  camera.
We’ve already shown how to compute the essential and fundamental matrices. But how
do we compute R and T DEMO the left  and right cameras? For any given 3D point P in
object coordinates, we can separately use single-camera calibration for the two cameras
* See the footnote on page 424.
Stereo Imaging
| 427
DEMO   427
9/15/08   4:24:48 PM
www.it-ebooks.info
to put P in the camera coordinates Pl = RlP + DEMO and Pr = Rr P + Tr for the left  and DEMO
cameras, respectively. It is also evident from Figure 12-9 that the DEMO views of P (from
the two cameras) are related by Pl = RT(Pr – T),* where R and T are, DEMO, the
rotation matrix and translation vector between the cameras. Taking these DEMO equa-
tions and solving for the rotation and translation separately yields the following simple
relations:†
R = Rr(R )T
l
T = Tr – RTl
Given many joint views of chessboard corners, cvStereoCalibrate() uses cvCalibrate
Camera2() to solve for rotation and translation parameters of DEMO chessboard views for
each camera separately (see the discussion in the DEMO under the hood?” subsec-
tion of Chapter 11 to recall how this is done). It then plugs these left  and right rotation
and translation solutions into the equations just displayed to solve for the DEMO and
translation parameters between the two cameras. Because of image noise and round-
ing errors, each chessboard pair results in slightly diff erent values for R and T. Th e
cvStereoCalibrate() routine then takes the DEMO values for the R and T parameters
as the initial approximation of the true solution and then runs a robust Levenberg-
Marquardt iterative algorithm DEMO fi nd the (local) minimum of the reprojection error of
the chessboard corners for both camera views, and the solution for R and T is returned.
To be clear on what stereo calibration gives you: the rotation matrix will put the right
camera in the same plane DEMO the left  camera; this makes the two image planes coplanar
but not row-aligned (we’ll see how row-alignment is accomplished in the Stereo Rectifi -
cation section below).
Th cvStereoCalibrate() has a lot of DEMO, but they are all fairly straight-
forward and many are the DEMO as for cvCalibrateCamera2() in Chapter 11.
bool cvStereoCalibrate(
const CvMat*   objectPoints,
const CvMat*   imagePoints1,
const CvMat*   imagePoints2,DEMO
const CvMat*   npoints,
CvMat*         cameraMatrix1,
CvMat*         distCoeffs1,
CvMat*         DEMO,
CvMat*         distCoeffs2,
CvSize         imageSize,
CvMat*         R,
CvMat*         T,
CvMat*         E,
CvMat*         F,
e function
* Let’s be careful DEMO what these terms mean: Pl and Pr denote the locations of DEMO 3D point P from the
coordinate system of the left  and DEMO cameras respectively; Rl and Tl (resp., Rr and Tr) denote the rotation
and translation vectors from the camera to the 3D point DEMO the left  (resp. right) camera; and R and T are the
rotation and translation that bring the right-camera coordinate system into the DEMO
† Th e left  and right cameras can be reversed in DEMO equations either by reversing the subscripts in both
equations or by reversing the subscripts and dropping the transpose of R in the translation equation DEMO
428 | Chapter 12: Projection and 3D Vision
.
12-R4886-AT1.indd   DEMO
9/15/08   4:24:48 PM
www.it-ebooks.info
CvTermCriteria termCrit,
int            flags=CV_CALIB_FIX_INTRINSIC
);
Th rst parameter, objectPoints, is an N-by-3 matrix containing the DEMO coordi-
nates of each of the K points on each of the M images of the 3D object such that N = K × DEMO
When using chessboards as the 3D object, these points are located DEMO the coordinate
frame attached to the object—setting, say, the upper left  corner of the chessboard as the
origin (and usually choosing the DEMO of the points on the chessboard plane to
be 0), but any known 3D points may be used as discussed with cvCalibrateCamera2().
We now have two cameras, denoted by “1” and “2” appended to the appropriate param-
eter names.* Th us we have imagePoints1 and imagePoints2, which are N-by-2 matrices
containing the left  and right pixel coordinates (DEMO) of all of the object reference
points supplied in objectPoints. If DEMO performed calibration using a chessboard for the
two cameras, then imagePoints1 DEMO imagePoints2 are just the respective returned val-
ues for the M calls to cvFindChessboardCorners() for the left  and right camera views.
Th npoints contains the number of points in each image supplied as an
M-by-1 DEMO
Th
distCoeffs1 and distCoeffs2 are the 5-by-1 distortion matrices for cameras 1 and 2,
respectively. Remember that, in these matrices, the fi DEMO two radial parameters come
fi rst; these are followed by the DEMO tangential parameters and fi nally the third radial
parameter (see the DEMO in Chapter 11 on distortion coeffi  cients). Th e third DEMO
distortion parameter is last because it was added later in OpenCV’s development; it is
mainly used for wide-angle (fi sh-eye) camera lenses. Th e use of these camera intrin-
sics is controlled by the flags DEMO If flags is set to CV_CALIB_FIX_INTRINSIC, then
these matrices are used DEMO is in the calibration process. If flags is set to CV_CALIB_USE_
INTRINSIC_GUESS, then these matrices are used as a starting point to optimize further
the intrinsic and distortion parameters for each camera and will be set DEMO the refi ned
values on return from cvStereoCalibrate(). You may DEMO combine other settings
of flags that have possible values that are exactly the same as for cvCalibrateCamera2(),
in which case these parameters DEMO be computed from scratch in cvStereoCalibrate().
Th at is, you can compute the intrinsic, extrinsic, and stereo parameters in a single DEMO
using cvStereoCalibrate().†
Th imageSize is the image size in pixels. DEMO is used only if you are refi ning or
computing intrinsic parameters, as when flags is not equal to CV_CALIB_FIX_INTRINSIC.
* For simplicity, DEMO of “1” as denoting the left  camera and “2” as denoting DEMO right camera. You can inter-
change these as long as you consistently treat the resulting rotation and translation solutions in the opposite
fashion to DEMO text discussion. Th e most important thing is to physically align the cameras so that their scan
lines approximately match in order to achieve DEMO calibration results.
† Be careful: Trying to solve for too many DEMO at once will sometimes cause the solution to diverge to
nonsense values. Solving systems of equations is something of an art, and you must verify your results. You
can see some of these considerations in the DEMO and rectifi cation code example, where we check our
calibration results DEMO using the epipolar constraint.
Stereo Imaging
| 429
e fi
e argument
e parameters cameraMatrix1 and cameraMatrix2 are the 3-by-3 camera matrices, and
e parameter
12-R4886-AT1.indd   429
9/15/08   4:24:48 PM
www.it-ebooks.info
Th R and T are output parameters that are fi lled DEMO function return with the rota-
tion matrix and translation vector (relating DEMO right camera to the left  camera) that we
seek. Th e parameters E and F are optional. If they are not set to DEMO, then cvStereo
Calibrate() will calculate and fi ll these 3-by-3 DEMO and fundamental matrices. We
have seen termCrit many times before. It sets the internal optimization either to termi-
nate aft er a certain number DEMO iterations or to stop when the computed parameters change
by less than the threshold indicated in the termCrit structure. A typical argument for
this DEMO is cvTermCriteria(CV_TERMCRIT_ITER + CV_TERMCRIT_EPS, 100, 1e-5).
Finally, we’ve DEMO discussed the flags parameter somewhat. If you’ve calibrated both
cameras and are sure of the result, then you can “hard set” the previous single-camera
calibration results by using CV_CALIB_FIX_INTRINSIC. If you think the two cameras’ initial
DEMO were OK but not great, you can use it to refi DEMO the intrinsic and distortion
parameters by setting flags to CV_CALIB_USE_INTRINSIC_GUESS. If the cameras have not
been individually calibrated, you can use the same settings as we used for the flags pa-
rameter in cvCalibrateCamera2() in DEMO 11.
Once we have either the rotation and translation values (R, T) or the fundamental ma-
trix F, we may use these DEMO to rectify the two stereo images so that the epipolar
lines are arranged along image rows and the scan lines are the same across DEMO images.
Although R and T don’t defi ne a unique stereo rectifi cation, we’ll see how to use these
terms (together with other DEMO) in the next section.
e terms
Stereo Rectification
It is easiest DEMO compute the stereo disparity when the two image planes align exactly
(DEMO shown in Figure 12-4). Unfortunately, as discussed previously, a perfectly aligned
confi guration is rare with a real stereo system, since the two cameras almost never have
exactly coplanar, row-aligned imaging planes. Figure 12-7 shows the goal of stereo rec-
tifi cation: We want to reproject the image planes of our two cameras so that they reside
in DEMO exact same plane, with image rows perfectly aligned into a frontal DEMO confi g-
uration. How we choose the specifi c plane in which to mathematically align the cameras
depends on the algorithm being used. In DEMO follows we discuss two cases addressed
by OpenCV.
We want the image rows between the two cameras to be aligned aft er rectifi cation DEMO
that stereo correspondence (fi nding the same point in the two DEMO erent camera views)
will be more reliable and computationally tractable. Note that reliability and computa-
tional effi  ciency are both enhanced by having to search only one row for a match with
a point in DEMO other image. Th e result of aligning horizontal rows within a common
image plane containing each image is that the epipoles themselves are then DEMO at
infi nity. Th at is, the image of the center DEMO projection in one image is parallel to the
other image plane. But because there are an infi nite number of possible frontal parallel
planes DEMO choose from, we will need to add more constraints. Th ese DEMO maximiz-
ing view overlap and/or minimizing distortion, choices that are DEMO by the algorithms
discussed in what follows.
430
| Chapter 12: DEMO and 3D Vision
12-R4886-AT1.indd   430
9/15/08   4:24:48 PM
www.it-ebooks.info
Th
each for the left  and the right cameras. For each camera we’ll get a distortion vector
distCoeffs , a rotation matrix Rrect (to apply to the image), and the rectifi ed and unrecti-
DEMO Mrect and M, respectively). From these terms, we can make a map,
using cvInitUndistortRectifyMap() (to be discussed shortly), of where to interpolate pix-
els from the original image in order to DEMO a new rectifi ed image.*
Th cation terms, of which OpenCV DEMO
two: (1) Hartley’s algorithm [Hartley98], which can yield uncalibrated stereo using just
the fundamental matrix; and (2) Bouguet’s algorithm,† which uses the rotation and
translation parameters from two calibrated cameras. Hartley’s algorithm DEMO be used to
derive structure from motion recorded by a single camera but may (when stereo recti-
fi
where you can employ calibration patterns—such as on a robot arm or for security cam-
era installations—Bouguet’s algorithm DEMO the natural one to use.
Uncalibrated stereo rectification: Hartley’s algorithm
Hartley’s DEMO attempts to fi nd homographies that map the epipoles to infi nity
while minimizing the computed disparities between the two stereo images; it does this
simply by matching points between two image pairs. Th us, we bypass having to com-
pute the camera intrinsics for the two cameras DEMO such intrinsic information is im-
plicitly contained in the point matches. Hence we need only compute the fundamental
matrix, which can be obtained from any matched set of seven or more points between
the two views DEMO the scene via cvFindFundamentalMat() as already described. Alterna-
tively, the DEMO matrix can be computed from cvStereoCalibrate().
Th
simply by observing DEMO in the scene. Th e disadvantage is that we have no sense of
image scale. For example, if we used a chessboard for generating point matches then
we would not be able to tell if the DEMO were 100 meters on each side and far away
or 100 centimeters on each side and nearby. Neither do we explicitly learn the intrinsic
DEMO matrix, without which the cameras might have diff erent focal lengths, skewed
pixels, diff erent centers of projection, and/or diff erent DEMO points. As a result, we
can determine 3D object reconstruction only DEMO to a projective transform. What this
means is that diff erent scales or projections of an object can appear the same to us (i.e.,
the feature points have the same 2D coordinates even though the DEMO objects diff er).
Both of these issues are illustrated in Figure 12-10.
e advantage of Hartley’s algorithm is that online stereo calibration can DEMO performed
* Stereo rectifi cation of an image in OpenCV is possible only when the epipole is outside of the image rect-
angle. Hence DEMO rectifi cation algorithm may not work with stereo confi gurations that are characterized by
either a very wide baseline or when the cameras point DEMO each other too much.
† Th e Bouguet algorithm is a completion and simplifi cation of the method fi rst presented by Tsai [Tsai87]
DEMO Zhang [Zhang99; Zhang00]. Jean-Yves Bouguet never published this algorithm beyond its DEMO
implementation in his Camera Calibration Toolbox Matlab.
Stereo Imaging
| 431
e result of the process of aligning the two image planes will be DEMO terms, four
ed camera matrices (
ere are many ways to compute our rectifi
ed) produce more distorted images than Bouguet’s calibrated algorithm. In situations
12-R4886-AT1.indd   431
9/15/08   4:24:49 DEMO
www.it-ebooks.info
Figure 12-10. Stereo reconstruction ambiguity: if we do not know object size, then diff erent size
objects can appear the same depending on their distance from the camera (left ); if we don’t know DEMO
camera instrinsics, then diff erent projections can appear the same—for example, by having diff erent
focal lengths and principal points
Assuming we have DEMO fundamental matrix F, which required seven or more points to
compute, Hartley’s algorithm proceeds as follows (see Hartley’s original paper [Hartley98]
for more details).
1. We use the fundamental matrix to compute the two DEMO via the relations Fel = 0
and ()eFr T = 0 for the left  and right epipoles, respectively.
2. We seek a DEMO rst homography Hr , which will map the right epipole to the 2D homo-
geneous point at infi nity (1, 0, 0)T. Since a homography has seven constraints (scale
is missing), and we use three to do the mapping to infi nity, we have 4 degrees of
freedom left  in which to choose our Hr . Th ese 4 degrees of freedom are mostly free-
dom to make a DEMO since most choices of Hr will result in highly distorted images.
To fi nd a good Hr , we choose a point in the DEMO where we want minimal distor-
tion to happen, allowing only rigid DEMO and translation not shearing there. A
reasonable choice for such a point is the image origin and we’ll further assume that
the epipole r DEMO = (, 01 lies on the x-axis (a rotation matrix will accomplish this
below). Given these coordinates, the matrix
10 0
01 0
−10 1/ k
⎛ ⎞
⎜ ⎟
G =⎜ ⎟
⎜ ⎟
⎝ ⎠
will take such an epipole to infi nity.
3. DEMO a selected point of interest in the right image (we chose DEMO origin), we compute
the translation T that will take that point to the image origin (0 in our case) and the
rotation DEMO that will take the epipole to () , )efr
will then be HGR= T.
432 | Chapter 12: Projection and 3D Vision
() , )ef
r
T = (, 01 . Th
e homography we want
12-R4886-AT1.indd   432
9/15/08   4:24:49 DEMO
www.it-ebooks.info
4. We next search for a matching homography Hl that will DEMO the left  epipole to
infi nity and align the rows of DEMO two images. Sending the left  epipole to infi nity
is easily DEMO by using up three constraints as in step 2. To align the rows, we just
use the fact that aligning the rows minimizes the total distance between all match-
ing points between the two images. Th DEMO is, we fi nd the Hl that minimizes the total
disparity DEMO left -right matching points ∑i dH p H p(, )lil DEMO . Th ese two homographies
defi ne the stereo rectifi cation.
Although the details of this algorithm are a bit tricky, cvStereoRectify Uncalibrated()DEMO
does all the hard work for us. Th e function is a bit misnamed because it does not rectify
uncalibrated stereo images; rather, DEMO computes homographies that may be used for rec-
tifi cation. Th e algorithm call is
int cvStereoRectifyUncalibrated(
const CvMat* points1,
const CvMat* DEMO,
const CvMat* F,
CvSize imageSize,
CvMat* Hl,
CvMat* Hr,
double threshold
);
In cvStereoRectifyUncalibrated(), the algorithm takes as input an array of 2-by-K cor-
responding points between the left  and right images in the arrays points1 and points2.
Th F. We DEMO familiar
with imageSize, which just describes the width and height of DEMO images that were used
during calibration. Our return rectifying homographies are returned in the function
variables Hl and Hr. Finally, if the distance from points to their corresponding epilines
exceeds a set threshold, the corresponding point is eliminated by the algorithm.*
If our cameras have roughly the same DEMO and are set up in an approximately
horizontally aligned frontal parallel confi guration, then our eventual rectifi ed outputs
from Hartley’s algorithm will look very much like the calibrated case described next.
If we know the DEMO or the 3D geometry of objects in the scene, we can DEMO the same
results as the calibrated case.
Calibrated stereo rectification: Bouguet’s DEMO
Given the rotation matrix and translation (R, T) between the DEMO images, Bouguet’s
algorithm for stereo rectifi cation simply attempts to minimize DEMO amount of change
reprojection produces for each of the two images (DEMO thereby minimize the resulting
reprojection distortions) while maximizing common viewing area.
DEMO minimize image reprojection distortion, the rotation matrix R that rotates the DEMO
camera’s image plane into the left  camera’s image plane is split DEMO half between the two
* Hartley’s algorithm works best for images that have been rectifi ed previously by single-camera calibration.
It won’t work at DEMO for images with high distortion. It is rather ironic that our “calibration-free” routine
works only for undistorted image inputs whose parameters are typically derived DEMO prior calibration. For
another uncalibrated 3D approach, see Pollefeys [Pollefeys99a].
Stereo DEMO | 433
e fundamental matrix we calculated above is passed as the array
12-R4886-AT1.indd   433
9/15/08   4:24:49 PM
www.it-ebooks.info
cameras; we call the two resulting rotation matrixes rl and rr for the left  and right cam-
era, respectively. Each camera rotates DEMO a rotation, so their principal rays each end up
parallel to DEMO vector sum of where their original principal rays had been pointing. As
we have noted, such a rotation puts the cameras into coplanar alignment but not into
row alignment. To compute the Rrect that will take DEMO left  camera’s epipole to infi nity
and align the epipolar lines DEMO, we create a rotation matrix by starting with
the direction of DEMO epipole e1 itself. Taking the principal point (cx, cy) as DEMO left  image’s
origin, the (unit normalized) direction of the epipole is directly along the translation
vector between the two cameras’ centers of DEMO:
e1 = T
T
Th
choosing a direction orthogonal to the principal ray (which will tend to be along the
image plane) DEMO a good choice. Th is is accomplished by using the cross product of e1 with
the direction of the principal ray and then normalizing DEMO that we’ve got another unit
vector:
e2 = []−TTyx 0 T
TT+
22
xy
Th e1 and e2; it can be found using the cross product:
e3 = e1 × e2
Our matrix that DEMO the epipole in the left
camera to infi
nity is then:
⎡
⎢
Rrect = ⎢
⎣⎢
()e
()e
()e
DEMO
2
3
T ⎤
T ⎥
⎥
⎦⎥
T
Th is matrix rotates the left  camera about the center of projection so that the epipolar
lines become horizontal and the epipoles are at infi nity. Th DEMO row alignment of the two
cameras is then achieved by setting:
Rl = Rrectrl
Rr = Rrectrr
We will also compute the rectifi DEMO left  and right camera matrices Mrect_l and Mrect_r but
return them DEMO with projection matrices P and P :
434
PM P=
lll
DEMO _
⎡
⎢
′= ⎢
⎢
⎣
⎤ ⎡1 000
⎥ ⎢
yl yl__ ⎥ ⎢01 0 0
⎥ ⎣⎢00 1 0
⎦
⎤
DEMO
⎥
⎦⎥
fcα
0 fc
00 1
xl l xl__
| Chapter 12: Projection and 3D Vision
l
r
e next vector, e2, must be orthogonal to e1 but is otherwise unconstrained. For e2,
DEMO third vector is just orthogonal to
12-R4886-AT1.indd   434
9/15/08   4:24:50 PM
www.it-ebooks.info
and
⎡ ⎤ ⎡10 0 Tx
⎢ ⎥ ⎢
′ = DEMO yr yr__ ⎥ ⎢ 01 0 0
⎢ ⎥ ⎣⎢ 00 1 0
⎣ ⎦
fcα
PM P= 0 fc
00 1
xr r DEMO
rrrrect _
(here αl and αr allow for a pixel skew DEMO that in modern cameras is almost always 0).
Th
homogeneous coordinates as follows:
⎡
⎢ ⎤
P ⎢ ⎥
⎢ ⎥ =
DEMO ⎥
⎣ ⎥
⎦
X x
Y y
Z w
1
where the screen coordinates can be calculated as (x/w, y/w)DEMO Points in two dimensions
can also then be reprojected into three dimensions given their screen coordinates and
the camera intrinsics matrix. Th e reprojection DEMO is:
⎡
⎢ ⎤
= ⎢ ⎥
Q ⎢ ⎥
⎢ ⎥
⎣⎢ ⎥
xx x x ⎦⎥
10 0 −cx
01 0 DEMO y
00 0 f
00 1/ ( )/−−Tc c T′
DEMO the parameters are from the left  image except for c x , which is the principal point
x coordinate in the right image. If the principal rays intersect at infi nity, then cx = c′ and
the term in the lower right corner is 0. Given a two-dimensional DEMO point
and its associated disparity d, we can project the point DEMO three dimensions using:
⎡ ⎤ ⎡ ⎤
⎢ ⎥ ⎢ ⎥
Q ⎢ ⎥ = ⎢ ⎥
⎢ ⎥ ⎢ Z ⎥
⎢ DEMO ⎢ ⎥
⎣ ⎦ ⎣W ⎦
x X
y Y
d
1
Th e 3D coordinates are then (X/W, Y/W, Z/W).
Applying the Bouguet rectifi cation method just described yields our DEMO stereo confi g-
uration as per Figure 12-4. New image centers and new image bounds are then chosen
for the rotated images so as DEMO maximize the overlapping viewing area. Mainly this just
sets a uniform camera center and a common maximal height and width of the two im-
DEMO areas as the new stereo viewing planes.
void cvStereoRectify(
const CvMat* cameraMatrix1,
const CvMat* cameraMatrix2,
Stereo Imaging
⎡
⎢
⎢
⎣⎢
DEMO
⎥
⎥
⎦⎥
⎤
⎥
⎥
⎦⎥
| 435
e projection matrices take a 3D point in homogeneous coordinates to a 2D point in
DEMO
12-R4886-AT1.indd   435
9/15/08   4:24:50 PM
www.it-ebooks.info
const CvMat* distCoeffs1,
const CvMat* distCoeffs2,
CvSize imageSize,
DEMO CvMat* R,
const CvMat* T,
CvMat* Rl,
CvMat* Rr,
CvMat* Pl,
CvMat* Pr,
CvMat* Q=0,
int    flags=CV_CALIB_ZERO_DISPARITY
);
For cvStereoRectify(),* we input the familiar original DEMO matrices and distortion
vectors returned by cvStereoCalibrate(). Th ese are DEMO by imageSize, the size of the
chessboard images used to perform DEMO calibration. We also pass in the rotation matrix
R and translation vector T between the right and left  cameras that was also returned by
cvStereoCalibrate().
Return parameters are Rl and Rr, the 3-by-3 row-aligned DEMO cation rotations for the
left  and right image planes as derived DEMO the preceding equations. Similarly, we get back
the 3-by-4 left  and right projection equations Pl and Pr. An optional return parameter is
Q, the 4-by-4 reprojection matrix described previously.
Th flags parameter is defaulted to DEMO disparity at infi nity, the normal case as per Fig-
ure DEMO Unsetting flags means that we want the cameras verging toward each other
(i.e., slightly “cross-eyed”) so that zero disparity occurs at a fi nite distance (this might
be necessary for greater depth resolution in the proximity of that particular distance).
If the flags parameter was not DEMO to CV_CALIB_ZERO_DISPARITY, then we must be more
careful about how we DEMO our rectifi
relative to the principal points (cx, cy) in DEMO left  and right cameras. Th us, our mea-
surements in Figure 12-4 must also be relative to these positions. Basically, we have
xx c rr=−
cxright (i.e., when CV_CALIB_ZERO_DISPARITY is passed to
cvStereoRectify()), and we can pass plain pixel coordinates (or disparity) to the formula
left
for depth. But if cvStereoRectify() is called without CV_CALIB_ZERO_DISPARITY DEMO cx
≠
cxright in general. Th erefore, even though the formula DEMO = fT/(xl – xr) remains the same,
one should keep in mind that xl and xr are not counted from the DEMO center but rather
left right
from the respective principal points cx and cx
, which could diff er from xl and xr.
Hence, if you computed disparity d = xl – xr then it should be DEMO before computing
left
Z: Z fT/(d – (cx – DEMO)).
Rectification map
Once we have our stereo calibration terms, we can pre-compute left  and right rectifi cation
lookup maps for the left  and right camera views using separate calls to cvInitUndistort
* Again, DEMO() is a bit of a misnomer because the function computes the terms that we can use
for rectifi cation but doesn’t actually rectify DEMO stereo images.
436
to modify the distances so that
=
left
been set to infi nity, we have cx
| Chapter 12: Projection DEMO 3D Vision
ed system. Recall that we rectifi
ed our system
right
x
and
xx c
ll
=− left. When disparity has
x
e
DEMO   436
9/15/08   4:24:51 PM
www.it-ebooks.info
RectifyMap(). As with any image-to-image mapping function, a forward DEMO (in
which we just compute where pixels go from the source DEMO to the destination image)
will not, owing to fl oating-point DEMO locations, hit all the pixel locations in the
destination image, which thus will look like Swiss cheese. So instead we work backward:
DEMO each integer pixel location in the destination image, we look up DEMO fl oating-point
coordinate it came from in the source image and then interpolate from its surrounding
source pixels a value to use in that DEMO destination location. Th is source lookup typi-
cally uses bilinear interpolation, DEMO we encountered with cvRemap() in Chapter 6.
Th cation is illustrated in Figure 12-11. As shown by the equation fl ow
in that DEMO gure, the actual rectifi cation process proceeds backward from (c) DEMO (a) in a
process known as reverse mapping. For each integer pixel in the rectifi ed image (c), we
fi nd its coordinates in the undistorted image (b) and use those to look DEMO the actual
(fl oating-point) coordinates in the raw image (a)DEMO Th e fl oating-point coordinate pixel
value is then interpolated from the nearby integer pixel locations in the original source
image, and that value is used to fi ll in the rectifi ed integer pixel location DEMO the destina-
tion image (c). Aft er the rectifi ed DEMO is fi lled in, it is typically cropped to emphasize
the DEMO areas between the left  and right images.
Th cvInitUndistort
RectifyMap(). We call this function twice, once for the left  and once DEMO the right image
of stereo pair.
void cvInitUndistortRectifyMap(
const CvMat* M,
const CvMat* distCoeffs,
const CvMat* Rrect,
const CvMat* Mrect,DEMO
CvArr*       mapx,
CvArr*       mapy
);
Th cvInitUndistortRectifyMap() function takes as input the 3-by-3 camera matrix
DEMO, the rectifi ed 3-by-3 camera matrix Mrect, the 3-by-3 rotation matrix Rrect, and the
5-by-1 camera distortion parameters in distCoeffs.
If we calibrated our stereo cameras using cvStereoRectify(), then we can read our in-
put to cvInitUndistortRectifyMap() straight out of cvStereoRectify() using fi rst DEMO left
parameters to rectify the left  camera and then the right DEMO to rectify the right
camera. For Rrect, use Rl or Rr DEMO cvStereoRectify(); for M, use cameraMatrix1 or
cameraMatrix2. For Mrect we could use the fi rst three columns of the 3-by-4 Pl or DEMO
from cvStereoRectify(), but as a convenience the function allows us DEMO pass Pl or Pr di-
rectly and it will read Mrect from them.
If, on the other hand, we used cvStereoRectifyUncalibrated() to DEMO our ste-
reo cameras, then we must preprocess the homography a DEMO Although we could—in
principle and in practice—rectify stereo without using the camera intrinsics, OpenCV
does not have a function for doing this directly. If we do not have Mrect from some
prior calibration, the proper procedure is to set Mrect equal to M. Th en, for Rrect in
Stereo Imaging
| 437
e process of rectifi
e function that implements DEMO math depicted in Figure 12-11 is called
e
12-R4886-AT1.indd   437
9/15/08   4:24:51 PM
www.it-ebooks.info
Figure 12-11. Stereo rectifi
and rectifi
rectifi
cation: for the left
ed (c) and fi
cation computation actually works backward from (c) to (a)
and right camera, the raw image (a) is undistorted (b)
nally cropped (d) to focus on overlapping areas between the two cameras; the
cvInitUndistortRectifyMap(), we need to DEMO Rrect_l = Mrect_–1 l H
HlMl if Mrect_–1 l is unavailable) DEMO Rrect_r = Mrect_–1 r Hr Mr (or just Rrect_r r is
DEMO) for the left  and the right rectifi cation, respectively. Finally, we will also need
the distortion coeffi  cients for each camera to fi ll in the 5-by-1 distCoeffs parameters.
Th cvInitUndistortRectifyMap() returns lookup DEMO mapx and mapy as output.
Th
destination image; the maps can DEMO be plugged directly into cvRemap(), a function we
fi rst DEMO in Chapter 6. As we mentioned, the function cvInitUndistortRectifyMap() is
DEMO separately for the left  and the right cameras so that we DEMO obtain their distinct
mapx and mapy remapping parameters. Th e function cvRemap() may then be called, using
the left  and then the DEMO maps each time we have new left  and right stereo images DEMO
rectify. Figure 12-12 shows the results of stereo undistortion and rectifi cation of a stereo
pair of images. Note how feature points become horizontally DEMO in the undistorted
rectifi ed images.
–1
lMl (or just Rrect_l DEMO Ml
–1
= Mr Hr Mr if Mrect_–1
Stereo Correspondence
Stereo correspondence—matching a 3D point in the two diff erent camera views—can
be computed DEMO over the visual areas in which the views of the two cameras overlap.
Once again, this is one reason why you will tend to get better results if you arrange your
cameras to be as nearly DEMO parallel as possible (at least until you become expert at
stereo DEMO). Th en, once we know the physical coordinates of the DEMO or the sizes
438
| Chapter 12: Projection and 3D Vision
DEMO function
ese maps indicate from where we should interpolate source pixels for each pixel of the
12-R4886-AT1.indd   438
9/15/08   4:DEMO:51 PM
www.it-ebooks.info
Figure 12-12. Stereo rectifi cation: original left  and right image DEMO (upper panels) and the stereo
rectifi ed left  and right DEMO pair (lower panels); note that the barrel distortion (in top of chessboard
patterns) has been corrected and the scan lines are aligned in the rectifi ed images
of objects in the scene, we can derive depth measurements from the triangulated dispar-
left
ity measures d = DEMO – xr (or d = xl – xr – (cx – cxright) if the principal rays intersect at a fi nite
distance) DEMO the corresponding points in the two diff erent camera views. Without
such physical information, we can compute depth only up to a scale factor. If we don’t
have the camera instrinsics, as when using Hartley’s algorithm, we can compute point
locations only up to a projective transform (DEMO Figure 12-10).
OpenCV implements a fast and eff ective block-matching stereo algorithm, cvFindStereo
CorrespondenceBM(), that is similar to the one developed DEMO Kurt Konolige [Konolige97];
it works by using small “sum of absolute diff erence” (SAD) windows to fi nd matching
points between the DEMO  and right stereo rectifi ed images.* Th is algorithm fi nds DEMO
strongly matching (high-texture) points between the two images. Th us, DEMO a highly tex-
tured scene such as might occur outdoors in a forest, every pixel might have computed
depth. In a very low-textured scene, such as an indoor hallway, very few points might
register depth. DEMO ere are three stages to the block-matching stereo correspondence algo-
rithm, DEMO works on undistorted, rectifi ed stereo image pairs:
* Th DEMO algorithm is available in an FPGA stereo hardware system from Videre (DEMO [Videre]).
Stereo Imaging | 439
12-R4886-AT1.indd   439
9/15/08   4:24:51 PM
www.it-ebooks.info
1. Prefi ltering to normalize image brightness and enhance texture.
2. DEMO search along horizontal epipolar lines using an SAD window.
3. Postfi ltering to eliminate bad correspondence matches.
In the prefi ltering step, the input images are normalized to reduce lighting diff erences
and to enhance image DEMO Th is is done by running a window—of size 5-by-5, 7-by-7
(the default), . . ., 21-by-21 (the maximum)—over the DEMO Th e center pixel Ic under the
window is replaced by min[max(Ic – –I , – Icap), Icap], where –I is the average value in the win-
dow and Icap is a positive numeric DEMO whose default value is 30. Th is method is invoked
by a CV_NORMALIZED_RESPONSE fl ag. Th e other possible fl ag is CV_LAPLACIAN_OF_GAUSSIAN,
DEMO runs a peak detector over a smoothed version of the image.
Correspondence is computed by a sliding SAD window. For each feature in the DEMO  im-
age, we search the corresponding row in the right image for a best match. Aft er rectifi -
cation, each row is an epipolar line, so the matching location in the right image must be
along the same row (same y-coordinate) as in the left  image; this matching location can
be found if the feature has enough texture to be detectable and if it is not occluded in
the DEMO camera’s view (see Figure 12-16). If the left  feature pixel coordinate is at (x0, y0)
then, for a horizontal frontal parallel camera arrangement, the match (if any) must be
found on the same row and at, or to the left  of, x0; see Figure 12-13. For frontal parallel
cameras, x0 is at zero DEMO and larger disparities are to the left . For cameras that are
angled toward each other, the match may occur at negative disparities (DEMO the right of
x0). Th e fi rst parameter that controls matching search is minDisparity, which is where
the matching search should start. Th e default for minDisparity is 0. Th e disparity search
is DEMO carried out over numberOfDisparities counted in pixels (the default is 64 DEMO).
Disparities have discrete, subpixel resolution that is set by the DEMO subPixelDis-
parities (the default is 16 subdisparities per pixel). Reducing DEMO number of disparities
to be searched can help cut down computation time by limiting the length of a search
for a matching point along DEMO epipolar line. Remember that large disparities represent
closer distances.
Setting the minimum disparity and the number of disparities to be searched establishes
the horopter, the 3D volume that is covered by the search range of the DEMO algorithm.
Figure 12-14 shows disparity search limits of fi ve pixels starting at three diff erent dis-
parity limits: 20, 17, and 16. Each disparity limit defi nes a plane at a fi xed depth DEMO
the cameras (see Figure 12-15). As shown in Figure 12-14, each disparity limit—together
with the number of disparities—sets a diff erent horopter DEMO which depth can be detected.
Outside of this range, depth will DEMO be found and will represent a “hole” in the depth
map where depth is not known. Horopters can be made larger by decreasing the DEMO
line distance T between the cameras, by making the focal length DEMO, by increasing
the stereo disparity search range, or by increasing the pixel width.
Correspondence within the horopter has one in-built constraint, called the order con-
straint, which simply states that the order of the features cannot change from the left
view to the right. Th ere may DEMO missing features—where, owing to occlusion and noise,
440 | Chapter DEMO: Projection and 3D Vision
12-R4886-AT1.indd   440
9/15/08   DEMO:24:52 PM
www.it-ebooks.info
Figure 12-13. Any right-image match of a left
to the left
(here, 0) and moves to the left
of window-based feature matching DEMO shown in the lower part of the fi
-image feature must occur on the same row and at (or
of) the same coordinate DEMO, where the match search starts at the minDisparity point
for the DEMO number of disparities; the characteristic matching function
gure
Figure 12-14. Each DEMO represents a plane of constant disparity in integer pixels from 20 to 12; a
disparity search range of fi ve pixels will cover diff erent horopter ranges, as shown by the vertical ar-
rows, and DEMO erent maximal disparity limits establish diff erent horopters
Stereo Imaging
| 441
12-R4886-AT1.indd   441
9/15/08   4:24:52 PM
www.it-ebooks.info
Figure 12-15. A fi
xed disparity forms a plane of fi
DEMO distance from the cameras
some features found on the left  cannot DEMO found on the right—but the ordering of those
features that are found remains the same. Similarly, there may be many features on the
right that were not identifi ed on the left  (these are called DEMO), but insertions do
not change the order of features although they may spread those features out. Th e proce-
dure illustrated in Figure DEMO refl ects the ordering constraint when matching features
on a horizontal scan line.
Given the smallest allowed disparity increment ∆d, we can determine smallest achiev-
able depth range resolution ∆Z by using the formula:
ΔΔZ DEMO Z 2
d
fT
It is useful to keep this formula in mind so that you know what kind of depth resolution
to expect DEMO your stereo rig.
Aft er correspondence, we turn to postfi ltering. DEMO e lower part of Figure 12-13 shows a
typical matching function response as a feature is “swept” from the minimum disparity
out to maximum DEMO Note that matches oft en have the characteristic of a strong
central peak surrounded by side lobes. Once we have candidate feature correspondences
between DEMO two views, postfi ltering is used to prevent false matches. OpenCV DEMO
use of the matching function pattern via a uniquenessRatio parameter (whose DEMO
value is 12) that fi lters out matches, where uniquenessRatio > (match_val–min_match)/
min_match.
442
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   442
9/15/08   4:24:52 PM
www.it-ebooks.info
Figure 12-16. Stereo correspondence starts by assigning point matches between corresponding DEMO
in the left  and right images: left  and right images DEMO a lamp (upper panel); an enlargement of a single
scan DEMO (middle panel); visualization of the correspondences assigned (lower panel).
To make sure that there is enough texture to overcome random noise DEMO matching,
OpenCV also employs a textureThreshold. Th is is just a limit on the SAD window re-
sponse such that no match is DEMO whose response is below the textureThreshold
(the default value is 12)DEMO Finally, block-based matching has problems near the boundar-
ies of objects DEMO the matching window catches the foreground on one side and
the background on the other side. Th is results in a local region of DEMO and small dis-
parities that we call speckle. To prevent these borderline matches, we can set a speckle
detector over a speckle window (DEMO in size from 5-by-5 up to 21-by-21) by setting
speckleWindowSize, which has a default setting of 9 for a 9-by-9 window. Within
the DEMO window, as long as the minimum and maximum detected disparities are
DEMO speckleRange, the match is allowed (the default range is set to 4).
Stereo vision is becoming crucial to surveillance systems, navigation, DEMO robotics, and
such systems can have demanding real-time performance requirements. Th DEMO, the ste-
reo correspondence routines are designed to run fast. Th DEMO, we can’t keep allocat-
ing all the internal scratch buff ers DEMO the correspondence routine needs each time we
call cvFindStereoCorrespondenceBM().
Th DEMO are kept in a data struc-
ture named CvStereoBMState:
typedef struct CvStereoBMState {
//pre filters (normalize input images):
Stereo Imaging DEMO 443
e block-matching parameters and the internal scratch buff
12-R4886-AT1.indd   443
9/15/08   4:24:53 PM
www.it-ebooks.info
int       preFilterType;
int       preFilterSize;//for 5x5 up to 21x21
int       preFilterCap;
//correspondence using Sum of Absolute Difference (SAD):
int       SADWindowSize; // Could be 5x5,7x7, ..., 21x21
int       minDisparity;
int       numberOfDisparities;//Number of DEMO to search
//post filters (knock out bad matches):
int       textureThreshold; //minimum allowed
float     uniquenessRatio;// Filter out if:
// [ match_val - min_match <
// uniqRatio*min_match ]
// over the corr window area
int       speckleWindowSize;//Disparity variation window
int       speckleRange;//Acceptable range of variation in window
// temporary buffers
CvMat* preFilteredImg0;
CvMat* preFilteredImg1;
CvMat* slidingSumBuf;
} CvStereoBMState;
Th cvCreateStereoBMState().
Th is function takes the parameter preset, which can be set to any one of the following.
e state structure is allocated and returned by DEMO function
CV_STEREO_BM_BASIC
Sets all parameters to their default values
CV_STEREO_BM_FISH_EYE
Sets parameters for dealing with wide-angle lenses
CV_STEREO_BM_NARROW
Sets parameters for stereo cameras with DEMO fi
eld of view
Th is function also takes the optional parameter numberOfDisparities; if nonzero, it
overrides the default value from the preset. DEMO is the specifi cation:
CvStereoBMState* cvCreateStereoBMState(
int presetFlag=CV_STEREO_BM_BASIC,
int numberOfDisparities=0
);
Th e state structure, CvStereoBMState{}, is released by DEMO
void cvReleaseBMState(
CvStereoBMState **BMState
);
Any stereo correspondence parameters can be adjusted at any time between cvFindStereo
CorrespondenceBM calls by directly assigning DEMO values of the state structure fi elds. Th e
correspondence function will take care of allocating/reallocating the internal buff ers as
needed.
Finally, cvFindStereoCorrespondenceBM() takes in rectifi ed image pairs and outputs a
disparity DEMO given its state structure:
void cvFindStereoCorrespondenceBM(
const CvArr     *leftImage,
444
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   444
9/15/08   4:24:54 PM
www.it-ebooks.info
const CvArr     *rightImage,
CvArr           *disparityResult,
CvStereoBMState *BMState
);
Stereo Calibration, Rectification, and Correspondence Code
Let’s put this all together with code in an example program DEMO will read in a number
of chessboard patterns from a fi le called list.txt. Th is fi le contains a list of alternating
left  and right stereo (chessboard) image pairs, which are used to calibrate the cameras
and then rectify the images. Note once again that we’re DEMO you’ve arranged the
cameras so that their image scan lines are roughly physically aligned and such that each
camera has essentially the same fi DEMO of view. Th is will help avoid the problem of the epi-
pole being within the image* and will also tend to maximize the DEMO of stereo overlap
while minimizing the distortion from reprojection.
In the code (Example 12-3), we fi rst read in the left  and DEMO image pairs, fi nd the chess-
board corners to subpixel accuracy, and set object and image points for the images
where all the DEMO could be found. Th is process may optionally be displayed.
Given this list of found points on the found good chessboard images, the code calls
cvStereoCalibrate() to calibrate the camera. Th is calibration gives us DEMO camera matrix
_M and the distortion vector _D for the two cameras; it also yields the rotation matrix _R,
the translation vector _T, the essential matrix _E, and the fundamental matrix _F.
Next comes DEMO little interlude where the accuracy of calibration is assessed by check-
ing how nearly the points in one image lie on the epipolar lines DEMO the other image. To
do this, we undistort the original points DEMO cvUndistortPoints() (see Chapter 11),
compute the epilines using cvComputeCorrespondEpilines(), and then compute the dot
product of the points with the lines (in the ideal case, these dot products would all be DEMO).
Th
Th cation maps using the un-
calibrated (Hartley) method cvStereoRectifyUncalibrated() or the calibrated (Bouguet)
method cvStereoRectify(). If DEMO rectifi cation is used, the code further allows
for either computing DEMO needed fundamental matrix from scratch or for just using the
fundamental matrix from the stereo calibration. Th e rectifi ed images are then computed
DEMO cvRemap(). In our example, lines are drawn across the image pairs to aid in seeing
how well the rectifi ed images are DEMO An example result is shown in Figure 12-12,
where we can see that the barrel distortion in the original images is largely corrected
DEMO top to bottom and that the images are aligned by horizontal scan lines.
Finally, if we rectifi ed the images then we initialize the block-matching state (internal
allocations and parameters) using cvCreateBMState(). We can then compute the dispar-
ity maps by using cvFindStereoCorrespondenceBM(). Our code example allows you to use
either horizontally aligned (left -right) or DEMO aligned (top-bottom) cameras; note,
* OpenCV does not (yet) deal with the case of rectifying stereo images when the epipole is within the image
frame. See, for example, Pollefeys, Koch, and DEMO [Pollefeys99b] for a discussion of this case.
Stereo Imaging | 445
e accumulated absolute distance forms the error.
e code then optionally moves on DEMO computing the rectifi
12-R4886-AT1.indd   445
9/15/08   4:24:54 PM
www.it-ebooks.info
however, that for the vertically aligned case the function cvFindStereoCorrespondenceBM()DEMO
can compute disparity only for the case of uncalibrated rectifi cation unless you add
code to transpose the images yourself. For horizontal camera arrangements, cvFind
StereoCorrespondenceBM() can fi nd disparity for calibrated or for uncalibrated DEMO ed
stereo image pairs. (See Figure 12-17 in the next section DEMO example disparity results.)
Example 12-3. Stereo calibration, rectifi
cation, and correspondence
#include "cv.h"
#include "cxmisc.h"
#include "highgui.h"
#include "cvaux.h"
#include <vector>
#include <string>
#include <algorithm>DEMO
#include <stdio.h>
#include <ctype.h>
using namespace std;
//
// Given a list of chessboard images, the number of DEMO (nx, ny)
// on the chessboards, and a flag called useCalibrated (0 for Hartley
// or 1 for Bouguet stereo DEMO). Calibrate the cameras and display the
// rectified results along DEMO the computed disparity images.
//
static void
StereoCalib(const char* imageList, int nx, int ny, int useUncalibrated)
{
int displayCorners = 0;
int showUndistorted = 1;
bool isVerticalStereo = false;//DEMO can handle left-right
//or up-down camera arrangements
const int maxScale = 1;
const float squareSize = 1.f; //Set this to your actual square size
FILE* f = fopen(imageList, "rt");
DEMO i, j, lr, nframes, n = nx*ny, N = DEMO;
vector<string> imageNames[2];
vector<CvPoint3D32f> objectPoints;
vector<CvPoint2D32f> points[2];
vector<int> npoints;
vector<uchar> active[2];
vector<CvPoint2D32f> temp(n);
CvSize imageSize = {0,0};
// ARRAY AND VECTOR STORAGE:
double M1[3][3], M2[3][3], D1[5], D2[5];
double R[3][3], T[3], E[3][3], F[3][3];
CvMat _M1 = cvMat(3, 3, CV_64F, M1 );
CvMat _M2 = cvMat(3, DEMO, CV_64F, M2 );
CvMat _D1 = cvMat(1, 5, CV_64F, D1 );
CvMat _D2 = cvMat(1, 5, CV_64F, D2 );
CvMat _R = cvMat(3, 3, CV_64F, DEMO );
CvMat _T = cvMat(3, 1, CV_64F, T );
CvMat _E = cvMat(3, 3, CV_64F, E );DEMO
446
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   446
DEMO/15/08   4:24:54 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
CvMat _F = cvMat(3, 3, CV_64F, F );
if( displayCorners )
cvNamedWindow( "corners", 1 );
// READ IN THE LIST OF CHESSBOARDS:
if( !f )
{
fprintf(DEMO, "can not open file %s\n", imageList );
return;
}
for(i=0;;i++)
{
char buf[1024];
int count = 0, result=0;
lr = i % 2;
vector<CvPoint2D32f>& pts = points[lr];
if( !fgets( buf, sizeof(buf)-3, DEMO ))
break;
size_t len = strlen(buf);
while( len > 0 && isspace(buf[len-1]))
buf[--len] = '\0';DEMO
if( buf[0] == '#')
continue;
IplImage* img = DEMO( buf, 0 );
if( !img )
break;
imageSize = cvGetSize(img);
imageNames[lr].push_back(buf);
//FIND CHESSBOARDS AND DEMO THEREIN:
for( int s = 1; s <= maxScale; s++ )
{
IplImage* timg = img;
if( s > DEMO )
{
timg = cvCreateImage(cvSize(img->width*s,img->height*s),
img->depth, img->nChannels );
cvResize( img, timg, CV_INTER_CUBIC );
}
result = cvFindChessboardCorners( timg, cvSize(nx, ny),
&temp[0], &count,
CV_CALIB_CB_ADAPTIVE_THRESH |
CV_CALIB_CB_NORMALIZE_IMAGE);
if( timg != img )
cvReleaseImage( &timg );
if( result || s DEMO maxScale )
for( j = 0; j < count; j++ )
{
temp[j].x /= s;
temp[j].y /= s;
}
if( result )
break;
}
if( displayCorners )
Stereo DEMO
| 447
12-R4886-AT1.indd   447
9/15/08   4:24:54 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
{
printf("%s\n", buf);
IplImage* cimg = cvCreateImage( imageSize, 8, 3 );
cvCvtColor( img, cimg, CV_GRAY2BGR );
cvDrawChessboardCorners( cimg, cvSize(nx, ny), &temp[0],
count, result );
cvShowImage( "corners", cimg );
cvReleaseImage( &cimg );
if( cvWaitKey(0) == 27 ) //Allow ESC to quit
exit(-1);
}
else
putchar('.');
N DEMO pts.size();
pts.resize(N + n, cvPoint2D32f(0,0));DEMO
active[lr].push_back((uchar)result);
//assert( result != 0 );DEMO
if( result )
{
//Calibration will suffer without subpixel interpolation
cvFindCornerSubPix( img, &temp[0], count,
cvSize(11, 11), cvSize(-1,-1),
cvTermCriteria(CV_TERMCRIT_ITER+CV_TERMCRIT_EPS,
30, 0.01) );
copy( temp.begin(), temp.end(), pts.begin() + N );
}
cvReleaseImage( &img );
}
fclose(f);
printf("\n");
// HARVEST CHESSBOARD 3D OBJECT POINT LIST:
nframes = DEMO();//Number of good chessboads found
objectPoints.resize(nframes*n);
for( i = 0; i < ny; i++ )
for( j DEMO 0; j < nx; j++ )
objectPoints[i*nx + j] =
DEMO(i*squareSize, j*squareSize, 0);
for( i = 1; i < nframes; i++ )
copy( objectPoints.begin(), objectPoints.begin() + n,
objectPoints.begin() + i*n );
npoints.resize(nframes,n);
N = nframes*n;
CvMat _objectPoints = cvMat(1, N, CV_32FC3, &DEMO );
CvMat _imagePoints1 = cvMat(1, N, CV_32FC2, &points[0][0] );
CvMat _imagePoints2 = cvMat(1, N, CV_32FC2, &points[1][0] );
CvMat _npoints = cvMat(1, npoints.size(), CV_32S, &npoints[0] );
cvSetIdentity(&_M1);
cvSetIdentity(&_M2);
cvZero(&_D1);
cvZero(&_D2);
// CALIBRATE THE STEREO CAMERAS
printf("Running stereo calibration ...");
448
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   448
9/15/08   4:24:54 DEMO
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
fflush(stdout);
cvStereoCalibrate( &_objectPoints, &_imagePoints1,
&_imagePoints2, &DEMO,
&_M1, &_D1, &_M2, &_D2,
imageSize, &DEMO, &_T, &_E, &_F,
cvTermCriteria(CV_TERMCRIT_ITER+
CV_TERMCRIT_EPS, 100, 1e-5),
CV_CALIB_FIX_ASPECT_RATIO +
CV_CALIB_ZERO_TANGENT_DIST +
CV_CALIB_SAME_FOCAL_LENGTH );
printf(" done\n");
// CALIBRATION QUALITY CHECK
// because the output fundamental DEMO implicitly
// includes all the output information,
// we can check the quality of calibration using the
// epipolar geometry constraint: DEMO
vector<CvPoint3D32f> lines[2];
points[0].resize(N);
points[1].resize(N);
DEMO = cvMat(1, N, CV_32FC2, &points[0][0] );
_imagePoints2 = cvMat(1, N, CV_32FC2, &points[1][0] );
lines[0].resize(N);
lines[1].resize(N);
CvMat _L1 = cvMat(1, N, CV_32FC3, &lines[0][0]);
CvMat _L2 = cvMat(1, N, CV_32FC3, &lines[1][0]);
//Always work in undistorted space
cvUndistortPoints( &_imagePoints1, &_imagePoints1,
&_M1, &_D1, 0, &_M1 );
cvUndistortPoints( &DEMO, &_imagePoints2,
&_M2, &_D2, 0, &_M2 );
cvComputeCorrespondEpilines( &_imagePoints1, 1, &_F, &_L1 );
cvComputeCorrespondEpilines( &_imagePoints2, 2, &_F, &_L2 );
double avgErr = 0;
for( i = 0; i < N; i++ )
DEMO
double err = fabs(points[0][i].x*lines[1][i].x +
points[0][i].y*lines[1][i].y + lines[1][i].z)
+ fabs(points[1][i].x*lines[0][i].x +
points[1][i].y*lines[0][i].y + lines[0][i].z);
avgErr += err;
}
printf( "avg err = %g\n", avgErr/(nframes*n) );
//DEMO AND DISPLAY RECTIFICATION
if( showUndistorted )
{
CvMat* mx1 = cvCreateMat( imageSize.height,
imageSize.width, CV_32F );
CvMat* my1 = cvCreateMat( imageSize.height,
imageSize.width, CV_32F );
CvMat* mx2 = cvCreateMat( imageSize.height,
DEMO, CV_32F );
CvMat* my2 = cvCreateMat( imageSize.height,
Stereo Imaging
| 449
12-R4886-AT1.indd   449
9/15/08   4:24:55 DEMO
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
imageSize.width, CV_32F );
CvMat* img1r = cvCreateMat( imageSize.height,
imageSize.width, CV_8U );
CvMat* img2r = cvCreateMat( imageSize.height,
imageSize.width, CV_8U );
CvMat* disp = cvCreateMat( imageSize.height,
imageSize.width, CV_16S );
CvMat* vdisp = cvCreateMat( imageSize.height,
imageSize.width, CV_8U );
CvMat* DEMO;
double R1[3][3], R2[3][3], P1[3][4], P2[3][4];
CvMat _R1 = DEMO(3, 3, CV_64F, R1);
CvMat _R2 = cvMat(3, 3, CV_64F, R2);
// IF BY CALIBRATED (BOUGUET'DEMO METHOD)
if( useUncalibrated == 0 )
{
CvMat _P1 = cvMat(3, 4, CV_64F, P1);
CvMat _P2 = cvMat(DEMO, 4, CV_64F, P2);
cvStereoRectify( &_M1, &_M2, &_D1, &_D2, imageSize,
&_R, &_T,
&_R1, &_R2, &_P1, &_P2, 0,
0/*CV_CALIB_ZERO_DISPARITY*/ );
DEMO = fabs(P2[1][3]) > fabs(P2[0][3]);
//Precompute maps for cvRemap()
cvInitUndistortRectifyMap(&_M1,&_D1,&_R1,&_P1,mx1,my1);DEMO
cvInitUndistortRectifyMap(&_M2,&_D2,&_R2,&_P2,mx2,my2);
}
//OR ELSE HARTLEY'S METHOD
else if( useUncalibrated == 1 || DEMO == 2 )
// use intrinsic parameters of each camera, DEMO
// compute the rectification transformation directly
// from the fundamental matrix
{
double H1[3][3], H2[3][3], iM[3][3];
CvMat _H1 = cvMat(3, 3, CV_64F, H1);
CvMat _H2 = cvMat(3, 3, CV_64F, H2);
CvMat _iM = cvMat(3, 3, CV_64F, iM);
//Just to show you could have independently used F
if( useUncalibrated == 2 )
cvFindFundamentalMat( &_imagePoints1,
&_imagePoints2, &_F);
cvStereoRectifyUncalibrated( &_imagePoints1,
&_imagePoints2, &_F,
imageSize,DEMO
&_H1, &_H2, 3);
cvInvert(&_M1, &_iM);DEMO
cvMatMul(&_H1, &_M1, &_R1);
cvMatMul(&_iM, &DEMO, &_R1);
cvInvert(&_M2, &_iM);
cvMatMul(&_H2, &_M2, &_R2);
cvMatMul(&_iM, &_R2, &_R2);
//Precompute map for cvRemap()
450
| Chapter 12: DEMO and 3D Vision
12-R4886-AT1.indd   450
9/15/08   4:24:55 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi cation, and correspondence (continued)
cvInitUndistortRectifyMap(&_M1,&_D1,&_R1,&_M1,mx1,my1);
cvInitUndistortRectifyMap(&DEMO,&_D1,&_R2,&_M2,mx2,my2);
}
else
assert(0);
cvNamedWindow( "rectified", 1 );
// RECTIFY THE DEMO AND FIND DISPARITY MAPS
if( !isVerticalStereo )
pair = cvCreateMat( DEMO, imageSize.width*2,
CV_8UC3 );
else
pair = cvCreateMat( imageSize.height*2, DEMO,
CV_8UC3 );
//Setup for finding stereo correspondences
CvStereoBMState *BMState = cvCreateStereoBMState();
assert(BMState != 0);
BMState->preFilterSize=41;
BMState->preFilterCap=31;
BMState->SADWindowSize=41;
BMState->minDisparity=-64;
BMState->numberOfDisparities=128;
DEMO>textureThreshold=10;
BMState->uniquenessRatio=15;
for( i = 0; i < nframes; i++ )
{
IplImage* img1=cvLoadImage(imageNames[0][i].c_str(),0);
IplImage* img2=cvLoadImage(imageNames[1][i].c_str(),0);
if( img1 && img2 )
DEMO
CvMat part;
cvRemap( img1, img1r, mx1, my1 );
cvRemap( img2, img2r, mx2, my2 );
if( !isVerticalStereo || useUncalibrated != 0 )
{
// When the stereo camera is DEMO vertically,
// useUncalibrated==0 does not transpose the
// image, DEMO the epipolar lines in the rectified
// images are vertical. Stereo DEMO
// function does not support such a case.
cvFindStereoCorrespondenceBM( img1r, DEMO, disp,
BMState);
cvNormalize( disp, vdisp, 0, 256, CV_MINMAX );
cvNamedWindow( "disparity" );
cvShowImage( "disparity", vdisp );
}
if( !isVerticalStereo )
{
cvGetCols( pair, &part, 0, imageSize.width );
cvCvtColor( img1r, &part, CV_GRAY2BGR );
cvGetCols( pair, &part, imageSize.width,
imageSize.width*2 );
DEMO Imaging
| 451
12-R4886-AT1.indd   451
9/15/08   4:24:55 PM
www.it-ebooks.info
Example 12-3. Stereo calibration, rectifi
cation, and correspondence (continued)
cvCvtColor( img2r, &part, CV_GRAY2BGR );
for( j = 0; j < imageSize.height; j += 16 )
cvLine( pair, cvPoint(0,j),
cvPoint(imageSize.width*2,j),
CV_RGB(0,255,0));
}
else
{
cvGetRows( pair, &part, 0, imageSize.height );
cvCvtColor( img1r, &part, CV_GRAY2BGR );
cvGetRows( pair, &part, imageSize.height,
imageSize.height*2 );
cvCvtColor( img2r, &part, DEMO );
for( j = 0; j < imageSize.width; j DEMO 16 )
cvLine( pair, cvPoint(j,0),
cvPoint(j,imageSize.height*2),
CV_RGB(0,255,0));
}
cvShowImage( "rectified", pair );
if( cvWaitKey() == 27 )
break;DEMO
}
cvReleaseImage( &img1 );
cvReleaseImage( &img2 );
}
cvReleaseStereoBMState(&BMState);
cvReleaseMat( &mx1 );
cvReleaseMat( &my1 );
cvReleaseMat( &mx2 );
cvReleaseMat( &my2 );
cvReleaseMat( &img1r );
cvReleaseMat( &img2r );
cvReleaseMat( &disp );
}
}
int main(void)
{
StereoCalib("list.txt", DEMO, 6, 1);
return 0;
}
Depth Maps from DEMO Reprojection
Many algorithms will just use the disparity map directly—for example, DEMO detect
whether or not objects are on (stick out from) a table. But for 3D shape matching, 3D
model learning, robot grasping, and so on, we need the actual 3D reconstruction or
depth map. Fortunately, all the stereo machinery we’ve built up so far makes this easy.
Recall the 4-by-4 reprojection matrix Q introduced in the section on DEMO stereo
rectifi cation. Also recall that, given the disparity d and DEMO 2D point (x, y), we can derive
the 3D depth using
452
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   452
9/15/08   4:24:55 PM
www.it-ebooks.info
⎡
⎢
Q ⎢
⎢
⎢
⎣
x
y
d
1
DEMO ⎡
⎥ ⎢
⎥ = ⎢
⎥ ⎢ Z
⎥ ⎢
⎦ ⎣W
X
Y
⎤
⎥
⎥
⎥
⎥
⎦
where the 3D DEMO are then (X/W, Y/W, Z/W). Remarkably, Q encodes whether
or not the cameras’ lines of sight were converging (cross eyed) as well as the camera
baseline and the principal points in both images. As a result, we need not explicitly ac-
count for converging or frontal parallel cameras and may instead simply extract depth
DEMO matrix multiplication. OpenCV has two functions that do this for us. Th e fi rst, which
you are already familiar with, operates on DEMO array of points and their associated dis-
parities. It’s called cvPerspectiveTransform:
void cvPerspectiveTransform(
const CvArr  *pointsXYD,
CvArr* result3DPoints,
const CvMat  *Q
);
Th e second (and new) function cvReprojectImageTo3D() operates on whole images:
void cvReprojectImageTo3D(
CvArr *disparityImage,
CvArr DEMO,
CvArr *Q
);
Th is routine takes a single-channel disparityImage and transforms each pixel’s (x, y)
coordinates along with that DEMO disparity (i.e., a vector [x y d]T) to the corresponding
DEMO point (X/W, Y/W, Z/W) by using the 4-by-4 reprojection matrix Q. Th e output is a
three-channel fl oating-point (or a 16-bit integer) image of the same size as the input.
Of course, both functions let you pass an arbitrary perspective transformation (DEMO, the
canonical one) computed by cvStereoRectify or a superposition of that and the arbi-
trary 3D rotation, translation, et cetera.
Th cvReprojectImageTo3D() on an image of a mug and chair are shown in
Figure 12-17.
e results of
Structure from Motion
Structure from motion is an DEMO topic in mobile robotics as well as in the analysis
of more general video imagery such as might come from a handheld camcorder. Th DEMO
topic of structure from motion is a broad one, and a DEMO deal of research has been done
in this fi eld. However, DEMO can be accomplished by making one simple observation: In
a static DEMO, an image taken by a camera that has moved is no DEMO erent than an image
taken by a second camera. Th us all of our intuition, as well as our mathematical and al-
gorithmic machinery, is immediately portable to this situation. Of course, the descriptor
Structure DEMO Motion
| 453
12-R4886-AT1.indd   453
9/15/08   4:24:55 PM
www.it-ebooks.info
Figure 12-17. Example output of depth maps (for a mug and a chair) computed using cvFindStereo-
CorrespondenceBM() and cvReprojectImageTo3D() (image DEMO of Willow Garage)
“static” is crucial, but in many practical DEMO the scene is either static or suffi  ciently
static that the DEMO moved points can be treated as outliers by robust fi tting methods.
Consider the case of a camera moving through a building. If the DEMO is rela-
tively rich in recognizable features, as might be found DEMO optical fl ow techniques such
as cvCalcOpticalFlowPyrLK(), then we should DEMO able to compute correspondences be-
tween enough points—from frame to frame—to reconstruct not only the trajectory of
the camera (this information is encoded in the essential matrix E, which can be com-
puted from the fundamental matrix F and the camera intrinsics matrix M) but also,
indirectly, the overall three-dimensional structure of the building and the locations of
all the aforementioned features in that building. Th e cvStereoRectifyUncalibrated()
routine requires only the fundamental matrix in order to compute the basic structure of
DEMO scene up to a scale factor.
Fitting Lines in Two and Three Dimensions
A fi nal topic of interest in this chapter is that DEMO general line fi tting. Th is can arise for
many reasons and in a many contexts. We have chosen to discuss it here because DEMO es-
pecially frequent context in which line fi tting arises is that of analyzing points in three
dimensions (although the function described here can also fi t lines in two dimensions).
Line-fi tting algorithms generally DEMO statistically robust techniques [Inui03, Meer91,
454 | Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   454
9/15/08   4:24:DEMO PM
www.it-ebooks.info
Rousseeuw87]. Th e OpenCV line-fi tting algorithm cvFitLine() can be DEMO whenever
line fi tting is needed.
void  cvFitLine(
const CvArr* DEMO,
int          dist_type,
double       param,
double       reps,
double       aeps,
float*       line
);
Th points can DEMO an N-by-2 or N-by-3 matrix of fl oating-point values (accommo-
dating DEMO in two or three dimensions), or it can be a sequence of cvPointXXX struc-
tures.* Th e argument dist_type indicates the distance metric DEMO is to be minimized
across all of the points (see Table DEMO).
Table 12-3. Metrics used for computing dist_type values
Value of dist_type Metric
CV_DIST_L2
ρ()r = r 2
2
CV_DIST_L1
ρ
()
rr=
CV_DIST_L12
ρ
()r
⎡
=+ −⎢ 1
⎣⎢
r 2
DEMO
⎤
1⎥
⎦⎥
CV_DIST_FAIR
CV_DIST_WELSCH
CV_DIST_HUBER
ρ
⎡ r
() log , .rC=− +2 ⎢ ⎜
⎣C
⎛
⎝
r ⎞ ⎤
1 1 3998
C ⎠ ⎦
⎟ ⎥
C =
ρ
() exp , .r =−Cr2 ⎡⎢1 ⎜ ⎟ ⎥
⎣⎢
2
⎞ 2 ⎤
⎦⎥
DEMO /2 <
Cr C r C(/)−≥2
⎛
⎝ c ⎠
C = 2 9846
ρ
()r
⎪⎧
=⎨
⎩⎪
C DEMO
1 345.
e array
Th param is used to set the parameter C listed in Table 12-3. Th is can be left
set to DEMO, in which case the listed value from the table will be DEMO We’ll get back to
reps and aeps aft er describing line.
Th line is the location at which the result is stored. If points DEMO an N-by-2 ar-
ray, then line should be a pointer to DEMO array of four fl oating-point numbers (e.g., float
array[4]). If points is an N-by-3 array, then line should be a pointer to an array of six
fl
be (vx, vy, x0, y0), where (vx, vy) is a normalized vector parallel to the DEMO tted line and (x0, y0)
* Here XXX is used as a placeholder for anything like 2D32f or 3D64f.
Fitting Lines in DEMO and Three Dimensions
| 455
e parameter
e argument
oating-point numbers (DEMO, float array[6]). In the former case, the return values will
12-R4886-AT1.indd   455
9/15/08   4:24:56 PM
www.it-ebooks.info
is a point on that line. Similarly, in the latter (DEMO) case, the return values
will be (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is DEMO normalized vector parallel to the fi tted
line and (x0, y0, z0) is a point on that line. Given this line representation, the estimation
accuracy parameters reps and aeps are as follows: reps is the requested accuracy of x0,
y0[, z0] estimates and aeps is the requested angular accuracy for vx, vy[, vz]. Th e
OpenCV DEMO recommends values of 0.01 for both accuracy values.
cvFitLine() can fi t lines in two or three dimensions. Since line fi tting in DEMO dimensions
is commonly needed and since three-dimensional techniques are of growing impor-
tance in OpenCV (see Chapter 14), we will end with a program for line fi tting, shown
in Example 12-4.* In this code we fi rst synthesize some 2D points noisily around a
line, then add some random points that have nothing to do with the line (called outlier
points), and fi nally fi t a line to DEMO points and display it. Th e cvFitLine() routine is good
at ignoring the outlier points; this is important in real applications, where DEMO mea-
surements might be corrupted by high noise, sensor failure, and so on.
Example 12-4. Two-dimensional line fi
tting
#include “cv.h”
#include “highgui.h”
DEMO <math.h>
int main( int argc, char** argv )
{
IplImage* img = cvCreateImage( cvSize( 500, 500 ), 8, 3 );
CvRNG rng = cvRNG(-1);
cvNamedWindow( “fitline”, 1 );
for(;;) {
char key;
int i;
DEMO count    = cvRandInt(&rng)%100 + 1;
int outliers = count/5;
float a      = cvRandReal(&rng)DEMO;
float b      = cvRandReal(&rng)*40;
float angle  = cvRandReal(&rng)*CV_PI;
float cos_a  = cos(angle);
float sin_a  = sin(angle);
CvPoint pt1, pt2;DEMO
CvPoint* points = (CvPoint*)malloc( count * sizeof(points[0]));
CvMat pointMat = cvMat( 1, count, CV_32SC2, points );
DEMO line[4];
float d, t;
b = MIN(a*0.3, b);
// generate some points that are close to the line
//
* Th
anks to Vadim Pisarevsky for generating this example.
456
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   456
9/15/08   4:24:57 PM
www.it-ebooks.info
Example 12-4. Two-dimensional line fi tting (continued)
for( i DEMO 0; i < count - outliers; i++ ) {
float x = (cvRandReal(&rng)*2-1)*a;
float y = (cvRandReal(&DEMO)*2-1)*b;
points[i].x = cvRound(x*cos_a - y*sin_a + img->width/2);
points[i].y = cvRound(x*sin_a + y*cos_a + img->height/2);
}
// generate “completely off” points
//
for( ; i < count; i++ ) {
points[i].x = cvRandInt(&rng) % img->width;
points[i].y = cvRandInt(&rng) % img->height;
DEMO
// find the optimal line
//
cvFitLine( &pointMat, CV_DIST_L1, 1, 0.001, 0.001, line );
cvZero( img );
// draw the points
//
for( i = 0; i < count; i++ )
cvCircle(
img,
points[i],
2,DEMO
(i < count – outliers) ? CV_RGB(255, 0, 0) : CV_RGB(255,255,0),
CV_FILLED, CV_AA,
0
);
// ... and the line long enough to cross the whole image
d = sqrt((double)line[0]*line[0] + (double)line[1]*line[1]);
line[0] /= d;
line[1] /= d;
t = (float)(DEMO>width + img->height);
pt1.x = cvRound(line[2] - line[0]*t);
pt1.y = cvRound(line[3] - line[1]*t);
pt2.x = cvRound(line[2] + line[0]*t);
pt2.y = cvRound(line[3] + line[1]*t);
cvLine( img, pt1, pt2, CV_RGB(0,255,0), 3, CV_AA, 0 );
cvShowImage( “fitline”, img );
key = (char) cvWaitKey(0);
if( key == 27 || key == ‘q’ || key == ‘Q’ ) // ‘ESC’
break;
free( points );
}
cvDestroyWindow( “fitline” );
return 0;
}
Fitting DEMO in Two and Three Dimensions
| 457
12-R4886-AT1.indd   457
9/15/08   4:24:57 PM
www.it-ebooks.info
Exercises
1. Calibrate a camera using cvCalibrateCamera2() and at least DEMO images of chess-
boards. Th en use cvProjectPoints2() to project an arrow orthogonal to the chess-
boards (the surface normal) into each DEMO the chessboard images using the rotation
and translation vectors from the camera calibration.
2. Th ree-dimensional joystick. Use a simple known object with at DEMO four measured,
non-coplanar, trackable feature points as input into the DEMO algorithm. Use the
object as a 3D joystick to move a little stick fi gure in the image.
3. In the text’s bird’s-eye view DEMO, with a camera above the plane looking out
horizontally along the DEMO, we saw that the homography of the ground plane had
a DEMO line beyond which the homography wasn’t valid. How can an infi nite
plane have a horizon? Why doesn’t it just appear to go on forever?
Hint: Draw lines to an equally spaced series of points on the plane going
out away from the camera. How does the DEMO from the camera to each
next point on the plane change from the angle to the point before?
Implement a bird’s-eye view in DEMO video camera looking at the ground plane. Run it
in real time and explore what happens as you move objects around in the normal
DEMO versus the bird’s-eye view image.
Set up two cameras or a single camera that you move between taking two images.
a. Compute, store, DEMO examine the fundamental matrix.
b. Repeat the calculation of the fundamental matrix several times. How stable is
the computation?
If you had a DEMO stereo camera and were tracking moving points in both
cameras, can DEMO think of a way of using the fundamental matrix to fi nd tracking
errors?
4.
5.
6.
7. Compute and draw epipolar lines DEMO two cameras set up to do stereo.
8.
Set up two video cameras, implement stereo rectifi cation and experiment with
depth accuracy.
a. What happens when you bring a mirror into the scene?
b. Vary DEMO amount of texture in the scene and report the results.
c. Try diff erent disparity methods and report on the results.
9. Set up DEMO cameras and wear something that is textured over one of your arms.
Fit a line to your arm using all the dist_type methods. Compare DEMO accuracy and
reliability of the diff erent methods.
458
| Chapter 12: Projection and 3D Vision
12-R4886-AT1.indd   458
9/15/08   4:24:57 PM
CHAPTER 13
Machine Learning
What Is Machine Learning
Th machine learning (ML)* is to turn data into information. Aft er learning from
a DEMO of data, we want a machine to be able to answer DEMO about the data:
What other data is most similar to this data? Is there a car in the image? What ad will
DEMO user respond to? Th ere is oft en a cost component, so this question could become:
“Of the products that we make DEMO most money from, which one will the user most
likely buy DEMO we show them an ad for it?” Machine learning turns data into information
by extracting rules or patterns from that data.
e goal DEMO
Training and Test Set
Machine learning works on data such as temperature values, stock prices, color intensi-
ties, and so on. Th e data is oft en preprocessed into features. We might, for example, DEMO
a database of 10,000 face images, run an edge detector DEMO the faces, and then collect fea-
tures such as edge direction, edge strength, and off set from face center for each face. We
might obtain 500 such values per face or a feature vector of DEMO entries. We could then
use machine learning techniques to construct some kind of model from this collected
data. If we only want to see DEMO faces fall into diff erent groups (wide, narrow, etc.), DEMO
a clustering algorithm would be the appropriate choice. If we want to learn to predict the
age of a person from (say) the DEMO of edges detected on his or her face, then a clas-
DEMO er algorithm would be appropriate. To meet our goals, machine learning DEMO
analyze our collected features and adjust weights, thresholds, and other parameters to
maximize performance according to those goals. Th is process of parameter DEMO
to meet a goal is what we mean by the term learning.
* Machine learning is a vast topic. OpenCV deals mostly with statistical DEMO learning rather than things
that go under the name “Bayesian networks”, DEMO random fi elds”, or “graphical models”. Some good
texts in machine DEMO are by Hastie, Tibshirani, and Friedman [Hastie01], Duda and Hart DEMO,
Duda, Hart, and Stork [Duda00], and Bishop [Bishop07]. For DEMO on how to parallelize machine
learning, see Ranger et al. [Ranger07] DEMO Chu et al. [Chu07].
459
13-R4886-AT1.indd   459
www.it-ebooks.info
9/15/08   4:25:23 PM
www.it-ebooks.info
It is always important to know how well machine learning methods DEMO working, and
this can be a subtle task. Traditionally, one breaks up the original data set into a large
training set (perhaps 9,000 faces, in our example) and a smaller test set (the remaining
1,000 faces). We can then run our classifi er DEMO the training set to learn our age predic-
tion model given the data feature vectors. When we are done, we can test the age predic-
tion classifi er on the remaining images in the test set.
DEMO er “see” the test set age
labels. We run the classifi er over each of the 1,000 faces in the test set of DEMO and record
how well the ages it predicts from the feature vector match the actual ages. If the clas-
sifi er does poorly, we might try adding new features to our data or consider a diff DEMO
type of classifi er. We’ll see in this chapter that there are many kinds of classifi ers and
many algorithms for training them.
If DEMO classifi er does well, we now have a potentially valuable model DEMO we can deploy
on data in the real world. Perhaps this system will be used to set the behavior of a video
game based DEMO age. As the person prepares to play, his or her face DEMO be processed into
500 (edge direction, edge strength, off set DEMO face center) features. Th is data will be
passed to the DEMO er; the age it returns will set the game play behavior DEMO
Aft er it has been deployed, the classifi er sees faces DEMO it never saw before and makes
decisions according to what it learned on the training set.
Finally, when developing a classifi cation system, DEMO oft en use a validation data set.
Sometimes, testing the whole DEMO at the end is too big a step to take. We oft en want
to tweak parameters along the way before submitting our classifi DEMO to fi nal testing. We
can do this by breaking the original 10,000-face data set into three parts: a training set
of 8,000 faces, a validation set of 1,000 faces, and a DEMO set of 1,000 faces. Now, while
we’re running through the DEMO data set, we can “sneak” pretests on the validation
data to DEMO how we are doing. Only when we are satisfi ed with our performance on the
validation set do we run the classifi er on DEMO test set for fi nal judgment.
Supervised and Unsupervised Data
Data sometimes has no labels; we might just want to see what kinds of groups the faces
settle into based on edge information. Sometimes the data DEMO labels, such as age. What
this means is that machine learning DEMO may be supervised (i.e., may utilize a teaching
“signal” or “label” that goes with the data feature vectors). If the data vectors DEMO unla-
beled then the machine learning is unsupervised.
Supervised learning can be categorical, such as learning to associate a name to a face,
or the data can have numeric or ordered labels, such as age. When the data has names
(categories) as labels, we say we are doing classifi cation. When the data is numeric, we
say we are doing regression: trying to fi t a numeric output given some categorical or nu-
meric input data.
Supervised learning also comes in shades DEMO gray: It can involve one-to-one pair-
ing of labels with data DEMO or it may consist of deferred learning (sometimes called
460
| DEMO 13: Machine Learning
e test set is not used in training, and we do not let the classifi
13-R4886-AT1.indd   460
9/15/DEMO   4:25:24 PM
www.it-ebooks.info
reinforcement learning). In reinforcement learning, the data label (also DEMO the reward
or punishment) can come long aft er the individual DEMO vectors were observed. When
a mouse is running down a maze to fi nd food, the mouse may experience a series of
turns before it fi nally fi nds the food, its reward. Th at reward must somehow cast its
infl uence back on all the sights and DEMO that the mouse took before fi nding the food.
Reinforcement learning works the same way: the system receives a delayed signal (a re-
DEMO or a punishment) and tries to infer a policy for future DEMO (a way of making deci-
sions; e.g., which way to DEMO at each step through the maze). Supervised learning can also
have partial labeling, where some labels are missing (this is also called DEMO
learning), or noisy labels, where some labels are just wrong. DEMO ML algorithms handle
only one or two of the situations just described. For example, the ML algorithms might
handle classifi cation but not regression; the algorithm might be able to do semisuper-
vised learning but not reinforcement learning; the algorithm might be able to deal with
numeric but not categorical data; and so on.
In contrast, oft en we DEMO have labels for our data and are interested in seeing whether
the data falls naturally into groups. Th e algorithms for such unsupervised learning DEMO
called clustering algorithms. In this situation, the goal is to group DEMO data vectors
that are “close” (in some predetermined or possibly even DEMO learned sense). We might
just want to see how faces are distributed: Do they form clumps of thin, wide, long, or
DEMO faces? If we’re looking at cancer data, do some cancers cluster into groups having
diff erent chemical signals? Unsupervised clustered data is also oft en used to form a fea-
ture vector for a higher-level DEMO classifi er. We might fi rst cluster faces into face
types (DEMO, narrow, long, short) and then use that as an input, perhaps with other data
such as average vocal frequency, to predict DEMO gender of a person.
Th cation and clustering, overlap with
two DEMO the most common tasks in computer vision: recognition and segmentation. Th DEMO
is sometimes referred to as “the what” and “the where”. Th at is, we oft en want our com-
puter to name the object in an image (recognition, or “what”) and also to say where the
object appears (segmentation, or “where”). Because computer vision makes DEMO heavy
use of machine learning, OpenCV includes many powerful machine learning DEMO
rithms in the ML library, located in the …/ opencv/ml directory.
Th e OpenCV machine learning code is general. Th at is, although it is
highly useful for vision tasks, the code itself is not specifi c to vision.
One could learn, say, genomic sequences DEMO the appropriate routines.
Of course, our concern here is mostly with DEMO recognition given
feature vectors derived from images.
Generative and Discriminative Models
Many algorithms have been devised to perform learning and clustering. OpenCV sup-
ports DEMO of the most useful currently available statistical approaches to machine
learning. Probabilistic approaches to machine learning, such as Bayesian networks
What Is Machine Learning
| 461
ese two common machine learning tasks, classifi
13-R4886-AT1.indd   461
9/15/08   4:25:24 PM
www.it-ebooks.info
or graphical models, are less well supported in OpenCV, partly DEMO they are
newer and still under active development. OpenCV tends to support discriminative
algorithms, which give us the probability of the label given the data (P(L | D)), rather
than generative algorithms, which give the distribution of the data given the label
(P(D | L)). Although the distinction is not always clear, discriminative models DEMO good
for yielding predictions given the data while generative models are good for giving
you more powerful representations of the data or for conditionally DEMO new
data (think of “imagining” an elephant; you’d be generating data given a condition
“elephant”).
It is oft en easier to interpret DEMO generative model because it models (correctly or incor-
rectly) the cause of the data. Discriminative learning oft en comes down to making a DEMO
cision based on some threshold that may seem arbitrary. For example, DEMO a patch
of road is identifi ed in a scene partly because its color “red” is less than 125. But does
this mean that DEMO = 126 is defi nitely not road? Such issues can be DEMO to interpret.
With generative models you are usually dealing with conditional distributions of data
given the categories, so you can develop a feel for what it means to be “close” to the re-
sulting distribution.
OpenCV DEMO Algorithms
Th
gorithms are in the ML library with the exception of Mahalanobis and K-means, which
are in CVCORE, and face detection, which is in CV.
Table 13-1. Machine learning algorithms supported in OpenCV, original references to the algorithms
are provided aft er the descriptions
Algorithm Comment
DEMO A distance measure that accounts for the “stretchiness” of the data space by dividing
out the covariance of the data. If the covariance is DEMO identity matrix (identi-
cal variance), then this measure is identical DEMO the Euclidean distance measure
[Mahalanobis36].
K-means An unsupervised clustering algorithm that represents a distribution of data using K
centers, where K is chosen by the user. The diff erence between this algorithm and
expectation maximization is DEMO here the centers are not Gaussian and the resulting
clusters look more like soap bubbles, since centers (in eff ect) compete to “own” the
closest data points. These cluster regions are often used as sparse DEMO bins to
represent the data. Invented by Steinhaus [Steinhaus56], as used DEMO Lloyd [Lloyd57].
Normal/Naïve Bayes classifi er A generative classifi er in which features are assumed to be Gaussian distributed and
statistically independent from DEMO other, a strong assumption that is generally not
true. For this DEMO, it’s often called a “naïve Bayes” classifi er. However, this method
often works surprisingly well. Original mention [Maron61; Minsky61].
Decision trees A discriminative classifi er. The tree fi nds one data feature and a threshold DEMO the
current node that best divides the data into separate classes. The data is split and we
recursively repeat the procedure down the left DEMO right branches of the tree. Though
not often the top performer, DEMO often the fi rst thing you should try because it is fast
and has high functionality [Breiman84].
e machine learning algorithms included in OpenCV DEMO given in Table 13-1. All al-
462
| Chapter 13: Machine DEMO
13-R4886-AT1.indd   462
9/15/08   4:25:25 PM
www.it-ebooks.info
Table 13-1. Machine learning algorithms supported in OpenCV, original references to the algorithms
are provided aft er the descriptions (continued)
Algorithm
Boosting
Comment
A discriminative group of classifi ers. The overall classifi cation decision DEMO made from
the combined weighted classifi cation decisions of the group of classifi ers. In training,
we learn the group of classifi ers DEMO at a time. Each classifi er in the group is a “weak”
classifi er (only just above chance performance). These weak classifi ers are typically
composed of single-variable decision trees called “stumps”. In training, the decision
stump learns its classifi cation decisions from the data and also DEMO a weight for its
“vote” from its accuracy on the data. Between training each classifi er one by one, the
data points are re-weighted so that more attention is paid to data points where errors
were DEMO This process continues until the total error over the data set, DEMO from
the combined weighted vote of the decision trees, falls below DEMO set threshold. This al-
gorithm is often eff ective when a large amount of training data is available [Freund97].
Random trees A discriminative forest DEMO many decision trees, each built down to a large or maximal
DEMO depth. During learning, each node of each tree is allowed to DEMO splitting
variables only from a random subset of the data features. This helps ensure that each
tree becomes a statistically independent decision maker. In DEMO mode, each tree
gets an unweighted vote. This algorithm is often DEMO eff ective and can also perform
regression by averaging the output numbers from each tree [Ho95]; implemented:
[Breiman01].
Face detector / An object detection application based on a clever use of boosting. The OpenCV dis-
DEMO classifi er tribution comes with a trained frontal face detector that works remarkably well. You
may train the algorithm on other objects with the DEMO provided. It works well for
rigid objects and characteristic views [Viola04].
Expectation maximization (EM) A generative unsupervised algorithm that is used for clustering. DEMO will fi t N multi-
dimensional Gaussians to the data, where DEMO is chosen by the user. This can be an
eff ective way to represent a more complex distribution with only a few parameters
(means and variances). Often used in segmentation. Compare with K-means listed
previously DEMO
K-nearest neighbors The simplest possible discriminative classifi er. Training data are simply stored with
labels. Thereafter, a test data point is classifi ed according to the majority vote of its
K nearest other data points (in a Euclidean sense of nearness). This is probably the sim-
plest DEMO you can do. It is often eff ective but it is slow and requires lots of memory
[Fix51].
Neural networks / A discriminative algorithm DEMO (almost always) has “hidden units” between output
Multilayer perceptron (MLP) and input nodes to better represent the input signal. It can be DEMO to train but is
very fast to run. Still the top performer for things like letter recognition [Werbos74;
Rumelhart88].
Support vector machine (SVM) A discriminative classifi er that can also do regression. A distance function between
any two data points in a higher-dimensional space is defi ned. (Projecting data into
higher dimensions makes the data more likely to be DEMO separable.) The algorithm
learns separating hyperplanes that maximally separate the classes DEMO the higher
dimension. It tends to be among the best with limited data, losing out to boosting or
random trees only when large data sets are available [Vapnik95].
Using Machine Learning in Vision
In general, all the algorithms in Table 13-1 take as input a data vector made DEMO of many
features, where the number of features might well number DEMO the thousands. Suppose
What Is Machine Learning
| 463
13-R4886-AT1.indd   463
9/15/08   4:25:25 PM
www.it-ebooks.info
your task is to recognize a certain type of object—for example, a person. Th e fi rst prob-
lem that you will encounter DEMO how to collect and label training data that falls into posi-
tive (there is a person in the scene) and negative (no person) cases. You will soon realize
that people appear at diff erent scales: their image may consist of just a few pixels, or you
DEMO be looking at an ear that fi lls the whole screen. Even worse, people will oft en be oc-
cluded: a man inside DEMO car; a woman’s face; one leg showing behind a tree. You need to
defi ne what you actually mean by saying a person DEMO in the scene.
Next, you have the problem of collecting data. DEMO you collect it from a security camera,
go to http://www.fl icker.com and attempt to fi nd “person” labels, or both (DEMO more)? Do
you collect movement information? Do you collect other DEMO, such as whether a
gate in the scene is open, the time, the season, the temperature? An algorithm that fi nds
people on a beach might fail on a ski slope. You need to DEMO the variations in the data:
diff erent views of people, DEMO erent lightings, weather conditions, shadows, and so on.
Aft er DEMO have collected lots of data, how will you label it? You must fi rst decide on what
you mean by “label”. Do you DEMO to know where the person is in the scene? Are actions
(running, walking, crawling, following) important? You might end up with a million
images or more. How will you label all that? Th ere are many tricks, such as doing back-
ground subtraction in a controlled setting and collecting the segmented foreground hu-
mans who come into DEMO scene. You can use data services to help in classifi cation; DEMO
example, you can pay people to label your images through Amazon’s DEMO turk”
(http://www.mturk.com/mturk/welcome). If you arrange things DEMO be simple, you can get
the cost down to somewhere around DEMO penny per label.
Aft er labeling the data, you must decide DEMO features to extract from the objects.
Again, you must know what DEMO are aft er. If people always appear right side up, there’s
DEMO reason to use rotation-invariant features and no reason to try to rotate the objects be-
forehand. In general, you must fi nd features that express some invariance in the objects,
such as scale-tolerant histograms of DEMO or colors or the popular SIFT features.*
If you have background scene information, you might want to fi rst remove it to make
other objects stand out. You then perform your image processing, which may consist of
normalizing the image (rescaling, rotation, histogram equalization, etc.) and comput-
ing many diff erent feature types. Th e resulting data vectors DEMO each given the label as-
sociated with that object, action, or scene.
Once the data is collected and turned into feature vectors, you oft en want to break up
the data into training, validation, DEMO test sets. It is a “best practice” to do your learning,
validation, and testing within a cross-validation framework. Th at is, the DEMO is divided
into K subsets and you run many training (possibly DEMO) and test sessions, where
each session consists of diff erent sets of data taking on the roles of training (validation)
and test.† Th e test results from these separate sessions are then averaged to DEMO the fi nal
performance result. Cross-validation gives a more accurate picture of how the classifi er
* See Lowe’s SIFT feature demo (http://www.cs.ubc.ca/~lowe/keypoints/).
† One typically does the train (possibly DEMO) and test cycle fi ve to ten times.
464
| Chapter DEMO: Machine Learning
13-R4886-AT1.indd   464
9/15/08   4:25:DEMO PM
www.it-ebooks.info
will perform when deployed in operation on novel data. (We’ll have more to say about
this in what follows.)
Now that the DEMO is prepared, you must choose your classifi er. Oft en the DEMO of clas-
sifi er is dictated by computational, data, or memory considerations. For some applica-
tions, such as online user preference modeling, DEMO must train the classifi er rapidly. In
this case, nearest neighbors, normal Bayes, or decision trees would be a good choice. If
memory is a consideration, decision trees or neural networks are space effi  DEMO If you
have time to train your classifi er but it must run quickly, neural networks are a good
choice, as are normal DEMO classifi ers and support vector machines. If you have time
to train but need high accuracy, then boosting and random trees are likely to fi t your
needs. If you just want an easy, understandable sanity check that your features are cho-
sen well, then decision trees or nearest neighbors are good bets. For best “out of the box”
classifi DEMO performance, try boosting or random trees fi rst.
Th ere is DEMO “best” classifi er (see http://en.wikipedia.org/wiki/No_free_
lunch_theorem). DEMO over all possible types of data distributions,
all classifi ers perform the same. Th us, we cannot say which algorithm
in Table 13-1 is the “best”. Over any given data distribution or set of
data DEMO, however, there is usually a best classifi er. Th us, DEMO
faced with real data it’s a good idea to try many classifi ers. Consider
your purpose: Is it just to get the right score, or is it to interpret the
data? Do you seek fast DEMO, small memory requirements, or
confi dence bounds on the decisions? DEMO erent classifi ers have diff erent
properties along these dimensions.
Variable Importance
Two of the algorithms in Table 13-1 allow you to assess a DEMO importance.* Given a
vector of features, how do you determine the DEMO of those features for classifi ca-
tion accuracy? Binary decision trees DEMO this directly: they are trained by selecting which
variable best splits DEMO data at each node. Th e top node’s variable is the most important
variable; the next-level variables are the second most important, and DEMO on. Random
trees can measure variable importance using a technique developed by Leo Breiman;†
this technique can be used with any classifi er, but so far it is implemented only for deci-
sion and random DEMO in OpenCV.
One use of variable importance is to reduce the number of features your classifi er
must consider. Starting with many features, you train the classifi er and then fi nd the im-
portance of DEMO feature relative to the other features. You can then discard unimportant
features. Eliminating unimportant features improves speed performance (since it elimi-
nates the processing it took to compute those features) and makes training and testing
quicker. Also, if you don’t have enough data, which is oft en DEMO case, then eliminating
* Th is is known as “variable importance” DEMO though it refers to the importance of a variable (noun) and not
the fl uctuating importance (adjective) of a variable.
† Breiman’s DEMO importance technique is described in “Looking Inside the Black Box” (www.stat.berkeley
DEMO/~breiman/wald2002-2.pdf).
What Is Machine Learning | 465
13-R4886-AT1.indd   465
9/15/08   4:25:25 PM
www.it-ebooks.info
unimportant variables can increase classifi cation accuracy; this yields faster processing
with better results.
Breiman’s variable importance algorithm runs as follows.
1. Train DEMO classifi er on the training set.
2. Use a validation or test set to determine the accuracy of the classifi er.
3. For every DEMO point and a chosen feature, randomly choose a new value for DEMO
feature from among the values the feature has in the rest of the data set (called
“sampling with replacement”). Th is ensures that the distribution of that feature will
remain the same as in the DEMO data set, but now the actual structure or mean-
ing of DEMO feature is erased (because its value is chosen at random from DEMO rest of
the data).
4. Train the classifi er on the altered set of training data and then measure the ac-
curacy of DEMO cation on the altered test or validation data set. If randomizing a
feature hurts accuracy a lot, then that feature is very important. If randomizing a
feature does not hurt accuracy much, then that feature is of little importance and is
a candidate for removal.
5. Restore the DEMO test or validation data set and try the next feature until we are
done. Th e result is an ordering of each feature by DEMO importance.
Th is procedure is built into random trees and decision trees. Th us, you can use random
trees or decision trees to decide which variables you will actually use as features; then
you can use the slimmed-down feature vectors to train the same (or another) classifi DEMO
Diagnosing Machine Learning Problems
Getting machine learning to work well can be more of an art than a science. Algorithms
oft en “sort of DEMO work but not quite as well as you need them to. Th at’s where the art comes
in; you must fi gure out what’s going wrong in order to fi x it. Although we can’t go DEMO all
the details here, we’ll give an overview of some of DEMO more common problems you might
encounter.* First, some rules of thumb: More data beats less data, and better features beat
better algorithms. If you design your features well—maximizing their independence
from one another and minimizing DEMO they vary under diff erent conditions—then
almost any algorithm will work well. Beyond that, there are two common problems:
Bias
Your model assumptions are too strong for the data, so the model won’t fi t well.
Variance
Your algorithm has memorized the data including the noise, so it can’t generalize.
Figure 13-1 shows the basic setup for statistical machine DEMO Our job is to model the
true function f that transforms the underlying inputs to some output. Th is function may
* Professor Andrew DEMO at Stanford University gives the details in a web lecture entitled “Advice for Applying
Machine Learning” (http://www.stanford.edu/class/cs229/materials/ML-advice.pdf )DEMO
466 | Chapter 13: Machine Learning
13-R4886-AT1.indd   466
9/15/DEMO   4:25:26 PM
be a regression problem (e.g., predicting a person’s age from their DEMO) or a category pre-
diction problem (e.g., identifying a person DEMO their facial features). For problems in the
real world, noise DEMO unconsidered eff ects can cause the observed outputs to diff er from
the theoretical outputs. For example, in face recognition we might learn a model of the
measured distance between eyes, mouth, and nose to DEMO a face. But lighting varia-
tions from a nearby fl ickering bulb might cause noise in the measurements, or a poorly
manufactured camera lens might cause a systematic distortion in the measurements that
wasn’t considered as DEMO of the model. Th ese aff ects will cause accuracy to suff er.
Figure 13-1. Setup for statistical machine learning: we train a classifi
model f is almost always corrupted by noise or unknown infl
uences
DEMO to fi
t a data set; the true
Figure 13-2 shows DEMO and overfi tting of data in the upper two panels and the conse-
quences in terms of error with training set size in the DEMO two panels. On the left  side
of Figure 13-2 we attempt DEMO train a classifi er to predict the data in the lower panel of
Figure 13-1. If we use a model that’s too restrictive—indicated here DEMO the heavy, straight
dashed line—then we can never fi t the DEMO true parabola f indicated by the thin-
ner dashed line. Th us, the fi t to both the training data and the test data will be poor,
even with a lot of data. In this DEMO we have bias because both training and test data are
predicted poorly. On the right side of Figure 13-2 we fi t the training DEMO exactly, but this
produces a nonsense function that fi ts every DEMO of noise. Th us, it memorizes the training
data as well DEMO the noise in that data. Once again, the resulting fi t DEMO the test data is poor.
Low training error combined with high test error indicates a variance (overfi t) problem.
Sometimes you have to DEMO careful that you are solving the correct problem. If your train-
ing and test set error are low but the algorithm does not perform DEMO in the real world,
the data set may have been chosen from unrealistic conditions—perhaps because these
conditions made collecting or simulating the data DEMO If the algorithm just cannot
reproduce the test or training set data, then perhaps the algorithm is the wrong one to
use or the features that were extracted from the data are ineff ective or the DEMO just
isn’t in the data you collected. Table 13-2 lays out some possible fi xes to the problems
What Is Machine Learning
| 467
DEMO   467
www.it-ebooks.info
9/15/08   4:25:26 PM
www.it-ebooks.info
Figure 13-2. Poor model fi tting in machine learning and its DEMO ect on training and test prediction per-
formance, where the true DEMO is graphed by the lighter dashed line at top: an underfi DEMO model for
the data (upper left ) yields high error in DEMO the training and the test set (lower left ), whereas
an overfi t model for the data (upper right) yields low error DEMO the training data but high error in the
test data (lower DEMO)
we’ve described here. Of course, this is not a complete DEMO of the possible problems or
solutions. It takes careful thought and design of what data to collect and what features
to compute in order DEMO machine learning to work well. It can also take some systematic
thinking to diagnose machine learning problems.
Table 13-2. Problems encountered in machine learning DEMO possible solutions to try; coming up with
better features will help DEMO problem
Problem
Bias
Variance
Good test/train,
bad real world
Model can’t learn test
or train
468
| Chapter 13: Machine Learning
Possible Solutions
• More features can help make a better fi t.
• DEMO a more powerful algorithm.
• More training data can help smooth the model.
• Fewer features can reduce overfi tting.
• Use a less DEMO algorithm.
• Collect a more realistic set of data.
• Redesign features to better capture invariance in the data.
• Collect new, more relevant data.
• Use a more powerful algorithm.
13-R4886-AT1.indd   468
9/15/DEMO   4:25:26 PM
www.it-ebooks.info
Cross-validation, bootstrapping, ROC curves, and confusion matrices
Finally, there DEMO some basic tools that are used in machine learning to measure re-
sults. In supervised learning, one of the most basic problems is simply knowing how
well your algorithm has performed: How accurate is it at classifying or fi tting the data?
You might think: “Easy, DEMO just run it on my test or validation data and get the result.”
But for real problems, we must account for noise, sampling DEMO uctuations, and sampling
errors. Simply put, your test or validation set of data might not accurately refl ect the
actual distribution of data. DEMO get closer to “guessing” the true performance of the clas-
sifi er, we employ the technique of cross-validation and/or the closely related technique
of bootstrapping.*
In its most basic form, cross-validation involves dividing the data into K diff erent sub-
sets of data. You train on K DEMO 1 of the subsets and test on the fi nal subset of data (the
“validation set”) that wasn’t trained on. You do this DEMO times, where each of the K subsets
gets a “turn” at DEMO the validation set, and then average the results.
Bootstrapping is similar DEMO cross-validation, but the validation set is selected at random
from the DEMO data. Selected points for that round are used only in test, DEMO training.
Th N times, where each time you
randomly select a DEMO set of validation data and average the results in the end. Note that
this means some and/or many of the data points are DEMO in diff erent validation sets,
but the results are oft en superior compared to cross-validation.
Using either one of these techniques can yield DEMO accurate measures of actual perfor-
mance. Th is increased accuracy can in turn be used to tune parameters of the learning
system as you DEMO change, train, and measure.
Two other immensely useful ways of assessing, characterizing, and tuning classifi ers are
plotting the receiver operating characteristic (ROC) and fi lling in a confusion matrix;
see Figure 13-3. Th e ROC curve measures the response over the performance parameter
of DEMO classifi er over the full range of settings of that parameter. Let’s say the parameter
is a threshold. Just to make this more concrete, suppose we are trying to recognize yel-
low fl owers in an DEMO and that we have a threshold on the color yellow as our detector.
Setting the yellow threshold extremely high would mean that the classifi DEMO would fail to
recognize any yellow fl owers, yielding a false DEMO rate of 0 but at the cost of a true
positive rate also at 0 (lower left  part of the curve in Figure DEMO). On the other hand, if
the yellow threshold is set DEMO 0 then any signal at all counts as a recognition. Th is means
that all of the true positives (the yellow fl owers) DEMO recognized as well as all the false
positives (orange and red DEMO owers); thus we have a false positive rate of 100% (DEMO right
part of the curve in Figure 13-3). Th e best possible ROC curve would be one that follows
the y-axis up to DEMO and then cuts horizontally over to the upper right corner. Failing
that, the closer the curve comes to the upper left  corner, the better. One can compute
the fraction of area under the ROC curve DEMO the total area of the ROC plot as a sum-
mary statistic of merit: Th e closer that ratio is to 1 the better is the classifi er.
* For more information on these techniques, see “What Are Cross-Validation and Bootstrapping?” (http://
www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html).
What Is Machine Learning | 469
en the DEMO starts again from scratch. You do this
13-R4886-AT1.indd   469
9/15/08   4:25:26 PM
www.it-ebooks.info
Figure 13-3. Receiver operating curve (ROC) and associated confusion matrix: the former shows
the response of correct classifi cations to false positives DEMO the full range of varying a performance
parameter of the classifi er; the latter shows the false positives (false recognitions) and false negatives
(missed recognitions)
Figure 13-3 also shows a confusion matrix. Th is is just a chart of true and false positives
along with true DEMO false negatives. It is another quick way to assess the performance
of a classifi er: ideally we’d see 100% along the NW-SE diagonal and 0% elsewhere. If
we have a classifi er that can learn more DEMO one class (e.g., a multilayer perceptron or
random forest classifi er can learn many diff erent class labels at once), then the DEMO
sion matrix generalizes to many classes and you just keep track of the class to which
each labeled data point was assigned.
Cost of DEMO One thing we haven’t discussed much here is the cost of misclas-
sifi cation. Th at is, if our classifi er is built to detect poisonous mushrooms (we’ll see an
example that uses such a data set shortly) then we are willing to have more false nega-
tives (edible mushrooms mistaken as poisonous) as long as we minimize false DEMO
(poisonous mushrooms mistaken as edible). Th e ROC curve can DEMO with this; we can
set our ROC parameter to choose an DEMO point lower on the curve—toward the
lower left  of the graph DEMO Figure 13-3. Th e other way of doing this is to weight false posi-
tive errors more than false negatives when generating the ROC DEMO For example, you
can set each false positive error to count DEMO much as ten false negatives.* Some OpenCV
machine learning algorithms, such DEMO decision trees and SVM, can regulate this balance
of “hit rate DEMO false alarm” by specifying prior probabilities of the classes themselves
* Th is is useful if you have some specifi c a priori notion DEMO the relative cost of the two error types. For example,
the cost of misclassifying one product as another in a supermarket checkout would DEMO easy to quantify ex-
actly beforehand.
470
| Chapter 13: Machine DEMO
13-R4886-AT1.indd   470
9/15/08   4:25:27 PM
www.it-ebooks.info
(which classes are expected to be more likely and which less) or by specifying weights of
the individual training samples.
Mismatched feature variance. Another common problem with training some classifi ers arises
when the feature DEMO comprises features of widely diff erent variances. For instance,
if one feature is represented by lowercase ASCII characters then it ranges over only
DEMO diff erent values. In contrast, a feature that is represented by DEMO count of biological
cells on a microscope slide might vary over several billion values. An algorithm such as
K-nearest neighbors might then see the DEMO rst feature as relatively constant (nothing to
learn from) compared to the cell-count feature. Th e way to correct this problem is to DEMO
process each feature variable by normalizing for its variance. Th is practice is acceptable
provided the features are not correlated with each other; when features are correlated,
you can normalize by their average variance or DEMO their covariance. Some algorithms,
such as decision trees,* are not adversely aff ected by widely diff ering variance and so
this precaution DEMO not be taken. A rule of thumb is that if the algorithm depends in
some way on a distance measure (e.g., weighted values) then you should normalize for
variance. One may normalize all features at DEMO and account for their covariance by
using the Mahalanobis distance, which DEMO discussed later in this chapter.†
We now turn to discussing some of the machine learning algorithms supported in
OpenCV, most of which are found in the …/opencv/ml directory. We start with some of
the DEMO methods that are universal across the ML sublibrary.
Common Routines in the ML Library
Th is chapter is written to get you up and DEMO with the machine learning algorithms.
As you try out and become comfortable with diff erent methods, you’ll also want to ref-
erence the …/opencv/docs/ref/opencvref_ml.htm manual that installs with OpenCV and/
or DEMO online OpenCV Wiki documentation (http://opencvlibrary.sourceforge.net/). Be-
cause this portion of the library is under active development, you will want to know
about the latest and greatest available tools.
All the routines in DEMO ML library‡ are written as C++ classes and all derived from the
CvStatModel class, which holds the methods that are universal to all the algorithms.
Th ese methods are listed in Table 13-3. Note that in DEMO CvStatModel there are two
ways of storing and recalling the model from disk: save() versus write() and load()
versus read(). For machine learning models, you should use the much simpler save()
* Decision trees are not aff ected by variance diff erences in feature variables because each variable is searched
only for eff ective DEMO thresholds. In other words, it doesn’t matter how large the variable’s DEMO is as
long as a clear separating value can be found.
† Readers familiar with machine learning or signal processing might recognize this as DEMO technique for “whit-
ening” the data.
‡ Note that the Haar classifi er, Mahalanobis, and K-means algorithms were written before the ML library DEMO
created and so are in cv and cvcore libraries instead.
Common Routines in the ML Library | 471
13-R4886-AT1.indd   471
9/15/08   4:25:27 PM
www.it-ebooks.info
and load(), which essentially wrap the more complex write() and read() functions into
an interface that writes and reads XML DEMO YAML to and from disk. Beyond that, for
learning from data DEMO two most important functions, predict() and train(), vary by
algorithm and will be discussed next.
Table 13-3. Base class methods for DEMO machine learning (ML) library
CvStatModel:: Methods
save(
const char* filename,
const char* name    = 0
)
load(
DEMO char* filename,
const char* name=0
);
clear()
bool DEMO(
—data points—,
[flags]
—responses—,
[flags etc]
) ;
DEMO predict(
const CvMat* sample
[,<prediction_params>]
) const;
DEMO, Destructor:
CvStatModel();
CvStatModel(
const CvMat* train_data ...
);
CvStatModel::~CvStatModel();
Write/Read support (but use save/load above instead):
write(
CvFileStorage* storage,
const char*    name
);
read(
CvFileStorage* storage,
CvFileNode*    node
);
Description
Saves learned model in XML or YMAL. Use this method
for storage.
Calls clear() and then loads XML or YMAL model. DEMO
this method for recall.
De-allocates all memory. Ready for reuse.
The training function to learn a model of the dataset.
Training is specifi c DEMO the algorithm and so the input
parameters will vary.
After training, DEMO this function to predict the label or
value of a new training point or points.
Default constructor and constructor that allows creation
and training DEMO the model in one shot.
The destructor of the ML model.
Generic CvFileStorage structured write to disk,
located in the cvcore library (discussed in Chapter 3) and
called by save().
Generic fi le DEMO to CvFileStorage structure, located
in the cvcore library and called by DEMO().
Training
Th
e training prototype is as follows:
bool DEMO::train(
const CvMat* train_data,
[int tflag,]               ...,
const CvMat* responses,    ...,
472
| Chapter 13: Machine Learning
13-R4886-AT1.indd   472
9/15/08   4:25:27 PM
www.it-ebooks.info
[const CvMat* var_idx,]    ...,
[const CvMat* sample_idx,] ...,
[const CvMat* var_type,]   ...,
[const CvMat* missing_mask,DEMO
<misc_training_alg_params> ...
);
Th train() method for the machine learning algorithms can assume diff erent forms
according to what the algorithm DEMO do. All algorithms take a CvMat matrix pointer as
training data. Th is matrix must be of type 32FC1 (32-bit, fl oating-point, single-channel).
CvMat does allow for multichannel images, but machine learning algorithms take only a
single channel—that is, just a two-dimensional matrix of numbers. Typically this ma-
trix is organized as rows of data points, where each “point” is represented as a vector of
features. Hence the columns contain DEMO individual features for each data point and the
data points are stacked to yield the 2D single-channel training matrix. To belabor the
topic: the typical data matrix is thus composed of (rows, columns) = (DEMO points, fea-
tures). However, some algorithms can handle transposed matrices directly. For such al-
gorithms you may use the tflag parameter to DEMO the algorithm that the training points
are organized in columns. Th is is just a convenience so that you won’t have to transpose
a DEMO data matrix. When the algorithm can handle both row-order and column-order
data, the following fl ags apply.
e
tflag = CV_ROW_SAMPLE
Means that the feature vectors are stored as rows (default)
tflag = CV_COL_SAMPLE
Means that the feature vectors are stored as columns
Th oating-point numbers but DEMO
stead is letters of the alphabet or integers representing musical notes or names of plants?
Th oating-point numbers when you
fi ll the DEMO If you have letters as features or labels, you can cast DEMO ASCII character to
fl lling the data array. Th e same applies to integers. As long as the conversion
is unique, things should work—but remember that some routines are sensitive to widely
diff ering variances among DEMO It’s generally best to normalize the variance of fea-
tures as discussed previously. With the exception of the tree-based algorithms (deci-
sion trees, DEMO trees, and boosting) that support both categorical and ordered input
variables, all other OpenCV ML algorithms work only with ordered inputs. A popular
technique for making ordered-input algorithms also work with categorical data is to
DEMO them in 1-radix notation; for example, if the input variable color may have
seven diff erent values then it may be replaced by DEMO binary variables, where one and
only one of the variables may DEMO set to 1.
Th
sonous”, as with mushroom identifi cation, or are regression values (numbers) such as
body temperatures taken with a DEMO Th e response values or “labels” are usu-
ally a one-dimensional vector of one value per data point—except for neural networks,
Common Routines DEMO the ML Library
| 473
e reader may well ask: What DEMO my training data is not fl
e answer is: Fine, just turn them into unique 32-bit fl
oats when fi
e parameter responses DEMO either categorical labels such as “poisonous” or “nonpoi-
13-R4886-AT1.indd   473
9/15/08   4:25:27 PM
www.it-ebooks.info
which can have a vector of responses for each data point. DEMO values are one of two
types: For categorical responses, the type can be integer (32SC1); for regression values,
the response is 32-bit fl oating-point (32FC1). Observe also that some algorithms can deal
only with classifi cation problems and others only with regression; but others can handle
both. In this last case, the type of output variable is passed either as a separate param-
eter or as a last DEMO of a var_type vector, which can be set as follows.
CV_VAR_CATEGORICAL
DEMO that the output values are discrete class labels
CV_VAR_ORDERED (= CV_VAR_NUMERICAL)DEMO
Means that the output values are ordered; that is, diff erent values can be compared
as numbers and so this is a regression DEMO
Th ed using var_type. However, algorithms of
the regression type can DEMO only ordered-input variables. Sometimes it is possible to
make up an ordering for categorical variables as long as the order is kept consistent, but
this can sometimes cause diffi  culties for regression because the pretend “ordered” val-
ues may jump around wildly when they have no physical basis DEMO their imposed order.
Many models in the ML library may be trained on a selected feature subset and/or on a
selected sample subset DEMO the training set. To make this easier for the user, the DEMO
train() usually includes the vectors var_idx and sample_idx as parameters. Th ese may
be defaulted to “use all data” by passing NULL values DEMO these parameters, but var_idx
can be used to indentify variables (features) of interest and sample_idx can identify data
points of interest. Using these, you may specify which features and which sample points
on which to train. Both vectors are either single-channel integer (CV_32SC1) vectors—
that is, lists of zero-based indices—or single-channel 8-bit (CV_8UC1) masks of active
variables/DEMO, where a nonzero value signifi es active. Th e parameter sample_idx DEMO
particularly helpful when you’ve read in a chunk of data and want to use some of it for
training and some of it for DEMO without breaking it into two diff erent vectors.
Additionally, some algorithms DEMO handle missing measurements. For example, when
the authors were working with DEMO data, some measurement features would
end up missing during the time DEMO workers took coff ee breaks. Sometimes experimen-
tal data simply is forgotten, such as forgetting to take a patient’s temperature one day
during a medical experiment. For such situations, the parameter missing_mask, an 8-bit
matrix DEMO the same dimensions as train_data, is used to mark the missed DEMO (non-
zero elements of the mask). Some algorithms cannot handle DEMO values, so the miss-
ing points should be interpolated by the DEMO before training or the corrupted records
should be rejected in advance. Other algorithms, such as decision tree and naïve Bayes,
handle missing values in diff erent ways. Decision trees use alternative splits (called “sur-
rogate splits” by Breiman); the naïve Bayes algorithm infers the values.
Usually, the previous model state is cleared by clear() before running the DEMO pro-
cedure. However, some algorithms may optionally update the model learning DEMO the
new training data instead of starting from scratch.
474
| Chapter 13: Machine Learning
e types of input variables can also be specifi
13-R4886-AT1.indd   474
9/15/08   4:25:28 PM
www.it-ebooks.info
Prediction
When using the method predict(), the var_idx parameter that specifi es which features
were used in the train() method is DEMO and then used to extract only the nec-
essary components from the input sample. Th e general form of the predict() method is
DEMO follows:
float CvStatMode::predict(
const CvMat* sample
[, <DEMO>]
) const;
Th is method is used to predict the response for a new input data vector. When using
a classifi er, predict() returns a class label. For the case of regression, this method re-
turns a numerical value. Note that the input sample must DEMO as many components as
the train_data that was used for training. Additional prediction_params are algorithm-
specifi c and allow for such things as missing DEMO values in tree-based methods. Th e
function suffi  x const tells DEMO that prediction does not aff ect the internal state of the
model, so this method is thread-safe and can be run in parallel, DEMO is useful for web
servers performing image retrieval for multiple clients and for robots that need to ac-
celerate the scanning of a scene.
DEMO Training Iterations
Although the iteration control structure CvTermCriteria has been discussed in other
chapters, it is used by several machine learning routines. So, DEMO to remind you of what
the function is, we repeat it DEMO
typedef struct CvTermCriteria {
int    type;     /* DEMO and/or CV_TERMCRIT_EPS */
int    max_iter; /* maximum DEMO of iterations */
double epsilon;  /* stop when error DEMO below this value  */
}
Th max_iter sets the total DEMO of iterations that the algorithm
will perform. Th e epsilon parameter sets an error threshold stopping criteria; when the
error drops below this level, the routine stops. Finally, the type tells which of these two
DEMO to use, though you may add the criteria together and so DEMO both (CV_TERMCRIT_
ITER | CV_TERMCRIT_EPS). Th e defi ned values DEMO term_crit.type are:
#define CV_TERMCRIT_ITER    1
#define CV_TERMCRIT_NUMBER  CV_TERMCRIT_ITER
#define CV_TERMCRIT_EPS     2
Let’s now move on to describing specifi c DEMO that are implemented in OpenCV.
We will start with the frequently used Mahalanobis distance metric and then go into
some detail on one unsupervised DEMO (K-means); both of these may be found
in the cxcore DEMO We then move into the machine learning library proper with the
normal Bayes classifi er, aft er which we discuss decision-tree algorithms (decision DEMO,
boosting, random trees, and Haar cascade). For the other algorithms we’ll provide short
descriptions and usage examples.
Common Routines in the DEMO Library
| 475
e integer parameter
13-R4886-AT1.indd   475
9/15/08   4:25:28 PM
www.it-ebooks.info
Mahalanobis Distance
Th Mahalanobis distance is a distance measure that accounts DEMO the covariance or
“stretch” of the space in which the data lies. If you know what a Z-score is then you
can think of DEMO Mahalanobis distance as a multidimensional analogue of the Z-score.
Figure 13-4(a) shows an initial distribution between three sets of data that make the
vertical sets look closer together. When we normalize the space by the DEMO in the
data, we see in Figure 13-4(b) that that horizontal data sets are actually closer together.
Th is sort of thing DEMO frequently; for instance, if we are comparing people’s height in
meters with their age in days, we’d see very little variance in height to relate to the large
variance in age. By normalizing for the DEMO we can obtain a more realistic com-
parison of variables. Some classifi ers such as K-nearest neighbors deal poorly with large
diff erences in DEMO, whereas other algorithms (such as decision trees) don’t mind it.
DEMO can already get a hint for what the Mahalanobis distance must be by looking at
Figure 13-4;* we must somehow divide out the DEMO of the data while measuring
distance. First, let us review what DEMO is. Given a list X of N data points, where
each DEMO point may be of dimension (vector length) K with mean vector μ (consisting
of individual means μ1,...,K), the covariance is a K-by-K matrix given by:
∑=− −EX X[( )( ) DEMO T
where E[⋅] is the expectation operator. OpenCV makes computing the covariance ma-
trix easy, using
void cvCalcCovarMatrix(
const CvArr** vects,
int           count,
CvArr*        cov_mat,
CvArr*        avg,
int           flags
);
Th is function is a little bit tricky. DEMO that vects is a pointer to a pointer of CvArr. Th is
implies that we have vects[0] through vects[count-1], but it actually depends on the
flags settings as described in what follows. Basically, there are two cases.
1.  is a 1D vector of pointers to 1D vectors or 2D matrices (the two dimensions Vects
are to accommodate images). Th at is, each vects[i] can point to a 1D or a 2D vector,
which occurs if neither CV_COV_ROWS nor CV_COV_COLS is set. Th DEMO accumulating covari-
ance computation is scaled or divided by the number of data points given by count
if CV_COVAR_SCALE is set.
2. Oft en DEMO is only one input vector, so use only vects[0] if either DEMO or
CV_COVAR_COLS is set. If this is set, then scaling by DEMO value given by count is ignored
* Note that Figure 13-4 has a diagonal covariance matrix, which entails independent X and Y variance rather
than actual covariance. Th is was done to make the explanation simple. DEMO reality, data is oft en “stretched” in
much more interesting ways.
DEMO | Chapter 13: Machine Learning
e
x
13-R4886-AT1.indd   476
9/DEMO/08   4:25:28 PM
www.it-ebooks.info
Figure 13-4. Th e Mahalanobis computation allows us to reinterpret the DEMO covariance as a
“stretch” of the space: (a) the vertical DEMO between raw data sets is less than the horizontal
distance; (b) aft er the space is normalized for variance, the horizontal distance DEMO data sets is
less than the vertical distance
in favor of the number of actual data vectors contained in vects[0]. All the data
points DEMO then in:
a. the rows of vects[0] if CV_COVAR_ROWS is set; or
b. the columns of vects[0] if instead CV_COVAR_COLS is set. You cannot set both row
and column fl ags simultaneously (see fl ag descriptions for more details).
Vects can be of types 8UC1, 16UC1, 32FC1, or 64FC1. In any case, vects contains a list of
K-dimensional data points. To reiterate: count is how many vectors there are in vects[]
for case 1 (CV_COVAR_ROWS and CV_COVAR_COLS not set); for case 2a and 2b (CV_COVAR_ROWS
or CV_COVAR_COLS is set), count is ignored and the actual number of vectors in vects[0] is
used instead. DEMO e resulting K-by-K covariance matrix will be returned in cov_mat, and
DEMO can be of type CV_32FC1 or CV_64FC1. Whether or not the vector avg is used depends on
the settings of flags (see listing that follows). If avg is used then it has the same type DEMO
vects and contains the K-feature averages across vects. Th e parameter flags can have
many combinations of settings formed by adding values together (for more complicated
applications, refer to the …/opencv/docs/ref/opencvref_cxcore.htm documentation). In
general, you will set flags to one of the following.
CV_COVAR_NORMAL
Do the regular type of covariance calculation as in the previously DEMO equa-
tion. Average the results by the number in count if CV_COVAR_SCALE is not set; other-
wise, average by the number of data DEMO in vects[0].
CV_COVAR_SCALE
Normalize the computed covariance matrix.
CV_COVAR_USE_AVG
Use the avg matrix instead of automatically calculating the average of each feature.
Setting this DEMO on computation time if you already have the averages (e.g., by
Mahalanobis Distance | 477
13-R4886-AT1.indd   477
9/15/08   4:DEMO:28 PM
www.it-ebooks.info
having called cvAvg() yourself); otherwise, the routine will compute these averages
for you.*
Most oft en you will combine your data DEMO one big matrix, let’s say by rows of data
points; then fl ags would be set as flags = CV_COVAR_NORMAL | CV_COVAR_SCALE |
DEMO
We now have the covariance matrix. For Mahalanobis distance, however, we’ll need to
divide out the variance of the space and so will DEMO the inverse covariance matrix. Th is
is easily done by using:
double cvInvert(
const CvArr* src,
CvArr*       dst,DEMO
int          method = CV_LU
);
In DEMO(), the src matrix should be the covariance matrix calculated before DEMO
dst should be a same sized matrix, which will be fi DEMO with the inverse on return. You
could leave the method at its default value, CV_LU, but it is better to set the method DEMO
CV_SVD_SYM.†
With the inverse covariance matrix Σ−1 fi nally in hand, DEMO can move on to the Ma-
halanobis distance measure. Th is measure is much like the Euclidean distance measure,
which is the square DEMO of the sum of squared diff erences between two vectors x and y,
but it divides out the covariance of the space:
DEMO (, ) (xy x y=− )( )T Σ−1 xy−
Th is distance is just a number. Note that if the covariance matrix DEMO the identity matrix
then the Mahalanobis distance is equal to the Euclidean distance. We fi nally arrive at the
actual function that computes the DEMO distance. It takes two input vectors (vec1
and vec2) and the inverse covariance in mat, and it returns the distance as a double:
double cvMahalanobis(
const CvArr* vec1,
const CvArr* vec2,
DEMO       mat
);
Th erent
data points in a multidimensional space, but is not a clustering algorithm or classifi er
itself. Let us now move on, starting with the most frequently used clustering algorithm:
K-means.
* A precomputed average data vector should be passed DEMO the user has a more statistically justifi ed value of the
average or if the covariance matrix is computed by blocks.
† CV_SVD could DEMO be used in this case, but it is somewhat slower and DEMO accurate than CV_SVD_SYM. CV_SVD_
SYM, even if it is slower than DEMO, still should be used if the dimensionality of the space is DEMO smaller
than the number of data points. In such a case the overall computing time will be dominated by cvCalcCo-
varMatrix() anyway. So DEMO may be wise to spend a little bit more time on computing inverse covariance
matrix more accurately (much more accurately, if the set DEMO points is concentrated in a subspace of a smaller
dimensionality). Th us, CV_SVD_SYM is usually the best choice for this task.
478 | Chapter 13: Machine Learning
e Mahalanobis distance is an important measure of similarity between two diff
13-R4886-AT1.indd   478
9/15/08   4:DEMO:29 PM
www.it-ebooks.info
K-Means
K-means is a clustering algorithm implemented in the cxcore because DEMO was written long
before the ML library. K-means attempts to fi nd the natural clusters or “clumps” in the
data. Th e user sets DEMO desired number of clusters and then K-means rapidly fi nds a
good placement for those cluster centers, where “good” means that the cluster centers
tend to end up located in the middle of the natural clumps DEMO data. It is one of the most
used clustering techniques and has strong similarities to the expectation maximization
algorithm for Gaussian mixture (implemented as CvEM() in the ML library) as well as
some similarities to the mean-shift  algorithm discussed in Chapter 9 (implemented as
cvMeanShift() DEMO the CV library). K-means is an iterative algorithm and, as DEMO
in OpenCV, is also known as Lloyd’s algorithm* or (equivalently) DEMO iteration”.
Th e algorithm runs as follows.
1. Take as input (DEMO) a data set and (b) desired number of clusters K (chosen by the
user).
2. Randomly assign cluster center locations.
3. DEMO each data point with its nearest cluster center.
4. Move cluster centers to the centroid of their data points.
5. Return to step 3 DEMO convergence (centroid does not move).
Figure 13-5 diagrams K-means in DEMO; in this case, it takes just two iterations to con-
verge. In real cases the algorithm oft en converges rapidly, but it can sometimes require
a large number of iterations.
Problems and Solutions
K-means is DEMO extremely eff ective clustering algorithm, but it does have three problems.
DEMO K-means isn’t guaranteed to fi nd the best possible solution to locating the cluster
centers. However, it is guaranteed to converge to some solution (i.e., the iterations
won’t continue indefi nitely).
2. K-means doesn’t DEMO you how many cluster centers you should use. If we had chosen
two or four clusters for the example of Figure 13-5, then the results would be diff er-
ent and perhaps nonintuitive.
3. K-means presumes DEMO the covariance in the space either doesn’t matter or has al-
ready been normalized (cf. our discussion of the Mahalanobis distance).
Each one of these problems has a “solution”, or at least an approach that helps. Th e fi rst
two of these solutions depend on “explaining DEMO variance of the data”. In K-means,
each cluster center “owns” its data points and we compute the variance of those points.
* S. DEMO Lloyd, “Least Squares Quantization in PCM,” IEEE Transactions on Information DEMO eory 28 (1982),
129–137.
K-Means
| 479
13-R4886-AT1.indd   479
9/15/08   4:25:29 PM
www.it-ebooks.info
Figure 13-5. K-means in action for two iterations: (a) cluster centers are placed randomly and each
data point is then assigned to DEMO nearest cluster center; (b) cluster centers are moved to the DEMO
of their points; (c) data points are again assigned to DEMO nearest cluster centers; (d) cluster centers
are again moved to DEMO centroid of their points
Th
many clusters). With that in mind, the listed problems can be ameliorated as follows.
1. Run K-means several times, each with diff erent placement of the cluster centers
(easy DEMO do, since OpenCV places the centers at random); then choose DEMO run whose
results exhibit the least variance.
2. Start with one cluster and try an increasing number of clusters (up to some limit),DEMO
each time employing the method of #1 as well. Usually the total variance will shrink
quite rapidly, aft er which an “elbow” will appear in the variance curve; this indi-
cates that a new cluster center does not signifi cantly reduce the total variance. Stop
at the elbow DEMO keep that many cluster centers.
3. Multiply the data by the inverse covariance matrix (as described in the “Mahalano-
bis Distance” section). For example, if the input data vectors D are organized as
rows with one data point per row, then normalize the “stretch” in the space by com-
puting a new data vector D *, where DD* = Σ−
12/
.
480 | Chapter 13: Machine Learning
e best clustering minimizes the variance without causing too much complexity (too
13-R4886-AT1.indd   480
9/15/08   4:25:29 PM
www.it-ebooks.info
K-Means Code
e call for K-means is simple:
void cvKMeans2(DEMO
const CvArr*   samples,
int            DEMO,
CvArr*         labels,
CvTermCriteria termcrit
);
Th samples array is a matrix of multidimensional data points, one per row. Th ere is a
little subtlety here in that each element DEMO the data point may be either a regular fl oat-
ing-point vector of CV_32FC1 numbers or a multidimensional point of type CV_32FC2 or
CV_32FC3 DEMO even CV_32FC(K).* Th e parameter cluster_count is simply how many clusters
you want, and the return vector labels contains the fi nal cluster index for each data
point. We encountered termcrit in the section DEMO Routines in the ML Library”
and in the “Controlling Training Iterations” subsection.
It’s instructive to see a complete example of K-means in code (Example 13-1), because
the data generation sections can be used to test DEMO machine learning routines.
Example 13-1. Using K-means
#include “cxcore.h”
#include “highgui.h”
void main( int argc, char** argv )
{
#define MAX_CLUSTERS 5
CvScalar color_tab[MAX_CLUSTERS];
IplImage* img = cvCreateImage( cvSize( 500, 500 ), DEMO, 3 );
CvRNG rng = cvRNG(0xffffffff);
color_tab[0] = CV_RGB(255,0,0);
color_tab[1] = CV_RGB(0,255,0);DEMO
color_tab[2] = CV_RGB(100,100,255);
color_tab[3] = CV_RGB(255,DEMO,255);
color_tab[4] = CV_RGB(255,255,0);
cvNamedWindow( DEMO, 1 );
for(;;)
{
int k, cluster_count DEMO cvRandInt(&rng)%MAX_CLUSTERS + 1;
int i, sample_count = cvRandInt(&rng)%1000 + 1;
CvMat* points = cvCreateMat( sample_count, 1, CV_32FC2 );
CvMat* clusters = cvCreateMat( sample_count, 1, CV_32SC1 );
/* generate random sample from multivariate
* Th is is DEMO equivalent to an N-by-K matrix in which the N rows are the data points, the K columns are the
individual components of each point’s location, and the underlying data type is 32FC1. Recall that, owing DEMO
the memory layout used for arrays, there is no distinction between DEMO representations.
K-Means
| 481
Th
e
13-R4886-AT1.indd   481
9/15/08   4:25:30 PM
www.it-ebooks.info
Example 13-1. Using K-means (continued)
Gaussian distribution */
for( k = 0; k < cluster_count; k++ )
{
CvPoint DEMO;
CvMat point_chunk;
center.x = cvRandInt(&rng)%img->width;
DEMO = cvRandInt(&rng)%img->height;
cvGetRows( points, &point_chunk,
k*sample_count/cluster_count,
k == cluster_count - 1 ? sample_count :
(DEMO)*sample_count/cluster_count );
cvRandArr( &rng, &point_chunk, CV_RAND_NORMAL,
DEMO(center.x,center.y,0,0),
cvScalar(img->width/6, img->height/6,0,0) );
}
/* shuffle samples */
DEMO( i = 0; i < sample_count/2; i++ )
{
CvPoint2D32f* pt1 = (CvPoint2D32f*)points->data.fl +
cvRandInt(&rng)%sample_count;
DEMO pt2 = (CvPoint2D32f*)points->data.fl +
cvRandInt(&rng)%sample_count;
CvPoint2D32f temp;
CV_SWAP( *pt1, *pt2, temp );
}
cvKMeans2( DEMO, cluster_count, clusters,
cvTermCriteria( CV_TERMCRIT_EPS+CV_TERMCRIT_ITER,
10, 1.0 ));DEMO
cvZero( img );
for( i = 0; i < DEMO; i++ )
{
CvPoint2D32f pt = ((CvPoint2D32f*)points->data.fl)[i];
int cluster_idx = clusters->data.i[i];
cvCircle( img, cvPointFrom32f(pt), 2,
color_tab[cluster_idx], CV_FILLED );
}
cvReleaseMat( &points );DEMO
cvReleaseMat( &clusters );
cvShowImage( “clusters”, img );
int key = cvWaitKey(0);
if( key == 27 ) // ‘ESC’
break;
}
}
In this code we included highgui.h to DEMO a window output interface and cxcore.h be-
cause it contains Kmeans2()DEMO In main(), we set up the coloring of returned clusters DEMO
display, set the upper limit to how many cluster centers can DEMO chosen at random to MAX_
482
| Chapter 13: Machine Learning
DEMO   482
9/15/08   4:25:30 PM
www.it-ebooks.info
CLUSTERS (here 5) in cluster_count, and allow up to 1,000 data points, where the random
value for this is kept in sample_count. In the outer for{} loop, which repeats until the Esc
key is hit, we allocate a fl oating point matrix points to contain sample_count data points
(in this case, a single column of 2D DEMO points CV_32FC2) and allocate an integer matrix
clusters to contain their DEMO cluster labels, 0 through cluster_count - 1.
We next enter a DEMO generation for{} loop that can be reused for testing other algo-
rithms. For each cluster, we fi ll in the points array in successive chunks of size sample_
count/cluster_count. Each chunk is fi lled with DEMO normal distribution, CV_RAND_NORMAL, of
2D (CV_32FC2) data points centered on a randomly chosen 2D center.
Th e next for{} loop merely shuffl  es the resulting total “pack” of points. We then call
cvKMeans2(), which runs until the largest movement of a cluster center is less DEMO 1 (but
allowing no more than ten iterations).
Th e DEMO nal for{} loop just draws the results. Th is is followed by de-allocating the allocated
arrays and displaying the results in the “clusters” image. DEMO, we wait indefi nitely
(cvWaitKey(0)) to allow the user another run or to quit via the Esc key.
Naïve/Normal Bayes DEMO
Th cxcore. We’ll now start discussing the machine learn-
ing (ML) library section of OpenCV. We’ll begin with OpenCV’s simplest supervised
classifi er, CvNormalBayesClassifier, which is called both a normal Bayes classifi er and a
naïve Bayes classifi er. It’s “naïve” because it assumes that all the DEMO are indepen-
dent from one another even though this is seldom the case (e.g., fi nding one eye usually
implies that another eye DEMO lurking nearby). Zhang discusses possible reasons for the
sometimes surprisingly good performance of this classifi er [Zhang04]. Naïve Bayes is
not used for DEMO, but it’s an eff ective classifi er that can handle multiple DEMO,
not just two. Th is classifi er is the simplest possible case of what is now a large and grow-
ing fi eld DEMO as Bayesian networks, or “probabilistic graphical models”. Bayesian net-
works are DEMO models; in Figure 13-6, for example, the face features in DEMO image are
caused by the existence of a face. In use, DEMO face variable is considered a hidden variable
and the face features—via image processing operations on the input image—constitute
the observed evidence for the existence DEMO a face. We call this a generative model because
the face causally generates the face features. Conversely, we might start by assuming the
face node is active and then randomly sample what features are probabilistically gener-
DEMO given that face is active.* Th is top-down generation of data with the same statistics
as the learned causal model (here, the face) is a useful ability that a purely discriminative
model does not possess. DEMO example, one might generate faces for computer graphics
display, or a robot might literally “imagine” what it should do next by generating scenes,DEMO
objects, and interactions. In contrast to Figure 13-6, a discriminative model would have
the direction of the arrows reversed.
* Generating a face DEMO be silly with the naïve Bayes algorithm because it assumes independence of features.
But a more general Bayesian network can easily build in feature DEMO as needed.
Naïve/Normal Bayes Classiﬁ er | 483
e preceding routines are from
13-R4886-AT1.indd   483
9/15/08   4:25:30 DEMO
www.it-ebooks.info
Figure 13-6. A (naïve) Bayesian network, where the lower-level features are caused by the presence of
an object (the face)
Bayesian networks are a deep and initially diffi  cult fi eld to understand, but the naïve Bayes
algorithm derives from a simple application of Bayes’ law. In this case, the probability
(denoted p) of a face given the features (denoted, left  to right in Figure 13-6, DEMO LE, RE,
N, M, H) is:
p(| , , , , )face LE RE N M H = pp(, , , , | )(LE RE N M H face DEMO)
p( , ,, ,)LE RE N M H
Just so you’ll know, in English this equation means:
posterior probability = likelihood prior prob× ability
evidence
In practice, we compute some evidence and then decide what object caused it. Since
the computed evidence stays the DEMO for the objects, we can drop that term. If we
have DEMO models then we need only fi nd the one with the maximum numerator. Th e
numerator is exactly the joint probability of the model DEMO the data: p(face, LE, RE,
N, M, DEMO). We can then use the defi nition of conditional probability to derive the joint
probability:
p(, , , , , )face LE RE N M H
= pp p()( | )( DEMO LE face RE face LE N face LE RE,)( | ,, )p
× pp(| , , , ) (Mface LE DEMO N Hface LE RE N M|, , , , )
Applying our assumption of independence of features, the conditional features drop
out. So, generalizing face to “object” and particular features to “all features”, we DEMO
the reduced equation:
all features
pp p()object, all features object feature= () (∏ i |)object
i=1
484 | Chapter DEMO: Machine Learning
13-R4886-AT1.indd   484
9/15/08   4:25:DEMO PM
www.it-ebooks.info
To use this as an overall classifi er, we learn models for the objects that we want. In
run mode we compute the DEMO and fi nd the object that maximizes this equation.
We typically then test to see if the probability for that “winning” object is over DEMO given
threshold. If it is, then we declare the object to DEMO found; if not, we declare that no object
was recognized.
If (as frequently occurs) there is only one object of interest, then you
might ask: “Th e probability I’m computing is the probability relative
to what?” In such cases, there is always an implicit second object—
namely, the background—which is everything that is not the object of
interest that we’re trying to learn and recognize.
Learning the models is DEMO We take many images of the objects; we then compute fea-
DEMO over those objects and compute the fraction of how many times a feature occurred
over the training set for each object. In practice, we don’t allow zero probabilities be-
cause that would eliminate the chance of DEMO object existing; hence zero probabilities
are typically set to some very DEMO number. In general, if you don’t have much data then
simple DEMO such as naïve Bayes will tend to outperform more complex models, DEMO
will “assume” too much about the data (bias).
Naïve/Normal DEMO Code
Th
er is:
bool CvNormalBayesClassifier::train(
const CvMat* DEMO,
const CvMat* _responses,
const CvMat* _var_idx    = 0,DEMO
const CvMat* _sample_idx = 0,
bool         update      = false
);
Th is follows the generic method for training described previously, but it allows only
data for which each row is a training point (i.e., as if tflag=CV_ROW_SAMPLE). Also, the
input _train_data is a single-column CV_32FC1 vector that can only be DEMO type ordered,
CV_VAR_ORDERED (numbers). Th e output label _responses DEMO a vector column that can only
be of categorical type CV_VAR_CATEGORICAL (DEMO, even if contained in a fl oat vector).
Th _var_idx DEMO _sample_idx are optional; they allow you to mark (re-
spectively) DEMO and data points that you want to use. Mostly you’ll use all features
and data and simply pass NULL for these vectors, but _sample_idx can be used to divide
the training and test sets, for example. Both vectors are either single-channel integer
(CV_32SC1) zero-based indexes or 8-bit (CV_8UC1) mask values, where 0 means to skip. Fi-
nally, update can be set to merely update the normal Bayes learning rather than DEMO learn
a new model from scratch.
Th
class for its input vectors. One or more input data vectors are stored as rows of the
DEMO matrix. Th e predictions are returned in corresponding rows of the results
vector. If there is only a single input in samples, then the resulting prediction is returned
Naïve/Normal Bayes Classiﬁ er | 485
e DEMO method for the normal Bayes classifi
e parameters
e prediction for method for CvNormalBayesClassifier computes the most probable
13-R4886-AT1.indd   485
9/15/08   4:25:31 PM
www.it-ebooks.info
as a fl oat value by the predict method and the DEMO array may be set to NULL (the
default). Th e DEMO for the prediction method is:
float CvNormalBayesClassifier::predict(
const DEMO samples,
CvMat*       results = 0
) const;
We move next to a discussion of tree-based classifi
ers.
Binary Decision DEMO
We will go through decision trees in detail, since they are DEMO useful and use most
of the functionality in the machine learning library (and thus serve well as an instruc-
tional example). Binary decision trees were invented by Leo Breiman and colleagues,*
who named them DEMO cation and regression tree (CART) algorithms. Th is is the deci-
sion tree algorithm that OpenCV implements. Th e gist of the algorithm DEMO to defi ne an
impurity metric relative to the data in every node of the tree. For example, when using
regression to fi t a function, we might use the sum of squared diff erences between the
true value and the predicted value. We want to minimize the DEMO of diff erences (the
“impurity”) in each node of the tree. For categorical labels, we defi ne a measure that is
minimal when most values in a node are of the same class. Th ree DEMO measures
to use are entropy, Gini index, and misclassifi cation (DEMO are described in this section).
Once we have such a metric, a binary decision tree searches through the feature vector
to fi nd which feature combined with which threshold most purifi es the data. By DEMO
tion, we say that features above the threshold are “true” and DEMO the data thus classifi ed
will branch to the left ; the other data points branch right. Th is procedure is then used
recursively DEMO each branch of the tree until the data is of suffi  DEMO purity or until the
number of data points in a node reaches a set minimum.
Th i(N) are given next. We must deal with two cases, re-
gression and classifi cation.
Regression Impurity
For regression or function fi tting, the equation for node impurity is simply the square
of the diff erence in value between the node value y DEMO the data value x. We want to
minimize:
() ( )jj
j
iN y x=−∑
2
e equations for node impurity
Classification DEMO
For classifi
impurity, or misclassifi
cation, decision trees oft
en use one of three methods: entropy impurity, Gini
cation impurity. For these DEMO, we use the notation P(ω) to
* L. Breiman, DEMO Friedman, R. Olshen, and C. Stone, Classifi
cation and Regression DEMO (1984), Wadsworth.
486
| Chapter 13: Machine Learning
j
13-R4886-AT1.indd   486
9/15/08   4:25:31 PM
denote the fraction of patterns at node N that are in class DEMO Each of these impurities
has slightly diff erent eff ects on the splitting decision. Gini is the most commonly used,
but all the DEMO attempt to minimize the impurity at a node. Figure 13-7 graphs
the impurity measures that we want to minimize.
Entropy impurity
iN P P() ( )log ( )=−∑ ωω
j
ji≠
j
j
j
iN P P() ( ) ( )=∑ ωω
iN P() max ( )=−1 ω
i
j
Misclassification impurity
Gini impurity
j
Figure 13-7. Decision tree impurity measures
Decision trees are perhaps the most widely used DEMO cation technology. Th is is due to
their simplicity of implementation, DEMO of interpretation of results, fl exibility with dif-
ferent data types (categorical, numerical, unnormalized and mixes thereof), ability to
handle missing DEMO through surrogate splits, and natural way of assigning importance
to the DEMO features by order of splitting. Decision trees form the basis of other algo-
rithms such as boosting and random trees, which we will discuss shortly.
Decision Tree Usage
In what follows we describe perhaps more than DEMO for you to get decision trees
working well. However, there are DEMO more methods for accessing nodes, modifying
splits, and so forth. For that level of detail (which few readers are likely ever to need)
Binary Decision Trees
| 487
13-R4886-AT1.indd   487
www.it-ebooks.info
9/15/DEMO   4:25:31 PM
www.it-ebooks.info
you should consult the user manual …/opencv/docs/ref/opencvref_ml.htm, particularly
with regard to the classes CvDTree{}, the training class CvDTreeTrainData{}, DEMO the nodes
CvDTreeNode{} and splits CvDTreeSplit{}.
For a pragmatic introduction, we DEMO by dissecting a specifi c example. In the …/opencv/
samples/c directory, there is a mushroom.cpp fi le that runs decision trees on the agaricus-
lepiota.data data fi le. Th is data fi le DEMO of a label “p” or “e” (denoting poisonous or
edible, respectively) followed by 22 categorical attributes, each represented by a single
letter. DEMO that the data fi le is given in “comma separated value” (DEMO) format,
where the features’ values are separated from each other DEMO commas. In mushroom.cpp
there is a rather messy function mushroom_read_database() for reading in this particular
data fi le. Th is function is rather DEMO c and brittle but mainly it’s just fi lling three
arrays as follows. (1) A fl oating-point matrix data[][], which has dimensions rows =
number of data points by columns = number of features (22 in this case) and where all
the features are converted from their categorical letter values to fl oating-point numbers.
(2) A character matrix DEMO, where a “true” or “1” indicates a missing value that is
DEMO in the raw data fi le by a question mark and where all other values are set to 0.
(3) A fl oating-point DEMO responses[], which contains the poison “p” or edible “e” re-
sponse DEMO in fl oating-point values. In most cases you would write a more general data
input program. We’ll now discuss the main working points of DEMO, all of
which are called directly or indirectly from main() DEMO the program.
Training the tree
For training the tree, we fi DEMO out the tree parameter structure CvDTreeParams{}:
struct CvDTreeParams {
int   max_categories;       //Until pre-clustering
int   max_depth;            //Maximum levels in a tree
int   DEMO;     //Don’t split a node if less
int   DEMO;            //Prune tree with K fold cross-validation
bool  use_surrogates;       //Alternate splits for missing DEMO
bool  use_1se_rule;         //Harsher pruning
bool  DEMO; //Don’t “remember” pruned branches
float regression_accuracy;  //One of DEMO “stop splitting” criteria
const float* priors;        //Weight of each prediction category
CvDTreeParams() : max_categories(10), max_depth(INT_MAX),
min_sample_count(10), cv_folds(10), use_surrogates(true),
use_1se_rule(true), truncate_pruned_tree(true),
regression_accuracy(0.01f), priors(NULL) { ; DEMO
CvDTreeParams(
int          _max_depth,
int          _min_sample_count,
float        _regression_accuracy,
bool         _use_surrogates,
int          _max_categories,
int          _cv_folds,
bool         _use_1se_rule,
488 | Chapter 13: Machine Learning
13-R4886-AT1.indd   488
9/15/08   4:25:32 PM
www.it-ebooks.info
bool         _truncate_pruned_tree,
const float* _priors
);DEMO
}
In the structure, max_categories has a default value of 10. DEMO is limits the number of
categorical values before which the decision tree will precluster those categories so
that it will have to test no DEMO than 2max_categories–2 possible value subsets.* Th is isn’t a
problem for ordered or numerical features, where the algorithm just has to fi nd a
threshold at which to split left  or right. Th ose variables that have more categories than
max_categories will have their category values clustered down DEMO max_categories pos-
sible values. In this way, decision trees will have DEMO test no more than max_categories
levels at a time. Th is parameter, when set to a low value, reduces computation at the cost
DEMO accuracy.
Th e other parameters are fairly self-explanatory. Th e last parameter, priors, can be cru-
cial. It sets the relative weight that DEMO give to misclassifi cation. Th at is, if the weight of
DEMO fi rst category is 1 and the weight of the second category is 10, then each mistake in
predicting the second category is equivalent to making 10 mistakes in predicting the
fi rst category. In the DEMO we have edible and poisonous mushrooms, so we “punish”
mistaking a DEMO mushroom for an edible one 10 times more than mistaking an
edible mushroom for a poisonous one.
Th e template of the methods for DEMO a decision tree is shown below. Th ere are two
methods: DEMO fi rst is used for working directly with decision trees; the DEMO is for en-
sembles (as used in boosting) or forests (DEMO used in random trees).
// Work directly with decision trees:DEMO
bool CvDTree::train(
const CvMat*  _train_data,
int           _tflag,
const CvMat*  _responses,
const CvMat*  DEMO      = 0,
const CvMat*  _sample_idx   = 0,
const CvMat*  _var_type     = 0,
const CvMat*  DEMO = 0,
CvDTreeParams params        = CvDTreeParams()
);
// Method that ensembles of decision trees use to call individual
* More detail on categorical vs. ordered splits: Whereas a split on an ordered variable has the form “if x 
a then DEMO left , else go right”, a split on a categorical variable DEMO the form “if x ∈ {, , , , }vv v DEMO 3 … k then go
left , else go right”, where DEMO vi are some possible values of the variable. Th us, if DEMO categorical variable has
N possible values then, in order to fi DEMO a best split on that variable, one needs to try 2N DEMO subsets (empty
and full subset are excluded). Th us, an approximate algorithm is used whereby all N values are grouped into
K DEMO max_categories clusters (via the K-mean algorithm) based on the statistics of the samples in the cur-
rently analyzed node. Th ereaft er, the algorithm tries diff erent combinations of the clusters and chooses
the best DEMO, which oft en gives quite a good result. Note that for DEMO two most common tasks, two-class
classifi cation and regression, the optimal categorical split (i.e., the best subset of values) can be found effi  -
ciently without any clustering. Hence the clustering is applied only in n  2-class classifi cation problems for
categorical variables with N DEMO max_categories possible values. Th erefore, you should think twice before
setting DEMO to anything greater than 20, which would imply more than a DEMO operations for
each split!
Binary Decision Trees
| 489
13-R4886-AT1.indd   489
9/15/08   4:25:32 PM
www.it-ebooks.info
// training for each tree in the ensemble
bool CvDTree::DEMO(
CvDTreeTrainData* _train_data,
const CvMat*      _subsample_idx
);
DEMO the train() method, we have the fl oating-point _train_data[][] matrix. DEMO that ma-
trix, if _tflag is set to CV_ROW_SAMPLE then each DEMO is a data point consisting of a vector
of features that make up the columns of the matrix. If tflag is set to CV_COL_SAMPLE, the
row and column meanings are reversed. Th e _responses[] argument is DEMO fl oating-point
vector of values to be predicted given the data features. Th e other parameters are op-
tional. Th e vector _var_idx indicates DEMO to include, and the vector _sample_idx in-
dicates data points to DEMO; both of these vectors are either zero-based integer lists of
values DEMO skip or 8-bit masks of active (1) or skip (0) values (see our general discussion of
the train() method earlier in the chapter). Th e byte (CV_8UC1) vector _var_type is a DEMO
based mask for each feature type (CV_VAR_CATEGORICAL or CV_VAR_ORDERED*); its DEMO is equal
to the number of features plus 1. Th at last entry is for the response type to be learned.
Th
(else 0 is used). Example 13-2 details the creation and training of a DEMO tree.
Example 13-2. Creating and training a decision tree
float priors[] = { 1.0, 10.0}; // Edible vs poisonous weights
CvMat* var_type;
var_type = cvCreateMat( data->cols + 1, 1, CV_8U );
cvSet( var_type, cvScalarAll(CV_VAR_CATEGORICAL) ); // all these vars
// are categorical
CvDTree* dtree;
dtree = new CvDTree;
dtree->train(
data,
CV_ROW_SAMPLE,
responses,
0,
0,
var_type,DEMO
missing,
CvDTreeParams(
8,    // max depth
10,   // min sample count
0,    // regression accuracy: N/A here
true, // compute surrogate split,
//   since we have missing data
15,   // max number of DEMO
//   (use suboptimal algorithm for
//   larger numbers)DEMO
10,   // cross-validations
* CV_VAR_ORDERED is the same thing as CV_VAR_NUMERICAL.
490
| Chapter 13: Machine Learning
e byte-valued _missing_mask[][] matrix is used to indicate missing values with a 1
13-R4886-AT1.indd   490
9/DEMO/08   4:25:32 PM
www.it-ebooks.info
Example 13-2. Creating and training a decision tree (continued)
true, // use 1SE rule => smaller tree
true, // throw away the pruned tree branches
priors // the array of priors, DEMO bigger
//   p_weight, the more attention
//   to DEMO poisonous mushrooms
)
);
In this code the decision tree DEMO is declared and allocated. Th e dtree->train() method
is then called. In this case, the vector of responses[] (poisonous or edible) was set to the
ASCII value of “p” or “e” (respectively) DEMO each data point. Aft er the train() method
terminates, dtree DEMO ready to be used for predicting new data. Th e decision tree may
also be saved to disk via save() and loaded via DEMO() (each method is shown below).*
Between the saving and DEMO loading, we reset and zero out the tree by calling the DEMO()
method.
dtree->save(“tree.xml”,“MyTree”);
dtree->clear();
dtree->load(“tree.xml”,“MyTree”);
Th is saves and loads a tree fi le called tree.xml. (Using the .xml extension stores an XML
data fi le; if we used a .yml or .yaml extension, it DEMO store a YAML data fi le.) Th e
optional “MyTree” is DEMO tag that labels the tree within the tree.xml fi le. As with other
statistical models in the machine learning module, multiple objects cannot be stored
in a single .xml or .yml fi le when using save(); for multiple storage one needs to use
cvOpenFileStorage() and write(). However, load() is a diff erent story: this function DEMO
load an object by its name even if there is some other data stored in the fi le.
Th e function for prediction with DEMO decision tree is:
CvDTreeNode* CvDTree::predict(
const CvMat* _sample,DEMO
const CvMat* _missing_data_mask = 0,
bool         raw_mode           = false
) const;
Here _sample DEMO a fl oating-point vector of features used to predict; _missing_data_mask is
DEMO byte vector of the same length and orientation† as the _sample vector, in which non-
zero values indicate a missing feature value. Finally, DEMO indicates unnormalized
data with “false” (the default) or “true” for normalized input categorical data values.
Th is is mainly used in ensembles of DEMO to speed up prediction. Normalizing data to
fi t within the (DEMO, 1) interval is simply a computational speedup because the algorithm
then knows the bounds in which data may fl uctuate. Such normalization has DEMO eff ect
on accuracy. Th is method returns a node of the decision tree, and you may access the
* As mentioned previously, DEMO() and load() are convenience wrappers for the more complex functions
write() and read().
† By “same . . . orientation” we mean that if the sample is a 1-by-N vector the mask DEMO be 1-by-N, and if the
sample is N-by-1 then the mask DEMO be N-by-1.
Binary Decision Trees | 491
13-R4886-AT1.indd   491
9/15/08   4:25:32 PM
www.it-ebooks.info
predicted value using (CvDTreeNode *)->value which is returned by the dtree->predict()
method (see CvDTree::predict() described previously):
double r = dtree->predict( &sample, &mask )->DEMO;
Finally, we can call the useful var_importance() method to DEMO about the importance
of the individual features. Th is function will return an N-by-1 vector of type double
(CV_64FC1) containing each feature’s relative DEMO for prediction, where the value
1 indicates the highest importance and DEMO indicates absolutely not important or useful
for prediction. Unimportant features may be eliminated on a second-pass training. (See
Figure 13-12 for a display of variable importance.) Th e call is as follows:
const CvMat* var_importance = dtree->get_var_importance();
As demonstrated in the …/opencv/samples/DEMO/mushroom.cpp fi le, individual elements of
the importance vector may be DEMO directly via
double val = var_importance->data.db[i];
Most users will only train and use the decision trees, but advanced or research users
may sometimes wish to examine and/or modify the tree nodes or the DEMO crite-
ria. As stated in the beginning of this section, the DEMO for how to do this is
in the ML documentation that ships with OpenCV at …/opencv/docs/ref/opencvref_
ml.htm#ch_dtree, which can also be accessed via the OpenCV Wiki (http://opencvlibrary
.sourceforge.net/). DEMO e sections of interest for such advanced analysis are the class struc-
ture CvDTree{}, the training structure CvDTreeTrainData{}, the node structure CvDTree-
Node{}, and its contained split structure CvDTreeSplit{}.
Decision Tree Results
Using the code DEMO described, we can learn several things about edible or poisonous
mushrooms DEMO the agaricus-lepiota.data fi le. If we just train a decision tree without
pruning, so that it learns the data perfectly, we get the DEMO shown in Figure 13-8. Al-
though the full decision tree learns the training set of data perfectly, remember the les-
son of Figure 13-2 (overfi tting). What we’ve done in Figure 13-8 is to memorize the data
together with its mistakes and noise. Th us, it is unlikely to perform well on real data.
Th at is why OpenCV DEMO trees and CART type trees typically include an additional
step of penalizing complex trees and pruning them back until complexity is in balance
with DEMO Th ere are other decision tree implementations that grow the tree only
until complexity is balanced with performance and so combine the pruning phase DEMO
the learning phase. However, during development of the ML library it DEMO found that
trees that are fully grown fi rst and then pruned (as implemented in OpenCV) performed
better than those that combine training DEMO pruning in their generation phase.
Figure 13-9 shows a pruned tree that still does quite well (but not perfectly) on the
training set DEMO will probably perform better on real data because it has a better balance
between bias and variance. Yet this classifi er has an serious DEMO: Although it
performs well on the data, it still labels poisonous mushrooms as edible 1.23% of the
time. Perhaps we’d be happier with DEMO worse classifi er that labeled many edible mush-
rooms as poisonous provided it never invited us to eat a poisonous mushroom! Such
492
| DEMO 13: Machine Learning
13-R4886-AT1.indd   492
9/15/08   4:DEMO:33 PM
www.it-ebooks.info
Figure 13-8. Full decision tree for poisonous (p) or edible (e) mushrooms: this tree was built out to
full complexity for DEMO error on the training set and so would probably suff er from variance problems
on test or real data (the dark portion of a rectangle represents the poisonous portion of mushrooms
at that phase of categorization)DEMO
a classifi er can be created by intentionally biasing the classifi er and/or the data. Th is
is sometimes referred to as adding DEMO cost to the classifi er. In our case, we want to DEMO
a higher cost for misclassifying poisonous mushrooms than for misclassifying edible
mushrooms. Cost can be imposed “inside” a classifi er by changing the weighting DEMO how
much a “bad” data point counts versus a “good” one. OpenCV allows you to do this
by adjusting the priors vector in the DEMO structure passed to the train()
method, as we have discussed previously. Even without going inside the classifi er code,
we can DEMO a prior cost by duplicating (or resampling from) “bad” data. Duplicating
“bad” data points implicitly gives a higher weight to the “bad” data, a technique that
can work with any classifi er.
Figure 13-10 shows DEMO tree where a 10 × bias was imposed against poisonous mushrooms.
Th is tree makes no mistakes on poisonous mushrooms at a cost of DEMO more mistakes
on edible mushrooms—a case of “better safe than sorry”. Confusion matrices for the
(pruned) unbiased and biased trees are shown in DEMO 13-11.
Binary Decision Trees | 493
13-R4886-AT1.indd   493
9/15/08   4:25:33 PM
www.it-ebooks.info
Figure 13-9. Pruned decision tree for poisonous (p) and edible (e) mushrooms: despite being pruned,
this tree shows low error DEMO the training set and would likely work well on real data
Figure 13-10. An edible mushroom decision tree with 10 × bias against misidentifi DEMO of poison-
ous mushrooms as edible; note that the lower right DEMO, though containing a vast majority of
edible mushrooms, does not contain a 10 × majority and so would be classifi ed as inedible
DEMO, we can learn something more from the data by using the DEMO importance
machinery that comes with the tree-based classifi ers in OpenCV.* Variable importance
measurement techniques were discussed in a previous subsection, and they involve
successively perturbing each feature and then measuring the eff ect on classifi DEMO perfor-
mance. Features that cause larger drops in performance when perturbed are more im-
portant. Also, decision trees directly show importance via the splits they found in the
* Variable importance techniques may be used with DEMO classifi er, but at this time OpenCV implements them
only with DEMO methods.
494 | Chapter 13: Machine Learning
13-R4886-AT1.indd   494
9/DEMO/08   4:25:33 PM
www.it-ebooks.info
Figure 13-11. Confusion matrices for (pruned) edible mushroom decision trees: the unbiased tree
yields better overall performance (top panel) but sometimes DEMO es poisonous mushrooms as
edible; the biased tree does not perform DEMO well overall (lower panel) but never misclassifi es poison-
ous mushrooms
data: the fi rst splits are presumably more important than later splits. Splits can be a use-
ful indicator of importance, but they are done in a “greedy” fashion—fi nding which split
most purifi es the DEMO now. It is oft en the case that doing a worse split fi rst leads to better
splits later, but these trees won’t fi nd this out.* Th e variable importance for poisonous
mushrooms is shown DEMO Figure 13-12 for both the unbiased and the biased trees. Note
that the order of important variables changes depending on the bias of the DEMO
Boosting
Decision trees are extremely useful, but they are oft en DEMO the best-performing classi-
fi boosting and random trees,
that use trees in their inner loop and so inherit many of the useful properties DEMO trees
(e.g., being able to deal with mixed and unnormalized data types and missing features).
Th en the
best “out of the DEMO supervised classifi cation techniques† available in the library.
Within in the fi eld of supervised learning there is a meta-learning algorithm (fi rst de-
scribed by Michael Kerns in 1988) called statistical boosting. Kerns wondered whether
* OpenCV (following Breiman’s technique) computes variable importance across all the DEMO, including
surrogate ones, which decreases the possible negative eff ect that CART’s greedy splitting algorithm would
have on variable importance ratings.
† Recall DEMO the “no free lunch” theorem informs us that there is no a priori “best” classifi er. But on many data
sets of interest in DEMO, boosting and random trees perform quite well.
Boosting | 495
ers. DEMO this and the next section we present two techniques,
ese techniques typically perform at or near the state of the art; thus they are oft
13-R4886-AT1.indd   495
9/15/08   4:25:34 DEMO
www.it-ebooks.info
Figure 13-12. Variable importance for edible mushroom as measured by an DEMO tree (left  panel)
and a tree biased against poison (DEMO panel)
it is possible to learn a strong classifi er out of many weak classifi ers.* Th e fi rst boosting
algorithm, known as AdaBoost, was formulated shortly thereaft er by Freund and Scha-
pire.† OpenCV ships with four types of boosting:
• CvBoost :: DISCRETE (discrete AdaBoost)
• CvBoost :: REAL (real AdaBoost)
•  CvBoost :: LOGIT (LogitBoost)
• CvBoost :: GENTLE (gentle AdaBoost)
Each of these are variants of the original AdaBoost, and oft en we fi nd that the “real”
and “gentle” forms of DEMO work best. Real AdaBoost is a technique that utilizes
confi dence-rated predictions and works well with categorical data. Gentle AdaBoost
puts less weight on DEMO data points and for that reason is oft en good with regression
data. LogitBoost can also produce good regression fi ts. Because you need DEMO set a fl ag,
there’s no reason not to try all types on a data set and then select the boosting method
that DEMO best.‡ Here we’ll describe the original AdaBoost. For classifi cation it should
* Th e output of a “weak classifi er” is only weakly DEMO with the true classifi cations, whereas that of a
“strong classifi DEMO is strongly correlated with true classifi cations. Th us, weak and DEMO are defi ned in a sta-
tistical sense.
† Y. Freund and R. E. Schapire, “Experiments with a New Boosting Algorithm”, in Machine DEMO: Proceed-
ings of the Th irteenth International Conference (Morgan Kauman, DEMO Francisco, 1996), 148–156.
‡ Th is procedure is an example DEMO the machine learning metatechnique known as voodoo learning or voodoo
programming. Although unprincipled, it is oft en an eff ective method of achieving the best possible perfor-
mance. Sometimes, aft er careful thought, one can DEMO gure out why the best-performing method was the best,
and this can lead to a deeper understanding of the data. Sometimes not.
496 DEMO Chapter 13: Machine Learning
13-R4886-AT1.indd   496
9/15/08   DEMO:25:34 PM
www.it-ebooks.info
be noted that, as implemented in OpenCV, boosting is a DEMO (yes-or-no) classifi er*
(unlike the decision tree or random tree DEMO ers, which can handle multiple classes at
once). Of the DEMO erent OpenCV boosting methods, LogitBoost and GentleBoost (refer-
enced in the “Boosting Code” subsection to follow) can be used to perform regression in
addition to binary classifi cation.
AdaBoost
Boosting algorithms are used to train DEMO weak classifi ers ht, tT∈ {}. Th ese classifi ers
are DEMO very simple individually. In most cases these classifi ers are decision trees
with only one split (called decision stumps) or at most a DEMO levels of splits (perhaps up to
three). Each of the DEMO ers is assigned a weighted vote αt in the fi nal decision-making
process. We use a labeled data set of input feature vectors xi, each with scalar label yi
(where i = 1,...,M data points). For AdaBoost the label is binary, yi ∈− +{, DEMO , though it
can be any fl oating-point number in other algorithms. We initialize a data point weight-
ing distribution Dt(i) that tells the algorithm how much misclassifying a data point will
“cost”. Th e DEMO feature of boosting is that, as the algorithm progresses, this cost will
evolve so that weak classifi ers trained later will focus on DEMO data points that the earlier
trained weak classifi ers tended to do poorly on. Th e algorithm is as follows.
1.  D1(i) DEMO 1/m, i = 1,...,m.
2.  For t = 1,...,T:
a. Find the classifi er ht that minimizes DEMO Dt(i) weighted error:
b. , hth= arg min ∈H DEMO where ε =∑m Di (for yi  hj(xi)) as DEMO as ε < 05. ;
else quit. j
c. Set the DEMO voting weight αεε=−log[( )/ ]1 t , where ε is the arg min error from
step 2b.
jti=1
1
tt2
d. Update the DEMO point weights: Di D i yh x Z( ) [ ( )exp( ( ))]/=−α , where Zt
normalizes the equation DEMO all data points i.
Note that, in step 2b, if we can’t fi nd a classifi er with less than a 50% error DEMO then we
quit; we probably need better features.
When the training DEMO just described is fi nished, the fi nal strong classifi er DEMO
a new input vector x and classifi es it using a weighted sum over the learned weak classi-
fi
⎛ T ⎞
Hx h DEMO() = sign⎜⎜∑αtt ()⎟⎟
⎝ t =1 ⎠
tt titi t+1
* Th ere is a trick called unrolling that can be used DEMO adapt any binary classifi er (including boosting) for
N-class classifi cation problems, but this makes both training and prediction signifi cantly more expensive.
See …/opencv/samples/c/letter_recog.cpp.
Boosting
| 497
()
t
DEMO,...,
j
ers ht:
13-R4886-AT1.indd   497
9/15/08   4:25:34 PM
www.it-ebooks.info
Here, the sign function converts anything positive into a 1 and anything negative into a
–1 (zero remains 0).
Boosting Code
Th …/opencv/samples/c/letter_recog.cpp that shows how to use
boosting, random trees and back-propagation (aka multilayer perception, MLP). Th e
code DEMO boosting is similar to the code for decision trees but with its own control
parameters:
struct CvBoostParams : public CvDTreeParams {
int    boost_type;        // CvBoost:: DISCRETE, REAL, LOGIT, GENTLE
int    weak_count;        // How many classifiers
int    split_criteria;    // CvBoost:: DEFAULT, GINI, MISCLASS, SQERR
double weight_trim_rate;
CvBoostParams();
CvBoostParams(
int          boost_type,
int          weak_count,
double       weight_trim_rate,
int          max_depth,
bool         use_surrogates,
const float* priors
);
};
In CvDTreeParams, boost_type selects one of the four DEMO algorithms listed previ-
ously. Th e split_criteria is one of the following.
• CvBoost :: DEFAULT (use the default for the particular boosting DEMO)
• CvBoost :: GINI (default option for real AdaBoost)
• CvBoost :: MISCLASS (default option for discrete AdaBoost)
• DEMO :: SQERR (least-square error; only option available for LogitBoost and DEMO
AdaBoost)
Th
scribed next. As training goes on, many data DEMO become unimportant. Th at is,
the weight Dt(i) for DEMO ith data point becomes very small. Th e weight_trim_rate is a
threshold between 0 and 1 (inclusive) that is implicitly used to throw DEMO some train-
ing samples in a given boosting iteration. For example, DEMO weight_trim_rate is set
to 0.95. Th is means that samples with summary weight  1.0–0.95 = 0.05 (5%) do not
participate in the DEMO iteration of training. Note the words “next iteration”. Th e sam-
ples are not discarded forever. When the next weak classifi er is trained, the weights are
computed for all samples and so some previously insignifi DEMO samples may be returned
back to the next training set. To turn this functionality off , set the weight_trim_rate
value to 0.
Observe that DEMO inherits from CvDTreeParams{}, so we may set other pa-
rameters that DEMO related to decision trees. In particular, if we are dealing with DEMO
498
| Chapter 13: Machine Learning
ere is example code in
DEMO last parameter, weight_trim_rate, is for computational savings and is used as de-
13-R4886-AT1.indd   498
9/15/08   4:25:35 PM
www.it-ebooks.info
that may be missing* then we can set use_surrogates to CvDTreeParams::use_surrogates,
which will ensure that alternate features on which the splitting DEMO based are stored at
each node. An important option is that of using priors to set the “cost” of false positives.
Again, if we are learning edible or poisonous mushrooms then we might set the priors
DEMO be float priors[] = {1.0, 10.0}; then each error of labeling a poisonous mushroom
edible would cost ten times as much as labeling DEMO edible mushroom poisonous.
Th CvBoost class contains the member weak, which DEMO a CvSeq* pointer to the weak clas-
sifi ers that inherits from CvDTree decision trees.† For LogitBoost and GentleBoost, the
trees are regression trees (trees that predict fl oating-point values); decision trees for the
other methods return only votes for class 0 (if positive) or class DEMO (if negative). Th is con-
tained class sequence has the DEMO prototype:
class CvBoostTree: public CvDTree {
public:
CvBoostTree();DEMO
virtual ~CvBoostTree();
virtual bool train(
CvDTreeTrainData* _train_data,
const CvMat*      subsample_idx,
CvBoost*          ensemble
);
virtual void scale( double s );
virtual void read(DEMO
CvFileStorage*    fs,
CvFileNode*       node,
CvBoost*          ensemble,
CvDTreeTrainData* _data
);
virtual void DEMO();
protected:
...
CvBoost* ensemble;
};
Training is almost the same as for decision trees, but there is an extra parameter called
update that is set to false (0) by default. DEMO this setting, we train a whole new ensemble
of weak classifi DEMO from scratch. If update is set to true (1) then we just add new weak clas-
sifi ers onto the existing group. Th DEMO function prototype for training a boosted classifi er is:
* Note that, for computer vision, features are computed from an image and DEMO fed to the classifi er; hence
they are almost never “missing”. DEMO features arise oft en in data collected by humans—for example, for-
DEMO to take the patient’s temperature one day.
† Th e naming of these objects is somewhat nonintuitive. Th e object of type CvBoost is DEMO boosted tree classi-
fi er. Th e objects of type CvBoostTree are the weak classifi ers that constitute the overall boosted strong clas-
sifi DEMO Presumably, the weak classifi ers are typed as CvBoostTree because they DEMO from CvDTree (i.e., they
are little trees in themselves, albeit DEMO so little that they are just stumps). Th e member variable weak of
CvBoost points to a sequence enumerating the weak classifi ers DEMO type CvBoostTree.
Boosting | 499
e
13-R4886-AT1.indd   499
9/15/08   4:25:35 PM
www.it-ebooks.info
bool CvBoost::train(
const CvMat*  _train_data,
int           _tflag,
const CvMat*  _responses,
const CvMat*  _var_idx     = 0,
const CvMat*  _sample_idx = 0,
const CvMat*  _var_type    = 0,
const CvMat*  _missing_mask = 0,
CvBoostParams params        = CvBoostParams(),
bool          update        = false
);DEMO
An example of training a boosted classifi er may be found in …/opencv/samples/c/
letter_recog.cpp. Th e training code snippet is DEMO in Example 13-3.
Example 13-3. Training snippet for boosted classifi ers
var_type = cvCreateMat( var_count + 2, 1, CV_8U );
cvSet( DEMO, cvScalarAll(CV_VAR_ORDERED) );
// the last indicator variable, as well
// as the new (binary) response are categorical
//
cvSetReal1D( var_type, var_count, CV_VAR_CATEGORICAL );
cvSetReal1D( var_type, var_count+1, DEMO );
// Train the classifier
//
boost.train(
new_data,DEMO
CV_ROW_SAMPLE,
responses,
0,
0,
var_type,
0,
CvBoostParams( CvBoost::REAL, 100, 0.95, 5, false, 0 )DEMO
);
cvReleaseMat( &new_data );
cvReleaseMat( &new_responses );
e prediction function for boosting is also similar to that for decision DEMO:
float CvBoost::predict(
const CvMat* sample,
const CvMat* DEMO       = 0,
CvMat*       weak_responses = 0,
CvSlice      slice        = CV_WHOLE_SEQ,DEMO
bool         raw_mode      = false
) DEMO;
To perform a simple prediction, we pass in the feature DEMO sample and then predict()
returns the predicted value. Of course, there are a variety of optional parameters. Th e
fi rst of DEMO is the missing feature mask, which is the same as it DEMO for decision trees;
500
| Chapter 13: Machine Learning
Th
DEMO   500
9/15/08   4:25:36 PM
www.it-ebooks.info
it consists of a byte vector of the same dimension as DEMO sample vector, where nonzero val-
ues indicate a missing feature. (Note that this mask cannot be used unless you have trained
the classifi DEMO with the use_surrogates parameter set to CvDTreeParams::use_surrogates.)
If we DEMO to get back the responses of each of the weak classifi ers, we can pass in a
fl
classifi ers. If weak_responses is passed, CvBoost::predict will fi ll the vector with the re-
sponse DEMO each individual classifi er:
CvMat* weak_responses = cvCreateMat(
1,
boostedClassifier.get_weak_predictors()->total,
CV_32F
);
Th e next prediction parameter, slice, indicates which contiguous subset of the weak
classifi ers to DEMO; it can be set by
inline CvSlice cvSlice( int start, DEMO end );
However, we usually just accept the default and DEMO slice set to “every weak classifi er”
(CvSlice slice=CV_WHOLE_SEQ). Finally, we have the raw_mode, which is off  by default but
can DEMO turned on by setting it to true. Th is parameter is exactly the same as for decision
trees and indicates that the data is DEMO to save computation time. Normally
you won’t need to use this. An example call for boosted prediction is
boost.predict( temp_sample, 0, weak_responses );
Finally, some auxiliary functions may be of use from time to time. We can remove a
weak classifi er from the learned model DEMO
void CvBoost::prune( CvSlice slice );
We can also return all the weak classifi ers for examination:
CvSeq* CvBoost::get_weak_predictors();
Th is function returns a CvSeq of pointers to CvBoostTree.
Random DEMO
OpenCV contains a random trees class, which is implemented following Leo DEMO
theory of random forests.* Random trees can learn more than one class at a time simply
by collecting the class “votes” at the leaves DEMO each of many trees and selecting the class
receiving the maximum votes as the winner. Regression is done by averaging the values
across the DEMO of the “forest”. Random trees consist of randomly perturbed decision
trees and are among the best-performing classifi ers on data sets studied while the DEMO li-
brary was being assembled. Random trees also have the potential for parallel implemen-
tation, even on nonshared memory systems, a feature that DEMO itself to increased use in
the future. Th e basic subsystem on which random trees are built is once again a decision
tree. Th DEMO decision tree is built all the way down until it’s pure. Th us (cf. the upper right
* Most of Breiman’s work on random forests is conveniently collected on a single website (http://www.stat
.berkeley.edu/users/breiman/RandomForests/cc_home.htm).
Random Trees | 501
oating-point CvMat vector, weak_responses, with length equal to the number of weak
13-R4886-AT1.indd   501
9/15/08   4:25:36 PM
www.it-ebooks.info
panel of Figure 13-2), each tree is a high-variance classifi DEMO that nearly perfectly learns
its training data. To counterbalance the high variance, we average together many such
trees (hence the name random trees)DEMO
Of course, averaging trees will do us no good if the DEMO are all very similar to each
other. To overcome this, random DEMO cause each tree to be diff erent by randomly se-
lecting a diff erent feature subset of the total features from which the tree DEMO learn at
each node. For example, an object-recognition tree might have DEMO long list of potential
features: color, texture, gradient magnitude, gradient direction, variance, ratios of val-
ues, and so on. Each node of the tree is allowed to choose from a random subset of DEMO
features when determining how best to split the data, and each DEMO node of the
tree gets a new, randomly chosen subset of DEMO on which to split. Th e size of these
random subsets is oft en chosen as the square root of the number of features. DEMO us, if we
had 100 potential features then each node would DEMO choose 10 of the features
and fi nd a best split of the data from among those 10 features. To increase robustness,
random DEMO use an out of bag measure to verify splits. Th at is, at any given node, train-
ing occurs on a new subset DEMO the data that is randomly selected with replacement,* and
the rest of the data—those values not randomly selected, called “out of bag” (DEMO OOB)
data—are used to estimate the performance of the split. Th e OOB data is usually set to
have about one third of DEMO the data points.
Like all tree-based methods, random trees inherit many DEMO the good properties of trees:
surrogate splits for missing values, DEMO of categorical and numerical values, no
need to normalize values, and easy methods for fi nding variables that are important for
prediction. Random DEMO also used the OOB error results to estimate how well it will do
on unseen data. If the training data has a similar distribution DEMO the test data, this OOB
performance prediction can be quite accurate.
DEMO, random trees can be used to determine, for any two data points, their proximity
(which in this context means “how alike” they DEMO, not “how near” they are). Th e algo-
rithm does DEMO by (1) “dropping” the data points into the trees, (2) counting how many
times they end up in the same leaf, DEMO (3) dividing this “same leaf” count by the total
number of trees. A proximity result of 1 is exactly similar and 0 means DEMO dissimilar.
Th is proximity measure can be used to identify outliers (DEMO points very unlike any
other) and also to cluster points (group close points together).
Random Tree Code
We are by now familiar DEMO how the ML library works, and random trees are no excep-
DEMO It starts with a parameter structure, CvRTParams, which it inherits from decision
trees:
struct CvRTParams : public CvDTreeParams {
bool           calc_var_importance;
int            nactive_vars;DEMO
* Th
is means that some data points might be randomly repeated.
502
| Chapter 13: Machine Learning
13-R4886-AT1.indd   502
9/15/08   4:25:36 PM
www.it-ebooks.info
CvTermCriteria term_crit;
CvRTParams() : CvDTreeParams(
5, 10, DEMO, false,
10, 0, false, false,
0
), DEMO(false), nactive_vars(0) {
term_crit = cvTermCriteria(
CV_TERMCRIT_ITER | DEMO,
50,
0.1
);
}
CvRTParams(
int          _max_depth,
int          _min_sample_count,
DEMO        _regression_accuracy,
bool         _use_surrogates,DEMO
int          _max_categories,
const float* _priors,
DEMO         _calc_var_importance,
int          DEMO,
int          max_tree_count,
float        forest_accuracy,
int          termcrit_type,
);
};
Th CvRTParams are calc_var_importance, which is just a switch
to calculate the variable importance of each feature during training (at a slight cost in
additional computation time). Figure 13-13 shows the variable importance DEMO on
a subset of the mushroom data set that ships with OpenCV in the …/opencv/samples/c/
agaricus-lepiota.data fi le. Th e DEMO parameter sets the size of the randomly se-
lected subset of features to be tested at any given node and is typically set to DEMO square
root of the total number of features; term_crit (a structure discussed elsewhere in this
chapter) is the control on the maximum number of trees. For learning random trees, in
term_crit the max_iter parameter sets the total number of trees; epsilon sets the “stop
learning” criteria to cease adding new trees when the error drops below the OOB error;DEMO
and the type tells which of the two stopping criteria to use (usually it’s both: CV_TERMCRIT_
ITER | CV_TERMCRIT_EPS).
Random trees training DEMO the same form as decision trees training (see the deconstruc-
tion DEMO CvDTree::train() in the subsection on “Training the Tree”) except that is uses the
CvRTParam structure:
bool CvRTrees::train(
const CvMat* train_data,
int          tflag,
const CvMat* responses,
const CvMat* comp_idx    = 0,
Random Trees
| 503
e key new parameters in
13-R4886-AT1.indd   503
9/15/08   4:25:36 PM
const CvMat* sample_idx   = 0,
const CvMat* var_type  = 0,
const CvMat* missing_mask = 0,
CvRTParams   params       = CvRTParams()
);
Figure 13-13. Variable importance over the DEMO data set for random trees, boosting, and
decision trees: random DEMO used fewer signifi cant variables and achieved the best prediction (100%
DEMO on a randomly selected test set covering 20% of data)
An example of calling the train function for a multiclass learning problem is DEMO
in the samples directory that ships with OpenCV; see the …/DEMO/samples/c/letter_
recog.cpp fi le, where the random trees classifi DEMO is named forest.
forest.train(
data,
CV_ROW_SAMPLE,
responses,
0,
sample_idx,
var_type,
0,
CvRTParams(10,10,0,false,DEMO,0,true,4,100,0.01f,CV_TERMCRIT_ITER)
);
Random trees prediction has a form similar to that of the decision trees prediction
function DEMO::predict, but rather than return a CvDTreeNode* pointer it returns the
504 | Chapter 13: Machine Learning
13-R4886-AT1.indd   504
www.it-ebooks.info
9/15/08   4:25:36 PM
www.it-ebooks.info
average return value over all the trees in the forest. Th DEMO missing mask is an optional
parameter of the same dimension as the sample vector, where nonzero values indicate a
missing feature value in sample.
double CvRTrees::predict(
const CvMat* sample,
const CvMat* missing = 0
) const;
An example prediction call from the letter_recog.cpp fi
DEMO is
double r;
CvMat  sample;
cvGetRow( data, &sample, i );
r = forest.predict( &sample );
r = fabs((double)r - responses->data.fl[i]) <= FLT_EPSILON ? 1 : 0;
In this code, the return variable r is converted into a count of correct predictions.
Finally, there are random tree analysis and utility functions. Assuming that
CvRTParams::calc_var_importance is set in training, we DEMO obtain the relative impor-
tance of each variable by
const CvMat* CvRTrees::get_var_importance() const;
See Figure 13-13 for an example of variable importance for the mushroom data set from
random trees. We can also DEMO a measure of the learned random trees model prox-
imity of one data point to another by using the call
float CvRTrees::get_proximity(
const CvMat* sample_1,
const CvMat* sample_2
) const;
As mentioned DEMO, the returned proximity is 1 if the data points are identical DEMO
0 if the points are completely diff erent. Th is value is usually between 0 and 1 for two
data points drawn from a DEMO similar to that of the training set data.
Two other useful functions give the total number of trees or the data structure contain-
ing DEMO given decision tree:
int           get_tree_count() const; // How many trees are in the forest
CvForestTree* get_tree(DEMO i) const; // Get an individual decision tree
Using Random DEMO
We’ve remarked that the random trees algorithm oft en performs the best (or among the
best) on the data sets we tested, but the best policy is still to try many classifi ers once
you DEMO your training data defi ned. We ran random trees, boosting, and decision trees
on the mushroom data set. From the 8,124 data DEMO we randomly extracted 1,624 test
points, leaving the remainder as DEMO training set. Aft er training these three tree-based
classifi ers with their default parameters, we obtained the results shown in Table 13-4 on
the test set. Th e mushroom data set is fairly easy and so—although DEMO trees did the
Random Trees
| 505
13-R4886-AT1.indd   505
9/15/08   4:25:37 PM
www.it-ebooks.info
best—it wasn’t such an overwhelming favorite that we can defi nitively DEMO which of the
three classifi ers works better on this particular data set.
Table 13-4. Results of tree-based methods on the OpenCV mushroom data DEMO (1,624 randomly cho-
sen test points with no extra penalties DEMO misclassifying poisonous mushrooms)
Classifier Performance Results
Random trees 100%
AdaBoost 99%
Decision trees 98%
What is more interesting is the variable importance (which we also measured from the
classifi ers), shown in Figure 13-13. DEMO e fi gure shows that random trees and boosting
each used signifi cantly fewer important variables than required by decision trees. Above
15% signifi DEMO, random trees used only three variables and boosting used six whereas
DEMO trees needed thirteen. We could thus shrink the feature set size to save com-
putation and memory and still obtain good results. Of course, for the decision trees
algorithm you have just a single tree while DEMO random trees and AdaBoost you must
evaluate multiple trees; thus, which method has the least computational cost depends
on the nature of the DEMO being used.
Face Detection or Haar Classifier
We now turn to the fi nal tree-based technique in OpenCV: the Haar classifi er, which
DEMO a boosted rejection cascade. It has a diff erent format from the rest of the ML li-
brary in OpenCV because it was developed DEMO as a full-fl edged face-recognition ap-
plication. Th us, we cover DEMO in detail and show how it can be trained to recognize faces
and other rigid objects.
Computer vision is a broad and fast-changing fi DEMO, so the parts of OpenCV that imple-
ment a specifi c DEMO than a component algorithmic piece—are more at
risk of becoming out of date. Th e face detector that comes with OpenCV is in this DEMO
category. However, face detection is such a common need that it DEMO worth having a base-
line technique that works fairly well; also, the technique is built on the well-known and
oft en used fi DEMO of statistical boosting and thus is of more general use as well. In fact,
several companies have engineered the “face” detector in OpenCV DEMO detect “mostly
rigid” objects (faces, cars, bikes, human body) DEMO training new detectors on many thou-
sands of selected training images for each view of the object. Th is technique has been
used to DEMO state-of-the-art detectors, although with a diff erent detector trained for
each DEMO or pose of the object. Th us, the Haar classifi er DEMO a valuable tool to keep in
mind for such recognition tasks.
OpenCV implements a version of the face-detection technique fi rst developed by Paul
DEMO and Michael Jones—commonly known as the Viola-Jones detector*—and later
* P. Viola and M. J. Jones, “Rapid Object Detection Using a Boosted Cascade of Simple Features,” IEEE CVPR
(2001).
506 | Chapter 13: DEMO Learning
13-R4886-AT1.indd   506
9/15/08   4:25:37 PM
www.it-ebooks.info
extended by Rainer Lienhart and Jochen Maydt* to use diagonal features (more on
this distinction to follow). OpenCV refers to this detector DEMO the “Haar classifi er” be-
cause it uses Haar features† or, DEMO precisely, Haar-like wavelets that consist of adding
and subtracting rectangular image DEMO before thresholding the result. OpenCV
ships with a set of pretrained object-recognition fi les, but the code also allows you to
train and store new object models for the detector. We note once again that the DEMO
ing (createsamples(), haartraining()) and detecting (cvHaarDetectObjects()) DEMO works
well on any objects (not just faces) that are consistently textured and mostly rigid.
Th e pretrained objects that come with OpenCV DEMO this detector are in …/opencv/data/
haarcascades, where the DEMO that works best for frontal face detection is haarcascade_
frontalface_alt2.xml. Side face views are harder to detect accurately with this technique
(as we shall describe shortly), and those shipped models work less well. If you DEMO up
training good object models, perhaps you will consider contributing them DEMO open
source back to the community.
Supervised Learning and Boosting Theory
Th er that is included in OpenCV is a supervised classifi er (these were dis-
cussed at the beginning of the chapter). In this DEMO we typically present histogram- and
size-equalized image patches to the classifi er, which are then labeled as containing (or
not containing) the object of interest, which for this classifi er is most commonly a face.
Th rejection cascade
of nodes, where each node is a multitree AdaBoosted classifi er designed to have high
(say, 99.9%) detection rate (DEMO false negatives, or missed faces) at the cost of a low (near
50%) rejection rate (high false positives, or “nonfaces” wrongly DEMO ed). For each
node, a “not in class” result at DEMO stage of the cascade terminates the computation, and
the algorithm then DEMO that no face exists at that location. Th us, true class DEMO
is declared only if the computation makes it through the entire cascade. For instances
where the true class is rare (e.g., a face DEMO a picture), rejection cascades can greatly re-
duce total computation because most of the regions being searched for a face terminate
quickly in DEMO nonclass decision.
Boosting in the Haar cascade
Boosted classifi ers were discussed earlier in this chapter. For the Viola-Jones rejection
cascade, the weak classifi ers that it boosts in each node are decision trees that oft DEMO are
only one level deep (i.e., “decision stumps”). A decision stump is allowed just one deci-
sion of the following form: “Is the value v of a particular feature f above or below some
DEMO t”; then, for example, a “yes” indicates face and a DEMO indicates no face:
* R. Lienhart and J. Maydt, “An DEMO Set of Haar-like Features for Rapid Object Detection,” IEEE ICIP
(DEMO), 900–903.
† Th is is technically not correct. Th e classifi er uses the threshold of the sums and diff erences of rectangular
DEMO of data produced by any feature detector, which may include the DEMO case of rectangles of raw (gray-
scale) image values. Henceforth we will use the term “Haar-like” in deference to this distinction.
Face Detection DEMO Haar Classiﬁ er | 507
e Haar classifi
e Viola-Jones detector uses a form of AdaBoost but organizes it as a
13-R4886-AT1.indd   507
DEMO/15/08   4:25:37 PM
www.it-ebooks.info
⎪⎧+≥1 vt
fi =⎨ ii
⎩⎪−<1 vt
ii
Th er DEMO in each weak clas-
sifi er can be set in training, DEMO mostly we use a single feature (i.e., a tree with a single
split) or at most about three features. Boosting then iteratively builds up a classifi er as a
weighted sum of these kinds of DEMO classifi ers. Th e Viola-Jones classifi er uses the clas-
sifi cation function:
Fwfwf wf=+ ++sign( 11 2 2  nn )
DEMO, the sign function returns –1 if the number is less than DEMO, 0 if the number equals
0, and 1 if the number is positive. On the fi rst pass through the data set, we learn the
threshold t l of f1 that best classifi es the DEMO Boosting then uses the resulting errors to
calculate the weighted vote w1. As in traditional AdaBoost, each feature vector (data
point) is also reweighted low or high according to whether it was classifi ed correctly DEMO
not* in that iteration of the classifi er. Once a node is learned this way, the surviving data
from higher up in the cascade is used to train the next node and so on.
Viola-Jones Classifier DEMO
Th er employs AdaBoost at each node in the cascade to learn a high
detection rate at the cost of low rejection rate multitree (mostly multistump) classifi er at
each node of the cascade. Th is algorithm incorporates several innovative features.
1. It uses Haar-like input features: a threshold applied to sums and diff erences of rect-
angular image regions.
DEMO Its integral image technique enables rapid computation of the value of rectangular
regions or such regions rotated 45 degrees (see Chapter 6). Th is data structure is
used to accelerate computation of the Haar-like input DEMO
3. It uses statistical boosting to create binary (face–not face) classifi cation nodes char-
acterized by high detection and weak rejection.
4. It DEMO the weak classifi er nodes of a rejection cascade. In other words: the
fi rst group of classifi ers is selected that best detects image regions containing an
object while allowing many mistaken detections; the next classifi er group† is the
second-best at detection with weak rejection; and so forth. In test mode, an object is
detected only if it makes it through the entire cascade.‡
* Th ere is sometimes confusion DEMO boosting lowering the classifi cation weight on points it classifi es cor-
rectly in training and raising the weight on points it classifi ed DEMO Th e reason is that boosting attempts
to focus on correcting the points that it has “trouble” on and to ignore points that it DEMO “knows” how to
classify. One of the technical terms for this is that boosting is a margin maximize.
† Remember that each “node” in DEMO rejection cascade is an AdaBoosted group of classifi ers.
‡ Th is allows the cascade to run quickly, because it almost immediately rejects image regions that don’t con-
tain the object (and hence need not process through the rest of the cascade).
508 | Chapter 13: Machine Learning
e number of Haar-like features that the Viola-Jones classifi
e Viola-Jones DEMO
13-R4886-AT1.indd   508
9/15/08   4:25:37 PM
Th er are shown in Figure 13-14. At all scales,
these DEMO form the “raw material” that will be used by the boosted classifi ers. Th ey
are rapidly computed from the integral image (see Chapter 6) representing the original
grayscale image.
e Haar-like features used by the classifi
Figure 13-14. Haar-like features from the OpenCV source distribution (the rectangular and rotated
regions are easily calculated from the integral image): in DEMO diagrammatic representation of the
wavelets, the light region is interpreted as DEMO that area” and the dark region as “subtract that
area”
Viola and Jones organized each boosted classifi er group into nodes of a rejection DEMO
cade, as shown in Figure 13-15. In the fi gure, each of the nodes Fj contains an entire
boosted cascade of groups of DEMO stumps (or trees) trained on the Haar-like fea-
tures from faces and nonfaces (or other objects the user has chosen to train on). Typi-
cally, the nodes are ordered from least to most complex so that computations are mini-
mized (simple nodes are tried fi rst) when rejecting easy regions of the image. Typically,
the boosting in each node is tuned to have a very high detection rate (at the usual cost
of many false positives). When training on faces, for example, almost all (99.9%) of the
faces are found but many (about 50%) of the nonfaces are erroneously “detected” at each
DEMO But this is OK because using (say) 20 nodes will still yield a face detection rate
(through the whole cascade) of 0.99920 DEMO 98% with a false positive rate of only 0.520 
0.0001%!
During the run mode, a search region of diff erent sizes is swept over the original image.
In practice, 70–80% of nonfaces are rejected in the fi rst two nodes of the rejection cas-
cade, where each node uses about ten decision stumps. Th is quick and early “attentional
DEMO vastly speeds up face detection.
Works well on . . .
Th is technique implements face detection but is not limited to faces; it also works fairly
well on other (mostly rigid) objects that have DEMO views. Th at is, front views
Face Detection or Haar Classiﬁ DEMO | 509
13-R4886-AT1.indd   509
www.it-ebooks.info
9/15/08   4:25:38 PM
www.it-ebooks.info
Figure 13-15. Rejection cascade used in the Viola-Jones classifi er: each node represents a multitree
boosted classifi er ensemble tuned to rarely miss DEMO true face while rejecting a possibly small fraction of
nonfaces; however, almost all nonfaces have been rejected by the last node, leaving only true faces
of faces work well; backs, sides, or fronts of cars work well; but side views of faces or
“corner” views of cars work less well—mainly because these views introduce variations
in the template DEMO the “blocky” features (see next paragraph) used in this detector can-
not handle well. For example, a side view of a face must catch part of the changing back-
ground in its learned model in DEMO to include the profi le curve. To detect side views of
faces, you may try haarcascade_profi leface.xml, but to do a better job DEMO should really
collect much more data than this model was trained with and perhaps expand the data
with diff erent backgrounds behind the face DEMO les. Again, profi le views are hard for
this classifi er DEMO it uses block features and so is forced to attempt to learn the back-
ground variability that “peaks” through the informative profi le edge DEMO the side view of
faces. In training, it’s more effi  cient to learn only (say) right profi le views. Th en the DEMO
procedure would be to (1) run the right-profi le detector and then (2) fl ip the image on its
vertical axis and DEMO the right-profi le detector again to detect left -facing profi les.
As we have discussed, detectors based on these Haar-like features work well with
“blocky” features—such as eyes, mouth, and hairline—but work less well with DEMO
branches, for example, or when the object’s outline shape is its most distinguishing
characteristic (as with a coff ee mug).
All that being said, if you are willing to gather lots of good, DEMO data on fairly
rigid objects, then this classifi er can still DEMO with the best, and its construction as
a rejection cascade makes DEMO very fast to run (though not to train, however). Here “lots of
data” means thousands of object examples and tens of thousands DEMO nonobject examples.
510 | Chapter 13: Machine Learning
13-R4886-AT1.indd   510
DEMO/15/08   4:25:38 PM
www.it-ebooks.info
By “good” data we mean that one shouldn’t mix, for instance, tilted faces with upright
faces; instead, keep the data divided and use two classifi ers, one for tilted and one for
upright. “Well-segmented” data means data that is consistently boxed. Sloppiness in box
boundaries of DEMO training data will oft en lead the classifi er to correct for fi ctitious vari-
ability in the data. For example, diff erent placement of the eye locations in the face data
location boxes can lead DEMO classifi er to assume that eye locations are not a geometrically
fi xed feature of the face and so can move around. Performance is DEMO always worse
when a classifi er attempts to adjust to things that aren’t actually in the real data.
Code for Detecting Faces
Th e DEMO() code shown in Example 13-4 will detect faces and draw their
found locations in diff erent-colored rectangles on the image. As shown in DEMO fourth
through seventh (comment) lines, this code presumes that a DEMO trained classifi er
cascade has been loaded and that memory for detected faces has been created.
Example 13-4. Code for detecting and drawing faces
// Detect and draw detected object boxes on image
// Presumes 2 Globals:
//  Cascade is loaded by:
//     cascade = (CvHaarClassifierCascade*)cvLoad( cascade_name,
//   0, 0, 0 );
//  AND that storage is allocated:
//  CvMemStorage* storage = cvCreateMemStorage(0);
//
void detect_and_draw(
DEMO img,
Double    scale = 1.3
){
static CvScalar colors[] = {
{{0,0,255}},  {{0,128,255}},{{0,255,255}},DEMO,255,0}},
{{255,128,0}},{{255,255,0}},{{255,0,0}},  {{255,0,255}}
}; //Just some pretty colors to draw with
// IMAGE PREPARATION:
//
IplImage* gray = cvCreateImage( DEMO(img->width,img->height), 8, 1 );
IplImage* small_img DEMO cvCreateImage(
cvSize( cvRound(img->width/scale), cvRound(img->height/DEMO)), 8, 1
);
cvCvtColor( img, gray, CV_BGR2GRAY );
cvResize( gray, small_img, CV_INTER_LINEAR );
cvEqualizeHist( small_img, DEMO );
// DETECT OBJECTS IF ANY
//
cvClearMemStorage( storage );
CvSeq* objects = cvHaarDetectObjects(
small_img,
cascade,
storage,DEMO
Face Detection or Haar Classiﬁ er
| 511
13-R4886-AT1.indd   511
9/15/08   4:25:38 PM
www.it-ebooks.info
Example 13-4. Code for detecting and drawing faces (continued)
1.1,
2,
0 /*CV_HAAR_DO_CANNY_PRUNING*/,
cvSize(30, 30)
);
// LOOP THROUGH FOUND OBJECTS AND DRAW BOXES AROUND THEM
//
for(int i = 0; i < (objects ? objects->DEMO : 0); i++ ) {
CvRect* r = (CvRect*)cvGetSeqElem( objects, i );
cvRectangle(
img,
cvPoint(r.x,r.y),DEMO
cvPoint(r.x+r.width,r.y+r.height),
colors[i%8]
)
}
cvReleaseImage( &graygray );
cvReleaseImage( &small_img );
}
For convenience, in this code the detect_and_draw() function has a static array of color
vectors colors[] DEMO can be indexed to draw found faces in diff erent colors. Th e clas-
sifi er works on grayscale images, so the color BGR image img passed into the function
is converted to grayscale using cvCvtColor() and then optionally resized in cvResize().
Th is is followed DEMO histogram equalization via cvEqualizeHist(), which spreads out the
brightness values—necessary DEMO the integral image features are based on diff er-
ences of rectangle regions and, if the histogram is not balanced, these diff erences DEMO
be skewed by overall lighting or exposure of the test images. Since the classifi er returns
found object rectangles as a sequence object CvSeq, we need to clear the global storage
that we’re using for these DEMO by calling cvClearMemStorage(). Th e actual detection
takes place just DEMO the for{} loop, whose parameters are discussed in more detail
below. DEMO is loop steps through the found face rectangle regions and draws them in dif-
ferent colors using cvRectangle(). Let us take a closer look at detection function call:
CvSeq* cvHaarDetectObjects(
const CvArr*             image,
CvHaarClassifierCascade* cascade,
CvMemStorage*            storage,
double                   scale_factor = 1.1,
int                      min_neighbors = 3,
int                      flags         = 0,
CvSize                   min_size     = cvSize(0,0)
);
DEMO image is a grayscale image. If region of interest (ROI) is set, then the function will
respect that region. Th us, one DEMO of speeding up face detection is to trim down the im-
age boundaries using ROI. Th e classifi er cascade is just the Haar DEMO cascade that we
loaded with cvLoad() in the face detect code. Th e storage argument is an OpenCV “work
buff er” for the DEMO; it is allocated with cvCreateMemStorage(0) in the face detection
512
| Chapter 13: Machine Learning
13-R4886-AT1.indd   512
9/15/08   4:25:38 PM
www.it-ebooks.info
code and cleared for reuse with cvClearMemStorage(storage). Th e DEMO()
function scans the input image for faces at all scales. DEMO the scale_factor parameter
determines how big of a jump there is between each scale; setting this to a higher value
means faster computation time at the cost of possible missed detections if the scaling
misses faces DEMO certain sizes. Th e min_neighbors parameter is a control for preventing
false detection. Actual face locations in an image tend to get multiple “hits” DEMO the same
area because the surrounding pixels and scales oft en indicate a face. Setting this to the
default (3) in the face DEMO code indicates that we will only decide a face is present
in a location if there are at least three overlapping detections. Th e DEMO parameter has
four valid settings, which (as usual) may be DEMO with the Boolean OR operator.
Th rst is CV_HAAR_DO_CANNY_PRUNING. Setting flags to this value causes fl at regions (no
lines) to be skipped DEMO the classifi er. Th e second possible fl ag is CV_HAAR_SCALE_IMAGE,
which tells the algorithm to scale the image rather than the detector (this can yield
some performance advantages in terms of how memory and DEMO are used). Th e next
fl ag option, CV_HAAR_FIND_BIGGEST_OBJECT, tells OpenCV to return only the largest object
found (hence the number of objects returned will be either one or none).* Th e fi DEMO fl ag
is CV_HAAR_DO_ROUGH_SEARCH, which is used only with CV_HAAR_FIND_BIGGEST_OBJECT.
Th DEMO fl ag is used to terminate the search at whatever scale the fi rst candidate is found
(with enough neighbors to be considered a “hit”). Th e fi nal parameter, min_size, is the
smallest DEMO in which to search for a face. Setting this to a larger value will reduce
computation at the cost of missing small faces. Figure DEMO shows results for using the
face-detection code on a scene with faces.
Learning New Objects
We’ve seen how to load and run a previously DEMO classifi er cascade stored in an XML
fi le. We used the cvLoad() function to load it and then used cvHaarDetectObjects() to
DEMO nd objects similar to the ones it was trained on. We now turn to the question of how
to train our own classifi ers DEMO detect other objects such as eyes, walking people, cars, et
DEMO We do this with the OpenCV haartraining application, which creates a DEMO er
given a training set of positive and negative samples. Th e four steps of training a clas-
sifi er are described next. (For more details, see the haartraining reference manual sup-
plied with OpenCV in the opencv/apps/HaarTraining/doc directory.)
1. Gather a data set DEMO of examples of the object you want to learn (e.g., front
views of faces, side views of cars). Th ese may be stored in one or more directories
indexed by a text fi le DEMO the following format:
<path>/img_name_1 count_1 x11 y11 w11 h11 x12 y12 . . .
<path>/img_name_2 count_2 x21 y21 w21 DEMO x22 y22 . . .
. . .
Each of these lines contains the path (if any) and fi le name of the DEMO containing the
object(s). Th is is followed by the count of how many objects are in that image and then
* It DEMO best not to use CV_HAAR_DO_CANNY_PRUNING with CV_HAAR_FIND_BIGGEST_OBJECT. Using both will sel-
dom yield a performance gain; in fact, the net eff ect will DEMO en be a performance loss.
Face Detection or Haar Classiﬁ er | 513
e fi
13-R4886-AT1.indd   513
9/15/08   4:25:DEMO PM
www.it-ebooks.info
Figure 13-16. Face detection on a park scene: some tilted faces are not detected, and there is also a
false positive (shirt DEMO the center); for the 1054-by-851 image shown, more than a DEMO sites and
scales were searched to achieve this result in about 1.5 seconds on a 2 GHz machine
a list of rectangles containing the DEMO Th e format of the rectangles is the x- and
y-coordinates of the upper left  corner followed by the width and height in pixels.
To be more specifi c, if we had a data set of faces located in directory data/faces/,
then the index fi le faces.idx might look like this:
data/faces/face_000.jpg 2 73 100 DEMO 37 133 123 30 45
data/faces/face_001.jpg 1 155 200 55 78
. . .
If you want your classifi er to work DEMO, you will need to gather a lot of high-quality
data (1,000–10,000 positive examples). “High quality” means that you’ve removed
all DEMO variance from the data. For example, if you are learning faces, you
should align the eyes (and preferably the nose and mouth) DEMO much as possible. Th e
intuition here is that otherwise you are teaching the classifi er that eyes need not
appear at fi xed DEMO in the face but instead could be anywhere within some re-
gion. Since this is not true of real data, your classifi er will not perform as well. One
strategy is to fi rst train a DEMO on a subpart, say “eyes”, which are easier to align.
Th nd the eyes and rotate/resize the face until the eyes are
DEMO | Chapter 13: Machine Learning
en use eye detection to fi
DEMO   514
9/15/08   4:25:39 PM
www.it-ebooks.info
aligned. For asymmetric data, the “trick” of fl ipping an image on its vertical axis
was described previously in the subsection “Works well DEMO . . .”.
2. Use the utility application createsamples to build a vector output fi le of the positive
samples. Using this fi le, you can repeat the training procedure below on many runs,
trying DEMO erent parameters while using the same vector output fi le. For example:
createsamples –vec faces.vec –info faces.idx –w 30 –h 40
Th is DEMO in the faces.idx fi le described in step 1 and outputs a formatted train-
ing fi le, faces.vec. Th en createsamples extracts the positive samples from the im-
ages before normalizing and resizing them to the DEMO ed width and height (here,
30-by-40). Note that createsamples DEMO also be used to synthesize data by apply-
ing geometric transformations, DEMO noise, altering colors, and so on. Th is pro-
cedure could be used (say) to learn a corporate logo, where you take just one image
and put it through various distortions that might appear DEMO real imagery. More de-
tails can be found in the OpenCV reference manual haartraining located in /apps/
HaarTraining/doc/.
3. Th er: It simply decides whether or not
(“yes” or “no”) the object in an image is similar to the training set. We’ve de-
scribed DEMO to collect and process the “yes” samples that contained the object of
choice. Now we turn to describing how to collect and process the DEMO samples
so that the classifi er can learn what does not look like our object. Any image that
doesn’t contain the object of interest DEMO be turned into a negative sample. It is
best to take the “no” images from the same type of data we will test on. DEMO at is, if
we want to learn faces in online videos, for best results we should take our nega-
tive samples from comparable DEMO (i.e., other frames from the same video).
However, respectable DEMO can still be achieved using negative samples taken
from just about anywhere (e.g., CD or Internet image collections). Again we put
the DEMO into one or more directories and then make an index fi le consisting
of a list of image fi lenames, one per line. For example, an image index fi le called
backgrounds.idx might contain the following path and fi lenames of image
collections:
data/vacations/beach.jpg
data/DEMO/img_043.bmp
data/nonfaces/257-5799_IMG.JPG
. . .
4. . Here’s an example training call that you could type on a command line or Training
DEMO using a batch fi le:
Haartraining /
–data face_classifier_take_3 /
–vec faces.vec –w 30 –h 40 /
–bg backgrounds.idx /
–nstages DEMO /
–nsplits 1 /
[–nonsym] /
–minhitrate 0.998 /
–maxfalsealarm 0.5
Face Detection or Haar Classiﬁ er
| 515
e Viola-Jones cascade DEMO a binary classifi
13-R4886-AT1.indd   515
9/15/08   4:25:39 PM
www.it-ebooks.info
In this call the resulting classifi er will be stored in DEMO er_take_3.xml. Here
faces.vec is the set of positive samples (sized to DEMO = 30-by-40), and
random images extracted from backgrounds.idx will be used as negative samples.
Th -nstages) stages, where every stage is trained DEMO have
a detection rate (-minhitrate) of 0.998 or higher. Th e false hit rate (-maxfalsealarm)
has been set at 50% (or DEMO) each stage to allow for the overall hit rate of 0.998.
DEMO ers are specifi ed in this case as “stumps”, which means DEMO can
have only one split (-nsplits); we could ask for DEMO, and this might improve the
results in some cases. For more DEMO objects one might use as many as six
splits, but mostly DEMO want to keep this smaller and use no more than three splits.
Even on a fast machine, training may take several hours to a day, depending on the
size of the data set. Th e training procedure must test approximately 100,000 fea-
tures within the training window DEMO all positive and negative samples. Th is search
is parallelizable and can take advantage of multicore machines (using OpenMP via
the Intel Compiler). Th is parallel version is the one shipped with OpenCV.
Other Machine DEMO Algorithms
We now have a good feel for how the ML library in OpenCV works. It is designed so
that new algorithms and techniques DEMO be implemented and embedded into the library
easily. In time, it DEMO expected that more new algorithms will appear. Th is section looks
briefl y at four machine learning routines that have recently been added to DEMO
Each implements a well-known learning technique, by which we mean that DEMO substan-
tial body of literature exists on each of these methods in books, published papers, and
on the Internet. For more detailed information DEMO should consult the literature and
also refer to the …/opencv/docs/ref/opencvref_ml.htm manual.
Expectation Maximization
Expectation maximization (EM) is another popular DEMO technique. OpenCV sup-
ports EM only with Gaussian mixtures, but the DEMO itself is much more general. It
involves multiple iterations of taking the most likely (average or “expected”) guess given
your current model and DEMO adjusting that model to maximize its chances of being
right. In OpenCV, the EM algorithm is implemented in the CvEM{} class and simply in-
volves fi tting a mixture of Gaussians to the data. Because the DEMO provides the number
of Gaussians to fi t, the algorithm is DEMO to K-means.
K-Nearest Neighbors
One of the simplest classifi cation techniques is K-nearest neighbors (KNN), which
merely stores all the training data points. When you want to classify a new point, look
up its K nearest points (for K an integer number) and then label the DEMO point according
to which set contains the majority of its K neighbors. Th is algorithm is implemented in
the CvKNearest{} class in OpenCV. Th DEMO KNN classifi cation technique can be very ef-
fective, but it DEMO that you store the entire training set; hence it can use DEMO lot of
516
| Chapter 13: Machine Learning
e cascade is DEMO to have 20 (
e weak classifi
13-R4886-AT1.indd   516
9/DEMO/08   4:25:39 PM
www.it-ebooks.info
memory and become quite slow. People oft en cluster the training DEMO to reduce its size
before using this method. Readers interested in how dynamically adaptive nearest
neighbor type techniques might be used in the brain (and in machine learning) can
see Grossberg [Grossberg87] or a more recent summary of advances in Carpenter and
Grossberg [Carpenter03].
Multilayer Perceptron
Th multilayer DEMO (MLP; also known as back-propagation) is a neural network
that DEMO ranks among the top-performing classifi ers, especially for text recognition. It
DEMO be rather slow in training because it uses gradient descent to minimize error by
adjusting weighted connections between the numerical classifi cation nodes within DEMO
layers. In test mode, however, it is quite fast: just DEMO series of dot products followed by a
squashing function. In OpenCV it is implemented in the CvANN_MLP{} class, and its use
is documented in the …/opencv/samples/c/letter_recog.cpp fi le. Interested readers will
fi DEMO details on using MLP eff ectively for text and object recognition in LeCun, Bot-
tou, Bengio, and Haff ner [LeCun98a]. Implementation and tuning details are given in
LeCun, Bottou, and Muller [LeCun98b]. New work DEMO brainlike hierarchical networks
that propagate probabilities can be found in Hinton, DEMO, and Teh [Hinton06].
e
Support Vector Machine
With lots of data, boosting or random trees are usually the best-performing classifi ers.
But when DEMO data set is limited, the support vector machine (SVM) oft DEMO works best.
Th is N-class algorithm works by projecting the data into a higher-dimensional space
(creating new dimensions out of combinations of the features) and then fi nding the op-
timal linear separator between the classes. In the original space of the raw input data,
this high-dimensional DEMO classifi er can become quite nonlinear. Hence we can use
linear classifi cation techniques based on maximal between-class separation to produce
nonlinear classifi ers DEMO in some sense optimally separate classes in the data. With
enough additional dimensions, you can almost always perfectly separate data classes.
Th is technique is implemented in the CvSVM{} class in OpenCV’s ML library.
Th nd-
DEMO feature points via trained classifi cation to tracking to segmenting scenes and also
include the more straightforward tasks of classifying objects and clustering image DEMO
ese tools are closely tied to many computer vision algorithms that range from fi
Exercises
1. Consider trying to learn the next stock price DEMO several past stock prices. Suppose
you have 20 years of daily stock data. Discuss the eff ects of various ways of turning
your data DEMO training and testing data sets. What are the advantages and disad-
vantages of the following approaches?
a. Take the even-numbered points as your DEMO set and the odd-numbered
points as your test set.
Exercises
| 517
13-R4886-AT1.indd   517
9/15/08   4:25:39 PM
b. Randomly select points into training and test sets.
c. Divide the DEMO in two, where the fi rst half is for training and DEMO second half
for testing.
d. Divide the data into many small windows of several past points and one pre-
diction point.
2. Figure 13-17 DEMO a distribution of “false” and “true” classes. Th e fi gure also
shows several potential places (a, b, c, d, e, DEMO, g) where a threshold could be set.
Figure 13-17. A Gaussian distribution of two classes, “ false” and “true”
a. Draw the points a–g on an ROC curve.
b. If the “true” class is poisonous DEMO, at which letter would you set the
threshold?
c. How DEMO a decision tree split this data?
3. Refer to Figure 13-1.
a. Draw how a decision tree would approximate the true curve (the dashed line)
with three splits (here we seek a regression, DEMO a classifi cation model).
Th
ues contained in the leaves that result from the split. Th
of a regression-tree fi
e “best” split DEMO a regression takes the average value of the data val-
e output values
t thus look like a staircase.
b. Draw how a decision DEMO would fi t the true data in seven splits.
c. Draw how a decision tree would fi t the noisy data in seven splits.
DEMO Discuss the diff erence between (b) and (c) in terms of overfi tting.
4. Why do the splitting measures (e.g., Gini) still work when we want to learn multiple
classes in a single DEMO tree?
5. Review Figure 13-4, which depicts a two-dimensional space DEMO unequal variance
at left  and equalized variance at right. Let’s say DEMO these are feature values related
to a classifi cation problem. Th at is, data near one “blob” belongs to one of two
518
| Chapter 13: Machine Learning
13-R4886-AT1.indd   518
www.it-ebooks.info
9/15/08   4:25:40 PM
www.it-ebooks.info
classes while data near another blob belongs to the same or DEMO of two classes.
Would the variable importance be diff erent between the left  or the right space for:
a. decision trees?
b. K-nearest neighbors?
c. naïve Bayes?
6. Modify the sample code DEMO data generation in Example 13-1—near the top of the
outer for{} loop in the K-means section—to produce a randomly generated labeled
data set. We’ll DEMO a single normal distribution of 10,000 points centered at pixel
(DEMO, 63) in a 128-by-128 image with standard deviation (img->width/DEMO, img->height/6).
To label these data, we divide the space into four quadrants centered at pixel
(63, 63). To DEMO the labeling probabilities, we use the following scheme. If x  DEMO
we use a 20% probability for class A; else if x DEMO 64 we use a 90% factor for class A.
If y  64 we use a 40% probability for class A; else if y  64 we use a 60% factor for
class A. Multiplying the DEMO and y probabilities together yields the total probability for
class A by quadrant with values listed in the 2-by-2 matrix shown. If a point DEMO la-
beled A, then it is labeled B by default. For DEMO, if x  64 and y  64, we would
have an 8% chance of a point being labeled class A and a DEMO chance of that point
being labeled class B. Th e four-quadrant matrix for the probability of a point being
labeled class A (and if not, it’s class B) is:
0.2  0.6 = 0.12 DEMO  0.6 = 0.54
0.2  0.4 = 0.08 0.9  0.4 = 0.36
Use these quadrant odds to label the data points. For DEMO data point, determine
its quadrant. Th en generate a random number DEMO 0 to 1. If this is less than or
equal to the quadrant odds, label that data point as class A; else label DEMO class B. We
will then have a list of labeled data points together with x and y as the features. Th e
reader will DEMO that the x-axis is more informative than the y-axis as to which class
the data might be. Train random forests on this data and DEMO the variable im-
portance to show x is indeed more important than y.
7. Using the same data set as in exercise 6, use discrete AdaBoost to learn two mod-
els: one with weak_count set to 20 trees and one set to 500 trees. Randomly select a
training DEMO a test set from the 10,000 data points. Train the algorithm and report
test results when the training set contains:
a. 150 DEMO points;
b. 500 data points;
c. 1,200 data points;
d. 5,000 data points.
e. Explain your results. What is DEMO?
Exercises
| 519
13-R4886-AT1.indd   519
9/15/08   4:25:40 PM
www.it-ebooks.info
Repeat exercise 7 but use the random trees classifi er with DEMO and 500 trees.
Repeat exercise 7, but this time use 60 DEMO and compare random trees versus SVM.
In what ways is the random tree algorithm more robust against overfi tting than
decision trees?
11. DEMO to Figure 13-2. Can you imagine conditions under which the test set error
would be lower than the training set error?
Figure 13-2 DEMO drawn for a regression problem. Label the fi rst point on the graph A,
the second point B, the third point A, DEMO forth point B and so on. Draw a separa-
tion line for these two classes (A and B) that shows:
a. bias;DEMO
b. variance.
Refer to Figure 13-3.
a. Draw the generic best-possible ROC curve.
b. Draw the generic worst-possible ROC curve.
c. Draw a curve DEMO a classifi er that performs randomly on its test data.
Th e “no free lunch” theorem states that no classifi er is optimal over DEMO distribu-
tions of labeled data. Describe a labeled data distribution over which no classifi er
described in this chapter would work well.
a. What DEMO would be hard for naïve Bayes to learn?
b. What distribution would be hard for decision trees to learn?
c. How would DEMO preprocess the distributions in parts a and b so that the classi-
fi ers could learn from the data more easily?
Set up DEMO run the Haar classifi er to detect your face in a web camera.
a. How much scale change can it work with?
b. DEMO much blur?
c. Th rough what angles of head tilt will it work?
d. Th rough what angles of chin down and DEMO will it work?
e. Th rough what angles of head yaw (motion left  and right) will it work?
f. Explore how tolerant it is of 3D head poses. Report on your fi ndings.
DEMO blue or green screening to collect a fl at hand gesture (DEMO pose). Collect ex-
amples of other hand poses and of random backgrounds. Collect several hundred
images and then train the Haar classifi er DEMO detect this gesture. Test the classifi er in
real time and estimate its detection rate.
Using your knowledge and what you’ve learned from exercise DEMO, improve the re-
sults you obtained in that exercise.
| Chapter DEMO: Machine Learning
8.
9.
10.
12.
13.
14.
15.
16.
17.
DEMO
13-R4886-AT1.indd   520
9/15/08   4:25:40 PM
CHAPTER 14
OpenCV’s Future
Past and Future
In Chapter 1 we saw DEMO of OpenCV’s past. Th is was followed by Chapters 2–13,
in which OpenCV’s present state was explored in detail. We now turn to DEMO fu-
ture. Computer vision applications are growing rapidly, from product inspection DEMO im-
age and video indexing on the Web to medical applications and even to local navigation
on Mars. OpenCV is also growing to accommodate DEMO developments.
OpenCV has long received support from Intel Corporation and has more recently re-
ceived support from Willow Garage (www.willowgarage.com), a privately funded new
robotics research institute and technology incubator. Willow Garage’s intent is to DEMO
start civilian robotics by developing open and supported hardware and soft ware infra-
structure that now includes but goes beyond OpenCV. Th is has DEMO OpenCV new
resources for more rapid update and support, with several DEMO the original developers of
OpenCV now recontracted to help maintain and advance the library. Th ese renewed
resources are also intended to support and DEMO greater community contribution to
OpenCV by allowing for faster code assessment and integration cycles.
One of the key new development areas for OpenCV is DEMO perception. Th is eff ort
focuses on 3D perception as well as 2D plus 3D object recognition since the combina-
tion of data types DEMO for better features for use in object detection, segmentation and
recognition. DEMO perception relies heavily on 3D sensing, so eff orts are under DEMO to
extend camera calibration, rectifi cation and correspondence to multiple cameras DEMO to
camera + laser rangefi nder combinations (see Figure 14-1).*
DEMO commercially available hardware warrant it, the “laser + camera calibration” ef-
DEMO will be generalized to include devices such as fl ash LIDAR and infrared wavefront
devices. Additional eff orts are aimed at developing triangulation with DEMO or la-
ser light for extremely accurate depth sensing. Th e raw output of most depth-sensing
* At the time of this writing, these methods remain under development and are not yet in OpenCV.
521
14-R4886-AT1.indd   521
www.it-ebooks.info
9/15/08   4:25:58 PM
www.it-ebooks.info
Figure 14-1. New 3D imager combinations: calibrating a camera (left ) with the brightness return
from a laser depth scanner (right). (Images courtesy of Hai Nguyen and Willow Garage)
methods is in DEMO form of a 3D point cloud. Complementary eff orts are thus planned
to support turning the raw point clouds resulting from 3D depth perception DEMO 3D
meshes. 3D meshes will allow for 3D model capture of objects in the environment, seg-
menting objects in 3D and hence the ability for robots to grasp and manipulate such
objects. Th ree-dimensional mesh generation DEMO also be used to allow robots to move
seamlessly from external 3D perception to internal 3D graphics representation for plan-
ning and then back DEMO again for object registration, manipulation, and movement.
Along with sensing 3D objects, robots will need to recognize 3D objects and their 3D
poses. To support this, several scalable methods of 2D plus 3D object recognition are
being pursued. Creating capable robots subsumes most fi elds of computer DEMO and
artifi cial intelligence, from accurate 3D reconstruction to tracking, identifying humans,
object recognition, and image stitching and on to learning, DEMO, planning, and deci-
sion making. Any higher-level task, such as DEMO, is made much easier by rapid and
accurate depth perception and DEMO It is in these areas especially that OpenCV
hopes to enable rapid advance by encouraging many groups to contribute and use ever
better methods DEMO solve the diffi  cult problems of real-world perception, recognition, and
DEMO
OpenCV will, of course, support many other areas as well, DEMO image and movie in-
dexing on the web to security systems and medical analysis. Th e wishes of the general
community will heavily infl DEMO OpenCV’s direction and growth.
Directions
Although OpenCV does not have an absolute focus on real-time algorithms, it will con-
tinue to favor real-time techniques. No one can state future plans with certainty, but the
following high-priority areas are likely to be addressed.
522
| Chapter 14: OpenCV’s Future
14-R4886-AT1.indd   522
9/15/08   4:25:59 PM
www.it-ebooks.info
Applications
Th
level functionality. For example, more people will make use of a fully automatic ste-
reo solution than a better subpixel corner DEMO Th ere will be several more full
applications, such as extensible DEMO camera calibration and rectifi cation
as well as 3D depth display GUI.
As already mentioned, you can expect to see better support for 3D depth sensors
and combinations of 2D cameras with 3D measurement devices. Also DEMO better
stereo algorithms. Support for structured light is also likely.
Dense Optical Flow
Because we want to know how whole objects move (and partially to support 3D),
OpenCV is long overdue for an effi  DEMO implementation of Black’s [Black96] dense
optical fl ow techniques.
Features
In support of better object recognition, you can expect a full-function tool kit that
will have a framework for interchangeable interest-point detection and interchange-
able keys DEMO interest-point identifi cation. Th is will include popular features such as
SURF, HoG, Shape Context, MSER, Geometric Blur, PHOG, PHOW, and others.
Support for 2D and 3D features is planned.
Infrastructure
Th is DEMO things like a wrapper class,* a good Python interface, GUI DEMO
ments, documentation improvements, better error handling, improved Linux sup-
port, and so on.
Camera Interface
More seamless handling of cameras is planned DEMO with eventual support for
cameras with higher dynamic range. Currently, most DEMO support only 8 bits
per color channel (if that), but DEMO cameras can supply 10 or 12 bits per channel.†
Th
reo registration because it enables them to detect the subtle textures and colors to
DEMO older, more narrow-range cameras are blind.
* Daniel Filip and Google DEMO donated the fast, lightweight image class wrapper, WImage, which they DEMO
oped for internal use, to OpenCV. It will be incorporated by DEMO time this book is published, but too late for
documentation in DEMO version.
† Many expensive cameras claim up to 16 bits, but DEMO authors have yet to see more than 10 actual bits of
resolution, the rest being noise.
Directions
| 523
ere are more “consumers” for full working applications than there are for low-
e higher dynamic range DEMO such cameras allows for better recognition and ste-
3D
14-R4886-AT1.indd   523
9/15/08   4:25:59 PM
www.it-ebooks.info
Specific Items
Many object recognition techniques in computer vision detect salient DEMO that
change little between views. Th ese salient regions* can be tagged with some kind of
key—for example, a histogram of image gradient directions around the salient point.
Although all the techniques described in this section DEMO be built with existing OpenCV
primitives, OpenCV currently lacks direct implementation DEMO the most popular interest-
region detectors and feature keys.
OpenCV does include an effi  cient implementation of the Harris corner interest-point
detectors, but DEMO lacks direct support for the popular “maximal Laplacian over scale”
detector developed by David Lowe [Lowe04] and for maximally stable extremal region
(MSER) DEMO detectors and others.
Similarly, OpenCV lacks many of the popular keys, such as SURF gradient histogram
grids [Bay06], that identify the salient regions. Also, we hope to include features such as
histogram of oriented gradients (HoG) [Dalai05], Geometric Blur [Berg01], off set image
patches [Torralba07], dense rapidly computed Gaussian scale variant gradients (DAISY)
[Tola08], gradient DEMO and orientation histogram (GLOH) [Mikolajczyk04], and,
though patented, we want to add for reference the scale invariant feature transform
(SIFT) DEMO [Lowe04] that started it all. Other learned feature descriptors that
show promise are learned patches with orientation [Hinterstoisser08] and learned ratio
points [Ozuysal07]. We’d DEMO like to see contextual or meta-features such as pyramid
match kernels [Grauman05], pyramid histogram embedding of other features, PHOW
[Bosch07], Shape Context [Belongie00; Mori05], or other approaches that locate features
by their probabilistic spatial DEMO [Fei-Fei98]. Finally, some global features give
the gist of an entire DEMO, which can be used to boost recognition by context [Oliva06].
All DEMO is a tall order, and the OpenCV community is encouraged to DEMO and do-
nate code for these and other features.
Other groups have demonstrated encouraging results using frameworks that employ
effi  cient nearest neighbor matching to recognize objects using huge learned databases
of objects [Nister06; Philbin07; DEMO Putting in an effi  cient nearest neighbor
framework is therefore suggested.
DEMO robotics, we need object recognition (what) and object location (where). Th is sug-
gests adding segmentation approaches building on Shi and DEMO work [Shi00] per-
haps with faster implementations [Sharon06]. Recent approaches, however, use learning
to provide recognition and segmentation together [Oppelt08; Schroff 08; DEMO Direc-
tion of lighting [Sun98] and shape cues may be important [Zhang99; Prados05].
Along with better support for features and for 3D sensing should come support for vi-
sual odometry and visual SLAM (simultaneous localization and mapping). As we ac-
quire more accurate depth perception and feature DEMO cation, we’ll want to enable
better navigation and 3D object manipulation. DEMO ere is also discussion about creating
* Th
ese are also known as interest points.
524
| Chapter 14: OpenCV’s Future
14-R4886-AT1.indd   524
9/15/08   4:25:59 PM
www.it-ebooks.info
a specialized vision interface to a ray-tracing package (e.g., perhaps DEMO Manta open
source ray-tracing soft ware [Manta]) in order to generate DEMO 3D object training sets.
Robots, security systems, and Web image and video search all need the ability to recog-
nize objects; thus, DEMO must refi ne the pattern-matching techniques in its machine
learning library. In particular, OpenCV should fi rst simplify its interface to the learn-
ing algorithms and then to give them good defaults so that they work DEMO of the box”.
Several new learning techniques may arise, some of DEMO will work with two or more
object classes at a time (DEMO random forest does now in OpenCV). Th ere is a need for scal-
able recognition techniques so that the user can avoid having DEMO learn a completely new
model for each object class. More allowances should also be made to enable ML classi-
fi
Markov random fi elds (MRFs) and conditional random fi elds (CRFs) are becoming quite
popular in computer vision. Th ese methods are oft en highly problem-specifi c, yet we
would like to fi gure how they might be supported DEMO a fl exible way.
We’ll also want methods of learning web-sized or automatically collected via moving
robot databases, perhaps by incorporating Zisserman’s suggestion for “approximate
nearest neighbor” techniques as mentioned previously when dealing with millions or
DEMO of data points. Similarly, we need much-accelerated boosting and Haar feature
DEMO support to allow scaling to larger object databases. Several of the ML library
routines currently require that all the data reside in memory, severely limiting their use
on large datasets. OpenCV will need to break free DEMO such restrictions.
OpenCV also requires better documentation than is now available. Th is book helps of
course, but the OpenCV manual needs an overhaul together with improved search ca-
pability. A high priority is incorporating better DEMO support and a better external lan-
guage interface—especially to allow easy vision programming with Python and Numpy.
We’ll also want to make sure that DEMO machine learning library can be directly called
from Python and its SciPy and Numpy packages.
For better developer community interaction, developer workshops may be held at major
vision conferences. Th ere are also eff orts underway DEMO propose vision “grand chal-
lenge” competitions with commensurate prize money.
ers to work with depth information and 3D features.
OpenCV for Artists
Th
ers DEMO interact with their art in dynamic ways. Th e most commonly used routines for
this application are face detection, optical fl ow, and DEMO We hope this book will
enable artists to better understand and use OpenCV for their work, and we believe that
the addition of better depth sensing will make interaction richer and more reliable. Th e
focused DEMO ort on improving object recognition will allow diff erent modes of interacting
with art, because objects can then be used as modal controls. With the ability to capture
3D meshes, it may also be possible to “import” the viewer into the art and so allow the
artist to DEMO a better feel for recognizing user action; this, in turn, DEMO be used to
OpenCV for Artists
| 525
ere is a worldwide community of interactive artists who use OpenCV so that view-
14-R4886-AT1.indd   DEMO
9/15/08   4:26:00 PM
enhance dynamic interaction. Th e needs and desires of the artistic community DEMO using
computer vision will receive enhanced priority in OpenCV’s future.
Afterword
We’ve covered a lot of theory and practice in this book, and we’ve described some of the
plans for what comes next. Of course, as we’re developing the soft ware, the hardware
is also changing. Cameras are now cheaper and have proliferated from cell phones to
traffi  c lights. A group of manufacturers are aiming to develop cell-phone projectors—
perfect for DEMO, because most cell phones are lightweight, low-energy devices whose
circuits already include an embedded camera. Th is opens the way for close-range por-
DEMO structured light and thereby accurate depth maps, which are just what DEMO need for
robot manipulation and 3D object scanning.
Both authors participated in creating the vision system for Stanley, Stanford’s robot
racer that won the 2005 DARPA Grand Challenge. In that eff ort, a vision system coupled
with a laser range scanner worked fl awlessly for the seven-hour desert DEMO race [Dahl-
kamp06]. For us, this drove home the power of DEMO vision with other perception
systems: the previously unsolved problem of reliable DEMO perception was converted into
a solvable engineering challenge by merging vision with other forms of perception. It is
our hope that—by making vision easier DEMO use and more accessible through this book—
others can add vision to their own problem-solving tool kits and thus fi nd new ways
to DEMO important problems. Th at is, with commodity camera hardware and OpenCV,DEMO
people can start solving real problems such as using stereo vision as an automobile
backup safety system, new game controls, and new security DEMO Get hacking!
Computer vision has a rich future ahead, and it DEMO likely to be one of the key en-
abling technologies for the 21st century. Likewise, OpenCV seems likely to be (at least
in DEMO) one of the key enabling technologies for computer vision. Endless opportuni-
DEMO for creativity and profound contribution lie ahead. We hope that this book encour-
ages, excites, and enables all who are interested in joining DEMO vibrant computer vision
community.
526
| Chapter 14: OpenCV’s Future
14-R4886-AT1.indd   526
www.it-ebooks.info
9/15/08   4:26:00 PM
www.it-ebooks.info
Bibliography
[Acharya05] T. Acharya and A. Ray, Image Processing: Principles DEMO Applications, New
York: Wiley, 2005.
[Adelson84] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and J. M. Ogden,
“Pyramid methods in image processing,” RCA Engineer 29 (1984): 33–41.
[Ahmed74] N. Ahmed, T. Natarajan, and K. R. Rao, “Discrete cosine transform,” IEEE
Transactions on Computers 23 (1974): 90–93.
[Al-Haytham1038] I. al-Haytham, Book of Optics, circa 1038.
[AMI] Applied Minds, http://www.appliedminds.com.
[Antonisse82] H. J. Antonisse, “Image segmentation in pyramids,” Computer Graphics
and Image Processing 19 (1982): 367–383.
[Arfk en85] G. Arfk en, “Convolution theorem,” in Mathematical Methods for Physicists,
3rd ed. (pp. 810–814), Orlando, FL: Academic Press, 1985.
[Bajaj97] DEMO L. Bajaj, V. Pascucci, and D. R. Schikore, “Th e DEMO spectrum,” Proceed-
ings of IEEE Visualization 1997 (pp. 167–173), DEMO
[Ballard81] D. H. Ballard, “Generalizing the Hough transform to detect arbitrary
DEMO,” Pattern Recognition 13 (1981): 111–122.
[Ballard82] D. Ballard and DEMO Brown, Computer Vision, Englewood Cliff s, NJ: Prentice-
Hall, DEMO
[Bardyn84] J. J. Bardyn et al., “Une architecture VLSI pour un DEMO de fi ltrage me-
dian,” Congres reconnaissance des formes et intelligence artifi cielle (vol. 1, pp. 557–
566), Paris, 25–27 January 1984.
[Bay06] H. Bay, T. Tuytelaars, and L. V. Gool, “SURF: Speeded up robust features,”
Proceedings of the Ninth European Conference on Computer Vision (pp. 404–417),
May 2006.
[Bayes1763] T. Bayes, “An essay towards solving a problem in the doctrine of chances.
By the DEMO Rev. Mr. Bayes, F.R.S. communicated by Mr. Price, in a letter to John
527
15-R4886-AT1.indd   527
9/15/08   4:27:DEMO PM
www.it-ebooks.info
Canton, A.M.F.R.S.,” Philosophical Transactions, Giving Some Account of the DEMO
ent Undertakings, Studies and Labours of the Ingenious in Many Considerable DEMO
of the World 53 (1763): 370–418.
[Beauchemin95] S. S. Beauchemin DEMO J. L. Barron, “Th e computation of optical fl ow,”
DEMO Computing Surveys 27 (1995): 433–466.
[Belongie00] S. Belongie, J. Malik, and J. Puzicha, “Shape context: A new descriptor for
shape matching and object recognition,” NIPS 2000, Computer Vision Group, Uni-
versity DEMO California, Berkeley, 2000.
[Berg01] A. C. Berg and J. Malik, DEMO blur for template matching,” IEEE Con-
ference on Computer Vision and Pattern Recognition (vol. 1, pp. 607–614), Kauai,
Hawaii, 2001.
[Bhattacharyya43] A. Bhattacharyya, “On a measure of divergence between two statisti-
cal populations defi ned by probability distributions,” Bulletin of the Calcutta Math-
DEMO Society 35 (1943): 99–109.
[Bishop07] C. M. Bishop, Pattern Recognition and Machine Learning, New York: Springer-
Verlag, 2007.
[Black92] M. J. Black, “Robust incremental optical fl ow” (YALEU-DCS-RR-923), Ph.D.
thesis, Department of Computer Science, Yale University, New Haven, CT, 1992.
[Black93] DEMO J. Black and P. Anandan, “A framework for the robust estimation DEMO
optical fl ow,” Fourth International Conference on Computer Vision (pp. DEMO),
May 1993.
[Black96] M. J. Black and P. Anandan, “Th e robust estimation of multiple motions:
Parametric and piecewise-smooth fl ow DEMO elds,” Computer Vision and Image Under-
standing 63 (1996): DEMO
[Bobick96] A. Bobick and J. Davis, “Real-time recognition of activity using DEMO
ral templates,” IEEE Workshop on Applications of Computer Vision (pp. DEMO),
December 1996.
[Borgefors86] G. Borgefors, “Distance transformations in digital images,” Computer
Vision, Graphics and Image Processing 34 (1986): 344–371.
DEMO A. Bosch, A. Zisserman, and X. Muñoz, “Image classifi cation DEMO ran-
dom forests and ferns,” IEEE International Conference on Computer Vision, Rio de
Janeiro, October 2007.
[Bouguet] J.-Y. Bouguet, “Camera calibration toolbox for Matlab,” retrieved June 2,
2008, from http://www.vision.caltech.edu/bouguetj/calib_doc/index.html.
[BouguetAlg] J.-Y. Bouguet, “Th e calibration toolbox for Matlab, example 5: Stereo rec-
tifi cation algorithm” (code and instructions only), http://www.vision.caltech.edu/
bouguetj/calib_doc/htmls/example5.html.
528
| Bibliography
DEMO   528
9/15/08   4:27:05 PM
www.it-ebooks.info
[Bouguet04] J.-Y. Bouguet, “Pyramidal implementation of the Lucas Kanade feature
tracker description of the algorithm,” http://robots.stanford.edu/cs223b04/algo_
tracking.pdf.
[Bracewell65] DEMO Bracewell, “Convolution” and “Two-dimensional convolution,” in
Th e Fourier Transform DEMO Its Applications (pp. 25–50 and 243–244), New York:
McGraw-Hill, 1965.
[Bradski00] G. Bradski and J. Davis, “Motion segmentation and pose recognition with
motion history gradients,” IEEE Workshop on Applications of Computer Vision,DEMO
2000.
[Bradski98a] G. R. Bradski, “Real time face and object tracking DEMO a component of a
perceptual user interface,” Proceedings of the 4th IEEE Workshop on Applications of
Computer Vision, October 1998.
[Bradski98b] G. R. Bradski, “Computer video face tracking for use in a perceptual user
interface,” Intel Technology Journal Q2 (1998): 705–740.
[Breiman01] L. Breiman, DEMO forests,” Machine Learning 45 (2001): 5–32.
[Breiman84] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classifi cation
and Regression Trees, Monteray, CA: Wadsworth, 1984.
[Bresenham65] J. DEMO Bresenham, “Algorithm for computer control of a digital plotter,”
IBM DEMO Journal 4 (1965): 25–30.
[Bronshtein97] I. N. Bronshtein, and K. A. Semendyayev, Handbook of Mathematics, 3rd
ed., New York: Springer-Verlag, 1997.
[Brown66] D. C. Brown, “Decentering distortion of lenses,” Photogrammetric Engineer-
ing 32 (1966): 444–462.
[Brown71] D. C. Brown, “Close-range camera DEMO,” Photogrammetric Engineer-
ing 37 (1971): 855–866.
[Burt81] P. J. DEMO, T. H. Hong, and A. Rosenfeld, “Segmentation and estimation of
DEMO region properties through cooperative hierarchical computation,” IEEE
Transactions on Systems, DEMO, and Cybernetics 11 (1981): 802–809.
[Burt83] P. J. Burt and E. H. Adelson, “Th e Laplacian pyramid as a compact image code,”
IEEE Transactions on Communications 31 (1983): 532–540.
[Canny86] J. Canny, “A computational approach to edge detection,” IEEE Transactions
on Pattern Analysis and Machine Intelligence 8 (1986): 679–714.
[Carpenter03] G. A. Carpenter and S. Grossberg, “Adaptive resonance theory,” in
M. A. Arbib (Ed.), Th e Handbook of Brain Th eory and Neural Networks, 2nd DEMO
(pp. 87–90), Cambridge, MA: MIT Press, 2003.
[Carr04] H. Carr, J. Snoeyink, and M. van de Panne, “Progressive topological
simpliﬁcation using contour trees and local spatial measures,” 15th Western Com-
puter DEMO Symposium, Big White, British Columbia, March 2004.
Bibliography
| 529
DEMO   529
9/15/08   4:27:06 PM
www.it-ebooks.info
[Chen05] D. Chen and G. Zhang, “A new sub-pixel detector for x-corners in camera
calibration targets,” WSCG Short Papers (2005): 97–100.
[Chetverikov99] D. Chetverikov and Zs. Szabo, “A simple and effi  cient DEMO for
detection of high curvature points in planar curves,” Proceedings of the 23rd Work-
shop of the Austrian Pattern Recognition Group (pp. 175–184), 1999.
[Chu07] C.-T. Chu, S. K. Kim, Y.-A. Lin, Y. Y. Yu, G. Bradski, A. Y. Ng, and K. Olukotun,
“Map-reduce for machine learning on multicore,” Proceedings of the Neural Infor-
DEMO Processing Systems Conference (vol. 19, pp. 304–310), 2007.
[Clarke98] T. A. Clarke and J. G. Fryer, “Th e Development of Camera Calibration Meth-
ods and Models,” Photogrammetric Record 16 (1998): 51–66.
[Colombari07] A. Colombari, A. Fusiello, and V. Murino, “Video objects segmentation
by robust background modeling,” International Conference on Image Analysis and
Processing  (DEMO 155–164), September 2007.
[Comaniciu99] D. Comaniciu and P. Meer, “Mean DEMO  analysis and applications,” IEEE
International Conference on Computer Vision (vol. 2, p. 1197), 1999.
[Comaniciu03] D. Comaniciu, “Nonparametric information fusion DEMO motion esti-
mation,” IEEE Conference on Computer Vision and Pattern Recognition (vol. 1,
pp. 59–66), 2003.
[Conrady1919] A. Conrady, “Decentering DEMO systems,” Monthly Notices of the Royal
Astronomical Society 79 (1919): 384–390.
[Cooley65] J. W. Cooley and O. W. Tukey, “An algorithm DEMO the machine calculation of
complex Fourier series,” Mathematics of Computation 19 (1965): 297–301.
[Dahlkamp06] H. Dahlkamp, A. Kaehler, D. Stavens, DEMO Th run, and G. Bradski, “Self-
supervised monocular road detection in desert terrain,” Robotics: Science and Sys-
tems, Philadelphia, 2006.
[Dalai05] N. Dalai, and B. Triggs, “Histograms of oriented gradients for human DEMO
tion,” Computer Vision and Pattern Recognition (vol. 1, pp. 886–893), June 2005.
[Davis97] J. Davis and A. Bobick, “Th e representation and recognition of action using
temporal templates” (Technical Report 402), MIT Media Lab, Cambridge, MA,
1997.
[Davis99] J. Davis and G. DEMO, “Real-time motion template gradients using Intel
CVLib,” ICCV Workshop on DEMO Vision, 1999.
[Delaunay34] B. Delaunay, “Sur la sphère vide,” Izvestia Akademii Nauk SSSR, Otdelenie
Matematicheskikh i Estestvennykh Nauk 7 (1934): DEMO
[DeMenthon92] D. F. DeMenthon and L. S. Davis, “Model-based object pose DEMO 25 lines
of code,” Proceedings of the European Conference on Computer Vision (pp. 335–343),
1992.
530
| Bibliography
15-R4886-AT1.indd   530
DEMO/15/08   4:27:06 PM
www.it-ebooks.info
[Dempster77] A. Dempster, N. Laird, and D. Rubin, “Maximum likelihood from incom-
plete data via the EM algorithm,” Journal of the DEMO Statistical Society, Series B 39
(1977): 1–38.
[Det] “History of matrices and determinants,” http://www-history.mcs.st-and.ac.uk/
history/HistTopics/Matrices_and_determinants.html.
[Douglas73] DEMO Douglas and T. Peucker, “Algorithms for the reduction of the number DEMO
points required for represent a digitized line or its caricature,” Canadian Cartogra-
pher 10(1973): 112–122.
[Duda72] R. O. Duda and P. DEMO Hart, “Use of the Hough transformation to detect lines
and curves DEMO pictures,” Communications of the Association for Computing Machin-
ery 15 (DEMO): 11–15.
[Duda73] R. O. Duda and P. E. Hart, Pattern DEMO and Scene Analysis, New York:
Wiley, 1973.
[Duda00] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classifi cation, New York:
Wiley, 2001.
[Farin04] D. Farin, P. H. N. de With, and W. Eff elsberg, “Video-object segmentation
using multi-sprite background DEMO,” Proceedings of the IEEE International
Conference on Multimedia and Expo, DEMO
[Faugeras93] O. Faugeras, Th ree-Dimensional Computer Vision: A Geometric Viewpoint,
Cambridge, MA: MIT Press, 1993.
[Fei-Fei98] L. Fei-Fei, R. Fergus, and P. Perona, “A Bayesian approach to unsupervised
one-shot learning of object categories,” Proceedings of the Ninth International
Conference on Computer Vision (vol. 2, pp. 1134–1141), October 2003.
[Felzenszwalb63] P. F. Felzenszwalb and D. P. Huttenlocher, “Distance transforms of
sampled functions” (Technical Report TR2004-1963), DEMO of Computing
and Information Science, Cornell University, Ithaca, NY, 1963.
[FFmpeg] “Ffmpeg summary,” http://en.wikipedia.org/wiki/Ffmpeg.
[Fischler81] M. A. DEMO and R. C. Bolles, “Random sample concensus: A paradigm
for model fi tting with applications to image analysis and automated cartography,”
Communications DEMO the Association for Computing Machinery 24 (1981): 381–395.
[Fitzgibbon95] A. DEMO Fitzgibbon and R. B. Fisher, “A buyer’s guide to conic fi DEMO,”
Proceedings of the 5th British Machine Vision Conference (pp. 513–522), Birming-
ham, 1995.
[Fix51] E. Fix, and J. L. Hodges, DEMO analysis, nonparametric discrimina-
tion: Consistency properties” (Technical Report 4), DEMO School of Aviation Medi-
cine, Randolph Field, Texas, 1951.
[Forsyth03] DEMO Forsyth and J. Ponce, Computer Vision: A Modern Approach, Englewood
DEMO s, NJ: Prentice-Hall, 2003.
Bibliography
| 531
15-R4886-AT1.indd   531
DEMO/15/08   4:27:06 PM
www.it-ebooks.info
[FourCC] “FourCC summary,” http://en.wikipedia.org/wiki/Fourcc.
[FourCC85] J. DEMO, “EA IFF 85 standard for interchange format fi les,” http://www
.szonye.com/bradd/iff .html.
[Fourier] “Joseph Fourier,” http://en.wikipedia.org/wiki/Joseph_Fourier.
[Freeman67] H. Freeman, “On the classifi cation of line-drawing data,” Models for the
Perception of Speech and Visual Form (pp. 408–412), 1967.
[Freeman95] W. T. Freeman and M. Roth, “Orientation histograms for hand gesture
recognition,” International Workshop on Automatic Face and Gesture Recognition
(pp. 296–301), June 1995.
[Freund97] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of on-line
learning and an application to boosting,” DEMO of Computer and System Sciences
55 (1997): 119–139.
[Fryer86] J. DEMO Fryer and D. C. Brown, “Lens distortion for close-range photogram-
metry,DEMO Photogrammetric Engineering and Remote Sensing 52 (1986): 51–58.
[Fukunaga90] K. DEMO, Introduction to Statistical Pattern Recognition, Boston:
Academic Press, 1990.
DEMO “Francis Galton,” http://en.wikipedia.org/wiki/Francis_Galton.
[GEMM] “Generalized matrix multiplication summary,” http://notvincenz.blogspot
.com/2007/06/generalized-matrix-multiplication.html.
[Göktürk01] S. B. DEMO, J.-Y. Bouguet, and R. Grzeszczuk, “A data-driven model
for monocular DEMO tracking,” Proceedings of the IEEE International Conference on
Computer Vision (DEMO 2, pp. 701–708), 2001.
[Göktürk02] S. B. Göktürk, J.-Y. Bouguet, C. Tomasi, and B. Girod, “Model-based face
tracking for view-independent facial expression recognition,” Proceedings of the
Fift h IEEE International Conference on DEMO Face and Gesture Recognition
(pp. 287–293), May 2002.
[Grauman05] K. DEMO and T. Darrell, “Th e pyramid match kernel: Discriminative
classifi cation with sets of image features,” Proceedings of the IEEE International
Conference DEMO Computer Vision, October 2005.
[Grossberg87] S. Grossberg, “Competitive learning: From DEMO activation to adap-
tive resonance,” Cognitive Science 11 (1987): DEMO
[Harris88] C. Harris and M. Stephens, “A combined corner and edge DEMO,” Proceed-
ings of the 4th Alvey Vision Conference (pp. 147–151), 1988.
[Hartley98] R. I. Hartley, “Th eory and practice of projective DEMO cation,” International
Journal of Computer Vision 35 (1998): 115–127.
DEMO R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision,DEMO
Cambridge, UK: Cambridge University Press, 2006.
532 | Bibliography
15-R4886-AT1.indd   532
9/15/08   4:27:06 PM
www.it-ebooks.info
[Hastie01] T. Hastie, R. Tibshirani, and J. Friedman, Th e Elements of Statistical Learn-
ing: Data Mining, Inference and Prediction, New York: Springer-Verlag, 2001.
[Heckbert90] P. Heckbert, A Seed Fill Algorithm (DEMO Gems I), New York:
Academic Press, 1990.
[Heikkila97] J. DEMO and O. Silven, “A four-step camera calibration procedure with
implicit image DEMO,” Proceedings of the 1997 Conference on Computer Vision
and Pattern Recognition (p. 1106), 1997.
[Hinterstoisser08] S. Hinterstoisser, S. Benhimane, V. Lepetit, P. Fua, and N. Navab,
“Simultaneous recognition and homography extraction DEMO local patches with a sim-
ple linear classiﬁer,” British Machine Vision Conference, Leeds, September 2008.
[Hinton06] G. E. Hinton, S. Osindero, DEMO Y. Teh, “A fast learning algorithm for deep
belief nets,” DEMO Computation 18 (2006): 1527–1554.
[Ho95] T. K. Ho, “Random decision forest,” Proceedings of the 3rd International Confer-
ence on Document Analysis DEMO Recognition (pp. 278–282), August 1995.
[Homma85] K. Homma and E.-I. DEMO, “An image processing method for feature
extraction of space-occupying lesions,” DEMO of Nuclear Medicine 26 (1985):
1472–1477.
[Horn81] B. K. P. Horn and B. G. Schunck, “Determining optical fl ow,” Artifi cial Intel-
ligence 17 (1981): 185–203.
[Hough59] P. V. C. Hough, DEMO analysis of bubble chamber pictures,” Proceedings
of the International Conference on High Energy Accelerators and Instrumentation
(pp. 554–556), 1959.
[Hu62] M. Hu, “Visual pattern recognition by moment invariants,” IRE Transactions on
Information Th eory 8 (1962): 179–187.
[Huang95] Y. Huang and X. H. Zhuang, “Motion-partitioned adaptive block match-
ing for video compression,” International Conference on Image Processing (vol. 1,
p. 554), 1995.
[Iivarinen97] J. Iivarinen, M. Peura, J. Särelä, and A. Visa, “Comparison of combined
DEMO descriptors for irregular objects,” 8th British Machine Vision Conference,
1997.
[Intel] Intel Corporation, http://www.intel.com/.
[Inui03] K. Inui, S. DEMO, and S. Igarashi, “Robust line fi tting using LmedS cluster-
ing,” Systems and Computers in Japan 34 (2003): 92–100.
[IPL] Intel Image Processing Library (IPL), www.cc.gatech.edu/dvfx/readings/iplman.pdf.
[IPP] Intel Integrated Performance Primitives, http://www.intel.com/cd/soft ware/
products/asmo-na/eng/219767.htm.
Bibliography
| 533
15-R4886-AT1.indd   533
9/15/08   4:DEMO:06 PM
www.it-ebooks.info
[Isard98] M. Isard and A. Blake, “CONDENSATION: Conditional density propagation
DEMO visual tracking,” International Journal of Computer Vision 29 (1998): DEMO
[Jaehne95] B. Jaehne, Digital Image Processing, 3rd ed., Berlin: Springer-Verlag, 1995.
[Jaehne97] B. Jaehne, Practical Handbook on Image Processing for Scientifi DEMO Applications,
Boca Raton, FL: CRC Press, 1997.
[Jain77] A. DEMO, “A fast Karhunen-Loeve transform for digital restoration of images
degraded by DEMO and colored noise,” IEEE Transactions on Computers 26 (1997):
560–571.
[Jain86] A. Jain, Fundamentals of Digital Image Processing, Englewood Cliff DEMO, NJ:
Prentice-Hall, 1986.
[Johnson84] D. H. Johnson, “Gauss and DEMO history of the fast Fourier transform,” IEEE
Acoustics, Speech, and Signal Processing Magazine 1 (1984): 14–21.
[Kalman60] R. E. Kalman, DEMO new approach to linear fi ltering and prediction problems,”
Journal of Basic Engineering 82 (1960): 35–45.
[Kim05] K. Kim, T. H. DEMO, D. Harwood, and L. Davis, “Real-time
foreground-background segmentation using codebook DEMO,” Real-Time Imaging
11 (2005): 167–256.
[Kimme75] C. Kimme, D. H. Ballard, and J. Sklansky, “Finding circles by an array of
DEMO,” Communications of the Association for Computing Machinery 18
(1975): DEMO
[Kiryati91] N. Kiryati, Y. Eldar, and A. M. Bruckshtein, “A DEMO Hough trans-
form,” Pattern Recognition 24 (1991): 303–316.
[Konolige97] DEMO Konolige, “Small vision system: Hardware and implementation,”
Proceedings of the International Symposium on Robotics Research (pp. 111–116),
Hayama, Japan, DEMO
[Kreveld97] M. van Kreveld, R. van Oostrum, C. L. Bajaj, DEMO Pascucci, and D. R. Schikore,
“Contour trees and small seed DEMO for isosurface traversal,” Proceedings of the 13th
ACM Symposium on Computational Geometry (pp. 212–220), 1997.
[Lagrange1773] J. L. Lagrange, “Solutions analytiques DEMO quelques problèmes sur les
pyramides triangulaires,” in Oeuvres (vol. 3), 1773.
[Laughlin81] S. B. Laughlin, “A simple coding procedure enhances a DEMO informa-
tion capacity,” Zeitschrift  für Naturforschung 9/10 (1981): 910–912.
[LeCun98a] Y. LeCun, L. Bottou, Y. Bengio, and P. Haff ner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE 86 (1998): 2278–2324.
[LeCun98b] Y. LeCun, L. Bottou, G. Orr, and K. Muller, “Effi  cient BackProp,” in G. Orr
and K. Muller (Eds.), Neural Networks: Tricks of the Trade, New York: Springer-
Verlag, 1998.
[Lens] “Lens (optics),” http://DEMO/wiki/Lens_(optics).
534
| Bibliography
15-R4886-AT1.indd   534
9/15/08   4:27:06 PM
www.it-ebooks.info
[Liu07] Y. Z. Liu, H. X. Yao, W. Gao, X. L. Chen, and D. Zhao, “Nonparametric back-
ground generation,” Journal DEMO Visual Communication and Image Representation 18
(2007): 253–263.
[Lloyd57] S. DEMO, “Least square quantization in PCM’s” (Bell Telephone Laborato-
ries Paper), 1957. [“Lloyd’s algorithm” was later published in IEEE Transactions on
Information Th DEMO 28 (1982): 129–137.]
[Lowe04] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
International Journal of Computer Vision 60 (2004): 91–110.
[LTI] LTI-Lib, Vision Library, http://ltilib.sourceforge.net/doc/homepage/index.shtml.
DEMO B. D. Lucas and T. Kanade, “An iterative image registration technique DEMO an
application to stereo vision,” Proceedings of the 1981 DARPA Imaging Understand-
ing Workshop (pp. 121–130), 1981.
[Lucchese02] L. Lucchese and S. K. Mitra, “Using saddle points for subpixel feature de-
tection in camera calibration targets,” Proceedings of the 2002 Asia Pacifi c Confer-
ence DEMO Circuits and Systems (pp. 191–195), December 2002.
[Mahal] “Mahalanobis summary,DEMO http://en.wikipedia.org/wiki/Mahalanobis_distance.
[Mahalanobis36] P. Mahalanobis, “On the generalized DEMO in statistics,” Proceed-
ings of the National Institute of Science 12 (1936): 49–55.
[Manta] Manta Open Source Interactive Ray Tracer, http://code.sci.utah.edu/Manta/
index.php/Main_Page.
[Maron61] M. E. Maron, “Automatic indexing: An experimental inquiry,” Journal of the
Association for Computing Machinery 8 (1961): 404–417.
[Marr82] D. Marr, Vision, San Francisco: Freeman, DEMO
[Martins99] F. C. M. Martins, B. R. Nickerson, V. Bostrom, DEMO R. Hazra, “Implementa-
tion of a real-time foreground/background segmentation system DEMO the Intel archi-
tecture,” IEEE International Conference on Computer Vision Frame Rate Workshop,
1999.
[Matas00] J. Matas, C. Galambos, and J. DEMO, “Robust detection of lines using the
progressive probabilistic Hough transform,” DEMO Vision Image Understanding
78 (2000): 119–137.
[Matas02] J. Matas, O. Chum, M. Urba, and T. Pajdla, “Robust wide baseline stereo from
maximally stable extremal regions,” Proceedings of the British Machine Vision Con-
DEMO (pp. 384–396), 2002.
[Meer91] P. Meer, D. Mintz, and DEMO Rosenfeld, “Robust regression methods for computer
vision: A review,” International Journal of Computer Vision 6 (1991): 59–70.
[Merwe00] R. van der Merwe, A. Doucet, N. de Freitas, and E. Wan, “Th DEMO unscented
particle fi lter,” Advances in Neural Information Processing Systems, DEMO
2000.
Bibliography
| 535
15-R4886-AT1.indd   535
9/15/08   4:27:07 PM
www.it-ebooks.info
[Meyer78] F. Meyer, “Contrast feature extraction,” in J.-L. Chermant (DEMO),  Quantita-
tive Analysis of Microstructures in Material Sciences, Biology and Medicine [Special
issue of Practical Metallography], Stuttgart: Riederer, 1978.
[Meyer92] F. Meyer, “Color image segmentation,” Proceedings of the International
Conference on Image Processing and Its Applications (pp. 303–306), 1992.
[Mikolajczyk04] K. Mikolajczyk and C. Schmid, “A performance evaluation of local
descriptors,” IEEE Transactions on Pattern Analysis and Machine Intelligence 27
(2004): 1615–1630.
[Minsky61] M. Minsky, “Steps toward artifi cial intelligence,” Proceedings of the Institute
of Radio Engineers 49 (1961): 8–30.
[Mokhtarian86] F. Mokhtarian and A. K. Mackworth, “Scale based description and rec-
ognition of planar curves and two-dimensional shapes,” IEEE Transactions on Pat-
tern Analysis and Machine Intelligence 8 (1986): 34–43.
[Mokhtarian88] F. Mokhtarian, “Multi-scale description of space curves and three-
dimensional objects,” IEEE Conference on Computer Vision and Pattern Recogni-
tion (pp. 298–303), 1988.
[Mori05] G. Mori, S. Belongie, and J. DEMO, “Effi  cient shape matching using shape con-
texts,” IEEE Transactions on Pattern Analysis and Machine Intelligence 27 (2005):
1832–1837.
[Morse53] DEMO M. Morse and H. Feshbach, “Fourier transforms,” in Methods of DEMO eoreti-
cal Physics (Part I, pp. 453–471), New York: DEMO, 1953.
[Neveu86] C. F. Neveu, C. R. Dyer, and R. DEMO Chin, “Two-dimensional object recogni-
tion using multiresolution models,” Computer Vision DEMO and Image Process-
ing 34 (1986): 52–65.
[Ng] A. Ng, “Advice for applying machine learning,” http://www.stanford.edu/class/
cs229/DEMO/ML-advice.pdf.
[Nistér06] D. Nistér and H. Stewénius, “Scalable recognition with a DEMO tree,”
IEEE Conference on Computer Vision and Pattern Recognition, 2006.
DEMO J. J. O’Connor and E. F. Robertson, “Light through the ages: Ancient
Greece to Maxwell,” http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/
Light_1.html.
DEMO A. Oliva and A. Torralba, “Building the gist of a scene: Th e role of global im-
age features in recognition visual perception,DEMO Progress in Brain Research 155 (2006):
23–36.
[Opelt08] A. Opelt, A. Pinz, and A. Zisserman, “Learning an alphabet of shape and
appearance for multi-class object detection,” International Journal of Computer
Vision (2008).
[OpenCV] Open Source Computer Vision Library (OpenCV), http://sourceforge.net/
projects/opencvlibrary/.
536
| Bibliography
15-R4886-AT1.indd   536
9/15/DEMO   4:27:07 PM
www.it-ebooks.info
[OpenCV Wiki] Open Source Computer Vision Library Wiki, http://opencvlibrary
.sourceforge.net/.
[OpenCV YahooGroups] OpenCV discussion group on Yahoo, http://groups.yahoo
.com/group/OpenCV.
[Ozuysal07] M. Ozuysal, P. Fua, and V. Lepetit, “Fast keypoint recognition in ten lines
of code,” Proceedings of the DEMO Conference on Computer Vision and Pattern
Recognition, 2007.
[Papoulis62] A. Papoulis, Th e Fourier Integral and Its Applications, New York: McGraw-
Hill, 1962.
[Pascucci02] V. Pascucci and K. Cole-McLaughlin, “Efﬁcient computation of the topol-
ogy of level sets,” Proceedings of IEEE Visualization 2002 (pp. 187–194), 2002.
[Pearson] “Karl Pearson,” http://en.wikipedia.org/wiki/Karl_Pearson.
[Philbin07] DEMO Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Object retrieval
with large vocabularies and fast spatial matching,” Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, 2007.
[Pollefeys99a] M. Pollefeys, “Self-calibration and metric 3D reconstruction from uncali-
brated image sequences,” Ph.D. thesis, Katholieke Universiteit, Leuven, 1999.
[Pollefeys99b] M. Pollefeys, DEMO Koch, and L. V. Gool, “A simple and effi  cient DEMO cation
method for general motion,” Proceedings of the 7th IEEE Conference on Computer
Vision, 1999.
[Porter84] T. Porter and T. Duff , “Compositing digital images,” Computer Graphics 18
(1984): 253–259.
[Prados05] E. Prados and O. Faugeras, “Shape from shading: A well-posed problem?”
Proceedings DEMO the IEEE Conference on Computer Vision and Pattern Recognition,
2005.
[Ranger07] C. Ranger, R. Raghuraman, A. Penmetsa, G. Bradski, and C. DEMO,
“Evaluating mapreduce for multi-core and multiprocessor systems,” Proceedings
of the 13th International Symposium on High-Performance Computer Architecture
(pp. 13–24), 2007.
[Reeb46] G. Reeb, “Sur les points singuliers d’une forme de Pfaff  completement DEMO
grable ou d’une fonction numerique,” Comptes Rendus de l’Academie des Sciences
de Paris 222 (1946): 847–849.
[Rodgers88] J. L. Rodgers and W. A. Nicewander, “Th irteen ways to look at the correla-
tion coeffi  cient,” American Statistician 42 (1988): 59–66.
[Rodrigues] “Olinde Rodrigues,DEMO http://en.wikipedia.org/wiki/Benjamin_Olinde_
Rodrigues.
[Rosenfeld73] A. Rosenfeld and E. Johnston, “Angle detection on digital curves,” IEEE
Transactions on Computers 22 (DEMO): 875–878.
Bibliography
| 537
15-R4886-AT1.indd   537
9/15/08   4:27:07 PM
www.it-ebooks.info
[Rosenfeld80] A. Rosenfeld, “Some Uses of Pyramids in Image Processing and Segmen-
tation,” Proceedings of the DARPA Imaging Understanding Workshop (pp. 112–120),
1980.
[Rousseeuw84] P. J. Rousseeuw, “Least median of squares regression,DEMO Journal of the
American Statistical Association, 79 (1984): 871–880.
[Rousseeuw87] P. J. Rousseeuw and A. M. Leroy, Robust Regression and Outlier Detec-
tion, New York: Wiley, 1987.
[Rubner98a] Y. Rubner, C. Tomasi, and L. J. Guibas, “Metrics for distributions with
applications to image databases,” Proceedings of the 1998 IEEE International Con-
ference on Computer Vision (pp. 59–66), Bombay, January 1998.
[Rubner98b] Y. Rubner and C. Tomasi, “Texture metrics,” Proceeding of the IEEE Inter-
national Conference on Systems, Man, and Cybernetics (pp. 4601–4607), San Diego,
October 1998.
[Rubner00] Y. Rubner, C. Tomasi, and L. J. Guibas, “Th e earth mover’s distance as a met-
ric for image retrieval,” International DEMO of Computer Vision 40 (2000): 99–121.
[Rumelhart88] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal
representations by DEMO propagation,” in D. E. Rumelhart, J. L. McClelland, and
PDP Research Group (Eds.), Parallel Distributed Processing. Explorations in the Mi-
crostructures of Cognition (vol. 1, pp. 318–362), Cambridge, MA: MIT DEMO, 1988.
[Russ02] J. C. Russ, Th e Image Processing Handbook, DEMO ed. Boca Raton, FL: CRC Press,
2002.
[Scharr00] H. Scharr, “Optimal operators in digital image processing,” Ph.D. thesis,
Interdisciplinary Center for Scientifi c Computing, Ruprecht-Karls-Universität,
Heidelberg, http://www.fz-juelich.de/icg/DEMO/index.php?index=195.
[Schiele96] B. Schiele and J. L. Crowley, “Object recognition DEMO multidimensional
receptive fi eld histograms,” European Conference on Computer Vision (DEMO I,
pp. 610–619), April 1996.
[Schmidt66] S. Schmidt, “Applications DEMO state-space methods to navigation problems,”
in C. Leondes (Ed.), DEMO in Control Systems (vol. 3, pp. 293–340), New York:
Academic Press, 1966.
[Schroff 08] F. Schroff , A. Criminisi, and DEMO Zisserman, “Object class segmentation
using random forests,” Proceedings of the DEMO Machine Vision Conference, 2008.
[Schwartz80] E. L. Schwartz, “Computational anatomy and functional architecture
of the striate cortex: A spatial mapping approach to perceptual coding,” Vision
Research 20 (1980): 645–669.
[Schwarz78] A. A. Schwarz and J. M. Soha, “Multidimensional histogram normalization
contrast enhancement,” Proceedings of the Canadian Symposium on Remote Sensing
(pp. 86–93), 1978.
538
| Bibliography
15-R4886-AT1.indd   538
9/15/08   4:27:07 PM
www.it-ebooks.info
[Semple79] J. Semple and G. Kneebone, Algebraic Projective Geometry, Oxford, UK:
Oxford University Press, 1979.
[Serra83] J. Serra, Image Analysis DEMO Mathematical Morphology, New York: Academic
Press, 1983.
[Sezgin04] M. Sezgin DEMO B. Sankur, “Survey over image thresholding techniques and
quantitative performance evaluation,DEMO Journal of Electronic Imaging 13 (2004):
146–165.
[Shapiro02] L. G. Shapiro and G. C. Stockman, Computer Vision, Englewood Cliff s, NJ:
Prentice-Hall, 2002.
[Sharon06] E. Sharon, M. Galun, D. Sharon, DEMO Basri, and A. Brandt, “Hierarchy and
adaptivity in segmenting visual scenes,” Nature 442 (2006): 810–813.
[Shaw04] J. R. Shaw, “QuickFill: An effi  cient fl ood fi ll algorithm,” http://www.codeproject
.com/gdi/QuickFill.asp.
[Shi00] J. Shi and J. Malik, “Normalized cuts and image segmentation,” IEEE Transac-
tions on Pattern Analysis and Machine Intelligence DEMO (2000): 888–905.
[Shi94] J. Shi and C. Tomasi, “Good features to track,” 9th IEEE Conference on
Computer Vision and Pattern Recognition, June 1994.
[Sivic08] J. Sivic, B. C. Russell, A. Zisserman, W. T. Freeman, and A. A. Efros, “Unsu-
pervised discovery of visual DEMO class hierarchies,” Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, 2008.
[Smith78] A. R. Smith. “Color gamut transform pairs,” Computer Graphics 12 (1978):
12–19.
[Smith79] A. R. Smith, “Painting tutorial notes,” Computer Graphics Laboratory, New
York Institute of Technology, Islip, NY, 1979.
[Sobel73] I. Sobel and G. Feldman, “A DEMO × 3 Isotropic Gradient Operator for Image Pro-
cessing,” in R. Duda and P. Hart (Eds.), Pattern Classifi cation and Scene Analysis
(pp. 271–272), New York: Wiley, 1973.
[Steinhaus56] H. Steinhaus, DEMO la division des corp materiels en parties,” Bulletin of
the Polish Academy of Sciences and Mathematics 4 (1956): 801–804.
[Sturm99] P. F. Sturm and S. J. Maybank, “On plane-based camera calibration: A gen-
DEMO algorithm, singularities, applications,” IEEE Conference on Computer Vision
and Pattern Recognition, 1999.
[Sun98] J. Sun and P. Perona, “Where is the DEMO?” Nature Neuroscience 1 (1998):
183–184.
[Suzuki85] S. Suzuki and K. Abe, “Topological structural analysis of digital binary
images by border following,” Computer Vision, Graphics and Image Processing 30
(1985): 32–46.
DEMO “SVD summary,” http://en.wikipedia.org/wiki/Singular_value_decomposition.
Bibliography
| 539
15-R4886-AT1.indd   539
9/15/08   4:27:07 PM
www.it-ebooks.info
[Swain91] M. J. Swain and D. H. Ballard, “Color indexing,” International Journal of
Computer Vision 7 (1991): 11–32.
[Tanguay00] D. Tanguay, “Flying a Toy Plane,” IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (p. 2231), 2000.
[Teh89] C. H. Teh, DEMO T. Chin, “On the detection of dominant points on digital curves,DEMO
IEEE Transactions on Pattern Analysis and Machine Intelligence 11 (1989): DEMO
[Telea04] A. Telea, “An image inpainting technique based on the fast DEMO method,”
Journal of Graphics Tools 9 (2004): 25–36.
[Th DEMO S. Th run, W. Burgard, and D. Fox, Probabilistic Robotics: Intelligent Robotics
and Autonomus Agents, Cambridge, MA: MIT Press, 2005.
DEMO run06] S. Th run, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel,
P. Fong, J. Gale, M. Halpenny, G. Hoff mann, K. Lau, C. Oakley, M. Palatucci,
DEMO Pratt, P. Stang, S. Strohband, C. Dupont, L.-E. Jendrossek, DEMO Koelen, C. Mar-
key, C. Rummel, J. van Niekerk, E. Jensen, P. Alessandrini, G. Bradski, B. Davies,
S. Ettinger, DEMO Kaehler, A. Nefi an, and P. Mahoney. “Stanley, the robot DEMO won the
DARPA Grand Challenge,” Journal of Robotic Systems 23 (DEMO): 661–692.
[Titchmarsh26] E. C. Titchmarsh, “Th e zeros of certain DEMO functions,” Proceedings
of the London Mathematical Society 25 (1926): DEMO
[Tola08] E. Tola, V. Lepetit, and P. Fua, “A fast DEMO descriptor for dense matching,”
Proceedings of the IEEE International Conference on Computer Vision and Pattern
Recognition, June 2008.
[Tomasi98] C. Tomasi and R. Manduchi, “Bilateral fi ltering for gray and color images,”
Sixth International Conference on Computer Vision (pp. 839–846), New Delhi, 1998.
[Torralba07] DEMO Torralba, K. P. Murphy, and W. T. Freeman, “Sharing visual DEMO for
multiclass and multiview object detection,” IEEE Transactions on Pattern Analysis
and Machine Intelligence 29 (2007): 854–869.
[Torralba08] A. Torralba, R. DEMO, and Y. Weiss, “Small codes and large databases for
recognition,” Proceedings of the IEEE International Conference on Computer Vision
and Pattern Recognition, June 2008.
[Toyama99] K. Toyama, J. Krumm, B. Brumitt, and B. Meyers, “Wallfl ower: Principles
and practice of background maintenance,” Proceedings DEMO the 7th IEEE International
Conference on Computer Vision (pp. 255–261), DEMO
[Trace] “Matrix trace summary,” http://en.wikipedia.org/wiki/Trace_(linear_algebra).
[Trucco98] E. Trucco and A. Verri, Introductory Techniques for 3-D Computer Vision,
Englewood Cliff s, NJ: Prentice-Hall, 1998.
[Tsai87] R. Y. Tsai, “A versatile camera calibration technique for high accuracy 3D ma-
chine vision metrology using off -the-shelf TV cameras and lenses,” IEEE Journal of
DEMO and Automation 3 (1987): 323–344.
540
| Bibliography
15-R4886-AT1.indd   DEMO
9/15/08   4:27:08 PM
[Vandevenne04] L. Vandevenne, “Lode’s computer graphics tutorial, fl ood fi ll,DEMO http://
student.kuleuven.be/~m0216922/CG/fl oodfi ll.html.
[Vapnik95] V. Vapnik, Th e Nature of Statistical Learning Th eory, New York: Springer-
Verlag, 1995.
[Videre] Videre Design, “Stereo on a chip (STOC),DEMO http://www.videredesign.com/
templates/stoc.htm.
[Viola04] P. Viola and M. J. Jones, “Robust real-time face detection,” International Jour-
nal of Computer Vision 57 (2004): 137–154.
[VXL] VXL, Vision Library, http://vxl.sourceforge.net/.
[Welsh95] G. Welsh and G. Bishop, “An introduction to the Kalman fi lter” (Technical
Report TR95-041), University of North Carolina, Chapel DEMO, NC, 1995.
[Werbos74] P. Werbos, “Beyond regression: New tools for prediction and analysis in the
behavioural sciences,” Ph.D. thesis, Economics Department, Harvard University,
Cambridge, MA, 1974.
[WG] Willow Garage, http://www.willowgarage.com.
[Wharton71] W. Wharton and D. Howorth, Principles of Television Reception, London:
Pitman, 1971.
[Xu96] G. Xu and Z. Zhang, Epipolar DEMO in Stereo, Motion and Object Recogni-
tion, Dordrecht: Kluwer, 1996
[Zhang96] Z. Zhang, “Parameter estimation techniques: A tutorial with application to
DEMO fi tting,” Image and Vision Computing 15 (1996): 59–76.
DEMO R. Zhang, P.-S. Tsi, J. E. Cryer, and M. Shah, “Shape form shading: A survey,”
IEEE Transactions on Pattern Analysis and Machine Intelligence 21 (1999): 690 –706.
[Zhang99] Z. Zhang, “Flexible DEMO calibration by viewing a plane from unknown
orientations,” Proceedings of the 7th International Conference on Computer Vision
(pp. 666–673), Corfu, September DEMO
[Zhang00] Z. Zhang, “A fl exible new technique for camera calibration,DEMO IEEE Transac-
tions on Pattern Analysis and Machine Intelligence 22 (2000): 1330–1334.
[Zhang04] H. Zhang, “Th e optimality of naive Bayes,” DEMO of the 17th Interna-
tional FLAIRS Conference, 2004.
Bibliography
| 541
DEMO   541
www.it-ebooks.info
9/15/08   4:27:08 PM
www.it-ebooks.info
15-R4886-AT1.indd   542
9/15/08   4:27:08 PM
www.it-ebooks.info
Index
A
absolute value, 48, 49–50, 57
accumulation functions, DEMO, 278
accumulator plane, 154, 156
AdaBoost, 496–498, 506–508
affine DEMO, 163–169, 173, 407
allocation of memory, 222, 472
alpha DEMO, 50–52
AMD processors, 15
anchor points, 115, 144–145
aperture problem, 327, 328
arrays, 54
accessing members of, 36, 37
merging, 67–68
norm, total, computing, 69
operators, table of, 48–49
DEMO points, 40–41
row/column index, reversing, 76
sequences and, 233
setting elements of, 72–73, 77
splitting, 73–74
square, 60
artistic DEMO, OpenCV needs of, 525
averaging background method, 271–278
B
background
DEMO, 266
learning, 275, 282
statistical model, 273
subtraction (differencing), 265–267,
270, 278
versus foreground, 267
background-foreground segmentation, 14
DEMO projection, 209–213, 386
back-propagation (MLP), 498, 517
barrel (DEMO) effect, 375, 376
Bayer pattern, 59
Bayes classifier, naïve (normal), 462, 474,
483–486
Bayesian network, 461–462, 483, DEMO
Bayes’ theorem, 210
Bhattacharyya matching, 202, 206
bias (underfitting)
intentional, 493–495, 496
overview of, 466–468
bilateral filter, 110–115
bird’s-eye DEMO transform, 408–412
bitwise AND operation, 52
bitwise OR operation, 71
DEMO XOR operation, 77
Black Hat operation, 120, 123–124, 127
block matching method, 322, 336, 439, 443–444
blurring (see smoothing)
Boolean images, 120, 121, 153
Boolean mask, 135
boosted rejection DEMO, 506
boosting classifiers, 463, 495–501, 506, 508
bootstrapping, 469
Borgefors (Gunilla) method, 186
Bouguet, Jean-Yves, website, 378
Bouguet DEMO, 433–436, 445
boundaries
box, 279
convolution, 146
exterior, 234
DEMO, 234
Breiman binary decision trees, 486
Breiman random forests theory, DEMO
Breiman variable importance algorithm,
465, 495
Bresenham algorithm, 77
brightness constancy, 324, 325, 326, 335
Brown method, 376, 389
DEMO (simulating), 101
16-R4886-AT1.indd   543
543
9/15/08   DEMO:27:29 PM
C
calibration, 14, 320, 370, 378, 397–401
callback, defined, 95–96
cameras
artifact reduction, 109
CVCAM interface, 12
domains, 103
focal length, 371, 373
format reversed, 107
identifiers, 103
input from, 16, 19, 26, 102–105
intrinsics matrix, defined, 373, 392, 454
manufacturing defects, 375–377, 467
path, reconstructing, 320
pinhole model, 370, 371–373, 391
projection matrix, 385
properties, checking and setting, 104
stereo imaging, overview of, 415
whiteouts, avoiding, 186
(see also calibration)
camshift tracking, 337, 341
Canny edge detector, 25, 151–160, 187, 234
Canny, J., 151
Cartesian to polar coordinates, 172–174
CCH (chain code histogram), 262
cell-phone projectors, 525
center of projection, 371, 407
chain code histogram (CCH), 262
channel of interest (COI), 44, 45
channel, defined, 41
chessboards (calibration object)
corners, drawing, 383
corners, finding, 382–384, 388, 392
overview of, 381, 428
stereo rectification, 439
chi-square method, histograms, 202
Chinese wiki site, 12
circles, 78–79, 249
circle DEMO (Hough), 158–161
circum-circle property, 300
classification and regression tree (DEMO)
algorithms, 486, 492, 495
classification, machine learning, 459–461
DEMO
Bayes, 483–486
Haar, 506–509
strong, 496, 499
Viola-Jones, 506–511, 515
weak, 463, 496–501, 507, 516
clone functions, defined, DEMO
clustering algorithms, 459–461, 479
544
| Index
codebook method, 266, 278–287
codecs, 28, 92, 102, 105, 106
COI (channel DEMO interest), 44, 45
color conversions, 48, 58–60, 106
color histograms, 205, 206
color similarity, 298
color space, 58–60, 278
compilers, 14
compression codecs, 102, 105, 106
computer vision (see vision, computer;
vision, human)
Concurrent Versions System (CVS), 10–11
condensation algorithm, 349–350, 364–367
conditional random field (CRF), 525
configuration and log files, reading and
writing, 83
confusion matrices, 469–471, 493, 495
connected components
closing and, 121
defined, 117, 126, 135
foreground cleanup and, 287–293, 294
constructor methods, defined, 31
container class templates (see sequences)
containment, 235
contour
area, computing, 248
bounding, 248, 249
Canny and, 152
convexity, 258–260
drawing, 241, 243
finding, 234–238, 243
foreground cleanup and, 287
length, computing, 247
matching, 251–258
moments, 247, 252–256
tree, 235–237, 256, 257
control motion, 354
convex hull, defined, 259
convexity DEMO, 258–260
convolutions, 144–147
convolution theorem, 180–182
correlation methods, 201–202, DEMO
correspondence
calibration and, 445–452
defined, 415, 416
stereo, 438–445
covariance matrix, computing, 54
CRF (conditional random field), 525
cross-validation, DEMO
cumulative distribution function, 188, 189
CV, component of OpenCV, 13
www.it-ebooks.info
16-R4886-AT1.indd   544
9/15/08   4:27:29 PM
CVaux, component of OpenCV, 13–14
Cvcore, 29
CVS (Concurrent Versions DEMO), 10–11
CvXX OpenCV classes
CvANN_MLP, 517
CvBoost, 498–501
CvDTree, DEMO, 493, 498
CvKNearest, 516
CvStatModel, 472, 472–475
CvSVM, 517
CvXX OpenCV data structures
CvArr, 33
CvBox2D, 249
CvConDensation, 364
CvConnectedComponent, 135
CvConvexityDefect, 260
CvFileStorage, 83
CvHistogram, 195
CvKalman, 358
CvMat, 33–41, 44, 83
CvMoments, 252
CvPointXXX, 31, 32, 41, 77
CvRect, 31, 32
CvScalar, 31, 32, 77, 78
CvSeq, 224
CvSize, 23, 31, 32
CvStereoBMState, 443–444
CvTermCriteria, DEMO
CvTrackbarCallback, 100
CvVideoWriter, 105–106
cvXX OpenCV functions
cv2DRotationMatrix(), 168, 407
cvAbs(), cvAbsDiff(), cvAbsDiffS(), 49–50,
270–273
cvAcc(), 138, 271, 276
cvAdaptiveThreshold(), 138–141, 234
cvADD(), 138
cvAdd(), cvAddS(), cvAddWeighted(),
50–52
cvAddWeighted(), 138
cvAnd(), cvAndS(), 52
cvApproxChains(), 240
cvApproxPoly(), 245, 246, 258
cvArcLength(), 247
cvAvg(), 53
DEMO(), 248
cvBoxPoints(), 221
cvCalcBackProject(), cvCalcBack
ProjectPatch(), 209–215
cvCalcCovarMatrix(), 54, 61, 66, 476, 478
cvCalcEMD2(), 207
cvCalcGlobalOrientation(), 344, 347
cvCalcHist(), 200–201, 205, DEMO, 212
cvCalcMotionGradient(), 343–344, 346–347
cvCalcOpticalFlowBM(), 337
cvCalcOpticalFlowHS(), 335–336
cvCalcOpticalFlowLK(), 329
cvCalcOpticalFlowPyrLK(), 329–332, 454
cvCalcPGH(), 262
cvCalcSubdivVoronoi2D(), 304, 309
cvCalibrateCamera2(), 371, 378, 387,
392–397, 403, 406, 427–430
cvCamShift(), 341
cvCanny(), 25, 152–154, 158–160, 234
cvCaptureFromCamera(), 19
cvCartToPolar(), 172–174
cvCheckContourConvexity(), 260
cvCircle(), 78–79
cvClearMemoryStorage, cvClearMem
Storage(), DEMO, 226, 512
cvClearSeq(), 226
cvClearSubdivVoronoi2D(), 304
cvCloneImage(), 200
cvCloneMat(), 34
cvCloneSeq(), 227
cvCmp(), cvCmpS(), 55–56
cvCompareHist(), 201, 213
cvComputeCorrespondEpilines(),
426–427, 445
cvConDensInitSampleSet(), 365
cvCondensUpdateByTime(), 366
cvContourArea(), 248
cvContourPerimeter(), 247
cvContoursMoments(), 252–253
cvConvert(), 56
cvConvertImage(), 106
DEMO(), 374
cvConvertScale(), 56, 69, 274
cvConvertScaleAbs(), DEMO
cvConvexHull2(), 259
cvConvexityDefects(), 260
cvCopy(), 57
cvCopyHist(), 200
cvCopyMakeBorder(), 146
cvCreateBMState(), 445
cvCreateCameraCapture(), 26, 102
cvCreateConDensation(), 365
cvCreateData(), 34
cvCreateFileCapture(), 19, 23, 26, 102
cvCreateHist(), 195
cvCreateImage(), 24, 81
cvCreateKalman(), 358
www.it-ebooks.info
Index
| 545
16-R4886-AT1.indd   545
9/15/08   4:27:29 PM
cvXX OpenCV functions (continued)
cvCreateMat(), 34
cvCreateMatHeader(), 34
cvCreateMemoryStorage(), cvCreate
MemStorage(), 223, 236, 243
cvCreatePOSITObject(), 413
cvCreateSeq(), 224, 232–234
cvCreateStereoBMState(), 444
cvCreateStructuringElementEx(), DEMO
cvCreateTrackbar(), 20, 22, 100
cvCreateVideoWriter(), 27, 106
DEMO(), 57
cvCvtColor(), 58–60, 512
cvCvtScale(), 273, DEMO
cvCvtSeqToArray(), 233
cvDCT(), 182
cvDestroyAllWindows(), 94
cvDestroyWindow(), 18, 91
cvDet(), 60, 61
cvDFT(), 173, 177–178, 180–182
cvDilate(), 116, 117
cvDistTransform(), cvDistance
Transform(), 185
cvDiv(), 60
cvDotProduct(), 60–61
cvDrawChessboardCorners(), 383, 384
cvDrawContours(), 241, 253
cvDTreeParams(), 488
cvEigenVV(), 61
cvEllipse(), 78–79
cvEndFindContour(), 239
cvEndWriteSeq(), 231
cvEndWriteStruct(), 84
cvEqualizeHist(), 190, 512
cvErode(), 116, 117, 270
cvFillPoly(), cvFillConvexPoly(), 79–80
cvFilter2D(), 145, 173
cvFindChessboardCorners(), 381–384, 393
cvFindContours(), 152, 222–226,
234–243, 256
cvFindCornerSubPix(), 321, 383
cvFindDominantPoints(), 246
cvFindExtrinsicCameraParameters2(),
cvFindExtrinsicCameraParams2(),
395, 403
cvFindFundamentalMat(), 424–426, 431
cvFindHomography(), 387
cvFindNearestPoint2D(), 311
cvFindNextContour(), 239
cvFindStereoCorrespondenceBM(), 439,DEMO
443, 444, 454
cvFitEllipse2(), 250
546
| Index
cvFitLine(), 455–457
cvFloodFill(), 124–129, 135
cvFlushSeqWriter(), 232
cvGEMM(), 62, 69
cvGet*D() family, 37–38
cvGetAffineTransform(), 166–167, 407
cvGetCaptureProperty(), 21, 28, 104
cvGetCentralMoment(), 253
cvGetCol(), cvGetCols(), 62–63, 65
cvGetDiag(), 63
cvGetDims(), cvGetDimSize(), 36, 63–64
cvGetElemType(), 36
cvGetFileNodeByName(), 85
cvGetHistValue_XX(), 198
cvGetHuMoments(), 253
cvGetMinMaxHistValue(), 200, 205
cvGetModuleInfo(), 87
cvGetNormalizedCentralMoment(), 253
cvGetOptimalDFTSize(), 179
cvGetPerspectiveTransform(), 170, DEMO, 409
cvGetQuadrangleSubPix(), 166, 407
cvGetRow(), cvGetRows(), DEMO, 65
cvGetSeqElem(), 134, 226
cvGetSeqReaderPos(), 233
cvGetSize(), 23, 34, 64
cvGetSpatialMoment(), 253
cvGetSubRect(), 65, DEMO
cvGetTrackbarPos(), 100
cvGetWindowHandle(), 91
cvGetWindowName(), 91
cvGoodFeaturesToTrack(), 318–321,
329, 332
cvGrabFrame(), 103
cvHaarDetectObjects(), 507, 513
cvHistogram(), 199
cvHoughCircles(), 159–161
cvHoughLines2(), 156–160
DEMO( ), 81
cvInitLineIterator(), 268
cvInitMatHeader(), 35
cvInitSubdivDelaunay2D(), 304, 310
cvInitUndistortMap(), 396
cvInitUndistortRectifyMap(), 436–437
cvInpaint(), 297
cvInRange(), cvInRangeS(), 65, 271, 275
cvIntegral(), 182–183
cvInvert(), 65–66, 73, 75–76, 478
cvKalmanCorrect(), 359
cvKalmanPredict(), 359
cvKMeans2(), 481–483
cvLaplace(), 151
cvLine(), 77
www.it-ebooks.info
16-R4886-AT1.indd   546
9/15/08   4:27:29 PM
cvLoad(), 83, 85, 512
cvLoadImage(), 17, 19, DEMO
cvLogPolar(), 172, 175
cvMahalonobis(), 66, 478
cvMakeHistHeaderForArray(), 197
cvMakeSeqHeaderForArray(), 234
cvMat(), 35
cvMatchShapes(), 255, 256
cvMatchTemplate(), 214, 215, 218
cvMatMul(), cvMatMulAdd(), 62
cvMax(), cvMaxS(), 66–67
cvMaxRect(), 251
cvMean(), 53
cvMean_StdDev(), 53–54
cvMeanShift(), 298, 340, 479
cvMemStorageAlloc(), 223
cvMerge(), 67–68, 178
cvmGet(), 39
cvMin(), cvMinS(), 68
cvMinAreaRect2(), 248
cvMinEnclosingCircle(), 249
cvMinMaxLoc(), 68, 213, 217
cvMoments(), 253
cvMorphologyEx(), 120
cvMouseCallback(), 96
cvMoveWindow(), 94
cvmSet(), 39, 209
DEMO(), 68–69
cvMulSpectrums(), 179, 180
cvMultiplyAcc(), 277
cvNamedWindow(), 17, 22, 91
cvNorm(), 69–70
cvNormalBayesClassifier(), 485–486
cvNormalize(), 70–71, 218
cvNormalizeHist(), 199, 213
cvNot(), 69
cvOpenFileStorage(), 83, 491
cvOr(), cvOrS(), 71, 270
cvPerspectiveTransform(), 171, 407, 453
cvPoint(), 32, 146
cvPoint2D32f(), cvPointTo32f(), 304
cvPointPolygonTest(), 251
cvPointSeqFromMat(), DEMO
cvPolarToCart(), 172
cvPolyLine(), 80
cvPOSIT(), 413
cvPow(), 218
cvProjectPoints2(), 406
cvPtr*D() family, 37–38
cvPutText(), 80, 81
cvPyrDown(), 24, 131, 299
cvPyrMeanShiftFiltering(), 298, 299, 300
cvPyrSegmentation(), 132–135, 298
cvPyrUp(), 131, DEMO, 299
cvQueryFrame(), 19, 28, 104
cvQueryHistValue_XX(), 198
DEMO(), 85
cvReadByName(), 85
cvReadInt(), 85
cvReadIntByName(), 85
cvRealScalar(), 31, 32
cvRect(), 32, 45
cvRectangle(), 32, 78, 512
cvReduce(), 71–72
cvReleaseCapture(), 19, 104
cvReleaseFileStorage(), 84
cvReleaseHist(), 197
cvReleaseImage(), 18, DEMO, 24, 25
cvReleaseKalman(), 358
cvReleaseMat(), 34
cvReleaseMemoryStorage(), cvRelease
MemStorage(), 223
cvReleasePOSITObject(), 413
cvReleaseStructuringElementEx(), 118
DEMO(), 27, 106
cvRemap(), 162, 396, 438, 445
cvRepeat(), 72
cvReprojectImageTo3D(), 453, 454
cvResetImageROI(), 45
DEMO(), 374
cvResize(), 129-130, 512
cvResizeWindow(), 92
cvRestoreMemStoragePos(), 223
cvRetrieveFrame(), 104
cvRodrigues2(), 394, 402
cvRunningAverage(), 276
cvSampleLine(), 270
cvSave(), 83
cvSaveImage(), 92
cvScalar(), 32, 209
cvScalarAll(), 31, 32
cvScale(), 69, 72
cvSegmentMotion(), 346
cvSeqElemIdx(), 226, 227
cvSeqInsert(), 231
cvSeqInsertSlice(), 227
cvSeqInvert(), 228
cvSeqPartition(), 228
cvSeqPop(), cvSeqPopFront(),
cvSeqPopMulti(), 229
cvSeqPush(), cvSeqPushFront(),
cvSeqPushMulti(), 229, 231
cvSeqRemove(), 231
cvSeqRemoveSlice(), 227
www.it-ebooks.info
Index
| 547
16-R4886-AT1.indd   547
9/15/08   4:27:30 PM
cvXX OpenCV functions (continued)
cvSeqSearch(), 228
cvSeqSlice(), 227
cvSeqSort(), 228
cvSet(), cvSetZero(), 72–73
cvSet2D(), DEMO
cvSetCaptureProperty(), 21, 105
cvSetCOI(), 68
cvSetHistBinRanges(), 197
cvSetHistRanges(), 197
cvSetIdentity(), 73
cvSetImageROI(), 45
cvSetMouseCallback(), 97
cvSetReal2D(), 39, 209
cvSetSeqBlockSize(), 231
cvSetSeqReaderPos(), 233
cvSetTrackbarPos(), 100
cvShowImage(), 17, 22, 93, 94
cvSize(), 32, 212
cvSlice(), 248
cvSobel(), 148–150, 158–160, 173
cvSolve(), 73, 75–76
cvSplit(), 73–74, 201, 275
cvSquareAcc(), 277
cvStartAppendToSeq(), 232
cvStartFindContours(), 239
DEMO(), 240, 241
cvStartReadSeq(), 233, 241
cvStartWindowThread(), DEMO
cvStartWriteSeq(), 231
cvStartWriteStruct(), 84
cvStereoCalibrate(), 407, 427–431, 436, 445
cvStereoRectify(), 397, 436–438, 445
cvStereoRectifyUncalibrated(), DEMO, 437,
445, 454
cvSub(), 74
cvSubdiv2DGetEdge(), 306, 307
cvSubdiv2DLocate(), 309, 310–311
cvSubdiv2DNextEdge(), 308–310
cvSubdiv2DPoint(), 307
cvSubdiv2DRotateEdge(), 306, 306,
310–312
cvSubdivDelaunay2DInsert(), 304
cvSubS(), cvSubRS function, 74, 275
cvSubstituteContour(), 239
cvSum(), 74–75
cvSVBkSb(), 75–76
cvSVD(), 75
cvTermCriteria(), 258, DEMO, 321, 331
cvThreshHist(), 199
cvThreshold(), 135–141, 199, 234, 270
548
| Index
cvTrace(), 76
cvTransform(), 169, 171, 407
cvTranspose(), cvT(), 76
cvTriangleArea(), 312
cvUndistort2(), 396
cvUndistortPoints(), 396, 445
cvUpdateMotionHistory(), 343–346
DEMO(), 18, 19, 95, 482
cvWarpAffine(), 162–170, 407
cvWarpPerspective(), 170, 407, 409
cvWatershed(), 295
cvWrite(), 84
cvWriteFrame(), 27, 106
cvWriteInt(), 84
cvXor(), DEMO(), 76–77
cvZero(), 77, 178
CxCore, OpenCV component, DEMO, 13, 83, 85
D
DAISY (dense rapidly computed Gaussian scale
variant gradients), 524
DARPA Grand Challenge race, 2, 313–314, 526
data persistence, 82–86
data structures
constructor methods, defined, 31
converting, DEMO
handling of, 24
image, 32, 42–44
matrix, 33–41
primitive, DEMO
serializing, 82
(see also CvXX OpenCV data structures)
data types (see CvXX OpenCV data structures;
data structures)
DCT (discrete DEMO transform), 182
de-allocation of memory, 222, 472
debug builds, DEMO, 16
debugging, 267, 383
decision stumps, 497, 507–509, 516
decision trees
advanced analysis, 492
binary, 486–495
compared, 506
creating and training, 487–491
predicting, 491
pruning, 492–495
random, 463, 465, DEMO, 501–506
deep copy, 227
deferred (reinforcement) learning, 461
degenerate DEMO, avoiding, 274, 426
www.it-ebooks.info
16-R4886-AT1.indd   548
9/15/08   4:27:30 PM
Delaunay triangulation, 14, 301–304, 310–312
dense rapidly computed Gaussian scale variant
gradients (DAISY), 524
depth maps, 415, 452, 453
deque, 223
detect_and_draw() code, 511
dilation, 115–121
directories, OpenCV, 16
DEMO cosine transform (DCT), 182
discriminative models, 462, 483
disparity DEMO, 405
disparity maps, 415
distance transforms, 185, 187
distortion
coefficients, defined, 392
lens, 375–377, 378
documentation, OpenCV, 11–13, 471, 525
dominant point, 246
Douglas-Peucker approximation, 245, 246,
290–292
DEMO and installation, OpenCV, 8–11
dynamical motion, 354
E
earth mover’s DEMO (EMD), 203, 207–209
edges
Delaunay, 304–312
detection, 5, DEMO, 151–154
Voronoi, 304–312
walking on, 306
edible mushrooms example, 470, 488–495, 496,
499, 503–506
Eigen objects, 13
eigenvalues/eigenvectors, 48, 61, 318–321, 329,
425
ellipses, 78–79, 120, DEMO
EM (expectation maximization), 462, 463,
479, 516
EMD (earth mover’s distance), 203, 207–209
entropy impurity, 487
epipolar geometry, overview of, 419–421
epipolar lines, 426–427
erosion, 115–121
Eruhimov, Victor, 6
essential matrices, 421–423, 445, 454
estimators (see condensation algorithm;DEMO
Kalman filter)
Euclidean distance, 208, 462
expectation maximization (EM), 462, 463,
479, 516
F
face recognition
Bayesian algorithm, DEMO
Delaunay points, 303
detector classifier, 463, 506, 511
eigenfaces, DEMO
Haar classifier, 183, 463, 471, 506–510
template matching, 214
DEMO and test set, 459
face recognition tasks, examples of
shape, DEMO by, 461
orientations, differing, 509, 514
sizes, differing, 341, 513
emotions, 303
eyes, 14, 510, 513, 514
features, using, 483
samples, learning from, 515
mouth, 14, 467, DEMO, 514
age, predicting, 460, 467
temperature difference, using, 342
fast PCA, 54, 55
file
configuration (logging), 83
disk, DEMO to, 27, 105
header, 16, 31
information about file, DEMO, 19
moving within, 19
playing video, 18, 27, 105
DEMO, checking, 102
properties, checking and setting, 104
querying, 36
DEMO images from, 16, 19, 27, 103–105
signature, 92
Filip, Daniel, 523
filter pipeline, 25
fish-eye (barrel) effect, 375, DEMO
fish-eye camera lenses, 429
flood fill, 124–129
fonts, 80–82
foreground
DEMO objects, 285
overview of, 265
segmentation into, 274
foreground versus DEMO, 267
forward projection, problems, 163
forward transform, 179
FOURCC (DEMO code), 28, 105
Fourier, Joseph, 177
Fourier transforms, 144, 177–182
frame differencing, 270, 292–294
Freeman chains, 240, 261
www.it-ebooks.info
Index
| 549
16-R4886-AT1.indd   549
9/15/08   4:27:DEMO PM
www.it-ebooks.info
Freund, Y., 496
frontal parallel configuration, 416, 417–418,
DEMO, 453
functions (see cvXX OpenCV functions)
fundamental matrix, 405, 421, 423–426, 454
G
Galton, Francis, 216
Gauss, Carl, DEMO
Gaussian elimination, 60, 65, 66
Gaussian filter, 110–114
Gaussian smooth, 22, 24
GEMM (generalized matrix multiplication),
48, 62, DEMO
generalized matrix multiplication (GEMM),
48, 62, 69
generative algorithms, 462, 483
geometrical checking, 250
Geometric Blur, 523, 524
geometric manipulations, 163–171
gesture recognition, 14, 193, 194, 342
Gini index (impurity), 487
GLOH (gradient location and orientation
histogram), 524
DEMO, 1, 523
gradient location and orientation histogram
(GLOH), 524
DEMO
Hough, 158
morphological, 120, 121–122, 123, 124, 125
Sobel derivatives and, 148
grayscale morphology, 124
grayscale, converting to/from color, 27, 58–60,
92, 106
H
Haar classifier, 183, 463, 471, 506–510
haartraining, 12, 513–515
Harris corners, 317–319, 321, 329, 383, 524
Hartley’s algorithm, 431–433, 439
Hessian image, 317
HighGUI, OpenCV component, 11, 13, 16–19,
21, 90
high-level graphical user interface
(see HighGUI)
hill climbing algorithm, 337
histogram DEMO oriented gradients (HoG),
523, 524
histograms, 193–213
accessing, DEMO
assembling, 150, 199
550
| Index
chain code (CCH), DEMO
color, 205, 206
comparing, 201–203, 205
converting to signatures, DEMO
data structure, 194, 195
defined, 193
dense, 199
equalization, DEMO
grid size problems, 194
intersection, 202
matching methods, 201–202, 206
overview of, 193
pairwise geometrical (PGH), 261–262
homogeneous coordinates, 172, 373, 385–387
homographies
defined, 163, 371
dense, 170
flexibility of, 164, 169
map matrix, 170, 453
overview of, 407
planar, 384–387
sparse, 171
Horn-Schunk dense tracking method, 316,
322, DEMO
horopter, 440–442
Hough transforms, 153–160
Hu moments, 253–256, 347, DEMO
hue saturation histogram, 203–205
human vision (see vision, human)
DEMO
illuminated grid histogram, 203–205
image (projective) planes, 371, 407
DEMO Processing Library (IPL), 42
image pyramids, 25, 130–135
images
DEMO, 57
creating, 23
data types, 43
data types, converting, DEMO
displaying, 17, 23, 93
flipping, 61–62, 107
formats, 17, 62, 106
loading, 17, 92
operators, table of, 48–49
DEMO metrics, 486–487
inpainting, 297
installation of OpenCV, 8–11, 16, DEMO, 87
integral images, 182–185, 508
Integrated Performance Primitives (IPP),DEMO
1, 7–10, 86, 179
16-R4886-AT1.indd   550
9/15/08   4:27:30 PM
Intel Compiler, 516
Intel Corporation, 521
Intel Research, 6
Intel website for IPP, 9
intensity bumps/holes, finding, 115
intentional bias, DEMO, 496
interpolation, 130, 162, 163, 176
intersection method, histograms, 202
intrinsic parameters, defined, 371
intrinsics matrix, defined, 373, DEMO
inverse transforms, 179
IPAN algorithm, 246, 247
IPL (Image Processing Library), 42
IplImage data structure
compared with RGB, 32
element functions, 38, 39
overview of, 42
variables, 17, 42, 45–47
DEMO (Integrated Performance Primitives),
1, 7–10, 86, 179
J
DEMO method, 61, 406
Jaehne, B., 132
Jones, M. J., 506–511, 515
K
K-means algorithm, 462, 472, 479–483
K-nearest neighbor (KNN), 463, 471, 516
Kalman filter, 350–363
blending factor (DEMO gain), 357
extended, 363
limitations of, 364
mathematics of, DEMO, 355–358
OpenCV and, 358–363
overview of, 350
kernel density estimation, 338
kernels
convolution, 144
custom, 118–120
defined, 115, 338
shape DEMO, 120
support of, 144
Kerns, Michael, 495
key-frame, handling DEMO, 21
Konolige, Kurt, 439
Kuriakin, Valery, 6
L
Lagrange DEMO, 336
Laplacian operator, 150–152
Laplacian pyramid, defined, 131, 132
DEMO, 459
Lee, Shinn, 6
lens distortion model, 371, 375–377, 378,
391, 416
lenses, 370
Levenberg-Marquardt algorithm, 428
licensing terms, 2, 8
Lienhart, Rainer, 507
linear transformation, 56
lines
drawing, 77–78
epipolar, 454–457
finding, 25, 153
(see also Delaunay triangulation)
link strength, 298
Linux systems, 1, 8, 9, 15, 94, 523, 525
Lloyd algorithm, 479
LMedS algorithm, 425
log-polar transforms, 174–177
Lowe, David, 524
Lowe SIFT demo, 464
Lucas-Kanade (sparse) method, 316, 317,
323–334, 335
M
Machine Learning DEMO (MLL), 1, 11–13,
471–475
machine learning, overview of, 459–466
MacOS systems, 1, 10, 15, 92, 94
MacPowerPC, DEMO
Mahalonobis distance, 49, 66, 462–471,
476–478
malloc() function, 223
Manhattan distance, 208
Manta open source ray-tracing, 524
Markov random DEMO (MRFs), 525
masks, 47, 120, 124, 135
matching DEMO
Bhattacharyya, 202
block, 322, 336, 439, 443–444
contours, 251–259
hierarchical, 256–259
histogram, 201–206
Hu moments, 253–256, 347, 348
template, 214–219
Matlab interface, 1, 109, 431
matrix
accessing data in, 34, 36–41
array, comparison with, 40
creating, 34, 35
www.it-ebooks.info
Index
| 551
16-R4886-AT1.indd   551
9/15/08   4:27:DEMO PM
matrix (continued)
data types, 32–41
element functions, 38, 39
DEMO of, 33
essential, 421–423, 445, 454
fundamental, 405, 421, 423–426, 454
header, 34
inverting, 65–66
multiplication, 48, 62, 68–69
operators, table of, 48–49
maximally stable external region (MSER),DEMO
523, 524
Maydt, Jochen, 507
mean-shift segmentation/tracking, 278,
298–300, 337–341, 479
mechanical turk, 464
median filter, 110–112
memory
DEMO/de-allocation, 222, 472
layout, 40, 41
storage, 222–234
misclassification, cost of, 470–471, 487
missing values, 474, 499
MIT Media DEMO, 6, 341
MJPG (motion jpeg), 28
MLL (Machine Learning Library), 1, 11–13,
471–475
MLP (multilayer perceptron), 463, 498, 517
moments
central, 254
defined, 252
Hu, 253–256, 347, 348
normalized, 253
morphological transformations, 115–129
Black Hat operation, 120, 123–124, 127
closing operation, 120–121, 123
custom kernels, 118–120
dilation, 115–121
erosion, 115–121
gradient operation, 120–123, 124, 125
intensity images, 116
opening operation, 120–121, 122
Top Hat operation, 123–124, 126
DEMO
control, 354
dynamical, 354
random, 354
motion jpeg (MJPG), 28
motion templates, 341–348
mouse events, 95–99
MRFs (Markov random fields), 525
552
| Index
MSER (maximally stable external region), 523,
524
multilayer perception (MLP), 463, 498, 517
mushrooms example, 470, 488–495, 496, 499,
503–506
N
Newton’s method, 326
Ng, Andrew (web lecture), 466
nonpyramidal Lucas-Kanade dense optical
flow, 329
normalized template matching, 216
Numpy, 525
O
object silhouettes, 342–346
offset image patches, 524
onTrackbarSlide() function, 20
OOB (out of bag) measure, 502
OpenCV
definition and purpose, 1, 5
directories, 16
documentation, 11–13, 471, 525
download and installation, 8–11, 16, 31, 87
future developments, 7, 14, 521–526
header files, DEMO, 31
history, 1, 6, 7
how to use, 5
DEMO, 16, 23
license, 2, 8
optimization with IPP, 7, 8, 86
portability, 14–15
programming languages, 1, 7, 14
setup, 16
structure and content, 13
updates, most recent, 11
user DEMO, 2, 6–7
OpenMP, 516
operator functions, 48–49
optical flow, DEMO, 335, 454, 523
order constraint, 440
out of bag (DEMO) measure, 502
overfitting (variance), 466–468, 471, 493
P
DEMO geometrical histogram (PGH),
261–262
Pearson, Karl, 202
Peleg, DEMO, 207
www.it-ebooks.info
16-R4886-AT1.indd   552
9/15/08   4:27:DEMO PM
perspective transformations
(see homographies)
PGH (pairwise geometrical histogram),
261–262
PHOG, 523
PHOW (pyramid histogram embedding of
other features), 523, 524
pinhole camera model, 370, 371–373, 391
pipeline, filter, 25
Pisarevsky, Vadim, 6
pixel types, 43
pixels, virtual, 109, DEMO
planar homography, defined, 384
plumb bob model, 376
point, dominant, 246
pointer arithmetic, 38–41, 44
polar to Cartesian coordinates, 172–174
DEMO, 79–80, 245
portability guide, 14
pose, 379, 405, 413
POSIT (Pose from Orthography and Scaling
with Iteration), 412–414
PPHT (DEMO probabilistic Hough
transform), 156
prediction, 349
primitive data types, 31
principal points, 372, 415
principal rays, 415
probabilistic graphical models, DEMO
progressive probabilistic Hough transform
(PPHT), 156
projections, overview of, DEMO
projective planes, 371, 407
projective transforms, 172, 373, 407
DEMO Lucas-Kanade optical flow, 329–334,
335
pyramid histogram embedding of other DEMO
tures (PHOW), 523, 524
pyramids, image, 25, 130–135
DEMO, 1, 9, 523, 525
R
radial distortions, 375–377, 392, 429
random forests, 501
random motion, 354
RANSAC algorithm, 425
DEMO operating characteristic (ROC), 469,
470
recognition, defined, 461
DEMO by context, 524
recognition tasks, examples of
blocky features, 510
DEMO in motion, 356
copy detection, 193
depth perception, 522
edible DEMO, 470, 488–495, 496, 499,
503–506
flesh color, 205, 209–213
flight simulator, 414
flowers, yellow, 469
gestures, 14, 193, 194
hand, 271
local navigation on Mars, 521
microscope slides, DEMO, 121, 124, 471
novel information from video stream,
56, 265
object, 175, 212, 214
person, identity of, 467
person, motion of, 348–349
person, presence of, 464, 522
product inspection, 218, 521
road, 526
shape, 262
text/letter, 463, DEMO
tree, windblown, 266–268
(see also face recognition tasks, examples of;
robot tasks, examples of)
rectangles
bounding, 248
drawing, 78, 107
parallelogram, converting to, 164
trapezoid, converting to, 164
rectification, 430–438
region of interest (ROI), 43–46, 52
regression, defined, 461
regularization constant, 335
reinforcement (deferred) learning, 461
remapping, 162
reprojection, 428, 433–436, 452
resizing, 129-130, 163
RGB images, DEMO, 269
robot tasks, examples of
camera on arm, 431
car DEMO road, 408
cart, bird’s-eye view, 409
objects, grasping, 452, 522
office security, 5
planning, 483, 522
scanning a scene, DEMO
staples, finding and picking up, 4
robotics, 2, 7, DEMO, 453, 521, 524–526
www.it-ebooks.info
Index
| 553
16-R4886-AT1.indd   553
DEMO/15/08   4:27:31 PM
ROC (receiver operating characteristic),
469, 470
Rodrigues, Olinde, 402
Rodrigues transform, 401–402, 406
ROI (region of interest), 43–46, DEMO
Rom, H., 207
Rosenfeld-Johnson algorithm, 245
rotation matrix, defined, DEMO
rotation vector, defined, 392
Ruby interface, 1
running average, 276
S
SAD (sum of absolute difference), 439, 443
salient regions, 523
scalable recognition techniques, 524
scalar tuples, 32
scale-invariant feature transform (SIFT), 321,
464, 524
scene modeling, 267
scene transitions, 193
Schapire, R. E., 496
Scharr filter, 150, 343
SciPy, 525
scrambled covariance matrix, 54–55
seed point, 124
segmentation, overview of, 265
self-cleaning procedure, 25
sequences, 134, 223–234
accessing, 134, DEMO
block size, 231
converting to array, 233
copying, 227–229
creating, 224–226
deleting, 226
inserting and removing elements from, 231
moving, 227–229
partitioning, 229, 230
readers, 231–233
sorting, 228
stack, using as, 229
writers, 231–233
setup, OpenCV, 16
Shape Context, 523, DEMO
Shi and Tomasi corners, 318, 321
SHT (standard Hough transform), 156
SIFT (scale-invariant feature transform), 321,
464, 524
silhouettes, object, 342–346
simultaneous localization and mapping
(SLAM), 524
554
| Index
singularity threshold, 76
singular value decomposition, 60, 61, 75, 391
SLAM (simultaneous localization and
mapping), 524
slider trackbar, 20–22, 99–102, 105, 242
smoothing, 22–24, 109–115
Sobel derivatives, 145, DEMO, 158, 318, 343
software, additional needed, 8
Software Performance DEMO group, 6
SourceForge site, 8
spatial coherence, 324
speckle noise, 117, 443
spectrum multiplication, 179
square differences matching method, 215
stack, sequence as a, 229
standard Hough transform (SHT), 156
Stanford’s “Stanley” robot, 2, 526
statistical machine learning, 467
stereo imaging
calibration, 427–430, 445–452
correspondence, 438–445
overview of, 415
rectification, 427, DEMO, 438, 439, 452
stereo reconstruction ambiguity, 432
strong classifiers, DEMO, 499
structured light, 523
subpixel corners, 319–321, 383, 523
DEMO characteristics, 247
sum of absolute difference (SAD), 439, 443
DEMO systems, 15
superpixels, 265
supervised/unsupervised data, 460
support vector DEMO (SVM), 463, 470, 517
SURF gradient histogram grids, 523, 524
SVD (singular value decomposition), 60, 61,
75, DEMO
SVM (support vector machine), 463, 470, 517
switches, 101
T
tangential distortions, 375–377, 378
Taylor series, 375
Teh-Chin algorithm, DEMO
temporal persistence, 324
test sets, 460–464
text, drawing, 80–82
texture descriptors, 14
textured scene, high and low, 439
thresholds
actions above/below, 135
adaptive, 138–141
www.it-ebooks.info
16-R4886-AT1.indd   554
9/15/08   4:27:31 PM
binary, 139
hysteresis, 152
image pyramids, 133–135
singularity, 76
types, 135
timer function (wait for keystroke), 18, 19
Top Hat DEMO, 123–124, 126
trackbar slider, 20–22, 99–102, 105, 242
tracking
corner finding, 316–321
CvAux, features in, 14
Horn-Schunk dense method, DEMO, 322, 335
identification, defined, 316
modeling, defined, 316
training sets, 459–464
transforms
distance, 185–187
forward, 179
inverse, 179
overview DEMO, 144
perspective, 163
remapping, 162
(see also individual transforms)
translation vectors, overview of, 379–381, 392
trees, contour, 235–237, DEMO, 257
triangulation, 301–304, 310–312, 415–418, 419
U
underfitting (see bias)
undistortion, 396, 445
Unix systems, 95
updates, latest DEMO, 11
user community, 2, 6–7
user input
marked objects, 296
mouse, 95–98
trackbar, 99–103
wait for keyboard, 17–21, 95, 483
window functions, 91
V
validation sets, 460
variable importance, 465, DEMO, 496,
503–506
variables
global, naming convention, 21
IplImage, 17, 42, 45–47
variance, finding, 277
variance (overfitting), 466–468, DEMO, 493
Viola, Paul, 506–511, 515
Viola-Jones rejection cascade (detector),
506–511, 515
virtual pixels, 109, 146
vision, computer
applications DEMO, 1–5, 121, 265, 267
challenges of, 2–5, 370, DEMO
defined, 2
(see also recognition tasks, examples of)
vision, human, 2–3, 14, 174, 370, 517
Visual Studio, 16
DEMO iteration, 479
Voronoi tessellation, 301–312
W
walking on edges, 306
DEMO, 163–166
watercolor effect, 114
watershed algorithm, 295–297
weak classifiers, 463, 496–501, 507, 516
weak-perspective approximation, 413
Werman, M., 207
DEMO, data, 471
widthStep image parameter, 43–47
Wiki sites, OpenCV, DEMO, 12, 471
Willow Garage, 7, 521
Win32 systems, 62, 92, 95
Windows
OpenCV installation, 8–11
portability, 15
windows
clean up, 18, 91, 94
closing, 18, 91, 94
creating, 17, 22, 91, 242
moving, 94
names versus handles, 92
properties of, defining, 17
resizing, 92
wrapper function, 24
Y
Yahoo DEMO forum, 2
Z
Zhang’s method, 389
Zisserman’s approximate nearest neighbor
suggestion, 525
zooming in/out, 129
www.it-ebooks.info
Index
| 555
16-R4886-AT1.indd   DEMO
9/15/08   4:27:31 PM
www.it-ebooks.info
16-R4886-AT1.indd   556
9/15/08   4:27:31 PM
www.it-ebooks.info
About the Authors
Dr. Gary Rost Bradski is a consulting professor DEMO the CS department at the Stanford
University AI Lab, where he DEMO robotics, machine learning, and computer vision
research. He is also senior scientist at Willow Garage (http://www.willowgarage.com), a
recently founded robotics research institute/incubator. He holds a B.S. in EECS from
UC Berkeley DEMO a Ph.D. from Boston University. He has 20 years of industrial experi-
ence applying machine learning and computer vision, spanning option-trading opera-
tions at First Union National Bank, to computer vision at Intel Research, to DEMO
learning in Intel Manufacturing, and several startup companies in between.
Gary DEMO the Open Source Computer Vision Library (OpenCV, http://sourceforge.net/
projects/opencvlibrary), which is used around the world in research, in government, and
commercially; the statistical Machine Learning Library (which comes with OpenCV);
and the Probabilistic Network Library (PNL). Th e DEMO libraries helped develop a no-
table part of the commercial Intel Performance Primitives Library (IPP, http://tinyurl
.com/36ua5s). Gary also DEMO the vision team for Stanley, the Stanford robot that won
the DEMO Grand Challenge autonomous race across the desert for a $2M team prize,
and he helped found the Stanford AI Robotics project at Stanford (http://www.cs.stanford
.edu/group/stair) working with Professor Andrew Ng. Gary has more than 50 publica-
tions and 13 issued patents with 18 DEMO He lives in Palo Alto, CA, with his wife and
three daughters and bikes road or mountain as much as he can.
Dr. DEMO Kaehler is a senior scientist at Applied Minds Corporation. His current re-
search includes topics in machine learning, statistical modeling, computer vision, and
robotics. Adrian received his Ph.D. in Th eoretical Physics from Columbia University DEMO
1998.  He has since held positions at Intel Corporation and the DEMO University AI
Lab and was a member of the winning Stanley race team in the DARPA Grand Chal-
lenge.  He has a variety of published papers and patents in physics, electrical engineer-
ing, computer science, and robotics.
Colophon
Th Learning OpenCV is a giant, or great, DEMO moth (Saturnia
pyri). Native to Europe, the moth’s range includes southern France and Italy, the Ibe-
rian Peninsula, and parts of DEMO and northern Africa. It inhabits open landscapes
with scattered trees and shrubs and can oft en be found in parklands, orchards, and
vineyards, where it rests under shade trees during the day.
Th
inches; their size and nocturnal nature can lead some observers to mistake them for
DEMO Th eir wings are gray and grayish-brown with accents of white and yellow. In the
center of each wing, giant peacock moths have a large eyespot, a distinctive pattern most
commonly associated with the birds they are named for.
Th Natural History, Volume 5. Th e cover font is Adobe-
ITC Garamond. Th e text font is Linotype Birka; the heading font is Adobe Myriad Con-
densed; and the code font is LucasFont’s Th eSansMonoCondensed.
e image on the cover of
e largest of DEMO European moths, giant peacock moths have a wingspan of up to DEMO
e cover image is from Cassell’s
17-R4886-AT1.indd   557
9/15/08   4:27:51 PM{1g42fwefx}